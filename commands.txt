python preprocess.py -train_src ../CitationLinking/training.txt \
                     -train_tgt ../CitationLinking/training.target.txt \
                     -valid_src ../CitationLinking/val.txt \
                     -valid_tgt ../CitationLinking/val.target.txt \
                     -save_data ../CitationLinking/preprocessed/scisumm \
                     -src_seq_length 10000 \
                     -tgt_seq_length 10000 \
                     -src_seq_length_trunc 400 \
                     -tgt_seq_length_trunc 100 \
                     -dynamic_dict \
                     -share_vocab \
                     -shard_size 100000



python train.py -save_model ../CitationLinking/models/scisumm \
                -data ../CitationLinking/preprocessed/scisumm \
                -copy_attn \
                -global_attention mlp \
                -word_vec_size 128 \
                -rnn_size 512 \
                -layers 1 \
                -encoder_type brnn \
                -train_steps 200000 \
                -max_grad_norm 2 \
                -dropout 0. \
                -batch_size 16 \
                -valid_batch_size 16 \
                -optim adagrad \
                -learning_rate 0.15 \
                -adagrad_accumulator_init 0.1 \
                -reuse_copy_attn \
                -copy_loss_by_seqlength \
                -bridge \
                -seed 777 \
                -world_size 2 \
                -gpu_ranks 0 1
