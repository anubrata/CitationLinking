python preprocess.py -train_src ../CitationLinking/training.txt \
                     -train_tgt ../CitationLinking/training.target.txt \
                     -valid_src ../CitationLinking/val.txt \
                     -valid_tgt ../CitationLinking/val.target.txt \
                     -save_data ../CitationLinking/preprocessed/scisumm \
                     -src_seq_length 10000 \
                     -tgt_seq_length 10000 \
                     -src_seq_length_trunc 400 \
                     -tgt_seq_length_trunc 100 \
                     -dynamic_dict \
                     -share_vocab \
                     -shard_size 100000



python train.py -save_model ../CitationLinking/models/scisumm \
                -data ../CitationLinking/preprocessed/scisumm \
                -copy_attn \
                -global_attention mlp \
                -word_vec_size 128 \
                -rnn_size 512 \
                -layers 1 \
                -encoder_type brnn \
                -train_steps 200000 \
                -max_grad_norm 2 \
                -dropout 0. \
                -batch_size 16 \
                -valid_batch_size 16 \
                -optim adagrad \
                -learning_rate 0.15 \
                -adagrad_accumulator_init 0.1 \
                -reuse_copy_attn \
                -copy_loss_by_seqlength \
                -bridge \
                -seed 777 \
                -world_size 2 \
                -gpu_ranks 0 1



### More reasonable preprocessing setup

python preprocess.py -train_src ../CitationLinking/training.txt \
                     -train_tgt ../CitationLinking/training.target.txt \
                     -valid_src ../CitationLinking/val.txt \
                     -valid_tgt ../CitationLinking/val.target.txt \
                     -save_data ../CitationLinking/preprocessed/rscisumm \
                     -src_seq_length 50000 \
                     -tgt_seq_length 5000 \
                     -src_seq_length_trunc 1000 \
                     -tgt_seq_length_trunc 500 \
                     -dynamic_dict \
                     -share_vocab \
                     -shard_size 100000

### Paramters are decided based on examining the data

## Changed training parameters
python train.py -save_model ../CitationLinking/models/rscisumm \
                -data ../CitationLinking/preprocessed/rscisumm \
                -copy_attn \
                -global_attention mlp \
                -word_vec_size 128 \
                -rnn_size 512 \
                -layers 4 \
                -encoder_type brnn \
                -train_steps 5000 \
                -max_grad_norm 2 \
                -dropout 0. \
                -batch_size 16 \
                -valid_batch_size 16 \
                -optim adagrad \
                -learning_rate 0.15 \
                -adagrad_accumulator_init 0.1 \
                -reuse_copy_attn \
                -copy_loss_by_seqlength \
                -bridge \
                -seed 777 \
                -world_size 2 \
                -gpu_ranks 0 1


#### Final Versions used

python preprocess.py -train_src ../CitationLinking/training.txt \
                     -train_tgt ../CitationLinking/training.target.txt \
                     -valid_src ../CitationLinking/val.txt \
                     -valid_tgt ../CitationLinking/val.target.txt \
                     -save_data ../CitationLinking/preprocessed/rscisumm \
                     -src_seq_length 10000 \
                     -tgt_seq_length 10000 \
                     -src_seq_length_trunc 1000 \
                     -tgt_seq_length_trunc 500 \
                     -dynamic_dict \
                     -share_vocab \
                     -shard_size 100000

python train.py -save_model ../CitationLinking/models/rscisumm \
                -data ../CitationLinking/preprocessed/rscisumm \
                -copy_attn \
                -global_attention mlp \
                -word_vec_size 128 \
                -rnn_size 512 \
                -layers 1 \
                -encoder_type brnn \
                -train_steps 5000 \
                -max_grad_norm 2 \
                -dropout 0 \
                -batch_size 2 \
                -valid_batch_size 2 \
                -optim adagrad \
                -learning_rate 0.15 \
                -adagrad_accumulator_init 0.1 \
                -reuse_copy_attn \
                -copy_loss_by_seqlength \
                -bridge \
                -seed 777 \
                -world_size 2 \
                -gpu_ranks 0 1




python translate.py -gpu 0 \
                    -batch_size 20 \
                    -beam_size 10 \
                    -attn_debug \
                    -report_rouge \
                    -model ../CitationLinking/models/rscisumm_step_5000.pt \
                    -log_file ../CitationLinking/log/OneTest.log \
                    -log_file_level INFO \
                    -src ../CitationLinking/oneTest.txt \
                    -output ../CitationLinking/out/oneTest.out \
                    -min_length 35 \
                    -verbose \
                    -stepwise_penalty \
                    -coverage_penalty summary \
                    -beta 5 \
                    -length_penalty wu \
                    -alpha 0.9 \
                    -verbose \
                    -block_ngram_repeat 3 \
                    -ignore_when_blocking "." "</t>" "<t>"


