Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs. For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined. For example, suppose bill has the following set of possible meanings: a piece of currency, pending legislation, or a bird jaw. When used in the context of The Senate bill is under consideration, a human reader immediately understands that bill is being used in the legislative sense. However, a computer program attempting to perform the same task faces a di√Ücult problem since it does not have the bene?t of innate common{sense or linguistic knowledge. Rather than attempting to provide computer programs with real{world knowledge comparable to that of humans, natural language processing has turned to corpus{based methods. These approaches use techniques from statistics and machine learning to induce models of language usage from large samples of text. These models are trained to perform particular tasks, usually via supervised learning. This paper describes an approach where a decision tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense{tag that denotes the most appropriate sense for that context. Prior to learning, the sense{tagged corpus must be converted into a more regular form suitable for automatic processing. Each sense{tagged occurrence of an ambiguous word is converted into a feature vector, where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process. Given the exibility and complexity of human language, there is potentially an in?nite set of features that could be utilized. However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources. These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy. It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution. One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements. This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning. Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison. The experimental data is discussed, and then the empirical results are presented. We close with an analysis of our ?ndings and a discussion of related work.
