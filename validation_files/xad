A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech. Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das RealitaÂ¨ t? (Is that reality?). The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun RealitaÂ¨ t (feminine), the article reading must be wrong. The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (âThe to be taxed income decreasesâ; The tËN N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases). This sentence should be tagged as shown in table 1. Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut p(tN , wN ) = n 1 1 i=1 p(ti|tiâ1 ) iâk p(wi|ti) le . (1) context prob. xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora. Qc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.) The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777â784 Manchester, August 2008 context probability of the third POS tag is therefore 0. If the probability is smoothed with the backoff distribution p(â¢|P ART .Z u), the most probable tag is ADJA.Pos.Acc.Sg.Fem rather than ADJA.Pos.Nom.Sg.Neut. Thus, the agreement between the article and the adjective is not checked anymore. A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut: â¢ All words appearing after an article (ART) and the infinitive particle zu (PART.zu) are attributive adjectives (ADJA) (10 of 10 cases). â¢ All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases). â¢ All adjectives appearing after a nominative article and a particle have nominative case (11 of 11 cases). â¢ All adjectives appearing after a singular article and a particle are singular (32 of 32 cases). â¢ All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases). By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) â p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) â p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) â p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) â p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the â p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) â p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1. Hence the context probability of the whole tag is. also 1. Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context. These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute. Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method. The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here. Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.
