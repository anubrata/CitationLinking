We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with ânon-localâ features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen 1 Informally, features are âpartsâ of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning. Here we take first steps toward such a âuniversalâ decoder, making the following contributions:Arbitrary feature model (Â§2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of âstring-to-tree,â âtree-to-string,â âtree- to-tree,â and âphrase-basedâ models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (Â§3â4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary ânon- localâ features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (Â§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (Â§6): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219â228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP Î£, T Trans : Î£ âª {NULL} â 2T s = (s0 , . . . , sn ) â Î£n t = (t1 , . . . , tm ) â Tm Ïs : {1, . . . , n} â {0, . . . , n} Ït : {1, . . . , m} â {0, . . . , m} a : {1, . . . , m} â 2{1,...,n} Î¸ source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where Ïs (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where Ït (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s; â denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (Â§2.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tjâN +1 ) language model features (Â§2.2): N -gram probabilities gsyn (t, Ït ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (Â§2.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I â {1, . . . , m} greor (s, Ïs , a, t, Ït ) f dist (i, j) reordering features (Â§2.4): distortion features for a source word at position i aligned to a target word at position j gtree 2 (Ïs , a, Ït ) f qg (i, il, j, k) tree-to-tree syntactic features (Â§3): configuration features for source pair si /sil being aligned to target pair tj /tk gcov (a) f scov (a), f zth (a), f sunc (a) coverage features (Â§4.2) counters for âcoveringâ each s word each time, the zth time, and leaving it âuncoveredâ Table 1: Key notation. Feature factorings are elaborated in Tab. 2. bination of the two. We quantify the effects of our approximate inference. We explore the effects of various ways of restricting syntactic non-isomorphism between source and target trees through the QG. We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research.
