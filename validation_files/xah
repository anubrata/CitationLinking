Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text pmtions by matching high frequency terms. Yaari ( 1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft ( 1997) used word co-occurrences to expand the number of terms for matching. Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget's Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations. In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus. Furthermore, this process was not automated and relied on subjective decision making. Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text. The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated. This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text. 1 Background Theory: Lexical Cohesion. Cohesion concerns how words in a text are related. The major work on cohesion in English was conducted by Halliday and Hasan (1976). An instance of cohesion between a pair of elements is referred to as a tie. Ties can be anaphoric or cataphoric, and located at both the sentential and suprasentential level. Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation. General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role. Consequently, it was not further considered. Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing). The effect of general word is difficult to automatically identify because no common referent exists between the general word and the word to which it refers. A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation.
