 Word Re-ordering and DP-based Search in Statistical Machine Translation  In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP). Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm. A search restriction especially useful for the translation direction from German to English is presented. The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.  The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model. Our approach uses word-to-word dependencies between source and target words. The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000). These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition. The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns. A simple extension will be used to handle this problem. In Section 2, we brie y review our approach to statistical machine translation. In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English. This approach is compared to another reordering scheme presented in (Berger et al., 1996). In Section 4, we present the performance measures used and give translation results on the Verbmobil task.  In this section, we brie y review our translation approach. In Eq. (1), Pr(eI 1) is the language model, which is a trigram language model in this case. For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word. The alignment model uses two kinds of parameters: alignment probabilities p(aj jajô1; I; J), where the probability of alignment aj for position j depends on the previous alignment position ajô1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ). When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect. In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone. 2.1 Inverted Alignments. To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000). An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi. What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijeiô1 iô2) max bI 1 I Yi=1 [p(bijbiô1; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijeiô1 iô2) p(bijbiô1; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijeiô1 iô2) is the trigram language model probability. The inverted alignment probability p(bijbiô1; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration. The details are given in (Och and Ney, 2000). The sentence length probability p(JjI) is omitted without any loss in performance. For the inverted alignment probability p(bijbiô1; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining. The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment. We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training. The word joining is done on the basis of a likelihood criterion. An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account. E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment. In the following, we assume that this word joining has been carried out.  Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup. In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962). The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1. A straightforward way to find the shortest tour is by trying all possible permutations of the n cities. The resulting algorithm has a complexity of O(n!). However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp. The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city. Subsets C of increasing cardinality c are processed. The algorithm works due to the fact that not all permutations of cities have to be considered explicitly. For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored. This algorithm can be applied to statistical machine translation. Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed. The advantage is that we can recombine search hypotheses by dynamic programming. The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation. input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Ã;e00 j02Cnfjg fp(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once. Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed. For a trigram language model, the partial hypotheses are of the form (e0; e; C; j). e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited. Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities. The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and eiô1 = e0. The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Ã = 0 or Ã = 1 new target words. For Ã = 1, a new target language word is generated using the trigram language model p(eje0; e00). For Ã = 0, no new target word is generated, while an additional source sentence position is covered. A modified language model probability pÃ(eje0; e00) is defined as follows: pÃ(eje0; e00) =  1:0 if Ã = 0 p(eje0; e00) if Ã = 1 : We associate a distribution p(Ã) with the two cases Ã = 0 and Ã = 1 and set p(Ã = 1) = 0:7. The above auxiliary quantity satisfies the following recursive DP equation: Qe0 (e; C; j) = Initial Skip Verb Final 1. In. 2. diesem 3. Fall. 4. mein 5. Kollege. 6. kann 7.nicht 8. besuchen 9. Sie. 10. am 11. vierten 12. Mai. 13. Figure 2: Order in which source positions are visited for the example given in Fig.1. = p(fj je) max Ã;e00 j02Cnfjg np(jjj0; J) p(Ã) pÃ(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j). The resulting algorithm is depicted in Table 1. The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary. 3.1 Word ReOrdering with Verbgroup. Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence. On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering. No: Predecessor coverage set Successor coverage set 1 (f1; ;mg n flg ; l0) ! (f1; ;mg ; l) 2 (f1; ;mg n fl; l1g ; l0) ! (f1; ;mg n fl1g ; l) 3 (f1; ;mg n fl; l1; l2g ; l0) ! (f1; ;mg n fl1; l2g ; l) 4 (f1; ;m ô 1g n fl1; l2; l3g ; l0) ! (f1; ;mg n fl1; l2; l3g ;m) German to English the monotonicity constraint is violated mainly with respect to the German verbgroup. In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words. Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup. A typical situation is shown in Figure 1. When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence. Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated. The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions. To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered. Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated. Final (F): The rest of the sentence is processed monotonically taking account of the already covered positions. While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position. The sequence of states needed to carry out the word reordering example in Fig. 1 is given in Fig. 2. The 13 positions of the source sentence are processed in the order shown. A position is presented by the word at that position. Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) ! (S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account. To be short, we omit the target words e; e0 in the formulation of the search hypotheses. There are 13 types of extensions needed to describe the verbgroup reordering. The details are given in (Tillmann, 2000). For each extension a new position is added to the coverage set. Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $). Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence. The search starts in the hypothesis (I; f;g; 0). f;g denotes the empty set, where no source sentence position is covered. The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Ã;e00 np(jjj0; J) p(Ã) pÃ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j). f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ ôL; ; Jg. The final score is obtained from: max e;e0 j2fJôL;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence. The complexity of the quasimonotone search is O(E3 J (R2+LR)). The proof is given in (Tillmann, 2000). 3.2 Reordering with IBM Style. Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996). A detailed description of the search procedure used is given in this patent. Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1. A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search. Here, we process only full-form words within the translation procedure. the number of permutations carried out for the word reordering is given. During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet. Only one of the first n positions which are not already aligned in a partial hypothesis may be chosen, where n is set to 4. The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set. This number must be less than or equal to n ô 1. Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions. Ignoring the identity of the target language words e and e0, the possible partial hypothesis extensions due to the IBM restrictions are shown in Table 2. In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction. Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction. A dynamic programming recursion similar to the one in Eq. 2 is evaluated. In this case, we have no finite-state restrictions for the search space. The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg. This approach leads to a search procedure with complexity O(E3 J4). The proof is given in (Tillmann, 2000).  4.1 The Task and the Corpus. We have tested the translation system on the Verbmobil task (Wahlster 1993). The Verbmobil task is an appointment scheduling task. Two subjects are each given a calendar and they are asked to schedule a meeting. The translation direction is from German to English. A summary of the corpus used in the experiments is given in Table 3. The perplexity for the trigram language model used is 26:5. Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this paper is the (more or less) correct orthographic transcription of the spoken sentences. Thus, the effects of spontaneous speech are present in the corpus, e.g. the syntactic structure of the sentence is rather less restricted, however the effect of speech recognition errors is not covered. For the experiments, we use a simple preprocessing step. German city names are replaced by category markers. The translation search is carried out with the category markers and the city names are resubstituted into the target sentence as a postprocessing step. Table 3: Training and test conditions for the Verbmobil task (*number of words without punctuation marks). German English Training: Sentences 58 073 Words 519 523 549 921 Words* 418 979 453 632 Vocabulary Size 7939 4648 Singletons 3454 1699 Test-147: Sentences 147 Words 1 968 2 173 Perplexity { 26:5 Table 4: Multi-reference word error rate (mWER) and subjective sentence error rate (SSER) for three different search procedures. Search CPU time mWER SSER Method [sec] [%] [%] MonS 0:9 42:0 30:5 QmS 10:6 34:4 23:8 IbmS 28:6 38:2 26:2 4.2 Performance Measures. The following two error criteria are used in our experiments: mWER: multi-reference WER: We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors. On average, 6 reference translations per automatic translation are available. The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken. This measure has the advantage of being completely automatic. SSER: subjective sentence error rate: For a more detailed analysis, the translations are judged by a human test person. For the error counts, a range from 0:0 to 1:0 is used. An error count of 0:0 is assigned to a perfect translation, and an error count of 1:0 is assigned to a semantically and syntactically wrong translation. 4.3 Translation Experiments. For the translation experiments, Eq. 2 is recursively evaluated. We apply a beam search concept as in speech recognition. However there is no global pruning. Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses. Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited. For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÃcient to consider only the best 50 words. We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2. Table 4 shows translation results for the three approaches. The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC). Here, the pruning threshold t0 = 10:0 is used. Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER). The monotone search performs worst in terms of both error rates mWER and SSER. The computing time is low, since no reordering is carried out. The quasi-monotone search performs best in terms of both error rates mWER and SSER. Additionally, it works about 3 times as fast as the IBM style search. For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy. The effect of the pruning threshold t0 is shown in Table 5. The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0. The negative logarithm of t0 is reported. The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors. Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors. Decreasing the threshold results in higher mWER due to additional search errors. Table 5: Effect of the beam threshold on the number of search errors (147 sentences). Search t0 CPU time #search mWER Method [sec] error [%] QmS 0.0 0.07 108 42:6 1.0 0.13 85 37:8 2.5 0.35 44 36:6 5.0 1.92 4 34:6 10.0 10.6 0 34:5 IbmS 0.0 0.14 108 43:4 1.0 0.3 84 39:5 2.5 0.8 45 39:1 5.0 4.99 7 38:3 10.0 28.52 0 38:2 Table 6 shows example translations obtained by the three different approaches. Again, the monotone search performs worst. In the second and third translation examples, the IbmS word reordering performs worse than the QmS word reordering, since it can not take properly into account the word reordering due to the German verbgroup. The German finite verbs 'bin' (second example) and 'konnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions). In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable.  In this paper, we have presented a new, eÃcient DP-based search procedure for statistical machine translation. The approach assumes that the word reordering is restricted to a few positions in the source sentence. The approach has been successfully tested on the 8 000-word Verbmobil task. Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word. 2) An improved language model, which takes into account syntactic structure, e.g. to ensure that a proper English verbgroup is generated. 3) A tight coupling with the speech recognizer output.  This work has been supported as part of the Verbmobil project (contract number 01 IV 601 A) by the German Federal Ministry of Education, Science, Research and Technology and as part of the Eutrans project (ESPRIT project number 30268) by the European Community. Table 6: Example Translations for the Verbmobil task. Input: Ja , wunderbar . Konnen wir machen . MonS: Yes, wonderful. Can we do . QmS: Yes, wonderful. We can do that . IbmS: Yes, wonderful. We can do that . Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin . Genaugenommen nur am dritten . Wie ware es denn am ahm Samstag , dem zehnten Februar ? MonS: That is too tight , because I from the third in Kaiserslautern . In fact only on the third . How about ahm Saturday , the tenth of February ? QmS: That is too tight , because I am from the third in Kaiserslautern . In fact only on the third . Ahm how about Saturday , February the tenth ? IbmS: That is too tight , from the third because I will be in Kaiserslautern . In fact only on the third . Ahm how about Saturday , February the tenth ? Input: Wenn Sie dann noch den siebzehnten konnten , ware das toll , ja . MonS: If you then also the seventeenth could , would be the great , yes . QmS: If you could then also the seventeenth , that would be great , yes . IbmS: Then if you could even take seventeenth , that would be great , yes . Input: Ja , das kommt mir sehr gelegen . Machen wir es dann am besten so . MonS: Yes , that suits me perfectly . Do we should best like that . QmS: Yes , that suits me fine . We do it like that then best . IbmS: Yes , that suits me fine . We should best do it like that .
 Named Entity Recognition: A Maximum Entropy Approach Using Global Information  This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.  Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC). A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information. In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task. Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999). We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier. By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data. We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information). As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework. The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors). These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task (Bikel et al., 1997; Bikel et al., 1999; Borth- wick, 1999). We believe it is natural for authors to use abbreviations in subsequent mentions of a named entity (i.e., first âPresident George Bushâ then âBushâ). As such, global information from the whole context of a document is important to more accurately recognize named entities. Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages.  Recently, statistical NERs have achieved results that are comparable to hand-coded systems. Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance. MUC7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al., 1998; Borthwick, 1999), notably Mikheev' s system, which achieved the best performance of 93.39% on the official NE test data. MENE (Maximum Entropy Named Entity) (Borth- wick, 1999) was combined with Proteus (a hand- coded system), and came in fourth among all MUC 7 participants. MENE without Proteus, however, did not do very well and only achieved an F measure of 84.22% (Borthwick, 1999). Among machine learning-based NERs, Identi- Finder has proven to be the best on the official MUC6 and MUC7 test data. MENE (without the help of hand-coded systems) has been shown to be somewhat inferior in performance. By using the output of a hand-coded system such as Proteus, MENE can improve its performance, and can even outperform IdentiFinder (Borthwick, 1999). Mikheev et al. (1998) did make use of information from the whole document. However, their system is a hybrid of hand-coded rules and machine learning methods. Another attempt at using global information can be found in (Borthwick, 1999). He used an additional maximum entropy classifier that tries to correct mistakes by using reference resolution. Reference resolution involves finding words that co-refer to the same entity. In order to train this error-correction model, he divided his training corpus into 5 portions of 20% each. MENE is then trained on 80% of the training corpus, and tested on the remaining 20%. This process is repeated 5 times by rotating the data appropriately. Finally, the concatenated 5 * 20% output is used to train the reference resolution component. We will show that by giving the first model some global features, MENERGI outperforms Borthwick' s reference resolution classifier. On MUC6 data, MENERGI also achieves performance comparable to IdentiFinder when trained on similar amount of training data. both MENE and IdentiFinder used more training data than we did (we used only the official MUC 6 and MUC7 training data). On the MUC6 data, Bikel et al. (1997; 1999) do have some statistics that show how IdentiFinder performs when the training data is reduced. Our results show that MENERGI performs as well as IdentiFinder when trained on comparable amount of training data.  The system described in this paper is similar to the MENE system of (Borthwick, 1999). It uses a maximum entropy framework and classifies each word given its features. Each name class is subdivided into 4 sub-classes, i.e., N begin, N continue, N end, and N unique. Hence, there is a total of 29 classes (7 name classes 4 sub-classes 1 not-a-name class). 3.1 Maximum Entropy. The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed. Such constraints are derived from training data, expressing some relationship between features and outcome. The probability distribution that satisfies the above property is the one with the highest entropy. It is unique, agrees with the maximum-likelihood distribution, and has the exponential form (Della Pietra et al., 1997): where refers to the outcome, the history (or context), and is a normalization function. In addition, each feature function is a binary function. For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972). This is an iterative method that improves the estimation of the parameters at each iteration. We have used the Java-based opennlp maximum entropy package1. In Section 5, we try to compare results of MENE, IdentiFinder, and MENERGI. However, 1 http://maxent.sourceforge.net 3.2 Testing. During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique). To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise. The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier. A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability.  The features we used can be divided into 2 classes: local and global. Local features are features that are based on neighboring tokens, as well as the token itself. Global features are extracted from other occurrences of the same token in the whole document. The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999). However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999). This might be because our features are more comprehensive than those used by Borthwick. In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used. In the maximum entropy framework, there is no such constraint. Multiple features can be used for the same token. Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used. We group the features used into feature groups. Each feature group can be made up of many binary features. For each token , zero, one, or more of the features in each feature group are set to 1. 4.1 Local Features. The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens. This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training. Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones). The zone to which a token belongs is used as a feature. For example, in MUC6, there are four zones (TXT, HL, DATELINE, DD). Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0. Case and Zone: If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1. If it is made up of all capital letters, then (allCaps, zone) is set to 1. If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1. A token that is allCaps will also be initCaps. This group consists of (3 total number of possible zones) features. Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1. For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword. If the token is the first word of a sentence, then this feature is set to 1. Otherwise, it is set to 0. Lexicon Feature: The string of the token is used as a feature. This group contains a large number of features (one for each token string present in the training data). At most one feature in this group will be set to 1. If is seen infrequently during training (less than a small count), then will not be selected as a feature and all features in this group are set to 0. Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1. If is not initCaps, then (not-initCaps, ) is set to 1. Same for . In the case where the next token is a hyphen, then is also used as a feature: (init- Caps, ) is set to 1. This is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter). Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1. Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task. The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al., 1999). The sources of our dictionaries are listed in Table 2. For all lists except locations, the lists are processed into a list of tokens (unigrams). Location list is processed into a list of unigrams and bigrams (e.g., New York). For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams. A list of words occurring more than 10 times in the training data is also collected (commonWords). Only tokens with initCaps not found in commonWords are tested against each list in Table 2. If they are found in a list, then a feature for that list will be set to 1. For example, if Barry is not in commonWords and is found in the list of person first names, then the feature PersonFirstName will be set to 1. Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1. For example, if is found in the list of person first names, the feature PersonFirstName is set to 1. Month Names, Days of the Week, and Numbers: If is initCaps and is one of January, February, . . . , December, then the feature MonthName is set to 1. If is one of Monday, Tuesday, . . . , Sun day, then the feature DayOfTheWeek is set to 1. If is a number string (such as one, two, etc), then the feature NumberString is set to 1. Suffixes and Prefixes: This group contains only two features: Corporate-Suffix and Person-Prefix. Two lists, Corporate-Suffix-List (for corporate suffixes) and Person-Prefix-List (for person prefixes), are collected from the training data. For corporate suffixes, a list of tokens cslist that occur frequently as the last token of an organization name is collected from the training data. Frequency is calculated by counting the number of distinct previous tokens that each token has (e.g., if Electric Corp. is seen 3 times, and Manufacturing Corp. is seen 5 times during training, and Corp. is not seen with any other preceding tokens, then the âfrequencyâ of Corp. is 2). The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List. A Person-Prefix-List is compiled in an analogous way. For MUC6, for example, Corporate- Suffix-List is made up of ltd., associates, inc., co, corp, ltd, inc, committee, institute, commission, university, plc, airlines, co., corp. and Person-Prefix- List is made up of succeeding, mr., rep., mrs., secretary, sen., says, minister, dr., chairman, ms. . For a token that is in a consecutive sequence of init then a feature Corporate-Suffix is set to 1. If any of the tokens from to is in Person-Prefix- List, then another feature Person-Prefix is set to 1. Note that we check for , the word preceding the consecutive sequence of initCaps tokens, since person prefixes like Mr., Dr., etc are not part of person names, whereas corporate suffixes like Corp., Inc., etc are part of corporate names. 4.2 Global Features. Context from the whole document can be important in classifying a named entity. A name already mentioned previously in a document may appear in abbreviated form when it is mentioned again later. Previous work deals with this problem by correcting inconsistencies between the named entity classes assigned to different occurrences of the same entity (Borthwick, 1999; Mikheev et al., 1998). We often encounter sentences that are highly ambiguous in themselves, without some prior knowledge of the entities concerned. For example: McCann initiated a new global system. (1) CEO of McCann . . . (2) Description Source Location Names http://www.timeanddate.com http://www.cityguide.travel-guides.com http://www.worldtravelguide.net Corporate Names http://www.fmlx.com Person First Names http://www.census.gov/genealogy/names Person Last Names Table 2: Sources of Dictionaries The McCann family . . . (3)In sentence (1), McCann can be a person or an orga nization. Sentence (2) and (3) help to disambiguate one way or the other. If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided. The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps. For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own. For example, in the sentence that starts with âBush put a freeze on . . . â, because Bush is the first word, the initial caps might be due to its position (as in âThey put a freeze on . . . â). If somewhere else in the document we see ârestrictions put in place by President Bushâ, then we can be surer that Bush is a name. Corporate Suffixes and Person Prefixes of Other Occurrences (CSPP): If McCann has been seen as Mr. McCann somewhere else in the document, then one would like to give person a higher probability than organization. On the other hand, if it is seen as McCann Pte. Ltd., then organization will be more probable. With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1. Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM). The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document. Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique. For example, if FCC and Federal Communications Commission are both found in a document, then Federal has A begin set to 1, Communications has A continue set to 1, Commission has A end set to 1, and FCC has A unique set to 1. Sequence of Initial Caps (SOIC): In the sentence Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement., a NER may mistake Even News Broadcasting Corp. as an organization name. However, it is unlikely that other occurrences of News Broadcasting Corp. in the same document also co-occur with Even. This group of features attempts to capture such information. For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified. For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1. Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document. needs to be in initCaps to be considered for this feature. If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears. As we will see from Table 3, not much improvement is derived from this feature.  The baseline system in Table 3 refers to the maximum entropy system that uses only local features. As each global feature group is added to the list of features, we see improvements to both MUC6 and MUC6 MUC7 Baseline 90.75% 85.22% + ICOC 91.50% 86.24% + CSPP 92.89% 86.96% + ACRO 93.04% 86.99% + SOIC 93.25% 87.22% + UNIQ 93.27% 87.24% Table 3: F-measure after successive addition of each global feature group Table 5: Comparison of results for MUC6 Systems MUC6 MUC7 No. of Articles No. of Tokens No. of Articles No. of Tokens MENERGI 318 160,000 200 180,000 IdentiFinder â 650,000 â 790,000 MENE â â 350 321,000 Table 4: Training Data MUC7 test accuracy.2 For MUC6, the reduction in error due to global features is 27%, and for MUC7,14%. ICOC and CSPP contributed the greatest im provements. The effect of UNIQ is very small on both data sets. All our results are obtained by using only the official training data provided by the MUC conferences. The reason why we did not train with both MUC6 and MUC7 training data at the same time is because the task specifications for the two tasks are not identical. As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3. In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999). IdentiFinder ' 99' s results are considerably better than IdentiFinder ' 97' s. IdentiFinder' s performance in MUC7 is published in (Miller et al., 1998). MENE has only been tested on MUC7. For fair comparison, we have tabulated all results with the size of training data used (Table 5 and Table 6). Besides size of training data, the use of dictionaries is another factor that might affect performance. Bikel et al. (1999) did not report using any dictionaries, but mentioned in a footnote that they have added list membership features, which have helped marginally in certain domains. Borth 2MUC data can be obtained from the Linguistic Data Consortium: http://www.ldc.upenn.edu 3Training data for IdentiFinder is actually given in words (i.e., 650K & 790K words), rather than tokens Table 6: Comparison of results for MUC7 wick (1999) reported using dictionaries of person first names, corporate names and suffixes, colleges and universities, dates and times, state abbreviations, and world regions. In MUC6, the best result is achieved by SRA (Krupka, 1995). In (Bikel et al., 1997) and (Bikel et al., 1999), performance was plotted against training data size to show how performance improves with training data size. We have estimated the performance of IdentiFinder ' 99 at 200K words of training data from the graphs. For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles. In fact, training on the official training data is not suitable as the articles in this data set are entirely about aviation disasters, and the test data is about air vehicle launching. Both BBN and NYU have tagged their own data to supplement the official training data. Even with less training data, MENERGI outperforms Borthwick' s MENE + reference resolution (Borthwick, 1999). Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results. The effect of a second reference resolution classifier is not entirely the same as that of global features. A secondary reference resolution classifier has information on the class assigned by the primary classifier. Such a classification can be seen as a not-always-correct summary of global features. The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document. We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre. Moreover, if we want to test on a huge test corpus, indexing the whole corpus might prove computationally expensive. Hence we decided to restrict ourselves to only information from the same document. Mikheev et al. (1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities. The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules.  We have shown that the maximum entropy framework is able to use global information directly. This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997). Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs. Information from a sentence is sometimes insufficient to classify a name correctly. Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier. We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources. Borth- wick (1999) successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results. However, such an approach requires a number of hand-coded systems, which may not be available in languages other than English. We believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously.
 Mining New Word Translations from Comparable Corpora  New words such as names, technical terms, etc appear frequently. As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations. Comparable corpora such as news documents of the same period from different news agencies are readily available. In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.  New words such as person names, organization names, technical terms, etc. appear frequently. In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations. Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003). But parallel corpora are scarce resources, especially for uncommon lan guage pairs. Comparable corpora refer to texts that are not direct translation but are about the same topic. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora. Previous research efforts on acquiring translations from comparable corpora include (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999). When translating a word w, two sources of information can be used to determine its translation: the word w itself and the surrounding words in the neighborhood (i.e., the context) of w. Most previous research only considers one of the two sources of information, but not both. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language. In this paper, we propose a new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information. Since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone. We fully implemented our method and tested it on ChineseEnglish comparable corpora. We translated Chinese words into English. That is, Chinese is the source language and English is the target language. We achieved encouraging results. While we have only tested our method on Chinese-English comparable corpora, our method is general and applicable to other language pairs.  The work of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) noted that if an English word e is the translation of a Chinese word c , then the contexts of the two words are similar. We could view this as a document retrieval problem. The context (i.e., the surrounding words) of c is viewed as a query. The context of each candidate translation e' is viewed as a document. Since the context of the correct translation e is similar to e , is considered as a document in IR. If an English word e is the translation of a Chinese word c , they will have similar contexts. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. The English word e document. We employ the language modeling approach (Ng, 2000; Ponte and Croft, 1998) for corresponding to that document translation of c . C (e* ) is the this retrieval problem. More details are given in Section 3. On the other hand, when we only look at the word w itself, we can rely on the pronunciation of w to locate its translation. We use a variant of Within IR, there is a new approach to document retrieval called the language modeling approach (Ponte & Croft, 98). In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998). More details are is estimated. The document with the highest given in Section 4. Each of the two individual methods provides a P(Q | D) is the one that best matches the query. ranked list of candidate words, associating with each candidate a score estimated by the particular method. If a word e in English is indeed the translation of a word c in Chinese, then we would expect e to be ranked very high in both lists in general. Specifically, our combination method is as follows: we examine the top M The language modeling approach to IR has been shown to give superior retrieval performance (Ponte & Croft, 98; Ng, 2000), compared with traditional vector space model, and we adopt this approach in our current work. To estimate P(Q | D) , we use the approach of (Ng, 2000). We view the document D as a multinomial distribution of terms and assume that words in both lists and finde1 , e2 ,..., ek that ap query Q is generated by this model: pear in top M positions in both lists. We then n! rank these words e1 , e2 ,..., ek according to the P (Q | D ) = â P (t | D ) c t average of their rank positions in the two lists. â t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output. If no words appear within the top M positions in both lists, then no translation is output. Since we are using comparable corpora, it is possible that the translation of a new word does not exist in the target corpus. In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations of times term t occurs in the query Q , n = ât ct is the total number of terms in query Q . For ranking purpose, the first fraction n! / ât ct ! can be omitted as this part depends on the query only and thus is the same for all the documents. in the English target corpus. In our translation problem, C(c) is viewed as the query and C(e) is viewed as a document. So  our task is to compute P(C (c) | C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c) | C (e)) , estimated as: most relevant to the query is returned from a document collection. â P(tc tcâC ( c ) | T (C (e)))q (tc ) For our task, the query is C (c) , the context Term tc is a Chinese word. q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form. Then we sum lingual dictionary. If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping. ble of pinyin, api is the ith sylla li is the English letter sequence estimation. We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) = Î± â Pml (tc | Tc (C (e))) + (1 âÎ± ) â Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem. We use the Pml (tc | Tc (C (e))) = dT (C (e )) (tc ) âdT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c tâTc (C ( e )) lables to English letter sequences. To reduce the search space, we limit the number of English letters that each pinyin syllable can map to as 0, where Pml (â¢) are the maximu m likelihood esti 1, or 2. Also. we do not allow cross mappin gs. mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus. Î± is set to 0.6 in our experiments.  must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e | c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus. Another difference is Onaizan and Knight, 2002b). Knight and Graehl (1998) proposed a probabilistic model for machine transliteration. In this model, a word in the target language (i.e., English in our task) is written and pronounced. This pronunciation is converted to source language pronunciation and then to source language word that our method estimates stead of P(c | e) and P(e) .  5.1 Resources. P(e | c)directly, in (i.e., Chinese in our task). AlOnaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters. Pinyin is the standard Romanization system of Chinese characters. It is phonetic-based. For transliteration, we estimate P(e | c) as follows: P(e | c) = P(e | pinyin) = â P(e, a | pinyin) a For the Chinese corpus, we used the Linguistic Data Consortium (LDC) Chinese Gigaword Corpus from Jan 1995 to Dec 1995. The corpus of the period Jul to Dec 1995 was used to come up with new Chinese words c for translation into English. The corpus of the period Jan to Jun 1995 was just used to determine if a Chinese word c from Jul to Dec 1995 was new, i.e., not occurring from Jan to Jun 1995. Chinese Giga- word corpus consists of news from two agencies: = ââ P(l a a i | pi ) Xinhua News Agency and Central News Agency. As for English corpus, we used the LDC English Gigaword Corpus from Jul to Dec 1995. The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service. To avoid accidentally using parallel texts, we did not use the texts of Xinhua News Agency them English translation candidate words. For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates. English Service. The size of the English corpus from Jul to Dec The context C(c)of a Chinese word c was col 1995 was about 730M bytes, and the size of the Chinese corpus from Jul to Dec 1995 was about 120M bytes. We used a ChineseEnglish dictionary which contained about 10,000 entries for translating the words in the context. For the training of transliteration probability, we required a ChineseEnglish name list. We used a list of 1,580 ChineseEnglish name pairs as training data for the EM algorithm. lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. The contexts of all occurrences of a word c were then concatenated together to form C(c) . The context of an English translation candidate word e, C (e) , was similarly collected. The window size of English context was 100 words.After all the counts were collected, we esti mated P(C (c) | C (e)) as described in Section 3, 5.2 Preprocessing. Unlike English, Chinese text is composed of Chinese characters with no demarcation for words. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). for each pair of Chinese source word and English translation candidate word. For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c) | C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e | c) (as described in from a half-month period. Then we determined the new Chinese words in each half-month period p. By new Chinese words, we refer to those words that appeared in this period p but not from Jan to Jun 1995 or any other periods that preceded p. Among all these new words, we selected those occurring at least 5 times. These words made up our test set. We call these words Chinese source words. They were the words that we were supposed to find translations from the English corpus. For the English corpus, we performed sentence segmentation and converted each word to its morphological root form and to lower case. We also divided the English corpus into 12 periods, each containing text from a half-month period. For each period, we selected those English words occurring at least 10 times and were not present in the 10,000-word ChineseEnglish dictionary we used and were not stop words. We considered these English words as potential translations of the Chinese source words. We call Section 4), which was used to rank the English candidate words based on transliteration. Finally, the English candidate word with the smallest average rank position and that appears within the top M positions of both ranked lists is the chosen English translation (as described in Section 2). If no words appear within the top M positions in both ranked lists, then no translation is output. Note that for many Chinese words, only one English word e appeared within the top M positions for both lists. And among those cases where more than one English words appeared within the top M positions for both lists, many were multiple translations of a Chinese word. This happened for example when a Chinese word was a non-English person name. The name could have multiple translations in English. For example, ç±³ æ´è¥¿å¨ was a Russian name. Mirochina and Miroshina both appeared in top 10 positions of both lists. Both were correct. 5.4 Evaluation. We evaluated our method on each of the 12 half- month periods. The results when we set M = 10 are shown in Table 1. We also investigated the effect of varying M . The results are shown in Table 2. Table 1. Accuracy of our system in each period (M = 10) In Table 1, period 1 is Jul 01 â Jul 15, period 2 is Jul 16 â Jul 31, â¦, period 12 is Dec 16 â Dec 31. #c is the total number of new Chinese source words in the period. #e is the total number of English translation candidates in the period. #o is the total number of output English translations. #Cor is the number of correct English translations output. Prec. is the precision. The correctness of the English translations was manually checked. Recall is somewhat difficult to estimate because we do not know whether the English translation of a Chinese word appears in the English part of the corpus. We attempted to estimate recall by manually finding the English translations for all the Chinese source words for the two periods Dec 01 â Dec 15 and Dec 16 â Dec 31 in the English part of the corpus. During the whole December period, we only managed to find English translations which were present in the English side of the comparable corpora for 43 Chinese words. So we estimate that English translations are present in the English part of the corpus for Table 2. Precision and recall for different values of M The past research of (Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) utilized context information alone and was evaluated on different corpora from ours, so it is difficult to directly compare our current results with theirs. Similarly, AlOnaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. To investigate the effect of the two individual sources of information (context and transliteration), we checked how many translations could be found using only one source of information (i.e., context alone or transliteration alone), on those Chinese words that have translations in the English part of the comparable corpus. As mentioned earlier, for the month of Dec 1995, there are altogether 43 Chinese words that have their translations in the English part of the corpus. This list of 43 words is shown in Table 3. 8 of the 43 words are translated to English multi-word phrases (denoted as âphraseâ in Table 3). Since our method currently only considers unigram English words, we are not able to find translations for these words. But it is not difficult to extend our method to handle this problem. We can first use a named entity recognizer and noun phrase chunker to extract English names and noun phrases. The translations of 6 of the 43 words are words in the dictionary (denoted as âcomm.â in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as âinsuffâ). Our method is not able to find 43 (329 + 205) Ã 4499 = 362words in all 12 pe these translations. But this is due to search space riods. And our program finds correct translations for 115 words. So we estimate that recall (for M = 10) is approximately 115 / 362 = 31.8% . pruning. If we are willing to spend more time on searching, then in principle we can find these translations. Table 3. Rank of correct translation for period Dec 01 â Dec 15 and Dec 16 â Dec 31. âCont. rankâ is the context rank, âTrans. Rankâ is the transliteration rank. âNAâ means the word cannot be transliterated. âinsuffâ means the correct translation appears less than 10 times in the English part of the comparable corpus. âcommâ means the correct translation is a word appearing in the dictionary we used or is a stop word. âphraseâ means the correct translation contains multiple English words. As shown in Table 3, using just context information alone, 10 Chinese words (the first 10) have their correct English translations at rank one position. And using just transliteration information alone, 9 Chinese words have their correct English translations at rank one position. On the other hand, using our method of combining both sources of information and setting M = â, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except å·´ä½äº,å©å,æ®å©æ³) have their correct English translations at rank one position. If M = 10, 15 Chinese words (i.e., the first 19 Chinese words in Table 3 except å¶çæ¯,å·´ä½äº,å©å,æ®å©æ³) have their correct English translations at rank one position. Hence, our method of using both sources of information outperforms using either information source alone.  As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information. For example, the work of (AlOnaizan and Knight, 2002a; AlOnaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Rapp, 1995; Rapp, 1999) used only the context of w to locate its translation in a second language. In contrast, our current work attempts to combine both complementary sources of information, yielding higher accuracy than using either source of information alone. Koehn and Knight (2002) attempted to combine multiple clues, including similar context and spelling. But their similar spelling clue uses the longest common subsequence ratio and works only for cognates (words with a very similar spelling). The work that is most similar to ours is the recent research of (Huang et al., 2004). They attempted to improve named entity translation by combining phonetic and semantic information. Their contextual semantic similarity model is different from our language modeling approach to measuring context similarity. It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.  In this paper, we proposed a new method to mine new word translations from comparable corpora, by combining context and transliteration information, which are complementary sources of information. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results.  We thank Jia Li for implementing the EM algorithm to train transliteration probabilities. This research is partially supported by a research grant R252000-125112 from National University of Singapore Academic Research Fund.
 Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging  We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.  A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags. Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features. For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate. The additional information may also help to disambiguate the (base) part of speech. Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das RealitaÂ¨ t? (Is that reality?). The word das is ambiguous between an article and a demonstrative. Because of the lack of gender agreement between das (neuter) and the noun RealitaÂ¨ t (feminine), the article reading must be wrong. The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall). Large tagsets aggravate sparse data problems. As an example, take the German sentence Das zu versteuernde Einkommen sinkt (âThe to be taxed income decreasesâ; The tËN N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases). This sentence should be tagged as shown in table 1. Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut p(tN , wN ) = n 1 1 i=1 p(ti|tiâ1 ) iâk p(wi|ti) le . (1) context prob. xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora. Qc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt. Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus. (Neither does the pair consisting of the first two tags.) The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777â784 Manchester, August 2008 context probability of the third POS tag is therefore 0. If the probability is smoothed with the backoff distribution p(â¢|P ART .Z u), the most probable tag is ADJA.Pos.Acc.Sg.Fem rather than ADJA.Pos.Nom.Sg.Neut. Thus, the agreement between the article and the adjective is not checked anymore. A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut: â¢ All words appearing after an article (ART) and the infinitive particle zu (PART.zu) are attributive adjectives (ADJA) (10 of 10 cases). â¢ All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases). â¢ All adjectives appearing after a nominative article and a particle have nominative case (11 of 11 cases). â¢ All adjectives appearing after a singular article and a particle are singular (32 of 32 cases). â¢ All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases). By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) â p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) â p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) â p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) â p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the â p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) â p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1. Hence the context probability of the whole tag is. also 1. Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context. These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute. Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method. The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here. Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.  Decision trees (Breiman et al., 1984; Quinlan, 1993) are normally used as classifiers, i.e. they assign classes to objects which are represented as attribute vectors. The non-terminal nodes are labeled with attribute tests, the edges with the possible outcomes of a test, and the terminal nodes are labeled with classes. An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating the test of the daughter node, and so on until a terminal node is reached whose class is assigned to the object. Decision Trees are turned into probability estimation trees by storing a probability for each possible class at the terminal nodes instead of a single result class. Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns. 2.1 Induction of Decision Trees. Decision trees are incrementally built by first selecting the test which splits the manually annotated training sample into the most homogeneous subsets with respect to the class. This test, which maximizes the information gain1 wrt. the class, is following expression for the context probability: 1 The information gain measures how much the test de-. p(ADJA | ART, PART.Zu) â p(Pos | 2:ART, 1:PART, 0:ADJA) â p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA) creases the uncertainty about the class. It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy yes 0:N.Name yes no 1:ART.Nom no 1:ADJA.Nom yes no which returns a probability of 0.3. The third tree for neuter has one non terminal and two terminal nodes returning a probability of 0.3 and 0.5, respectively. The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1. This problem 2:N.Reg p=0.999 0:N.Name 0:N.Name yes no p=0.571 p=0.938 yes no p=0.948 p=0.998 .... is solved by renormalizing the probabilities. The probability of an attribute (such as âNomâ) is always conditioned on the respective base POS (such as âNâ) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns. The test 1:ART.Nom checks if the preceding word is a nominative article. assigned to the top node. The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class. In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data. Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value. The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not. The best test for each node is selected with the standard information gain criterion. The recursive tree building process terminates if the information gain is 0. The decision tree is pruned with the pruning criterion described below. Since the tagger creates a separate tree for each attribute, the probabilities of a set of competing attributes such as masculine, feminine, and neuter will not exactly sum up to 1. To understand why, assume that there are three trees for the gender attributes. Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS. All context attributes other than the base POS are always used in combination with the base POS. A typical context attribute is â1:ART.Nomâ which states that the preceding tag is an article with the attribute âNomâ. â1:ARTâ is also a valid attribute specification, but â1:Nomâ is not. The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k (i=position of the predicted POS tag, k â¥ 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined. This restriction improved the tagging accuracy for large contexts. 2.2 Pruning Criterion. The tagger applies3 the critical-value pruning strategy proposed by (Mingers, 1989). A node is pruned if the information gain of the best test multiplied by the size of the data subsample is below a given threshold. To illustrate the pruning, assume that D is the data of the current node with 50 positive and 25 negative elements, and that D1 (with 20 positive and 20 negative elements) and D2 (with 30 positive and 5 negative elements) are the two subsets induced by the best test. The entropy of D is â2/3 log22/3 â 1/3 log21/3 = 0.92, the entropy of D1 is â1/2 log21/2â1/2 log21/2 = 1, and the entropy of D2 is â6/7 log26/7 â 1/7 log21/7 = 0.59. The information gain is therefore 0.92 â (8/15 â 1 â 7/15 â 0.59) = 0.11. The resulting score is 75 â 0.11 = 8.25. Given a threshold of 6, the node is therefore not pruned. We experimented with pre-pruning (where a node is always pruned if the gain is below the in the two subsets. The weight of each subset is proportional to its size. 2 We did not directly compare the two alternatives (two- valued vs. multi-valued tests), because the implementational effort required would have been too large. 3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was never larger than 0.1% for any context size. Thus, the simpler pruning strategy presented here was chosen. threshold) as well as post-pruning (where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes). The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold. A threshold of 6 consistently produced optimal or near optimal results for pre-pruning. Thus, pre-pruning with a threshold of 6 was used in the experiments.  The tagger treats dots in POS tag labels as attribute separators. The first attribute of a POS tag is the main category. The number of additional attributes is fixed for each main category. The additional attributes are category-specific. The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature. The attributes occurring at a certain position constitute the value set of the feature.  Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities. The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|tiâ1 ) is internally computed as a product of attribute probabili ties. In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold. With athreshold of 10â3 or lower, the influence of prun ing on the tagging accuracy was negligible. 4.1 Supplementary Lexicon. The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data. If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags. The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities. If the word w was observed with N different tags, and f (w, t) is the joint frequency of w and POS tag t, and p(t|[w]) is the average probability of t among words with the same set of possible tags as w, then the smoothed probability of t given w is defined as follows: f (w, t) + N p(t|[w]) the preceding attributes of the predicted POS tag is estimated with a decision tree as described be p(t|w) = f (w) + N fore. The probabilities at the terminal nodes of the decision trees are smoothed with the parent node probabilities (which themselves were smoothed in the same way). The smoothing is implemented by adding the weighted class probabilities pp(c) of the parent node to the frequencies f (c) before normalizing them to probabilities: p(c) = f (c) + Î±pp(c) Î± + ï¿½c f (c) The weight Î± was fixed to 1 after a few experiments on development data. This smoothing strategy is closely related to Witten-Bell smoothing. The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature (see section 2.1). The best tag sequence is computed with the Viterbi algorithm. The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).5 4.2 Unknown Words. The lexical probabilities of unknown words are obtained as follows: The unknown words are divided into four disjoint classes6 with numeric expressions, words starting with an uppercase letter, words starting with a lowercase letter, and a fourth class for the other words. The tagger builds a suffix trie for each class of unknown words using the known word types from that class. The maximal length of the suffixes is 7. The suffix tries are pruned until (i) all suffixes have a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous. Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences. 6 In earlier experiments, we had used a much larger number of word classes. Decreasing their number to 4 turned out to be better. a threshold of 1. More precisely, if TÎ± is the set of POS tags that occurred with suffix Î±, |T | is the size of the set T , fÎ± is the frequency of suffix Î±, and pÎ±(t) is the probability of POS tag t among the words with suffix Î±, then the following condition must hold: tion between definite and indefinite articles, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other symbols which the Tiger treebank annotates with â$(â. A supplementary lexicon was created by analyzing a word list which included all words from the faÎ± paÎ± (t) log paÎ±(t) < 1 training, development, and test data with a German computationa l morphology. The analyses gener |TaÎ±| tâTaÎ± pÎ±(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.  Our tagger was first evaluated on data from the German Tiger treebank. The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (GimeÂ´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day. Therefore it was not possible to optimize the parameters systematically. We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (HladkaÂ´ et al., 2007) and compared to the TnT tag- ger. 5.1 Tiger Corpus. The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens. It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood. After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left. The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data. Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23. Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name. Some lexically decidable distinctions missing in the Tiger corpus have been tagset. Note that only the words, but not the POS tags from the test and development data were used, here. Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed. In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown. This strategy returned the best results on the development data. In case of the SVM- Tool, we were not able to successfully integrate the supplementary lexicon. 5.1.1 Refined Tagset Prepositions are not annotated with case in the Tiger treebank, although this information is important for the disambiguation of the case of the next noun phrase. In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc). Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in). The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or). For evaluation purposes, the refined tags are mapped back to the original tags. This mapping is unambiguous. 7 It was planned to include also the Stanford tagger. (Toutanova et al., 2003) in this comparison, but it was not possible to train it on the Tiger data. 8 In German, the genitive case of arguments is more and. more replaced by the dative. Table 2: Tagging accuracies on development data in percent. Results for 2 and for 10 preceding POS tags as context are reported for our tagger. much smaller. Table 3 shows the results of an evaluation based on the plain STTS tagset. The first result was obtained with TnT trained on Tiger data which was mapped to STTS before. The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS. The third row gives the corresponding figures for our tagger. 5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data. The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon. The TnT tagger achieves 86.3% accuracy on the default tagset. A tag is considered correct if all attributes are correct. The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%. The SVMTool is slightly better than the TnT tagger on the default tagset, but shows little improvement from the tagset refinement. Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement. With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outperforms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon. A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively. de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7. 2 8 9 7. 1 7 97.26 97.51 9 7. 3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset. These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features. This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization. Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size. The best results are obtained with a context size of 10. What type of information is relevant across a distance of ten words? A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order. Since German is a verb-final language, these tests clearly make sense. Table 4 shows the performance on the test data. Our tagger was used with a context size of 10. The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon. These values were optimal on the development data. The accuracy of our tagger is lower than on the development data. This could be due to the higher rate of unknown words (10.0% vs. 7.7%). Relative to the TnT tagger, however, the accuracy is quite similar for test and development data. The differences between the two taggers are significant.10 ta gg er de fa ult refined ref.+lexicon Tn T ou r ta gg er 8 3. 4 5 84.11 89.14 8 5. 0 0 85.92 91.07 Table 4: Tagging accuracies on test data. By far the most frequent tagging error was the confusion of nominative and accusative case. If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger. The resulting score of a binomial test is below 0.001. this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%. Our tagger is quite fast, although not as fast as the TnT tagger. With a context size of 3 (10), it annotates 7000 (2000) tokens per second on a computer with an Athlon X2 4600 CPU. The training with a context size of 10 took about 4 minutes. 5.2 Czech Academic Corpus. We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags. The data was divided into 80% training data, 10% development data and 10% test data. 89 88.9 88.8 Provost & Domingos (2003) noted that well- known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al., 1984) fail to produce accurate probability estimates. They proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the Laplace correction). Ferri et al. (2003) describe a more complex backoff smoothing method. Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0). Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes. These two-class trees can be pruned with a fixed pruning threshold. Hence there is no need to put aside training data for parameter tuning. 88.7 88.6 88.5 â c o n t e x t d a t a 2 â 2 3 4 5 6 7 8 9 10 A n ope n que stio n is wh eth er the SV MT ool (or oth er dis cri min ativ ely trai ned tag ger s) cou ld out - perf orm the pre sen ted tag ger if the sa me dec om positi on of PO S tag s and the sa me con text size wasFigure 3: Accuracy on development data depend ing on context size The best accuracy of our tagger on the development set was 88.9% obtained with a context of 4 preceding POS tags. The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5. The corresponding figures for the test data are. 89.53% for our tagger and 88.88% for the TnT tag- ger. The difference is significant.  Our tagger combines two ideas, the decomposition of the probability of complex POS tags into a product of feature probabilities, and the estimation of the conditional probabilities with decision trees. A similar idea was previously presented in Kempe (1994), but apparently never applied again. The tagging accuracy reported by Kempe was below that of a traditional trigram tagger. Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used. Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability. Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model. used. We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train the SVMTool (or other discriminatively trained tag- gers) without such a restriction given the difficulties to train it with the standard context size. Czech POS tagging has been extensively studied in the past (HajicË and VidovaÂ´-HladkaÂ´, 1998; HajicË et al., 2001; Votrubec, 2006). Spoustov et al. (2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (MorcËe), and evaluated them on the Prague Dependency Treebank (PDT 2.0). MorcËeâs tagging accuracy was 95.12%, 0.3% better than the n-gram tagger. A hybrid system based on four different tagging methods reached an accuracy of 95.68%. Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult. Furthermore, our tagger uses no corpus-specific heuristics, whereas MorcËe e.g. is optimized for Czech POS tagging. The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.  We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees. In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).
 Better Arabic Parsing: Baselines, Evaluations, and Analysis  In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design. First, we identify sources of syntactic ambiguity understudied in the existing parsing literature. Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic. Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG. Fourth, we show how to build better models for three different parsers. Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1.  It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Simaâan, 2008), and the effect of variable word order (Collins et al., 1999). Certainly these linguistic factors increase the difficulty of syntactic disambiguation. Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; KuÂ¨ bler, 2005). 1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (HajicË and ZemaÂ´nek, 2004; Habash and Roth, 2009). To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply âArabicâ) because of the unusual opportunity it presents for comparison to English parsing results. The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993). Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages. But Arabic contains a variety of linguistic phenomena unseen in English. Crucially, the conventional orthographic form of MSA text is unvocalized, a property that results in a deficient graphical representation. For humans, this characteristic can impede the acquisition of literacy. How do additional ambiguities caused by devocalization affect statistical learning? How should the absence of vowels and syntactic markers influence annotation choices and grammar development? Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering. Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (Â§2). Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (Â§3). We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (Â§4). To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (Â§5). Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6). We quantify error categories in both evaluation settings. To our knowledge, ours is the first analysis of this kind for Arabic parsing.  Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages. The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics. Particles are uninflected. Word Head Of Complement POS 1 '01 inna âIndeed, trulyâ VP Noun VBP 2 '01 anna âThatâ SBAR Noun IN 3 01 in âIfâ SBAR Verb IN 4 01 an âtoâ SBAR Verb IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an. The distinctions in the ATB are linguistically justified, but complicate parsing. Table 8a shows that the best model recovers SBAR at only 71.0% F1. Diacritics can also be used to specify grammatical relations such as case and gender. But diacritics are not present in unvocalized text, which is the standard form of, e.g., news media documents.3 VBD she added VP PUNC S VP VBP NP ... VBD she added VP PUNC â SBAR IN NP 0 NN. Let us consider an example of ambiguity caused by devocalization. Table 1 shows four words â 0 Indeed NN Indeed Saddamwhose unvocalized surface forms 0 an are indistinguishable. Whereas Arabic linguistic theory as Saddam (a) Reference (b) Stanford signs (1) and (2) to the class of pseudo verbs 01 +i J>1ï¿½ inna and her sisters since they can beinflected, the ATB conventions treat (2) as a com plementizer, which means that it must be the head of SBAR. Because these two words have identical complements, syntax rules are typically unhelpful for distinguishing between them. This is especially true in the case of quotationsâwhich are common in the ATBâwhere (1) will follow a verb like (2) (Figure 1). Even with vocalization, there are linguistic categories that are difficult to identify without semantic clues. Two common cases are the attribu tive adjective and the process nominal _; maSdar, which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals; they are inflected for gender, number, case, and definiteness. Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order. However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009). In particular, the decision to represent arguments in verb- initial clauses as VP internal makes VSO and VOS configurations difficult to distinguish. Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop). 3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007). However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance. 4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun ï¿½ '.i . Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1). more frequently than is done in English. Process nominals name the action of the transitive or ditransitive verb from which they derive. The verbal reading arises when the maSdar has an NP argument which, in vocalized text, is marked in the accusative case. When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct ï¿½ ?f iDafa. Gabbard and Kulick (2008) show that there is significant attachment ambiguity associated with iDafa, which occurs in 84.3% of the trees in our development set. Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase. All three models evaluated in this paper incorrectly analyze the constituent as iDafa; none of the models attach the attributive adjectives properly. For parsing, the most challenging form of ambiguity occurs at the discourse level. A defining characteristic of MSA is the prevalence of discourse markers to connect and subordinate words and phrases (Ryding, 2005). Instead of offsetting new topics with punctuation, writers of MSA in sert connectives such as ï¿½ wa and ï¿½ fa to link new elements to both preceding clauses and the text as a whole. As a result, Arabic sentences are usually long relative to English, especially after Length English (WSJ) Arabic (ATB) â¤ 20 41.9% 33.7% â¤ 40 92.4% 73.2% â¤ 63 99.7% 92.6% â¤ 70 99.9% 94.9% Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2â23) and the ATB (p1â3). English parsing evaluations usually report results on sentences up to length 40. Arabic sentences of up to length 63 would need to be. evaluated to account for the same fraction of the data. We propose a limit of 70 words for Arabic parsing evaluations. ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks. Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Negra (Dubey and Keller, 2003); English, sections 221 (train) and section 23 (test). Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction. segmentation (Table 2). The ATB gives several different analyses to these words to indicate different types of coordination. But it conflates the coordinating and discourse separator functions of wa (<..4.b ï¿½ ï¿½) into one analysis: conjunction(Table 3). A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990). We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).  3.1 Gross Statistics. Linguistic intuitions like those in the previous section inform language-specific annotation choices. The resulting structural differences between tree- banks can account for relative differences in parsing performance. We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4). The ATB is disadvantaged by having fewer trees with longer average 5 LDC A-E catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1). We map the ATB morphological analyses to the shortened âBiesâ tags for all experiments. yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (Î¼ Constituents / Î¼ Length). Evalb, the standard parsing metric, is biased toward such corpora (Sampson and Babarczy, 2003). Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic. In general, several gross corpus statistics favor the ATB, so other factors must contribute to parsing underperformance. 3.2 Inter-annotator Agreement. Annotation consistency is important in any supervised learning task. In the initial release of the ATB, inter-annotator agreement was inferior to other LDC treebanks (Maamouri et al., 2008). To improve agreement during the revision process, a dual-blind evaluation was performed in which 10% of the data was annotated by independent teams. Maamouri et al. (2008) reported agreement between the teams (measured with Evalb) at 93.8% F1, the level of the CTB. But Rehbein and van Genabith (2007) showed that Evalb should not be used as an indication of real differenceâ or similarityâbetween treebanks. Instead, we extend the variation n-gram method of Dickinson (2005) to compare annotation error rates in the WSJ and ATB. For a corpus C, let M be the set of tuples ân, l), where n is an n-gram with bracketing label l. If any n appears 6 Generative parsing performance is known to deteriorate with sentence length. As a result, Habash et al. (2006) developed a technique for splitting and chunking long sentences. In application settings, this may be a profitable strategy. NN ï¿½ .e NP NNP NP DTNNP NN ï¿½ .e NP NP NNP NP Table 5: Evaluation of 100 randomly sampled variation nuclei types. The samples from each corpus were independently evaluated. The ATB has a much higher fraction of nuclei per tree, and a higher type-level error rate. summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add ân, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity. Human evaluation is one way to distinguish between the two cases. Following Dickinson (2005), we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error. The human evaluators were a non-native, fluent Arabic speaker (the first author) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus. The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ. The results clearly indicate increased variation in the ATB relative to the WSJ, but care should be taken in assessing the magnitude of the difference. On the one hand, the type-level error rate is not calibrated for the number of n-grams in the sample. At the same time, the n-gram error rate is sensitive to samples with extreme n-gram counts. For example, one of the ATB samples was the determiner -"" ; dhalikâthat.â The sample occurred in 1507 corpus po sitions, and we found that the annotations were consistent. If we remove this sample from the evaluation, then the ATB type-level error rises to only 37.4% while the n-gram error rate increases to 6.24%. The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions. 7 Unlike Dickinson (2005), we strip traces and only con-. Figure 2: An ATB sample from the human evaluation. The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a). But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).  We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003). Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions. A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7). In our grammar, features are realized as annotations to basic category labels. We start with noun features since written Arabic contains a very high proportion of NPs. genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter. This is the form of recursive levels in iDafa constructs. We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008). For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead). Base NPs are the other significant category of nominal phrases. markBaseNP indicates these non-recursive nominal phrases. This feature includes named entities, which the ATB marks with a flat NP node dominating an arbitrary number of NNP pre-terminal daughters (Figure 2). For verbs we add two features. First we mark any node that dominates (at any level) a verb sider POS tags when pre-terminals are the only intervening nodes between the nucleus and its bracketing (e.g., unaries, base NPs). Since our objective is to compare distributions of bracketing discrepancies, we do not use heuristics to prune the set of nuclei. 8 We use head-finding rules specified by a native speaker. of Arabic. This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses. termined by the category of the word that follows it. Because conjunctions are elevated in the parse trees when they separate recursive constituents, we choose the right sister instead of the category of the next word. We create equivalence classes for verb, noun, and adjective POS categories.  Table 6: Incremental dev set results for the manually annotated grammar (sentences of length â¤ 70). phrase (markContainsVerb). This feature has a linguistic justification. Historically, Arabic grammar has identified two sentences types: those that begin with a nominal (ï¿½ '.i ï¿½u _.. ), and thosethat begin with a verb (ï¿½ ub..i ï¿½u _.. But for eign learners are often surprised by the verbless predications that are frequently used in Arabic. Although these are technically nominal, they have become known as âequationalâ sentences. mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences. We also mark all nodes that dominate an SVO configuration (containsSVO). In MSA, SVO usually appears in non-matrix clauses. Lexicalizing several POS tags improves performance. splitIN captures the verb/preposition idioms that are widespread in Arabic. Although this feature helps, we encounter one consequence of variable word order. Unlike the WSJ corpus which has a high frequency of rules like VP âVB PP, Arabic verb phrases usually have lexi calized intervening nodes (e.g., NP subjects and direct objects). For example, we might have VP â VB NP PP, where the NP is the subject. This annotation choice weakens splitIN. We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers. All experiments use ATB parts 1â3 divided according to the canonical split suggested by Chiang et al. (2006). Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text. At the phrasal level, we remove all function tags and traces. We also collapse unary chains withidentical basic categories like NP â NP. The pre terminal morphological analyses are mapped to the shortened âBiesâ tags provided with the tree- bank. Finally, we add âDTâ to the tags for definite nouns and adjectives (Kulick et al., 2006). The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents. We retain segmentation markersâwhich are consistent only in the vocalized section of the treebankâto differentiate between e.g. ï¿½ âtheyâ and ï¿½ + âtheir.â Because we use the vocalized section, we must remove null pronoun markers. In Table 7 we give results for several evaluation metrics. Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag. For parsing, this is a mistake, especially in the case of interrogatives. splitPUNC restores the convention of the WSJ. We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine). To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC). The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-. able at http://nlp.stanford.edu/projects/arabic.shtml. 10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. 11 taTweel (-) is an elongation character used in Arabic script to justify text. It has no syntactic function. Variants of alif are inconsistently used in Arabic texts. For alif with hamza, normalization can be seen as another level of devocalization. 12 For English, our Evalb implementation is identical to the most recent reference (EVALB20080701). For Arabic we M o d e l S y s t e m L e n g t h L e a f A n c e s t o r Co rpu s Sent Exact E v a l b L P LR F1 T a g % B a s e l i n e 7 0 St an for d (v 1. 6. 3) all G o l d P O S 7 0 0.7 91 0.825 358 0.7 73 0.818 358 0.8 02 0.836 452 80. 37 79. 36 79. 86 78. 92 77. 72 78. 32 81. 07 80. 27 80. 67 95. 58 95. 49 99. 95 B a s e li n e ( S e lf t a g ) 70 a l l B i k e l ( v 1 . 2 ) B a s e l i n e ( P r e t a g ) 7 0 a l l G o l d P O S 70 0.7 70 0.801 278 0.7 52 0.794 278 0.7 71 0.804 295 0.7 52 0.796 295 0.7 75 0.808 309 77. 92 76. 00 76. 95 76. 96 75. 01 75. 97 78. 35 76. 72 77. 52 77. 31 75. 64 76. 47 78. 83 77. 18 77. 99 94. 64 94. 63 95. 68 95. 68 96. 60 ( P e tr o v, 2 0 0 9 ) all B e r k e l e y ( S e p . 0 9 ) B a s e l i n e 7 0 a l l G o l d P O S 70 â â â 0 . 8 0 9 0.839 335 0 . 7 9  0 . 8 3 1 0.859 496 76. 40 75. 30 75. 85 82. 32 81. 63 81. 97 81. 43 80. 73 81. 08 84. 37 84. 21 84. 29 â 95. 07 95. 02 99. 87 Table 7: Test set results. Maamouri et al. (2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length â¤ 40. The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them. We are unaware of prior results for the Stanford parser. F1 85 Berkeley 80 Stanford. Bikel 75 training trees 5000 10000 15000 Figure 3: Dev set learning curves for sentence lengths â¤ 70. All three curves remain steep at the maximum training set size of 18818 trees. The Leaf Ancestor metric measures the cost of transforming guess trees to the reference (Sampson and Babarczy, 2003). It was developed in response to the non-terminal/terminal bias of Evalb, but Clegg and Shepherd (2005) showed that it is also a valuable diagnostic tool for trees with complex deep structures such as those found in the ATB. For each terminal, the Leaf Ancestor metric extracts the shortest path to the root. It then computes a normalized Levenshtein edit distance between the extracted chain and the reference. The range of the score is between 0 and 1 (higher is better). We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores along add a constraint on the removal of punctuation, which has a single tag (PUNC) in the ATB. Tokens tagged as PUNC are not discarded unless they consist entirely of punctuation. with the number of exactly matching guess trees. 5.1 Parsing Models. The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1. Presence of the determiner J Al. 2. Contains digits. 3. Ends with the feminine affix :: p. 4. Various verbal (e.g., ï¿½, .::) and adjectival. suffixes (e.g., ï¿½=) Other notable parameters are second order vertical Markovization and marking of unary rules. Modifying the Berkeley parser for Arabic is straightforward. After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization. We use the default inference parameters. Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings. However, when we pre- tag the inputâas is recommended for Englishâ we notice a 0.57% F1 improvement. We use the log-linear tagger of Toutanova et al. (2003), which gives 96.8% accuracy on the test set. 5.2 Discussion. The Berkeley parser gives state-of-the-art performance for all metrics. Our baseline for all sentence lengths is 5.23% F1 higher than the best previous result. The difference is due to more careful S-NOM NP NP NP VP VBG :: b NP restoring NP ADJP NN :: b NP NN NP NP ADJP DTJJ ADJP DTJJ NN :: b NP NP NP ADJP ADJP DTJJ J ..i NN :: b NP NP NP ADJP ADJP DTJJ NN _;ï¿½ NP PRP DTJJ DTJJ J ..i _;ï¿½ PRP J ..i NN _;ï¿½ NP PRP DTJJ NN _;ï¿½ NP PRP DTJJ J ..i role its constructive effective (b) Stanford (c) Berkeley (d) Bik el (a) Reference Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmentation). The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals. Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa (Fassi Fehri, 1993). In the ATB, :: b astaâadah is tagged 48 times as a noun and 9 times as verbal noun. Consequently, all three parsers prefer the nominal reading. Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify. None of the models attach the attributive adjectives correctly. pre-processing. However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009). Moreover, the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1% below the Bikel model, which uses pre-tagged input. In Figure 4 we show an example of variation between the parsing models. We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8. The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models. 6 Joint Segmentation and Parsing. Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words. Since these are distinct syntactic units, they are typically segmented. The ATB segmentation scheme is one of many alternatives. Until now, all evaluations of Arabic parsingâincluding the experiments in the previous sectionâhave assumed gold segmentation. But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline. Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance. Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic. We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton. To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a). Formally, for a lexicon L and segments I â L, O â/ L, each word automaton accepts the language Iâ(O + I)Iâ. Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed. Our evaluation includes both weighted and un- weighted lattices. We weight edges using a unigram language model estimated with Good- Turing smoothing. Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005). MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer. For each 13 Of course, this weighting makes the PCFG an improper distribution. However, in practice, unknown word models also make the distribution improper. Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths â¤ 70 (dev set, gold segmentation). (a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse. We showed in Â§2 that lexical ambiguity explains the underperformance of these categories. (b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ). Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing. (c) Coordination ambiguity is shown in dependency scores by e.g., âSSS R) and âNP NP NP R). âNP NP PP R) and âNP NP ADJP R) are both iDafa attachment. input token, the segmentation is then performed deterministically given the 1-best analysis. Since guess and gold trees may now have different yields, the question of evaluation is complex. Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric. But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal. Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace. Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1. However, MADA is language-specific and relies on manually constructed dictionaries. Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality. Nonetheless, parse quality is much lower in the joint model because a lattice is effectively a long sentence. A cell in the bottom row of the parse chart is required for each potential whitespace boundary. As we have said, parse quality decreases with sentence length. Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew. Table 9: Dev set results for sentences of length â¤ 70. Coverage indicates the fraction of hypotheses in which the character yield exactly matched the reference. Each model was able to produce hypotheses for all input sentences. In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.  By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English. We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors. With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus. Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations. Acknowledgments We thank Steven Bethard, Evan Rosen, and Karen Shiells for material contributions to this work. We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions. The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship. This paper is based on work supported in part by DARPA through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.
 Strategic Lazy Incremental Copy Graph Unification  The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures. One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory. The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation. The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing systems mainly based on I.yped feature structure unification such as natural language analysis and generation sysl~ems.  Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871. These furnmlisms were developed relatively independentIy but actually had common properties; th'~t is, they used data structures called ftmctional structures or feature structures and they were based on unilieathm operation on these data structures. These formalisms were applied in the field of natural language processing and, based on these formalisms, ~:~ystems such as machine translation systems were developed [l<ol;u, e et a l 8gJ. In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation. The efficiency of systems based on ..~uch formalisms, such as natural language analysis and generation systems very much depends on their FS ~lnifieatlon efficiencies. Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO[Pollard and Sag 861 and JPSG[Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs. For example, a spoken Present. affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories. lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan. Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification. Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871. These methods uses rooted directed graphs (DGs) to represent FSs. These methods take two DGs as their inputs and give a unification result DG. Previous research identified DG copying as a significant overhead. Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying). Ile proposed an incremental copy graph unification method to avoid over copying and early copying. itowever, the problem with his method is that a unitication result graph consists only of newly created structures. This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph. Copying sharable parts is called redundant copying. A better method would nfinimize the copying of sharable varts. The redundantly copied parts are relatively large when input graphs have few common feature paths. In natural language processing, such cases are ubiquitous. I"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h'equent, ly. In Kasper's disjunctive feature description unification [Kasper 861, such cases occur very h'equently in unifying definite and disjunct's definite parts. Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency. I)eveloping a method which avoids memory wastage is very important. Pereira's structure sharing FS unification method can avoid this problem. The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721. The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information. 3'he skeleton part is shared by one of the input FSs and the result FS. Therefore, Pereira's method needs relatively few new structures when two input FSs are difference in size and which input is larger are known before unification. However, Pereira's method can create skeleton-enviromnent structures that are deeply embedded, for example, in reeursively constructing large phrase structure fl'om their parts. This causes O(log d) graph node access time overhead in assembling the whole DG from the skeleton and environments where d is the number of nodes in the DG. Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing. This paper proposes an FS unification method that allows structure sharing with constant m'der node access time. This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method. The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short). In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms. The advantages of such a system include: (1)rule writers are not required to describe control infimnation such as eonstraiut application order in a rule, and (12)rule descriptions can be used iu different processing directions, i.e., analysis and general,ion. However, these advantages in describing rules are disadvantages in applying them because of tt~e lack of control information. For example, when constructing a phrase structure from its parts (e.g., a sentence fi'om a subject NP and VP), unueeessary computation can be reduced if the semantic representation is assembled after checking constraints such as grammatical agreements, which can fail. This is impossible in straightforward unification-based formalisms. In contrast, in a procedure-based system which uses IF-TItEN style rules (i.e., consisting of explicit test and structure-building operations), it is possible to construct the semantic representation (TIIEN par'g) after checking the agreement (IF part). Such a system has the advantage of processing efficiency but the disadvantage of lacking multidirectionality. In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system. That is, an FS unification method is proposed that introduces a strategy called the e_arly failure Â£inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures. This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method). These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method). Section 2 explains typed feature structures (TFSs) and unification on them. Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method. The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method. Section 3 and 4 introduce the LING method and the SING method, respectively.  Ordinary FSs used in unification-based grammar formalisms such as PAT].{[Shieher 851 arc classified into two classes, namely, atomic leSs and complex FSs. An atomic FS is represented by an atomic symbol and a complex FS is represented by a set of feature-value pairs. Complex FSs are used to partially describe objects by specifying values for certain features or attributes of described objects. Complex FSs can have complex FSs as their feature values and can share certain values among features. For ordinary FSs, unification is defined by using partial ordering based on subsumption relationships. These properties enable flexible descriptions. An extension allows complex FSs to have type symbols which define a lattice structure on them, for example, as in [Pollard and Sag 8"11. The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol. An example of a type symbol lattice is shown in Fig. 1. An extended complex FS is represented by a type symbol and a set of feature-value pairs. Once complex IeSs are extended as above, an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs. Extended complex FSs are called typed feature structures (TFSs). TFSs are denoted by feature-value pair matrices or rooted directed graphs as shown in Fig. 2. Among such structures, unification c'm be defined IAP,- Kaci 861 by using the following order; ATFS tl is less than or equal to a TFS t2 if and only if: Â the type symbol of tl is less than or equal to the type syn'bol of/2; and Â each of the features of t2 exists in t1 and. has as its value a TFS which is not less than its counterpart in tl ; and each of the coreference relationships in t2 is also held in tl. Top Sign Syn Head List POS /77 Lexical Phrase Sign NonEmpty Empty V N P ADV Slgn Li. Lis~ ust I I I I NonEmpty Emply I I i I Sign Sign I I/ / List List 5/ /5 .... U_ Bottom Figure 1: Exainple of a type symbol lattice --2-- peSymbÂ°10 eaturel TypeSymboll ] ]] I feature2 TypeSymbol2 I feature3 ?Tag T ypeSymbol3 ] ]feature4 TypeSymbol4 L [.feature5 TypeSymbol5 TIeature3 7Tag (a) feature-value matrix notation "?" i~ the prefix for a tag and TFSs with the same tag are token-identical. TypeSym bol/~ feo~.,o/ I TypeSymboll ~ [. TypeSymbol2 4Â¢" '~Â°~'~/.~ypeSymbol3 featury "X~ature5 TypeSymbol4 4r "~TypeSymbol5 (b) directed graph notation Figure 2: TFS notations Phrase [sub(at ?X2 SignList ] dtrs CHconst Sign U Syn i'oo I syn I head ?Xl . ] ubcat NonEmptySignLIst | ['first ]1 ?Ã3 Lrest ?X2 J j Phrase -dtrs CHconst hdtr LexicalSignsyn Syn -head Head pos P orm Ga subcat NonEmptySignList Sign ,11 yn Synead Head L~,os N] Irest EmptySignkist Phrase "syn Syn head ?X1 Head Fpos P Lform Ga ] Lsubcat ?X2 Empl.ySignList dtrs CHconst ccltr ?X3 Sign syn iyn head Head _ [pos N hdtr LexicalSign l-syn Syn l I F head :x~ 7/ Lsubcat [ NonEinptySignList l l P"" ~Ã~ llll Lrest ?X2 JJjJ Figure 3: Example of TFS unification Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet. A unification example is shown in Fig. 3. In tile directed graph notation, TFS unification corresponds to graph mergi ng. TFSs are very convenient for describing linguistic information in unlfication-based formalisms.  In TFS unification based on Wrobtewski's method, a DG is represented by tile NODE and ARC structures corresponding to a TFS and a feature-value pair respectively, as shown in Fig. 4. The NODE structure has the slots TYPESYMBOL to represent a type symbol, ARCS to represent a set of feature-value pairs, GENERATION to specify the unification process in which the structure has been created, FORWARD, and COPY. When a NODE's GENERATION value is equal to the global value specifying the current unit]cation process, the structure has been created in the current process or that the structure is currel~l. The characteristics which allow nondestructive incremental copy are the NODE's two different slots, FORWARD and COPY, for representing forwarding relationships. A FORWARD slot value represents an eternal relationship while a COPY slot value represents a temporary relationship. When a NODE node1 has a NODE node2 as its FORWARD value, the other contents of tile node1 are ignored and tim contents of node2 are used. t{owever, when a NODE has another NODE as its COPY value, the contents of the COPY value are used only when the COPY value is cub:rent. After the process finishes, all COPY slot values are ignored and thus original structures are not destroyed. The unification procedure based on this method takes as its input two nodes which are roots of the DGs to be unified. The procedure incrementally copies nodes and ares on the subgraphs of each input 1)G until a node with an empty ARCS value is found. The procedure first dereferences both root nodes of the input DGs (i.e., it follows up FORWARD and COPY slot values). If the dereferenee result nodes arc identical, the procedure finishes and returns one of the dereference result nodes. Next, the procedure calculates the meet of their type symbol. If the meet is Bottom, which means inconsistency, the procedure finishes and returns Bottom. Otherwise, the procedure obtains the output node with the meet as its TYPESYMBOL. The output node has been created only when neither input node is current; or otherwise the output node is an existing current node. Next, the procedure treats arcs. The procedure assumes the existence of two procedures, namely, SharedArcs and ComplementArcs. The SharedArcs procedure takes two lists of arcs as its arguments and gives two lists of arcs each of which contains arcs whose labels exists in both lists with the same arc label order. The ComplementArcs procedure takes two lists of arcs as NODE TYPESYMBOL: <symbol> [ ARCS: <a list of ARC structures > FORWARD: "<aNODEstructure orNIL> / COPY: < a NODEstructure or Nil, > GENERATION: <an integer> ARC LABEL: <symbol> VALUE: <:a NODEstructure> Figure 4: Data Structures for Wroblewski's method Input graph GI Input graph 62 Â¢ .......'77 ........ i : Sobg,'aphs not required to be copied L ........................................... Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted. its arguments and gives one list of arcs whose labels are unique to one input list. The unification procedure first treats arc pairs obtained by SharedArcs. The procedure applies itself ,'ecursively to each such arc pair values and adds to the output node every arc with the same label as its label and the unification result of their values unless the tmification result is Bottom. Next, the procedure treats arcs obtained by ComplementArcs. Each arc value is copied and an arc with the same label and the copied value is added to the output node. For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig. 5. The unification procedure is applied recursively to feature a values of the input nodes. The node specified by the feature path <a> fi'om input graph G1 (Gl/<a>) has an arc with the label c and the corresponding node of input graph G2 does not. The whole subgraph rooted by 6 l/<a c> is then copied. This is because such subgraphs can be modified later. For example, the node Y(G3/<o c g>) will be modified to be the unification result of G 1/<a c g> (or G1/<b d>) and G2/<b d> when the feature path <b d> will be treated. Incremental Copy Graph Unification PROCEDURE Unify(node1, node2) node1 = Dereference(nodel). node2 = Dereferencelnode2). IF Eq?(nodel, node2) THEN Return(node1). ELSE meet = Meet(nodel.typesymbol, node2.typesymbol) IF Equal?(meet, Bottom) THEN Return(Bottom). ELSE outnode = GetOutNode(nodel, node2, meet). (sharedst, shareds2) = SharedArcs(nodel.arcs, node2.arcs). complements1 = ComplementArcs(node|.arcs, node2.arcs). complements2 = ComplementArcs(node2.arcs, nodel.arcs). FOR ALL (sharedt, shared2) IN (sharedsl, shareds2) DO arcnode = Unify(sharedl.value, shared2.value). IF Equal?(arcnode, Bottom) ]HEN Return(Bottom). ELSE AddArc(outnode, sharedl.label, arcnode). ENDIF IF Eq?(outnode, node1) THEN coi'nplements = complement2. ELSE IF Eq?(outnode, node2) THEN complements = complementL ELSE complements = Append(complements1, complements2]. ENDIF FORALL complement IN complements DO newnode = CopyNode(complement.value). AddArc(outnode, complement.label, newnode). Return(outnode). ENDIF ENDIE ENDPROCEDURE Figure 6: Incremental copy graph unification procedure The problem with Wroblewski's method is that tile whole result DG is created by using only newly created structures. In the example in Fig. 5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels. This order is related to the unification failure tendency. Unification fails in treating arcs with common labels more often than in treating arcs with unique labels. Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation. This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels. In Section 5, a method which uses this generalized strategy is proposed.  In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig. 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used. With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig. 5 due to a change of node Y G3/<a c g>). To achieve this, I, he LING unification method, which uses copy dependency information, was developed. The LING unification procedure uses a revised CopyNode procedure which does not copy structures immediately. The revised procedure uses a newly introduced slot COPY-DEPENDENCY. The slot has pairs consisting of nodes and arcs as its value. The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig. 7): (1) if nodel ', the dereference result of node/, is current, then CopyNode returns node l" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1" and if it returns ,~;everal arc copies, CopyNode creates a new copy node. It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1" and returns Nil_. ,',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results. When a new copy of a node is needed later, the LING unification procedure will actually copy structures using the COPY-DEPENDENCY slot value of the node (in GetOutNode procedure in lJ'ig. 6). It substitutes arcs with newly copied nodes for existing arcs. That is, antecedent nodes in the COPY-DEPENDENCY values are also copied. In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity. ]lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously. The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test. Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method. The efficiency of the LING unification method depends on the proportion of newly created structures in the unification result structures. Two worst eases can be considered: (t) If there are no arcs whose labels are unique to an input node witlh respect to each other, the procedure in LING unification method behaves in the same way as the procedure in the Wroblewski's method. (2) In the worst eases, in which there are unique label arcs but all result structures are newly created, the method CopyNode PROCEDURE CopyNode(node, arc, ancestor) node = Dereference(node). IF Current?(node) THEN Return(node). ELSE IF NotEmpty?(newarcs = CopyArcs(node)) THEN newnode = Create(node.typesymbol). node.copy = newnode. FOR ALL arc IN node.arcs DO IF NotNIL?(newarc = FindArc(arc.label, newarcs)) THEN AddArc(newnode, newarc.label, newarc.value}. ELSE AddArc(newnode, arc.label, arc.value). ENDIF Returo(newnode). ELSE node.copy-dependency = node.copy-dependency U {Cons(ancestor, arc)}. Return(Nil_). ENDIF ENDPROCEDURE CopyArcs PROCEDURE AlcsCopied(node) newarcs = O- FOR ALL arc IN node.arcs DO newnode = CopyNode(arc.value, arc, node). IF NotNIL?(newnode) THEN newarc = CreateArc(arc.label, newnode). newarcs = {newarc} U newarcs. ENDIF Return(newarcs). ENDPROCEDURE Figure 7: The revised CopyNode procedure has the disadvantage of treating copy dependency information. However, these two cases are very rare. Usually, the number of features in two input structures is relatively small and the sizes of the two input structures are often very different. For example, in Kasper's disjunctive feature description unification, a definite part ["S is larger than a disjunet definite part t"S.  Method In a system where FS unification is applied, there are features whose values fail relatively often in unification with other values and there are features whose values do not fail so often. For example, in Japanese sentence analysis, unification of features for conjugation forms, case markers, and semantic selectional restrictions tends to fail but unification of features for semantic representations does not fail. In such cases, application of the EFF strategy, that is, treating features tending to fall in unification first, reduces unnecessary computation when the unification finally fails. For example, when unification of features for case markers does fail, treating these features first avoids treating features for senmntic representations. The SING unification method uses this failure tendency infornmtion. These unification failure tendencies depend on systems such as analysis systems or generation systems. Unlike the analysis case, unification of features for semantic representations tends to fail. in this method, theretbre, the failure tendency information is acquired by a learning process. That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process. in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction. As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag. This is because the type symbol of a 'rFS represents salient information on the whole TFS. By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail. This is achieved by the sorting procedure of common label arc pairs attached to the meet type symbol. The arc pairs obtained by the SharedArcs procedure are sorted before treating arcs. The efficiency of the SING unification method depends on the following factors: (1) The overall FS unification failure rate of the process: in extreme cases, if Go unification failure occurs, the method has no advantages except the overhead of feature unification order sorting. However, such cases do not occur in practice. (2) Number of features FSs have: if each FS has only a small number of features, the efficiency gain from the SING unification method is small. (3) Unevenness of FS unification failure tendency: in extreme cases, if every feature has the same unification failure tendency, this method has no advantage. However, such cases do not occur or are very rare, and for example, in many cases of natural language analysis, FS unification failures occur in treating only limited kinds of features related to grammatical agreement such as number and/or person agreement and semantic selectional constraints. In such cases, the SING unification method obtains efl]ciency gains. The above factors can be examined by inspecting failure tendency information, from which the efficiency gain from the SING method can be predicted. Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not.  The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method. The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method. Structure sharing avoids memory wastage'. Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them. This reduces repeated calculation of substructures. The SING unification method introduces the concept of feature unification strategy. 'the method treats features tending to fail in unification first. Thus, the efficiency gain fi'om this method is high when the overall FS unification failure rate of the application process is high. The combined method Inakes each FS unification efficient and also reduces garbage collection and page swapping occurrences by avoiding memory wastage, thus increasing the total efficiency of li'S unification-based natural language processing systems such aa analysis and generation systems based on IlI'SG.
 THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES  in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch. !['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation.  Unification lbrmMisms ma.y be either un-typed (DCC~s, PATRII, 1,F(;) or typed (npsG). A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;l's(:: [5] in order to rule out nonexista.nt tyl)es of objects. For example, there a.re no verbs which have the [km.ture +R. The simplest way to express such restrictions is by mea.ns of a.n a.ppropria.teness pa.r-tim flmction Approp: Type × Feat ~ Type. With such a.n a.pl)rol)riatleness specifica.- tion lrla.tly Sllch restrictioi,s may be expressed, though no restrictions involving reentrancies ma.y be expressed. In this pal)er, we will first in §2 survey the range of type eonstra.ints tha.t ma.y be expressed with just a. type hiera.rchy and *']'he resea.rch pl'eS(!lllL('d ill |,his; paper was pay tia.lly sponsored hy '[kfilprojekt B4 "(;onsl.rahH.s on Grammar fl~r Efficient Ck:neration" of the Soi,der forschungsbereich 340 of the Deutsche ["orschungsgemeinscha, ft. "VVe would also like to thank 'l'hilo GStz for helph,l comments ou thc ideas present.ed here. All mistakes a.rc of collrsc our OWll. IKI. Wilhehnstr. 113, |)-721174Tfilfi,,ge,, (ler- ma.ny, {rig,King} g'~sfs.n phil.uni-I uebingen.de. a.n N)propria.teness specification. Then in ~3, we discuss how such type cons|fronts linty be mainta.ined under unification as exemplilied in the na.tura.1 language D~rs- ing/generation system '.l'ro]l [7]. 1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities. The a.lnount of dis- junction is a.lso kept small by the technique of unlilli,g described in [9]. This strategy a.ctua.lly ma.inta.ins apl)ropri~tteness conditions in some ca.ses in which a. type in-ferencing stra.tegy would fa.il, l)'inMly, in §4, we discuss the possibilities for genera lizillg this a.pl)roa.ch to ha.ndle a bro~Mer r~tnge of constra.ints, including constraints inw)lving reentran cies.  As discussed iu Gerdemann ,~ King [8], one ca.n view a.pl}rol)ria.teness CO[lditions as (lelining GPSG style fea,1;tl re cooccurence restrict:ions (FCRs). In [8], we divided FCRs into co,j,,ctive and di.q,,~ctive ct~sses. A conjunctive FCI/. is a constra.int of the following fornl : i[' a.n object is of ;~ cert;fin kind then ill deserves certa.in fea.tures with wdues of cert~till kinds An FCI~ stat:ing tha,2: a. verb must h~we v and N t'eatures with values A- and -respectively is a.ll example of a. conjunctive FCI{. A disjunctive I"CI{. is of the form: l rl'he "]'roll ,qysl.em was implemented in Quintus Prolog by Dale (lerdemann and '['hilo (]Stz. if an object is of a. cel'taiu kiud then it deserves cerl;a.in [ca,1;tll'C~s with vMues of certa.hi kinds, or it deserves cerl.ahi (pei'ha.liS other) fea.1;u res \vil, h viiiues of terra.in (perlla.ps other) kinds, or ... (31:it i:leserw.;s i:erl;a.in (lmrhal)S other) fea,1;llres wil.h Vi, l.[ll(~S o[ certain (perha.ps other) khi,<ls lo::I exa~]nple, the following F(',|/. sl.a.t.iug tha,t inverCed verbs lilt|S1, lie a.uxili;tries is disjunctive: a verb Ilitisl; ha.re the ['(~il.l.tll'(~s INV and AUX with va.l/ies d a.Iid I, -a.iitl i L-, or -;Mid -respectivel.y. Both o| these |el'illS or l,'(',lls iiHly I)(! expressed in a. foi'llla.iiSlli euiployhi<~ fiiiil.e lia,rtia.[ order (Type, E) o| types tllldel' sub- 8illnptioli> a, finite sel. Feat of ro;./.t;tll.(~s, and an a.pprol)ria.teness parl, ial rliilcl.ion Approp:Type X Feat -~ Type. [uluitively; the l, ypes fornla.lize I;lie notion ol" kinds +,j" objecl, t g: t,' ill' ca.oh oil|eel, of tyl>e t' i~<i Mso of l;Ylle L, il, ll(] Approp(l, f) = lI ill' (!;i('[I object oF type t deserves [eaA.urt~ f wil.]i :i. Vi./.]lle or type ft. ~@'e call S/IC]I it. [Ol'tll;liiSlll i-i, ii ;I,])l)l"Opl']al, olio,~/ fOl'lllil]i~;lll. (',iLl'- peliLel",s AI,F, and (,erdeliia. i ;ill(| (i(~t,z's Troll are ex:-t.niples o| illilllenienl.a.Lions o| a,pF, ro]) ria, Loliess |or illa.[iSlil,s. l low an a.i)ln'oprhi.teness [orniaJisnl enco<les a conjunctive I:(',R is ob\.'i<>us~ bll(. llOW it encodes a disjuiictive I"(',1{ is less so. Ali exa.niple i|]usl;ral;es best how it. is done. ~Ul)pOS0 that F( ',1{ [i sl.al.es l.hal, ob- .iecls (if type t deserw! [(!a.[./ll'(!S f 'and .q, I)oth with boolea.I/ wdues a.ll(I ['lll'l,[lel'lllOF(~ that the va.hies of f aild g iil/lSl al~r(!e, [> is the disjunct]w! I"(111. if a,u object is o[ type l then it deserw:s f with va.lue -I- and q with wdue +, or it deserw.~s f with va.lue a.nd 9 with value - To 0ncode [3> first iul,rodLiCe sul/l.yltes , t ~ ;+l.ll([ l" of I (1 E I/, 1.##), O11(! SUl)tyl)e ['()l' ea,ch disjuuct iu the cousequenl, of'p. Then encode the ]'ea.tli['e/wthl~.~ <'on(!il.illliS in l, he [irst disjunct ILy putthlg Approp(t', ./) :: ~-a,nd Approp(//~ q) -+, and encode the I'eature/value conditions in the second dis-juu(:t by putting Approp(t',f) = -. and Approp(t',g) = . .'2 This a pproa,ch Ina, kes two inll)ort;a, lll, closed-world type assumptious a, bouL (.he types tli~d; Slll)SlllIle 11o ogher types (hellCe- forth species), l:irst, the p;i.rtition conditiOII states tha.t for each type t, if a.n object is (31' type t theu the object is of ex-ax-I.ly o11(2 species subsulned by t. Second, the all-or-nothing cclndition sta, tes that 1'(31' each species ,q a.itd fea.ture f, either every el" IIO ol>,iecl, or species s deserves feature .#c.3 All a.l)ltroltriM,eliess [orli+ia.lisill sllc]l a.s ALl:, ([2], [3])ti,;t.l. does not uieet both c.ouditions llla.y llOt; ]lroper[y el|cOde a, disjull('- five l"(:l/. For exalnple, consider disjunctive I"CI{. p. An a.I)prl;)pria.l, elleSS [ornia.l--iSlli I/lily l/O( properly encode 1,hi~t t / a.lld t" i'el)rt,selil, MI a.lid oilly the disjuncl, s ill the COll.qeqll(Hlt or [i wiLhout the i)a.rl,ition COll-d]tion. <till a.llln'ol)riill.eness [orlila.liSlll llia,y IIOl. llrOl)erly encode the [t~ii.l.llle/vii.hle (:(lll-<liiriOii: deinanded liy em'h disjuncl, hi the COli.~t!qllelil. o| p wilhoul, the a.i[-Ol'-liot;hilig c(m(til.ion. As indicat.ed a.bove, AI, I.; is iLIi exa.tlli)le o| it. f(n'liialiSlU I.ha.l, does it(it ineel; llol;h o| 1.hese closed world aS,glllnlil,iOli.g. In AI+E :-/. ['eli.l.tlr(~ st.i'llCtlile i.<4 won typed ifl' for ea.ch arc iit the te:+d.ure sI.l'tlCl;tlr0, if' 1,he SOtll'('(~ node is labelled wil.h type /., the targel; node is lallelled with 1;ype l / a.lld the il.i'c is IMlelled with [ea.tlll'(~ f 1,lien Approp(/.> .f) [ l/. Furl.her|note> a ['eal, urt~ strut(tire is >l'lds exanll)h: I:(JR is, for eXlmsil.ory l)nrl)oses, quilt simph'. "l'hc prolileni o[ c.xpr('.sshig F(Jl/'s, however, is a l'Cal Iiuguisl.ic i)rol)lcin. As noted I)y Copcstakc. ct al. [4], it. was inipossihlc I.o c.xpress CV('II Ihc .~ilii[)]oM. forilis o[ l"(JRs in l.hc.ii7 c×tciidcd VCISiOII (it' AI.E. '['hc basic principle of expressing l"Clls also ex lends Io I"(',[(s iuvolviug longer palhs. For example, to (:llSllt't: thai. for the type l, I.he path (fg) lakes a vahie subsuuied I)y .% one nlust tirst hll, ro ducc the chaiu Approp(/, f) = .,, Approp('a, g) = .~. Silch ilil.crlllCdialc I.'~'l)lts COllid ll(! hll.rodllced a.<-; part o[ a (onilli[al.iou sl.age.. 4 Nob: I.hal. Ihesc cl,>s<,d world assulnplions art' explicitly made in Pollard ,t,. Sag (rorthcoming) [14].. well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed. Unfortunately, well-typability is not sufficient to ensure that disjunctive FCRs are satisfied. Consider, For exam- pie, our encoding of the disjunctive FCR p and suppose that 99 is the fe, ature structure t[f : +,9 : -]. 90 is well-typed, and hence trivially well-typable. Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed. By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure. We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively. Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'. Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0. T$ is satisfial~le if[' 7~(F) 7 ~ (7). C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl" the feature structure is satisfiable. The Troll system, which is based on this idea, effectively inqflements type resolution. Why does type resohttion succeed where. type inferencing fails? Consider again the encoding of p and the feature structure 9~. Loosely speaking, the appropriateness sl)eeifieations for type t encode the part of p that sta, tes that an object of tyl)e t deserves features f and g, both with boolean vahles. However, the appropriateness specifications for the speci- ate sul)types t' and t" of type t encode the part of p that states that these val-lies lnust agree. Well-typability only considers species if forced to. In the case of ~, well-typability can be estahlished by consklering type t alone, without the l)artition condition forcing one to find a well-typed species subsumed hy t. Consequently, well-tyl)ahility overlooks the part offl exehisively encoded by the ai)propriateness specifications for t' and t". Type resolution, on the other hand, always considers species. Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'.  APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability. It would, of course, not be very efficient to work with such large disjunctions of feature structures. 4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t" a.nd 1'" 6 fS then "R ( F) tJ 1"(1"') = "R ( F tO F'). Uni/ication of sets of fca.ture structures is defined here ill the standard way: S t2 ,S" = {1"[ I"' 6 S and l"" G S" and 1" = 1"' H 1""}. (!rty a.llows a. disjultctivo fesolv(,d featur(, structti re to I)e r(;l)rosetd,(~d more et[icieutly a,s ~t sitlgle untyl)(~d l'eatur(' st.l'll(:l.llfe plus a, sel; of d(;pondlmt node la.h(~liugs, which ca.n be further (;oml)a,(:t(~d using mi, Nie(l dis. junction a.s in (',(~rdemann [(i], I)i'~['re t(: Fo]' exanH)le , SUl)l)OS(~ \v(~ I,,yl)(~ r(~solvc the [ea, l, urc st, ructure t[,f ; bool,fl; bool] using our encoding of p. ()he can (rosily see tha.t this fea.tur(~ strut:fur(, has only two I'e solwl, nts, which ca, n I)e colla.ps(~d iuto one fea,1;ure strlll:ttlro with llallV2d d]sjunci.ion a,s shown below: f:k , : :> f: (I t -) II'll;1} ["'"' ] 0:t-LU: J ,u: (I t ) We now ha,vo a, [;(mSolml)ly COml)a(:l l'q)-resentaJ;ion hi which t.ho l"(il{, ha.s lie(Hi tl';tllsl;I,t(~([ iul,o a. Ila, ill(!(I ([iS.]llll(:l.ioli. Ih,w O,V(H'> (Hie should note tha, t fills dis.iun(: l;ion is only l)l'eSeUl; b(~(:aats(~ the ['oaJ, tli'O~i .f a,]l(l g ha>l)l)en 1:o I)o Fir(~s(HIt. Tilt!S(! I(,a tures would .eed l;o Im l)res(mt il w(~ wtwe enforchl<ej (Jaxpcnl,(H"s [:7] lcil, al w(ql i.yl)iug r(xluiroti]oilt ~ whhth ,qa,y's 1.1ial [(!al:ilr('s I. lial a,l:e a.llowed ilillSt 1)o pres,.ml., lllil. Iol.a[ well I.yping is, hi fax:t> incoinl)a.lib]e ;villi lype resolul, ioli~ since I;hore lilil$' w(ql I)o all inli llit;(~ seL of tota, lly w(,ll iyl)od I'esolvalil.s of ;1 l'(;a, Lllr(J st]'llcttir('~, For (~xa.llipi(~, a.ll illi(lei'.- Sl)ocifiod list stl'u('tlir(' couhl be iT(~S()/v0(I 1.o ;~ list of length (L a. list of h:ngl.h 1, el.c, ,qhlce I, ota.I well I,yliin g is liOt i'(!quir(!([, we lm~y i~s well a.ctiwqy un[il[ r0(lulid;lnt ['0a, tlires, 5 ill this (!Xalli[)l(!> i[ t, li(' f ail(l (7 fo.a, tllrOS ;~l'e reliiovod, we a,lO lell wil, h lh(, simple, disjunction {if,/'~}: which is (!quiv- a,lent, to l;]le or(lillaJ'y l,Yl)(' l.(; Thus, iu lliis ca, so> ]lO (lisjtulcl, ion a.t all ix rc!(llliro(l 10 (!11" force the I"CIL All th',tt is requirc(I is tim ~qntuil, ively, [eat, ui'cs arc rodundaui it Ilwir val llCS art'. eul,h'cl 5 predictaldc fl'oui ihc approluiaic .ross Sl>eCificatim,..%'c GStz [1)], (',cr,lemam, [7] k,r ;I. IIlOl;('. [HXX:iHCforUllllalioii. °[n this casc, il. would also have b(:ml l)~>,~iblc to unlill Lhc oi'i<eiuai teal, life Sll'tl<ltllc I,.I.ie I*' solviug. /Snforl, unai,e, ly, llmvcvcr, this i~; l.>i ;ihvay~. the (:asc, as C;lll |)(! S(!t'II in the [ollowiug (!Xalll])lC: t{j: +] :> {C/: +]} ~ ~'. asSUml)tion tha.t t will only be ext(mded I)y unil'ying il with a.lmther (t;Oml)a.ct(~d) m(mll)(!r o[' "l)']?.Jr,_c,. This, h.w(wer, wa.s a. simple ca.se iu which a.I1 of the named dis.jun(:tion could ho removed. It would not lmve I)('en i)os sihle to relnov(' tim fea.tur('s f ~tll(I g if thest~ 17,atu['es had I)oen involved iu re(m-tranci(+s of i[' tlt(,se lim.tures ha.d ha.d t:om- i)h+x va.lu('s, lu gt+tlera.I, howover, our eXl)e- ri(!ll(:(~ ha,s I)(~(ql that, eV(;l! wil, li very (:()tit pl('x type hi(~ra, rchi(~s a.nd |'(m, tur(; SLI'UC-l, lll'eS [()1" liPS(i, very i'ow named (lisjunc-lions a, re introdu('e(l. 7 q'hus~ uuilica.1;ion is e;(merally uo more (~xp(msive tha.n unifica.- li,:)H with unlylmd l(mt.ur(~ sl.fu(:l.ur('s.  \% havc~ sh,:Y, vu in this i~al),:~r tha.t the kind of consl raints ,:~Xl)r.t~ssihlo Ity api)Vol)rh~,l;or.~ss c~mdit.ions call he imlflemc'.nted iN a i.'actical .,.D, sle]n e,ul)loyinK typ,M featu r,:'~ st.ru(:t.uf(,s and utdlica.Lion a.s I.he I:,ritna.ry Ol)(U'a,t, ic:,n on t(>;t,l, ur<+ ,'-;t, ruct, ure~. Ilut what. Of IIlOl'(' COIII[)I(~N l;yp(~ CC'IIH|,F.~LilI|,,q it~v'.)l',.' h~y; r(~enl;ram:ies': [ntro(IL~ciug reeJH.ra.ncies illl. ,::<rest ralid.s allows E.' the F,O~sihillty of d(~liNiu/,, recursivc l.yl),:~s ~ such a.s the (leli nitkm of append in [I]. (;lea['ly the re ~olv;-~nl.~, o[ such a. recursiv(~ l.yl)(', could Not I)(~ l,reCOmlfiled a.s r,.~quiI'oxl in Troll. Oue might, uew'rtholoss, considm' a l- [OWil]l[ f('(Hl(, f a, ll('y- ('OIls t f a hI| S oll llollrecursiv(qy defiltcd l.ypcs. A ])ro/)leul still arises; nantcly, il lhe l'eSo[va.itts of a Frail, till't1 .qll'tlCI411"(~ ill(:ludcd sonic with a pa.rticu lar r(~onll'all(:y a.nd s()Tn(~ \viLh(',ul, then the (:,.)mliti()ll iliad, a.II resc)lva.uts ha.v(~ th,:~ same shal)(~ would m)lon~e[' hold. ()ue v.,ottkl l.her(q'or,.~ no(~(l i.o eml)loy a moue COml)l(,x vorsion .r ,a.med (lis.it, f,t:tio, (Ill], [12], tit)I). It. ig (i,.L(~sti,.malfl(~ wh('thef such a.d ditional (:()mpl(~xit.y would I)e justified to 'Our CXl)ericl~(:c is derived l,'imarily flora test-i.I" Ihc 'l'loll system (m a tat, her lar<e,e e, ramul;G for (',(!l>lll;lll imfiial vcrh I>lHases, which was wiit-t('n I)y I'hhard Ilillrichs a.d Tsum:ko Na, kazawa aud iinl)lclncut,cd by I)clmar McuH_:J's. handle this limited class of reentrancy- constraints. It seems then, that the class of constraints that can be expressed by appropriateness conditions corresponds closely to the class of constraints that can be efficiently preeompiled. We take this as a justification for appropriateness formalisms in general. It makes sense to ~d)straet out the efficiently processable constraints and then allow another mechalfiSm, such as attachments of definite clauses, to express more complex constraints.
 Text Segmentation Using Reiteration and Collocation  A method is presented for segmenting text into subtopic areas. The proportion of related pairwise words is calculated between adjacent windows of text to determine their lexical similarity. The lexical cohesion relations of reiteration and collocation are used to identify related words. These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights. This method is shown to successfully detect known subject changes in text and corresponds well to the segmentations placed by test subjects.  Many examples of heterogeneous data can be found in daily life. The Wall Street Journal archives, for example, consist of a series of articles about different subject areas. Segmenting such data into distinct topics is useful for information retrieval, where only those segments relevant to a user's query can be retrieved. Text segmentation could also be used as a pre-processing step in automatic summarisation. Each segment could be summarised individually and then combined to provide an abstract for a document. Previous work on text segmentation has used term matching to identify clusters of related text. Salton and Buckley (1992) and later, Hearst (1994) extracted related text pmtions by matching high frequency terms. Yaari ( 1997) segmented text into a hierarchical structure, identifying sub-segments of larger segments. Ponte and Croft ( 1997) used word co-occurrences to expand the number of terms for matching. Reynar ( 1994) compared all Lindsay J. Evett Department of Computing Nottingham Trent University Nottingham NGI 4BU, UK lje@doc.ntu.ac.uk words across a text rather than the more usual nearest neighbours. A problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (Salton et al., 1994). Another approach to text segmentation is the detection of semantically related words. Hearst (1993) incorporated semantic information derived from WordNet but in later work reported that this information actually degraded word repetition results (Hearst, 1994). Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. Another approach extracted semantic information from Roget's Thesaurus (RT). Lexical cohesion relations (Halliday and Hasan, 1976) between words were identified in RT and used to construct lexical chains of related words in five texts (Morris and Hirst, 1991 ). It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. However, RT does not capture all types of lexical cohesion relations. In previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus. Furthermore, this process was not automated and relied on subjective decision making. Following Morris and Hirst's work, a segmentation algorithm was developed based on identifying lexical cohesion relations across a text. The proposed algorithm is fully automated, and a quantitative measure of the association between words is calculated. This algorithm utilises linguistic features additional to those captured in the thesaurus to identify the other types of lexical cohesion relations that can exist in text. 1 Background Theory: Lexical Cohesion. Cohesion concerns how words in a text are related. The major work on cohesion in English was conducted by Halliday and Hasan (1976). An instance of cohesion between a pair of elements is referred to as a tie. Ties can be anaphoric or cataphoric, and located at both the sentential and suprasentential level. Halliday and Hasan classified cohesion under two types: grammatical and lexical. Grammatical cohesion is expressed through the grammatical relations in text such as ellipsis and conjunction. Lexical cohesion is expressed through the vocabulary used in text and the semantic relations between those words. Identifying semantic relations in a text can be a useful indicator of its conceptual structure. Lexical cohesion is divided into three classes: general noun, reiteration and collocation. General noun's cohesive function is both grammatical and lexical, although Halliday and Hasan's analysis showed that this class plays a minor cohesive role. Consequently, it was not further considered. Reiteration is subdivided into four cohesive effects: word repetition (e.g. ascent and ascent), synonym (e.g. ascent and climb) which includes near-synonym and hyponym, superordinate (e.g. ascent and task) and general word (e.g. ascent and thing). The effect of general word is difficult to automatically identify because no common referent exists between the general word and the word to which it refers. A collocation is a predisposed combination of words, typically pairwise words, that tend to regularly co-occur (e.g. orange and peel). All semantic relations not classified under the class of reiteration are attributed to the class of collocation.  To automatically detect lexical cohesion tics between pairwise words, three linguistic features were considered: word repetition, collocation and relation weights. The first two methods represent lexical cohesion relations. Word repetition is a component of the lexical cohesion class of reiteration, and collocation is a lexical cohesion class in its entirety. The remaining types of lexical cohesion considered, include synonym and superordinate (the cohesive effect of general word was not included). These types can be identified using relation weights (Jobbins and Evett, 1998). Word repetition: Word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem. An inflected word was reduced to its stem by lookÂ­ up in a lexicon (Keenan and Evett, 1989) comprising inflection and stem word pair records (e.g. "orange oranges"). Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon. Collocations were automatically located in a text by looking up pairwise words in this lexicon. Figure 1 shows the record for the headword orange followed by its collocates. For example, the pairwise words orange and peel form a collocation. orange free green lemon peel red state yellow Figure 1. Excerpt from the collocation lexicon. Relation Weights: Relation weights quantify the amount of semantic relation between words based on the lexical organisation of RT (Jobbins and Evett, 1995). A thesaurus is a collection of synonym groups, indicating that synonym relations are captured, and the hierarchical structure of RT implies that superordinate relations are also captured. An alphabetically-ordered index of RT was generated, referred to as the Thesaurus Lexicon (TLex). Relation weights for pairwise words are calculated based on the satisfaction of one or more of four possible connections in TLex.  The proposed segmentation algorithm compares adjacent windows of sentences and determines their lexical similarity. A window size of three sentences was found to produce the best results. Multiple sentences were compared because calculating lexical similarity between words is too fine (Rotondo, 1984) and between individual sentences is unreliable (Salton and Buckley, 1991). Lexical similarity is calculated for each window comparison based on the proportion of related words, and is given as a normalised score. Word repetitions are identified between identical words and words derived from the same stem. troughs placed subject change linguistic feature points located average std. dev. (out of 42 poss.) word repetition 7.1 3.16 41 collocation (97.6%) word repetition 7.3 5.22 41 relation weights (97.6%) 41 Collocations are located by looking up word pairs in the collocation lexicon. Relation weights are word repetition 8.5 3.62 (97.6%) calculated between pairwise words according to their location in RT. The lexical similarity score indicates the amount of lexical cohesion demonstrated by two windows. Scores plotted on a graph show a series of peaks (high scores) and troughs (low scores). Low scores indicate a weak collocation 5.8 3.70 40 relation weights (95.2%) word repetition 40 collocation 6.4 4.72 (95.2%) relation weights 39 level of cohesion. Hence, a trough signals a potential subject change and texts can be relation weights 7 4.23 (92.9%) segmented at these points.  An investigation was conducted to determine whether the segmentation algorithm could reliably locate subject change in text. Method: Seven topical articles of between 250 to 450 words in length were extracted from the World Wide Web. A total of 42 texts for test data were generated by concatenating pairs of these articles. Hence, each generated text consisted of two articles. The transition from the first article to the second represented a known subject change point. Previous work has identified the breaks between concatenated texts to evaluate the performance of text segmentation algorithms (Reynar, 1994; Stairmand, 1997). For each text, the troughs placed by the segmentation algorithm were compared to the location of the known subject change point in that text. An error margin of one sentence either side of this point, determined by empirical analysis, was allowed. Results: Table I gives the results for the comparison of the troughs placed by the segmentation algorithm to the known subject change points. collocation 6.3 3.83 35 (83.3%) Table 1. Comparison of segmentation algorithm using different linguistic features. Discussion: The segmentation algorithm using the linguistic features word repetition and collocation in combination achieved the best result. A total of 41 out of a possible 42 known subject change points were identified from the least number of troughs placed per text (7.I). For the text where the known subject change point went undetected, a total of three troughs were placed at sentences 6, 11 and 18. The subject change point occurred at sentence 13, just two sentences after a predicted subject change at sentence 11. In this investigation, word repetition alone achieved better results than using either collocation or relation weights individually. The combination of word repetition with another linguistic feature improved on its individual result, where less troughs were placed per text.  The objective of the current investigation was to determine whether all troughs coincide with a subject change. The troughs placed by the algorithm were compared to the segmentations identified by test subjects for the same texts. Method: Twenty texts were randomly selected for test data each consisting of approximately 500 words. These texts were presented to seven test subjects who were instructed to identify the sentences at which a new subject area commenced. No restriction was placed on the number of subject changes that could be identified. Segmentation points, indicating a change of subject, were determined by the agreement of three or more test subjects (Litman ami Passonneau, 1996). Adjacent segmentation points were treated as one point because it is likely that they refer to the same subject change. The troughs placed by the segmentation algorithm were compared to the segmentation points identified by the test subjects. In Experiment 1, the top five approaches investigated identified at least 40 out of 42 known subject change points. Due to that success, these five approaches were applied in this experiment. To evaluate the results, the information retrieval metrics precision and recall were used. These metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide a scale of correctness (Beeferman et al., 1997). The degree to which a segmentation point was 'missed' by a trough, for instance, is not considered. Allowing an error margin provides some degree of flexibility. An error margin of two sentences either side of a segmentation point was used by Hearst (1993) and Reynar ( 1994) allowed three sentences. In this investigation, an error margin of two sentences was considered. Results: Table 2 gives the mean values for the comparison of troughs placed by the segmentation algorithm to the segmentation points identified by the test subjects for all the texts. Discussion: The segmentation algorithm usmg word repetition and relation weights in combination achieved mean precision and recall rates of 0.80 and 0.69, respectively. For 9 out of the 20 texts segmented, all troughs were relevant. Therefore, many of the troughs placed by the segmentation algorithm represented valid subject Table 2. Comparison of troughs to segmentation points placed by the test subjects. changes. Both word repetition in combination with collocation and all three features in combination also achieved a precision rate of 0.80 but attained a lower recall rate of 0.62. These results demonstrate that supplementing word repetition with other linguistic features can improve text segmentation. As an example, a text segmentation algorithm developed by Hearst ( 1994) based on word repetition alone attained inferior precision and recall rates of 0.66 and 0.61. In this investigation, recall rates tended to be lower than precision rates because the algorithm identified fewer segments (4.1 per text) than the test subjects (4.5). Each text was only 500 words in length and was related to a specific subject area. These factors limited the degree of subject change that occurred. Consequently, the test subjects tended to identify subject changes that were more subtle than the algorithm could detect. Conclusion The text segmentation algorithm developed used three linguistic features to automatically detect lexical cohesion relations across windows. The combination of features word repetition and relation weights produced the best precision and recall rates of 0.80 and 0.69. When used in isolation, the performance of each feature was inferior to a combined approach. This fact provides evidence that different lexical relations are detected by each linguistic feature considered. Areas for improving the segmentation algorithm include incorporation of a threshold for troughs. Currently, all troughs indicate a subject change, however, minor fluctuations in scores may be discounted. Future work with this algorithm should include application to longer documents. With trough thresholding the segments identified in longer documents could detect significant subject changes. Having located the related segments in text, a method of determining the subject of each segment could be developed, for example, for information retrieval purposes.
 Feature-Rich Translation by Quasi-Synchronous Lattice Parsing  We present a machine translation framework that can incorporate arbitrary features of both input and output sentences. The core of the approach is a novel decoder based on lattice parsing with quasi- synchronous grammar (Smith and Eisner, 2006), a syntactic formalism that does not require source and target trees to be isomorphic. Using generic approximate dynamic programming techniques, this decoder can handle ânon-localâ features. Similar approximate inference techniques support efficient parameter estimation with hidden variables. We use the decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on non- isomorphism.  We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with ânon-localâ features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen 1 Informally, features are âpartsâ of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning. Here we take first steps toward such a âuniversalâ decoder, making the following contributions:Arbitrary feature model (Â§2): We define a sin gle, direct log-linear translation model (Papineni et al., 1997; Och and Ney, 2002) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of âstring-to-tree,â âtree-to-string,â âtree- to-tree,â and âphrase-basedâ models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (Â§3â4): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary ânon- localâ features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009).Parameter estimation (Â§5): We exploit simi lar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (Â§6): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219â228, Singapore, 67 August 2009. Qc 2009 ACL and AFNLP Î£, T Trans : Î£ âª {NULL} â 2T s = (s0 , . . . , sn ) â Î£n t = (t1 , . . . , tm ) â Tm Ïs : {1, . . . , n} â {0, . . . , n} Ït : {1, . . . , m} â {0, . . . , m} a : {1, . . . , m} â 2{1,...,n} Î¸ source and target language vocabularies, respectively function mapping each source word to target words to which it may translate source language sentence (s0 is the NULL word) target language sentence, translation of s dependency tree of s, where Ïs (i) is the index of the parent of si (0 is the root, $) dependency tree of t, where Ït (i) is the index of the parent of ti (0 is the root, $) alignments from words in t to words in s; â denotes alignment to NULL parameters of the model gtrans (s, a, t) f lex (s, t) j f phr (si , tk ) lexical translation features (Â§2.1): word-to-word translation features for translating s as t phrase-to-phrase translation features for translating sj as t i k glm (t) j f N (tjâN +1 ) language model features (Â§2.2): N -gram probabilities gsyn (t, Ït ) f att (t, j, tl, k) f val (t, j, I ) target syntactic features (Â§2.3): syntactic features for attaching target word tl at position k to target word t at position j syntactic valence features with word t at position j having children I â {1, . . . , m} greor (s, Ïs , a, t, Ït ) f dist (i, j) reordering features (Â§2.4): distortion features for a source word at position i aligned to a target word at position j gtree 2 (Ïs , a, Ït ) f qg (i, il, j, k) tree-to-tree syntactic features (Â§3): configuration features for source pair si /sil being aligned to target pair tj /tk gcov (a) f scov (a), f zth (a), f sunc (a) coverage features (Â§4.2) counters for âcoveringâ each s word each time, the zth time, and leaving it âuncoveredâ Table 1: Key notation. Feature factorings are elaborated in Tab. 2. bination of the two. We quantify the effects of our approximate inference. We explore the effects of various ways of restricting syntactic non-isomorphism between source and target trees through the QG. We do not report state-of-the-art performance, but these experiments reveal interesting trends that will inform continued research.  (Table 1 explains notation.) Given a sentence s and its parse tree Ïs, we formulate the translation on the feasibility of inference, including decoding. Typically these feature functions are chosen to factor into local parts of the overall structure. We next define some key features used in current MT systems, explaining how they factor. We will use subscripts on g to denote different groups of features, which may depend on subsets of the structures t, Ït, a, s, and Ïs. When these features factor into parts, we will use f to denote the factored vectors, so that if x is an object that breaks into parts {xi}i, then g(x) = ï¿½i f (xi). 4 problem as finding the target sentence tâ (along with its parse tree Ï â source tree) such that3 and alignment aâ to the 2.1 Lexical. Translations Classical lexical translation features depend on s and t and the alignment a between them. The sim (tâ, Ï â, aâ) = argmax p(t, Ït, a | s, Ïs) (1) t,Ït ,aa In order to include overlapping features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model. That is, p(t, Ït, a | s, Ïs) = exp{Î¸Tg(s, Ïs, a, t, Ït)} plest are word-to-word features, estimated as the conditional probabilities p(t | s) and p(s | t) for s â Î£ and t â T. Phrase-to-phrase features generalize these, estimated as p(tl | sl) and p(sl | tl) where sl (respectively, tl) is a substring of s (t). A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into ï¿½al,tl,Ï l exp{Î¸Tg(s, Ïs, al, tl, Ï l)} (2) disjoint parts of the source and target sentences t t 4 There are two conventional definitions of feature func-. where the g are arbitrary feature functions and the Î¸ are feature weights. If one or both parse trees or the word alignments are unavailable, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this work that s is parsed. In principle, we might include source-side parsing as part of decoding. tions. One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002). These estimates are usually heuristic and inconsistent (Koehn et al., 2003). An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008). This offers more expressive power but may require much more training data to avoid overfitting. For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice. (Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any func g (s, a, t) = Pm iâa(j) f lex (si , tj ) (3) tion of words and alignments, we permit features + P f (slast (i,j) , tj ) that consider phrase pairs in which a target word g (t) = P i,j:1â¤i<jâ¤m Pm+1 phr first (i,j) i j lm N â{2,3} j=1 f N (tjâN +1 ) (4) outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, gsyn (t, Ït ) = Pm j Ï (j) , Ït (j)) val t (j)) (5) 2007). g (s, Ïs , a, t, Ït ) = Pm m P iâa(j) f dist (i, j) (6) Lexical translation features factor as in Eq. 3 (Tab. 2). We score all phrase pairs in a sentence pair that pair a target phrase with the smallest gtree 2 (Ïs , a, Ït ) = X f qg (a(j), a(Ït (j)), j, Ït (j)) (7) j=1 source phrase that contains all of the alignments in Table 2: Factoring of global feature collections g into f . xj denotes (xi , . . . xj ) in sequence x = (x1 , . . .). the target phrase; if k:iâ¤kâ¤j a(k) = â, no phrase i first (i, j) = mink:iâ¤kâ¤j (min(a(k))) and last (i, j) = feature fires for tj . maxk:iâ¤kâ¤j (max(a(k))). 2.2 N -gram Language Model N -gram language models have become standard in machine translation systems. For bigrams and trigrams (used in this paper), the factoring is in Eq. 4 (Tab. 2). 2.3 Target Syntax. There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and source- side dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, Ït, along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as âsyntactic language modelâ features (Shen et al., 2008). 5 Segmentation might be modeled as a hidden variable in future work. 2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007). In syntax-based systems, reordering is typically parameterized by grammar rules. For generality we permit these features to âseeâ all structures and denote them greor (s, Ïs, a, t, Ït). Eq. 6 (Tab. 2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the âbackboneâ model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features.  Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, Ït, a | s, Ïs). Given a source sentence s and its parse Ïs, a QDG induces a probabilistic monolingual dependency grammar over sentences âinspiredâ by the source sentence and tree. We denote this grammar by Gs,Ïs ; its (weighted) language is the set of translations of s. Each word generated by Gs,Ïs is annotated with a âsense,â which consists of zero or more words from s. The senses imply an alignment (a) between words in t and words in s, or equivalently, between nodes in Ït and nodes in Ïs. In principle, any portion of Ït may align to any portion of Ïs, but in practice we often make restrictions on the alignments to simplify computation. Smith and Eisner, for example, restricted |a(j)| for all words tj to be at most one, so that each target word aligned to at most one source word, which we also do here.6 lem.) As usual, the normalization constant is not required for decoding; it suffices to solve: t , a ) = argmax Î¸ g(s, Ï , a, t, Ï ) (8)Which translations are possible depends heav ily on the configurations that the QDG permits. (tâ, Ï â â T s t t,Ït ,aa Formally, for a parent-child pair (tÏt (j), tj ) in Ït, we consider the relationship between a(Ït(j)) and a(j), the source-side words to which tÏt (j) and tj align. If, for example, we require that, for all j, a(Ït(j)) = Ïs(a(j)) or a(j) = 0, and that the root of Ït must align to the root of Ïs or to NULL, then strict isomorphism must hold between Ïs and Ït, and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005). Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (âa(Ït(j)) = Ïs(a(j))â corresponds to their âparent-childâ configuration; see Fig. 3 in Smith and Eisner (2006) for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2). Note that the QDG instantiates the model in Eq. 2. Of the features discussed in Â§2, f lex , f att , f val , and f dist can be easily incorporated into theQDG as described while respecting the indepen dence assumptions implied by the configuration features. The others (f phr , f 2, and f 3) are non- local, or involve parts of the structure that, from the QDGâs perspective, are conditionally independent given intervening material. Note that ânon localityâ is relative to a choice of formalism; in Â§2 we did not commit to any formalism, so it is only now that we can describe phrase and N -gram features as non-local. Non-local features will present a challenge for decoding and training (Â§4.3).  Given a sentence s and its parse Ïs, at decoding time we seek the target sentence tâ, the target tree For a QDG model, the decoding problem has not been addressed before. It equates to finding the most probable derivation under the s/Ïs-specific grammar Gs,Ïs . We solve this by lattice parsing, assuming that an upper bound on m (the length of t) is known. The advantage offered by this approach (like most other grammar-based translation approaches) is that decoding becomes dynamic programming (DP), a technique that is both widely understood in NLP and for which practical, efficient, generic techniques exist. A major advantage of DP is that, with small modifications, summing over structures is also possible with âinsideâ DP algorithms. We will exploit this in training(Â§5). Efficient summing opens up many possibilities for training Î¸, such as likelihood and pseudo likelihood, and provides principled ways to handle hidden variables during learning. 4.1 Translation as Monolingual Parsing. We decode by performing lattice parsing on a lattice encoding the set of possible translations. The lattice is a weighted âsausageâ lattice that permits sentences up to some maximum length Â£; Â£ is derived from the source sentence length. Let the states be numbered 0 to Â£; states from lÏÂ£J to Â£ are final states (for some Ï â (0, 1)). For every position between consecutive states j â 1 and j (0 < j â¤ Â£), and for every word si in s, and for every word t â Trans(si), we instantiate an arc annotated with t and i. The weight of such an arc is exp{Î¸Tf }, where f is the sum of feature functions that fire when si translates as t in target position j (e.g., f lex (si, t) and f dist (i, j)). Given the lattice and Gs,Ïs , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms Ït , and the alignments aâ that are most probable, (Eisner, 1997). This decoder accounts for f lex ,as defined in Eq. 1.7 (In Â§5 we will consider kbest and all-translations variations on this prob 6 I.e., from here on, a : {1, . . . , m} â {0, . . . , n} where 0 denotes alignment to NULL. f att , f val , f dist , and f qg as local features. Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice repre 7 Arguably, we seek argmax. p(t | s), marginalizing out senting possible translations. In each bundle, the everything else. Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work. arcs are listed in decreasing order according to weight and for clarity only the first five are shown. The output of the decoder consists of lattice arcs Source: $ konnten sie es Ã¼bersetzen ? Reference: could you translate it ? Decoder output: $ konnten:could konnten:could es:it ?:? Ã¼bersetzen: ?:? Ã¼bersetzen: sie:you sie:you konnten:could translate Ã¼bersetzen: translate Ã¼bersetzen: konnten:couldn es:it sie :you translated translated konnten:might es:it sie:let sie:them ?:? Ã¼bersetzen: translate es:it konnten:could es:it NULL:to ... Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and thicker blue arcs forming a dependency tree over them. selected at each position and a dependency tree over them. 4.2 Source-Side Coverage Features. Most MT decoders enforce a notion of âcoverageâ covered the zth time (z â {2, 3, 4}) and fire again all subsequent times it is covered; these are denoted f 2nd, f 3rd, and f 4th. â¢ A counter of uncovered source words: of the source sentence during translation: all parts f sunc (a) = ï¿½n Î´(|aâ1(i)|, 0). of s should be aligned to some part of t (alignment to NULL incurs an explicit cost). Phrase-based systems such as Moses (Koehn et al., 2007) explicitly search for the highest-scoring string in which all source words are translated. Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in Ït (or a deliberate choice is made by the decoder to translate it to NULL). In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder. Our QDG decoder has no way to enforce coverage; it does not track any kind of state in Ïs apart from a single recently aligned word. This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993). Thissacrifice is the result of our choice to use a condi Of these, only f scov is local. 4.3 Non-Local Features. The lattice QDG parsing decoder incorporates many of the features we have discussed, but not all of them. Phrase lexicon features f phr , language model features f N for N > 1, and most coverage features are non-local with respect to our QDG. Recently Chiang (2007) introduced âcube pruningâ as an approximate decoding method that extends a DP decoder with the ability to incorporate features that break the Markovian independence assumptions DP exploits. Techniques like cube pruning can be used to include the non-local features in our decoder.8  Training requires us to learn values for the parameters Î¸ in Eq. 2. Given T training examples of the tional model (Â§2). form (t (i) , Ï (i), s (i) , Ï (i)), for i = 1, ..., T , max The solution is to introduce a set of coverageimum likelihood estimation for this model con 9 features gcov (a). Here, these include: sists of solving Eq. 9 (Tab. 3). Note that the â¢ A counter for the number of times each source 8 A full discussion is omitted for space, but in fact we use âcube decoding,â a slightly less approximate, slightly more word is covered: f scov (a) = ï¿½n |aâ1(i)|. expensive method that is more closely related to the approximate inference methods we use for training, discussed in Â§5. â¢ Features that fire once when a source word is 9 In practice, we regularize by including a term âc Î¸ 2 .. T T P exp{Î¸Tg(s(i) , Ï (i) , a, t(i) , Ï (i) )} T ânumeratorâ LL(Î¸) = X log p(t(i) , Ï (i) | s(i) , Ï (i) ) = X log a s t = X log (9) i=1 t s i=1 T t,Ït ,a exp{Î¸ g(s (i) , Ï (i) , a, t, Ï )} T i=1 âdenominatorâ PL(Î¸) = X logâX p(t(i) , a Â« Ï (i) , s(i) , Ï (i) ) X logâX p(Ï (i) , a Â« t(i) , s(i) , Ï (i) ) (10) âdenominatorâ of i=1 a n X X 1 | t s l n T ` + t | i=1 a l l s o (11) term 1 in Eq. 10 = i=0 tl âTrans(si ) S(Ït (0), i, t ) Ã exp Î¸ f lex (si , t ) + f att ($, 0, t , k) + f qg (0, i, 0, k)Â´ n S(j, i, t) = Y X X S(k, il, tl) Ã exp Î¸T â lex (sil , tl) + f 1 att (t, j, tl, k)+ Â«ff (12) kâÏ â1 (j) il =0 tl âTrans(sil ) f val (t, j, Ï â (j)) + f qg (i, il, j, k) S(j, i, t) = exp nÎ¸T `f (t, j, Ï â1 (j))Â´o if Ï â1 (j) = â (13) val t t Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. Î¸. Eqs. 11â13: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008). Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA). This requires us to calculate the functionâs gradient (vector of first derivatives) with respect to Î¸.11 Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast âinsideâ DP solution is known (Smith and Eisner, 2006; Wang et al., 2007). It runs in O(mn2) time and O(mn) space. Computing the denominator in Eq. 9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences. With a maximum length imposed, this is tractable using the âinsideâ version of the maximizing DP algorithm of Sec. 4, but it is prohibitively expensive. We therefore optimize pseudo-likelihood instead, making the following approximation (Be 10 Alignments could be supplied by automatic word alignment algorithms. We chose to leave them hidden so that we could make the best use of our parsed training data when configuration constraints are imposed, since it is not always possible to reconcile automatic word alignments with automatic parses. 11 When the functionâs value is computed by âinsideâ DP, the corresponding âoutsideâ algorithm can be used to obtain the gradient. Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al. (2005). sag, 1975): p(t, Ït | s, Ïs) â p(t | Ït, s, Ïs) Ã p(Ït | t, s, Ïs) Plugging this into Eq. 9, we arrive at Eq. 10 (Tab. 3). The two parenthesized terms in Eq. 10 each have their own numerators and denominators (not shown). The numerators are identical to each other and to that in Eq. 9. The denominators are much more manageable than in Eq. 9, never requiring summation over more than two structures at a time. We must sum over target word sequences and word alignments (with fixed Ït), and separately over target trees and word alignments (with fixed t). 5.1 Summing over t and a. The summation over target word sequences and alignments given fixed Ït bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992). Let S(j, i, t) denote the sum of all translations rooted at position j in Ït such that a(j) = i and tj = t. Tab. 3 gives the equations for this DP: Eq. 11 is the quantity of interest, Eq. 12 is the recursion, and Eq. 13 shows the base cases for leaves of Ït.Letting q = max0â¤iâ¤n |Trans(si)|, this algo rithm runs in O(mn2q2) time and O(mnq) space. For efficiency we place a hard upper bound on q during training (details in Â§6). 5.2 Summing over Ït and a. For the summation over dependency trees and alignments given fixed t, required for p(Ït | t, s, Ïs), we perform âinsideâ lattice parsing with Gs,Ïs . The technique is the summing variant of the decoding method in Â§4, except for each state j, the sausage lattice only includes arcs from j â 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3) time and requires O(a2) space. Because we use a hard upper bound on |Trans(s)| for all s â Î£, this summation is much faster in practice than the one over words and alignments. 5.3 Handling Non-Local Features. So far, all of our algorithms have exploited DP, disallowing any non-local features (e.g., f phr , f N for N > 1, f zth, f sunc ). We recently proposed âcube summing,â an approximate technique that permits the use of non-local features for inside DP algorithms (Gimpel and Smith, 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item. Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features. Using the machinery of cube summing, it is straightforward to include the desired non-local features in the summations required for pseudo- likelihood, as well as to compute their approximate gradients. ment set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. 6.2 Features. Our base system uses features as discussed in Â§2. To obtain lexical translation features gtrans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the âgrow-diag-final-andâ heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t | s) and p(s | t) estimated from the symmetrized align ments. After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words Ã phrase conditional and âlexical smoothingâ probabilities Ã two conditional directions. Bigram and trigam language model features, f 2 and f 3, are estimated using the SRI toolkit (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998).Our approach permits an alternative to mini mum error-rate training (MERT; Och, 2003); it is For our target-language syntactic features g syn , discriminative but handles latent structure and regularization in more principled ways. The pseudo- likelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGAâs inner loop faster than MERTâs inner loop.  Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output. 6.1 Data and Evaluation. We use the GermanEnglish portion of the Basic Travel Expression Corpus (BTEC). The corpus has approximately 100K sentence pairs. We filter sentences of length more than 15 words, which only removes 6% of the data. We end up with a training set of 82,299 sentences, a develop we use features similar to lexicalized CFG events (Collins, 1999), specifically following the dependency model of Klein and Manning (2004). These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ). These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003). The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003). In total, there are 7 lexical and 7 word-class syntax features. For reordering, we use a single absolute distortion feature f dist (i, j) that returns |iâj| whenever a(j) = i and i, j > 0. (Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.) The tree-to-tree syntactic features gtree 2 in our model are binary features f qg that fire for particular QG configurations. We use one feature for each of the configurations in (Smith and Eisner, 2006), adding 7 additional features that score configura Phrase Syntactic Features: features: +f att âª f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU). tions involving root words and NULL-alignments more finely. There are 14 features in this category. Coverage features gcov are as described in Â§4.2. In all, 46 feature weights are learned. 6.3 Experimental Procedure. Our model permits training the system on the full set of parallel data, but we instead use the parallel data to estimate feature functions and learn Î¸ on the development set.12 We trained using three iterations of SGA over the development data with a batch size of 1 and a fixed step size of 0.01. We used Â£2 regularization with a fixed, untuned coefficient of 0.1. Cube summing used a 10-best list for training and a 7-best list for decoding unless otherwise specified. To obtain the translation lexicon (Trans) we first included the top three target words t for each s using p(s | t) Ã p(t | s) to score target words. For any training sentence (s, t) and tj for which gcov . The results are shown in Table 4. The second row contains scores when adding in the eight f phr features. The second column shows scores when adding the 14 target syntax features (f att and f val ), and the third column adds to them the 14 additional tree-to-tree features (f qg ). We find large gains in BLEU by adding more features, and find that gains obtained through phrase features and syntactic features are partially additive, suggesting that these feature sets are making complementary contributions to translation quality. 6.5 Varying k During Decoding. For models without syntactic features, we constrained the decoder to produce dependency trees in which every wordâs parent is immediately to its right and ignored syntactic features while scoring structures. This causes decoding to proceed left- to-right in the lattice, the way phrase-based decoders operate. Since these models do not search over trees, they are substantially faster during decoding than those that use syntactic features and do not require any pruning of the lattice. Therefore, we explored varying the value of k used during k-best cube decoding; results are shown in Fig. 2. Scores improve when we increase k up tj /â n Trans(si), we added tj to Trans(si) to 10, but not much beyond, and there is still a i l l substantial gap (2.5 BLEU) between using phrase for = arg ma xilâ I p(si |tj ) Ã p(tj |si ), wh ere I i i n Trans(si)| < qi}. features with k = 20 and using all features with We used q0 = 10 and q>0 = 5, restricting k = 5. Models without syntax perform poorly |Trans(NULL)| â¤ 10 and |Trans(s)| â¤ 5 for anys â Î£. This made 191 of the development sentences unreachable by the model, leaving 743 sen tences for learning Î¸. During decoding, we generated lattices with all t â Trans(si) for 0 â¤ i â¤ n, for every position. We used Ï = 0.9, causing states within 90% of the source sentence length to be final states. Between each pair of consecutive states, we pruned edges that fell outside a beam of 70% of the sum of edge weights (see Â§4.1; edge weights use f lex , f dist , and f scov ) of all edges between those two states. 6.4 Feature Set Comparison. Our first set of experiments compares feature sets commonly used in phrase- and syntax-based trans when using a very small k, due to their reliance on non-local language model and phrase features. By contrast, models with syntactic features, which are local in our decoder, perform relatively well even with k = 1. 6.6 QG Configuration Comparison. We next compare different constraints on isomorphism between the source and target dependency 0.55 0.50 0.45 0.40 0.35 Phrase + Syntactic lation. In particular, we compare the effects of combining phrase features and syntactic features. The base model contains f lex , glm , greor , and 12 We made this choice both for similarity to standard MT. 0.30 0.25 0.20 Phrase Syntactic Neither 0 5 10 15 20 Value of k for Decoding systems and a more rapid experiment cycle. Figure 2: Comparison of size of k-best list for cube decoding with various feature sets. QD G Co nfi gu rati on s BL E U M ET E O R sy nc hr on ou s + nul ls, root an y + child par ent , sa me no de + sib lin g + gr an dp ar ent /ch ild + c co m ma nd + oth er 0.4 00 8 0.4 10 8 0.4 33 7 0.4 88 1 0.5 01 5 0.5 15 6 0.5 14 2 0 . 6 9 4 9 0 . 6 9 3 1 0 . 6 8 1 5 0 . 7 2 1 6 0 . 7 3 6 5 0 . 7 4 4 1 0 . 7 4 7 2 Table 5: QG configuration comparison. The name of each configuration, following Smith and Eisner (2006), refers to the relationship between a(Ït (j)) and a(j) in Ïs . trees. To do this, we impose harsh penalties on some QDG configurations (Â§3) by fixing their feature weights to â1000. Hence they are permit ted only when absolutely necessary in training and rarely in decoding.13 Each model uses all phrase and syntactic features; they differ only in the sets of configurations which have fixed negative weights. Tab. 5 shows experimental results. The base âsynchronousâ model permits parent-child (a(Ït(j)) = Ïs(a(j))), any configuration where a(j) = 0, including both words being linked to NULL, and requires the root word in Ït to be linked to the root word in Ïs or to NULL(5 of our 14 configurations). The second row allows any configuration involving NULL, including those where tj aligns to a non-NULL word in s and its parent aligns to NULL, and allows the root in Ït to be linked to any word in Ïs. Each subsequent row adds additional configurations (i.e., trains its Î¸ rather than fixing it to â1000). In general, wesee large improvements as we permit more con figurations, and the largest jump occurs when we add the âsiblingâ configuration (Ïs(a(Ït(j))) = Ïs(a(j))). The BLEU score does not increase, however, when we permit all configurations in the final row of the table, and the METEOR score increases only slightly. While allowing certain categories of non-isomorphism clearly seems helpful, permitting arbitrary violations does not appear to be necessary for this dataset. 6.7 Discussion. We note that these results are not state-of-the- art on this dataset (on this task, Moses/MERT achieves 0.6838 BLEU and 0.8523 METEOR with maximum phrase length 3).14 Our aim has been to 13 In fact, the strictest âsynchronousâ model used the almost-forbidden configurations in 2% of test sentences; this behavior disappears as configurations are legalized. 14 We believe one cause for this performance gap is the generation of the lattice and plan to address this in future work by allowing the phrase table to inform lattice generation. illustrate how a single model can provide a controlled experimental framework for comparisons of features, of inference methods, and of constraints. Our findings show that phrase features and dependency syntax produce complementary improvements to translation quality, that tree-to- tree configurations (a new feature in MT) are helpful for translation, and that substantial gains can be obtained by permitting certain types of non- isomorphism. We have validated cube summing and decoding as practical methods for approximate inference. Our framework permits exploration of alternative objectives, alternative approximate inference techniques, additional hidden variables (e.g., Mosesâ phrase segmentation variable), and, of course, additional feature representations. The system is publicly available at www.ark.cs. cmu.edu/Quipu.  We presented feature-rich MT using a principled probabilistic framework that separates features from inference. Our novel decoder is based on efficient DP-based QG lattice parsing extended to handle ânon-localâ features using generic techniques that also support efficient parameter estimation. Controlled experiments permitted with this system show interesting trends in the use of syntactic features and constraints.  We thank three anonymous EMNLP reviewers, David Smith, and Stephan Vogel for helpful comments and feedback that improved this paper. This research was supported by NSF IIS0836431 and IIS0844507, a grant from Google, and computational resources provided by Yahoo.
 A Fast Fertility Hidden Markov Model forWord Alignment Using MCMC  A word in one language can be translated to zero, one, or several words in other languages. Using word fertility features has been shown to be useful in building word alignment models for statistical machine translation. We built a fertility hidden Markov model by adding fertility to the hidden Markov model. This model not only achieves lower alignment error rate than the hidden Markov model, but also runs faster. It is similar in some ways to IBM Model 4, but is much easier to understand. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4.  IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). There are three kinds of important information for word alignment models: lexicality, locality and fertility. IBM Model 1 uses only lexical information; IBM Model 2 and the hidden Markov model take advantage of both lexical and locality information; IBM Models 4 and 5 use all three kinds of information, and they remain the state of the art despite the fact that they were developed almost two decades ago. Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Nevertheless, we believe that IBM Model 4 is essentially a better model because it exploits the fertility of words in the tar get language. However, IBM Model 4 is so complex that most researches use the GIZA++ software package (Och and Ney, 2003), and IBM Model 4 itself is treated as a black box. The complexity in IBM Model 4 makes it hard to understand and to improve. Our goal is to build a model that includes lexicality, locality, and fertility; and, at the same time, to make it easy to understand. We also want it to be accurate and computationally efficient. There have been many years of research on word alignment. Our work is different from others in essential ways. Most other researchers take either the HMM alignments (Liang et al., 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4. Directly modeling fertility makes our model fundamentally different from others. Most models have limited ability to model fertility. Liang et al. (2006) learn the alignment in both translation directions jointly, essentially pushing the fertility towards 1. ITG models (Wu, 1997) assume the fertility to be either zero or one. It can model phrases, but the phrase has to be contiguous. There have been works that try to simulate fertility using the hidden Markov model (Toutanova et al., 2002; Deng and Byrne, 2005), but we prefer to model fertility directly. Our model is a coherent generative model that combines the HMM and IBM Model 4. It is easier to understand than IBM Model 4 (see Section 3). Our model also removes several undesired properties in IBM Model 4. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596â605, MIT, Massachusetts, USA, 911 October 2010. Qc 2010 Association for Computational Linguistics estimation. Our distortion parameters are similar to IBM Model 2 and the HMM, while IBM Model 4 uses inverse distortion (Brown et al., 1993). Our model assumes that fertility follows a Poisson distribution, while IBM Model 4 assumes a multinomial distribution, and has to learn a much larger number of parameters, which makes it slower and less reliable. Our model is much faster than IBM Model 4. In fact, we will show that it is also faster than the HMM, and has lower alignment error rate than the HMM. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. Brown et al. (1993) and Och and Ney (2003) first compute the Viterbi alignments for simpler models, then consider only some neighbors of the Viterbi alignments for modeling fertility. If the optimal alignment is not in those neighbors, this method will not be able find the opti total of I + 1 empty words for the HMM model1. Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. After we add I + 1 empty words to the target sentence, the alignment is a mapping from source to target word positions: a : j â i, i = aj where j = 1, 2, . . . , J and i = 1, 2, . . . , 2I + 1. Words from position I + 1 to 2I + 1 in the target sentence are all empty words. We allow each source word to align with exactly one target word, but each target word may align with multiple source words. The fertility Ïi of a word ei at position i is defined as the number of aligned source words: J mal alignment. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, Ïi = j=1 Î´(aj , i) which has nice probabilistic guarantees. DeNero et al. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility.  where Î´ is the Kronecker delta function: ( 1 if x = y Î´(x, y) = 0 otherwise In particular, the fertility of all empty words in 2.1 Alignment and Fertility. the target sentence is "Â£2I +1 "Â£2I +1 Ïi. We define ÏÇ« â¡ 2I +1 i=I +1 Ïi. For a bilingual sentence pair e1 and Given a source sentence f J = f1, f2, . . . , fJ and a f J , we have "Â£I Ïi + ÏÇ« = J . target sentence eI 1 = e1, e2, . . . , eI , we define the 1 i=1 The inverted alignments for position i in the tar alignments between the two sentences as a subset of the Cartesian product of the word positions. Following Brown et al. (1993), we assume that each source word is aligned to exactly one target word. get sentence are a set Bi, such that each element in Bi is aligned with i, and all alignments of i are in Bi. Inverted alignments are explicitly used in IBM Models 3, 4 and 5, but not in our model, which is We denote as aJ = a1, a2, . . . , aJ the alignments one reason that our model is easier to understand. between f J and eI . When a word fj is not aligned 1 1 with any word e, aj is 0. For convenience, we add an empty word Ç« to the target sentence at position 0 (i.e., e0 = Ç«). However, as we will see, we have to add more than one empty word for the HMM. 2.2 IBM Model 1 and HMM. IBM Model 1 and the HMM are both generative models, and both start by defining the probability of alignments and source sentence given the In order to compute the âjump probabilityâ in the target sentence: P (aJJ 1 ); the data likeli HMM model, we need to know the position of the 1 , f1 |e2I +1 hood can be computed by summing over alignments: aligned target word for the previous source word. If the previous source word aligns to an empty word, 1 If fj. â1 does not align with an empty word and fj alignswe could use the position of the empty word to indi with an empty word, we want to record the position of the target word that fjâ1 aligns with. There are I + 1 possibilities: fj is cate the nearest previous source word that does not align to an empty word. For this reason, we use a the first word in the source sentence, or fj the target word. â1 aligns with one ofP (f J |e2I +1) = "Â£ J P (aJ , f J |e2I +1). The alignwhere the first two equations imply that the proba 1 1 a1 1 1 1 ments aJ are the hidden variables. The expectation maximization algorithm is used to learn the parameters such that the data likelihood is maximized. Without loss of generality, P (aJ , f J |e2I +1) can bility of jumping to an empty word is either 0 or p0, and the third equation implies that the probability of jumping from a nonempty word is the same as the probability of jumping from the corespondent empty 1 1 1 be decomposed into length probabilities, distortion probabilities (also called alignment probabilities), and lexical probabilities (also called translation probabilities): P (aJ , f J |e2I +1) 1 1 1 J word. The absolute position in the HMM is not important, because we re-parametrize the distortion probability in terms of the distance between adjacent alignment points (Vogel et al., 1996; Och and Ney, 2003): = P (J |e2I +1) n P (aj , fj |f jâ1, ajâ1, e2I +1) c(i â iâ²) 1 j=1 1 1 1 P (i|iâ², I ) = "Â£ iâ²â² c(iâ²â² â iâ²) J = P (J |e2I +1) n P (aj |f jâ1, ajâ1, e2I +1) Ã where c( ) is the count of jumps of a given distance. 1 j=1 1 1 1 In IBM Model 1, the word order does not mat ter. The HMM is more likely to align a source P (fj |f jâ1, aj , e2I +1)l 1 1 1 where P (J |e2I +1) is a length probability, word to a target word that is adjacent to the previous aligned target word, which is more suitable than IBM Model 1 because adjacent words tend to form (aj |f jâ1, ajâ1 2I +1P 1 1 , e1 ) is a distortion prob phrases. ability and P (fj |f j probability. â1, aj , e 2I +1 1 ) is a lexical For these two models, in theory, the fertility for a target word can be as large as the length of the IBM Model 1 assumes a uniform distortion probability, a length probability that depends only on the length of the target sentence, and a lexical probability that depends only on the aligned target word: J source sentence. In practice, the fertility for a target word in IBM Model 1 is not very big except for rare target words, which can become a garbage collector, and align to many source words (Brown et al., 1993; Och and Ney, 2003; Moore, 2004). The HMM is P (aJ , f J |e2I +1) = P (J |I ) n P (f |e ) less likely to have this garbage collector problem be 1 1 1 (2I + 1)J j=1 j aj cause of the alignment probability constraint. However, fertility is an inherent cross language propertyThe hidden Markov model assumes a length prob ability that depends only on the length of the target sentence, a distortion probability that depends only on the previous alignment and the length of the target sentence, and a lexical probability that depends only on the aligned target word: P (aJ , f J |e2I +1) = 1 1 1 J P (J |I ) n P (aj |ajâ1, I )P (fj |ea ) j=1 In order to make the HMM work correctly, we enforce the following constraints (Och and Ney, 2003): and these two models cannot assign consistent fertility to words. This is our motivation for adding fertility to these two models, and we expect that the resulting models will perform better than the baseline models. Because the HMM performs much better than IBM Model 1, we expect that the fertility hidden Markov model will perform much better than the fertility IBM Model 1. Throughout the paper, âour modelâ refers to the fertility hidden Markov model. Due to space constraints, we are unable to provide details for IBM Models 3, 4 and 5; see Brown et al. (1993) and Och and Ney (2003). But we want to point out that the locality property modeled in the HMM is missing in IBM Model 3, and is modeled invertedly in IBM Model 4. IBM Model 5 removes deficiency (Brown et al., 1993; Och and Ney, 2003) from IBM Model 4, but it is computationally very expensive due to the larger number of parameters than IBM Model 4, and IBM Model 5 often provides no improvement on alignment accuracy.  Our fertility IBM Model 1 and fertility HMM are both generative models and start by defining the probability of fertilities (for each nonempty target word and all empty words), alignments, and the source sentence given the target sentence: P (ÏI , ÏÇ«, aJ , f J |e2I +1); 1 1 1 1 are further away from the mean have low probability. IBM Models 3, 4, and 5 use a multinomial distribution for fertility, which has a much larger number of parameters to learn. Our model has only one parameter for each target word, which can be learned more reliably. In the fertility IBM Model 1, we assume that the distortion probability is uniform, and the lexical probability depends only on the aligned target word: P (ÏI , ÏÇ«, aJ , f J |e2I +1) the data likelihood can be computed by 1 1 1 I Ïi 1 Î»(ei ) summing over fertilities and alignments: n Î»(ei) eâ Ã P (f J |e2I +1) = "Â£ I J P (ÏI , ÏÇ«, aJ , f J |e2I +1). i=1 Ïi! 1 1 Ï1 ,ÏÇ« ,a1 1 1 1 1 The fertility for a nonempty word ei is a random variable Ïi, and we assume Ïi follows a Poisson distribution Poisson(Ïi; Î»(ei)). The sum of the fer (I Î»(Ç«))ÏÇ« eâ(I Î»(Ç«)) ÏÇ«! Ã J tilities of all the empty words (ÏÇ«) grows with the length of the target sentence. Therefore, we assume that ÏÇ« follows a Poisson distribution with parameter I Î»(Ç«). Now P (ÏI , ÏÇ«, aJ , f J |e2I +1) can be decomposed 1 (2I + 1)J n P (fj | j=1 eaj ) (1) 1 1 1 1 in the following way: P (ÏI , ÏÇ«, aJ , f J |e2I +1) In the fertility HMM, we assume that the distor tion probability depends only on the previous alignment and the length of the target sentence, and that 1 1 1 1 = P (ÏI |e2I +1)P (ÏÇ«|ÏI , e2I +1) Ã 1 1 1 1 J the lexical probability depends only on the aligned target word: n P (aj , fj |f jâ1, ajâ1, e2I +1, ÏI , ÏÇ«) j=1 1 1 1 1 P (ÏI , ÏÇ«, aJ , f J |e2I +1) = n Î»(ei) eâÎ»(ei ) 1 1 1 I Ï 1 Î»(e ) Ïi! Ã = n Î»(ei) i eâ i i=1 (I Î»(Ç«))ÏÇ« eâI Î»(Ç«) ÏÇ«! Ã Ï i=1 (I Î»(Ç«))ÏÇ« ! Ã eâ(I Î»(Ç«)) J n P (aj |f jâ1, ajâ1, e2I +1 I ÏÇ«! Ã J j=1 1 1 1 , Ï1 , ÏÇ«) Ã n P (aj | j=1 ajâ1 , I )P (fj | eaj ) (2) P (fj |f jâ1, aj , e2I +1, ÏI , ÏÇ«)l 1 1 1 1 Superficially, we only try to model the length 1 |e2I +1probability more accurately. However, we also en When we compute P (f J 1 ), we only sum force the fertility for the same target word across the corpus to be consistent. The expected fertility for a nonempty word ei is Î»(ei), and the expected fertil over fertilities that agree with the alignments: ity for all empty words is I Î»(Ç«). Any fertility value P (f J |e2I +1) = P (aJ , f J |e2I +1) has a nonzero probability, but fertility values that 1 1 1 1 1 J 1 where P (aJ , f J |e2I +1) auxiliar y functio n is: L(P (f |e), P (a|a ), Î»(e), Î¾1(e) , Î¾2(a )) 1 1 1 = P (ÏI , ÏÇ«, aJ , f J |e2I +1) = PË â² aJ e 2I +1, f J ) log â² P (aJ , f J | e2I +1) 1 1 ,ÏÇ« 1 1 1 1 1 1 J 1 1 1 1 â P (ÏI , ÏÇ«, aJ , f J |e2I +1) Ã â Î¾1(e)( P (f |e) â 1) 1 1 1 1 I ï£« J ï£¶ e f n Î´ ï£­ i=1 j=1 Î´(aj , i), Ïiï£¸ Ã â Î¾2(aâ²)( aâ² a P (a|aâ²) â 1) ï£« 2I +1 J ï£¶ Because P (aJ , f J |e2I +1) is in the exponential 1 1 1 Î´ ï£­ i=I +1 j=1 Î´(aj , i), ÏÇ«ï£¸ (3) family, we get a closed form for the parameters from expected counts: In the last two lines of Equation 3, ÏÇ« and each P (f |e) = "Â£s c (f |e; f (s), e(s)) (4) Ïi are not free variables, but are determined by f s c(f |e; f (s), e(s))the alignments. Because we only sum over fer tilities that are consistent with the alignments, we P (a|aâ²) = "Â£s c (a|aâ²; f (s), e(s)) (5)have "Â£f J P (f J |e2I +1) < 1, and our model is de "Â£ "Â£ a s c(a|aâ²; f (s), e(s)) 1 1 1 "Â£ (s) (s) ficient, similar to IBM Models 3 and 4 (Brown et al., 1993). We can remove the deficiency for fertility IBM Model 1 by assuming a different distortion Î»(e) = s c(Ï| e; f , e ) s c(k|e; f (s), e(s)) (6) probability: the distortion probability is 0 if fertility where s is the number of bilingual sentences, andis not consistent with alignments, and uniform oth c(f |e; f J 2I +1 Ë J J 2I +1 erwise. The total number of consistent fertility and 1 , e1 ) = P (a1 |f1 , e1 ) Ã J alignments is J ! . Replacing 1 with a1 ÏÇ« ! J i ! ÏÇ« ! J i ! (2I +1)J Î´(fj , f )Î´(ei, e) J ! , we have: c(a|aâ²; f J , e2I +1) = j PË(aJ |f J , e2I +1) Ã P (ÏI , ÏÇ«, aJ , f J |e2I +1) 1 1 1 1 1 J 1 1 1 1 a1 I = n Î»(ei)Ïi eâÎ»(ei ) Ã i=1 (I Î»(Ç«))ÏÇ« eâ(I Î»(Ç«)) Ã c(Ï|e; f1 , e1 ) = Î´(aj , a)Î´(ajâ1, aâ²) j PË(a1 |f1 , e1 ) Ã J 2I +1 J J 2I +1 J n P (fj |ea ) J 1 Ï Î´(e , e) J ! j i i j=1 i c(k|e; f J , e2I +1) = k(ei)Î´(ei, e) In our experiments, we did not find a noticeable 1 1 change in terms of alignment accuracy by removing the deficiency.  We estimate the parameters by maximizing P (f J |e2I +1) using the expectation maximization These equations are for the fertility hidden Markov model. For the fertility IBM Model 1, we do not need to estimate the distortion probability.  Although we can estimate the parameters by using 1 1 (EM) algorithm (Dempster et al., 1977). The the EM algorithm, in order to compute the expected counts, we have to sum over all possible alignments1 , which is, unfortunately, exponential. We devel Algorithm 1: One iteration of E-step: draw t samples for each aj for each sentence pairoped a Gibbs sampling algorithm (Geman and Ge (f J 1 ) in the corpus man, 1984) to compute the expected counts. 1 , e2I +1 J 2I +1 For each target sentence e2I +1 and source sentence f J , we initialize the alignment aj for each source word fj using the Viterbi alignments from IBM Model 1. During the training stage, we try all 2I + 1 possible alignments for aj but fix all other alignments.2 We choose alignment aj with probabil J 2I +1 for (f1 , e1 ) in the corpus do Initialize aJ with IBM Model 1; for t do for j do for i do aj = i; Compute P (aJ , f J |e2I +1); ity P (aj |a1, Â· Â· Â· ajâ1, aj+1 Â· Â· Â· aJ , f1 , e1 ), which can be computed in the following way: end 1 1 1 P (aj |a1, Â· Â· Â· , aj 1, a , Â· Â· Â· , a , f J , e2I +1) â j+1 J 1 1 J J 2I +1 Draw a sample for aj using Equation 7; Update counts; = P (a1 , f1 |e1 ) (7) end "Â£ J J 2I +1 aj P (a1 , f1 |e1 ) For each alignment variable aj , we choose t samples. We scan through the corpus many times until we are satisfied with the parameters we learned using Equations 4, 5, and 6. This Gibbs sampling method updates parameters constantly, so it is an âonline learningâ algorithm. However, this sampling method needs a large amount of communication between machines in order to keep the parameters up to date if we compute the expected counts in parallel. Instead, we do âbatch learningâ: we fix the parameters, scan through the entire corpus and compute expected counts in parallel (E-step); then combine all the counts together and update the parameters (M- step). This is analogous to what IBM models and end end We also consider initializing the alignments using the HMM Viterbi algorithm in the E-step. In this case, the fertility hidden Markov model is not faster than the HMM. Fortunately, initializing using IBM Model 1 Viterbi does not decrease the accuracy in any noticeable way, and reduces the complexity of the Gibbs sampling algorithm. In the testing stage, the sampling algorithm is the same as above except that we keep the alignments 1 that maximize P (a1 , f1 |e2I +1). We need more the HMM do in the EM algorithms. The algorithm aJ J J 1 for the E-step on one machine (all machines are independent) is in Algorithm 1. For the fertility hidden Markov model, updating P (aJ , f J |e2I +1) whenever we change the alignment 1 1 1 aj can be done in constant time, so the complexity of choosing t samples for all aj (j = 1, 2, . . . , J ) is O(tI J ). This is the same complexity as the HMM if t is O(I ), and it has lower complexity if t is a constant. Surprisingly, we can achieve better results than the HMM by computing as few as 1 sample for each alignment, so the fertility hidden Markov model is much faster than the HMM. Even when choosing t such that our model is 5 times faster than the HMM, we achieve better results. 2 For fertility IBM Model 1, we only need to compute I + 1. values because e2I +1 are identical empty words. samples in the testing stage because it is unlikely to get to the optimal alignments by sampling a few times for each alignment. On the contrary, in the above training stage, although the samples are not accurate enough to represent the distribution defined by Equation 7 for each alignment aj , it is accurate enough for computing the expected counts, which are defined at the corpus level. Interestingly, we found that throwing away the fertility and using the HMM Viterbi decoding achieves same results as the sampling approach (we can ignore the difference because it is tiny), but is faster. Therefore, we use Gibbs sampling for learning and the HMM Viterbi decoder for testing. Gibbs sampling for the fertility IBM Model 1 is similar but simpler. We omit the details here. Al ig n m en t M o d e l P R A E R e n â c n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 49 .6 55 .4 62 .6 65 .4 66 .8 67 .8 66 .8 55 .3 57 .1 59 .5 59 .1 60 .8 62 .3 64 .1 4 7. 8 4 3. 8 3 9. 0 3 7. 9 3 6. 2 3 4. 9 3 4. 5 c n â e n I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M MF 3 0 I B M 4 52 .6 55 .9 66 .1 68 .6 71 .1 71 .1 69 .3 53 .7 56 .4 62 .1 60 .2 62 .2 62 .7 68 .5 4 6. 9 4 3. 9 3 5. 9 3 5. 7 3 3. 5 3 3. 2 3 1. 1 Table 1: AER results. IBM1F refers to the fertility IBM1 and HMMF refers to the fertility HMM. We choose t = 1, 5, and 30 for the fertility HMM. 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 1: AER comparison (enâcn) 0.48 0.46 0.44 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 0.42 0.4 0.38 0.36 0.34 0.32 Figure 2: AER comparison (cn âen) 5000 4000 I B M 1 I B M 1 F H M M H M M F 1 H M M F 5 H M M F 3 0 I B M 4 3000 2000 1000 0 Figure 3: Training time comparison. The training time for each model is calculated from scratch. For example, the training time of IBM Model 4 includes the training time of IBM Model 1, the HMM, and IBM Model 3.  We evaluated our model by computing the word alignment and machine translation quality. We use the alignment error rate (AER) as the word alignment evaluation criterion. Let A be the alignments output by word alignment system, P be a set of possible alignments, and S be a set of sure alignments both labeled by human beings. S is a subset of P . Precision, recall, and AER are defined as follows: recall = |A â© S| |S| precision = |A â© P | |A| AER(S, P, A) = 1 |A â© S| + |A â© P | |A| + |S| AER is an extension to F-score. Lower AER is better. We evaluate our fertility models on a ChineseEnglish corpus. The ChineseEnglish data taken from FBIS newswire data, and has 380K sentence pairs, and we use the first 100K sentence pairs as our training data. We used hand-aligned data as reference. The ChineseEnglish data has 491 sentence pairs. We initialize IBM Model 1 and the fertility IBM Model 1 with a uniform distribution. We smooth all parameters (Î»(e) and P (f |e)) by adding a small value (10â8), so they never become too small. We run both models for 5 iterations. AER results are computed using the IBM Model 1 Viterbi alignments, and the Viterbi alignments obtained from the Gibbs sampling algorithm. We initialize the HMM and the fertility HMM with the parameters learned in the 5th iteration of IBM Model 1. We smooth all parameters (Î»(e), P (a|aâ²) and P (f |e)) by adding a small value (10â8). We run both models for 5 iterations. AER results are computed using traditional HMM Viterbi decoding for both models. It is always difficult to determine how many samples are enough for sampling algorithms. However, both fertility models achieve better results than their baseline models using a small amount of samples. For the fertility IBM Model 1, we sample 10 times for each aj , and restart 3 times in the training stage; we sample 100 times and restart 12 times in the testing stage. For the fertility HMM, we sample 30 times for each aj with no restarting in the training stage; no sampling in the testing stage because we use traditional HMM Viterbi decoding for testing. More samples give no further improvement. Initially, the fertility IBM Model 1 and fertility HMM did not perform well. If a target word e only appeared a few times in the training corpus, our model cannot reliably estimate the parameter Î»(e). Hence, smoothing is needed. One may try to solve it by forcing all these words to share a same parameter Î»(einfrequent). Unfortunately, this does not solve the problem because all infrequent words tend to have larger fertility than they should. We solve the problem in the following way: estimate the parameter Î»(enon empty ) for all nonempty words, all infrequent words share this parameter. We consider words that appear less than 10 times as infrequent words. Table 1, Figure 1, and Figure 2 shows the AER results for different models. We can see that the fertility IBM Model 1 consistently outperforms IBM Model 1, and the fertility HMM consistently outperforms the HMM. The fertility HMM not only has lower AER than the HMM, it also runs faster than the HMM. Figure 3 show the training time for different models. In fact, with just 1 sample for each alignment, our model archives lower AER than the HMM, and runs more than 5 times faster than the HMM. It is possible to use sampling instead of dynamic programming in the HMM to reduce the training time with no decrease in AER (often an increase). We conclude that the fertility HMM not only has better AER results, but also runs faster than the hidden Markov model. We also evaluate our model by computing the machine translation BLEU score (Papineni et al., 2002) using the Moses system (Koehn et al., 2007). The training data is the same as the above word alignment evaluation bitexts, with alignments for each model symmetrized using the grow-diag-final heuristic. Our test is 633 sentences of up to length 50, with four references. Results are shown in Table 2; we see that better word alignment results do not lead to better translations. Model BLEU HMM 19.55 HMMF30 19.26 IBM4 18.77 Table 2: BLEU results  We developed a fertility hidden Markov model that runs faster and has lower AER than the HMM. Our model is thus much faster than IBM Model 4. Our model is also easier to understand than IBM Model 4. The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4. While better word alignment results do not necessarily correspond to better translation quality, our translation results are comparable in translation quality to both the HMM and IBM Model 4. Acknowledgments We would like to thank Tagyoung Chung, Matt Post, and the anonymous reviewers for helpful comments. This work was supported by NSF grants IIS0546554 and IIS0910611.
 Simple Type-Level Unsupervised POS Tagging  Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus. Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy. However, in existing systems, this expansion come with a steep increase in model complexity. This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments. In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training. Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts. On several languages, we report performance exceeding that of more complex state-of-the art systems.1  Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits âone tag per discourseâ sparsity â words are likely to select a single predominant tag in a corpus, even when several tags are possible. Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank. This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/. â similar results have been observed across multiple languages. Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary. In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (MeÂ´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; GracÂ¸a et al., 2009; Berg-Kirkpatrick et al., 2010). These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable. By design, they readily capture regularities at the token-level. However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity. Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (GracÂ¸a et al., 2009; Ravi and Knight, 2009). In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time. In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model. The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word. Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag. In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag. Across all languages, high performance can be attained by selecting a single tag per word type. token-level HMM to reflect lexicon sparsity. This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007). There are two key benefits of this model architecture. First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns). Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference. We evaluate our model on seven languages exhibiting substantial syntactic variation. On several languages, we report performance exceeding that of state-of-the art systems. Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naÂ¨Ä±veBayes approach to incorporate features. The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts.  Recent work has made significant progress on unsupervised POS tagging (MeÂ´rialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John son, 2008; Ravi and Knight, 2009). Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process. This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity. The extent to which this constraint is enforced varies greatly across existing methods. On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010). These clusters are computed using an SVD variant without relying on transitional structure. While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context. Other approaches encode sparsity as a soft constraint. For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags. This design does not guarantee âstructural zeros,â but biases towards sparsity. A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (GracÂ¸a et al., 2009). This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. The use of ILP in learning the desired grammar significantly increases the computational complexity of this method. In contrast to these approaches, our method directly incorporates these constraints into the structure of the model. This design leads to a significant reduction in the computational complexity of training and inference. Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009). These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training. In our work, we demonstrate that using a simple naÂ¨Ä±veBayes approach also yields substantial performance gains, without the associated training complexity.  We consider the unsupervised POS induction problem without the use of a tagging dictionary. A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1. As is standard, we use a fixed constant K for the number of tagging states. Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word. Conditioned on T , features of word types W are drawn. We refer to (T , W ) as the lexicon of a language and Ï for the parameters for their generation; Ï depends on a single hyperparameter Î². Once the lexicon has been drawn, the model proceeds similarly to the standard token-level HMM: Emission parameters Î¸ are generated conditioned on tag assignments T . We also draw transition parameters Ï. Both parameters depend on a single hyperparameter Î±. Once HMM parameters (Î¸, Ï) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from Ï. The corresponding token words w are drawn conditioned on t and Î¸.2 Our full generative model is given by: K P (Ï, Î¸|T , Î±, Î²) = n (P (Ït|Î±)P (Î¸t|T , Î±)) t=1 The transition distribution Ït for each tag t is drawn according to DIRICHLET(Î±, K ), where Î± is the shared transition and emission distribution hyperparameter. In total there are O(K 2) parameters associated with the transition parameters. In contrast to the Bayesian HMM, Î¸t is not drawn from a distribution which has support for each of the n word types. Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then Î¸t is drawn from DIRICHLET(Î±, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4). Note that while the standard HMM, has O(K n) emission parameters, our model has O(n) effective parameters.3 Token Component Once HMM parameters (Ï, Î¸) have been drawn, the HMM generates a token-level corpus w in the standard way: P (w, t|Ï, Î¸) = P (T , W , Î¸, Ï, Ï, t, w|Î±, Î²) = P (T , W , Ï|Î²) [Lexicon] ï£« n n ï£­ (w,t)â(w,t) j ï£¶ P (tj |Ïtjâ1 )P (wj |tj , Î¸tj )ï£¸ P (Ï, Î¸|T , Î±, Î²) [Parameter] P (w, t|Ï, Î¸) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively. Since the parameter and token components will remain fixed throughout experiments, we briefly describe each. Parameter Component As in the standard Bayesian HMM (Goldwater and Griffiths, 2007), all distributions are independently drawn from symmetric Dirichlet distributions: 2 Note that t and w denote tag and word sequences respectively, rather than individual tokens or tags. Note that in our model, conditioned on T , there is precisely one t which has nonzero probability for the token component, since for each word, exactly one Î¸t has support. 3.1 Lexicon Component. We present several variations for the lexical component P (T , W |Ï), each adding more complex pa rameterizations. Uniform Tag Prior (1TW) Our initial lexicon component will be uniform over possible tag assignments as well as word types. Its only purpose is 3 This follows since each Î¸t has St â 1 parameters and. P St = n. Î² T VARIABLES Ï Y W : Word types (W1 ,. .., Wn ) (obs) P T : Tag assigns (T1 ,. .., Tn ) T W Ï E w : Token word seqs (obs) t : Token tag assigns (det by T ) PARAMETERS Ï : Lexicon parameters Î¸ : Token word emission parameters Ï : Token tag transition parameters Ï Ï t1 t2 Î¸ Î¸ w1 w2 K Ï T tm O K Î¸ E wN m N N Figure 1: Graphical depiction of our model and summary of latent variables and parameters. The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters Î¸. The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure. The hyperparameters Î± and Î² represent the concentration parameters of the token- and type-level components of the model respectively. They are set to fixed constants. to explore how well we can induce POS tags using only the one-tag-per-word constraint. Specifically, the lexicon is generated as: P (T , W |Ï) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010). Past work however, has typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated these features with token occurrences, typically in an HMM. In our model, we associate these features at the type-level in the lexicon. Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint. Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution Ï over tag assignments drawn from DIRICHLET(Î², K ). This alters generation of T as follows: n P (T |Ï) = n P (Ti|Ï) i=1 Note that this distribution captures the frequency of a tag across word types, as opposed to tokens. The P (T |Ï) distribution, in English for instance, should have very low mass for the DT (determiner) tag, since determiners are a very small portion of the vocabulary. In contrast, NNP (proper nouns) form a large portion of vocabulary. Note that these observa sider suffix features, capitalization features, punctuation, and digit features. While possible to utilize the feature-based log-linear approach described in Berg-Kirkpatrick et al. (2010), we adopt a simpler naÂ¨Ä±ve Bayes strategy, where all features are emitted independently. Specifically, we assume each word type W consists of feature-value pairs (f, v). For each feature type f and tag t, a multinomial Ïtf is drawn from a symmetric Dirichlet distribution with concentration parameter Î². The P (W |T , Ï) term in the lexicon component now decomposes as: n P (W |T , Ï) = n P (Wi|Ti, Ï) i=1 n ï£« ï£¶ tions are not modeled by the standard HMM, which = n ï£­ n P (v|ÏTi f )ï£¸ instead can model token-level frequency. i=1 (f,v)âWi  For inference, we are interested in the posterior probability over the latent variables in our model. During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, Î±, Î²) â P (T , t, W , w|Î±, Î²) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, Ï, Î¸, Ï, w|Î±, Î²)dÏdÎ¸dÏ Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior. Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i). The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5). Performance typically stabilizes across languages after only a few number of iterations. to represent the ith word type emitted by the HMM: P (t(i)|Ti, t(âi), w, Î±) â n P (w|Ti, t(âi), w(âi), Î±) (tb ,ta ) P (Ti, t(i)|T , W , t(âi), w, Î±, Î²) = P (T |tb, t(âi), Î±)P (ta|T , t(âi), Î±) âi (i) i i (âi) P (Ti|W , T âi, Î²)P (t |Ti, t , w, Î±) All terms are Dirichlet distributions and parameters can be analytically computed from counts in t(âi)where T âi denotes all type-level tag assignment ex cept Ti and t(âi) denotes all token-level tags except and w (âi) (Johnson, 2007). t(i). The terms on the right-hand-side denote the type-level and token-level probability terms respectively. The type-level posterior term can be computed according to, P (Ti|W , T âi, Î²) â Note that each round of sampling Ti variables takes time proportional to the size of the corpus, as with the standard token-level HMM. A crucial difference is that the number of parameters is greatly reduced as is the number of variables that are sampled during each iteration. In contrast to results reported in Johnson (2007), we found that the per P (Ti|T âi, Î²) n (f,v)âWi P (v|Ti, f, W âi, T âi, Î²) formance of our Gibbs sampler on the basic 1TW model stabilized very quickly after about 10 full it All of the probabilities on the right-hand-side are Dirichlet, distributions which can be computed analytically given counts. The token-level term is similar to the standard HMM sampling equations found in Johnson (2007). The relevant variables are the set of token-level tags that appear before and after each instance of the ith word type; we denote these context pairs with the set {(tb, ta)} and they are contained in t(âi). We use w erations of sampling (see Figure 2 for a depiction).  We evaluate our approach on seven languages: English, Danish, Dutch, German, Portuguese, Spanish, and Swedish. On each language we investigate the contribution of each component of our model. For all languages we do not make use of a tagging dictionary. Mo del Hy per par am . E n g li s h1 1 m-1 D a n i s h1 1 m-1 D u t c h1 1 m-1 G er m a n1 1 m-1 Por tug ues e1 1 m-1 S p a ni s h1 1 m-1 S w e di s h1 1 m-1 1T W be st me dia n 45. 2 62.6 45. 1 61.7 37. 2 56.2 32. 1 53.8 47. 4 53.7 43. 9 61.0 44. 2 62.2 39. 3 68.4 49. 0 68.4 48. 5 68.1 34. 3 54.4 33.  36. 0 55.3 34. 9 50.2 +P RI OR be st me dia n 47. 9 65.5 46. 5 64.7 42. 3 58.3 40. 0 57.3 51. 4 65.9 48. 3 60.7 50.  41. 7 68.3 56. 2 70.7 52. 0 70.9 42.  37. 1 55.8 38.  36. 8 57.3 +F EA TS be st me dia n 50. 9 66.4 47. 8 66.4 52. 1 61.2 43. 2 60.7 56. 4 69.0 51. 5 67.3 55. 4 70.4 46. 2 61.7 64. 1 74.5 56. 5 70.1 58. 3 68.9 50. 0 57.2 43. 3 61.7 38. 5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5). For each language and setting, we report one-to-one (11) and many- to-one (m-1) accuracies. For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 11 metric. The second row represents the performance of the median hyperparameter setting. Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3). La ng ua ge # To ke ns # W or d Ty pe s # Ta gs E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics for various corpora utilized in experiments. See Section 5. The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task. 5.1 Data Sets. Following the setup of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English. For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora (Buchholz and Marsi, 2006) which include gold POS tags (used for evaluation). We train and test on the CoNLL-X training set. Statistics for all data sets are shown in Table 2. 5.2 Setup. Models To assess the marginal utility of each component of the model (see Section 3), we incremen- tally increase its sophistication. Specifically, we (+FEATS) utilizes the tag prior as well as features (e.g., suffixes and orthographic features), discussed in Section 3, for the P (W |T , Ï) component. Hyperparameters Our model has two Dirichlet concentration hyperparameters: Î± is the shared hyperparameter for the token-level HMM emission and transition distributions. Î² is the shared hyperparameter for the tag assignment prior and word feature multinomials. We experiment with four values for each hyperparameter resulting in 16 (Î±, Î²) combinations: Î± Î² 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations In each run, we performed 30 iterations of Gibbs sampling for the type assignment variables W .4 We use the final sample for evaluation. Evaluation Metrics We report three metrics to evaluate tagging performance. As is standard, we report the greedy one-to-one (Haghighi and Klein, 2006) and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags. We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-. encodes the one tag per word constraint and is uni form over type-level tag assignments. The second model (+PRIOR) utilizes the independent prior over type-level tag assignments P (T |Ï). The final model tions. 5 We choose these two metrics over the Variation Information measure due to the deficiencies discussed in Gao and Johnson (2008). we perform five runs with different random initialization of sampling state. Hyperparameter settings are sorted according to the median one-to-one metric over runs. We report results for the best and median hyperparameter settings obtained in this way. Specifically, for both settings we report results on the median run for each setting. Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set. The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags. We tokenize MWUs and their POS tags; this reduces the tag set size to 12. See Table 2 for the tag set size of other languages. With the exception of the Dutch data set, no other processing is performed on the annotated tags. 6 Results and Analysis. We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings. Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness. Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al. (2010) and the posterior regular- ization HMM of GracÂ¸a et al. (2009). The system of Berg-Kirkpatrick et al. (2010) reports the best unsupervised results for English. We consider two variants of Berg-Kirkpatrick et al. (2010)âs richest model: optimized via either EM or LBFGS, as their relative performance depends on the language. Our model outperforms theirs on four out of five languages on the best hyperparameter setting and three out of five on the median setting, yielding an average absolute difference across languages of 12.9% and 3.9% for best and median settings respectively compared to their best EM or LBFGS performance. While Berg-Kirkpatrick et al. (2010) consistently outperforms ours on English, we obtain substantial gains across other languages. For instance, on Spanish, the absolute gap on median performance is 10%. Top 5 Bot to m 5 Go ld NN P NN JJ CD NN S RB S PD T # â , 1T W CD W RB NN S VB N NN PR P$ W DT : MD . +P RI OR CD JJ NN S WP $ NN RR B- , $ â . +F EA TS JJ NN S CD NN P UH , PR P$ # . â Table 5: Type-level English POS Tag Ranking: We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting. Our second point of comparison is with GracÂ¸a et al. (2009), who also incorporate a sparsity constraint, but does via altering the model objective using posterior regularization. We can only compare with GracÂ¸a et al. (2009) on Portuguese (GracÂ¸a et al. (2009) also report results on English, but on the reduced 17 tag set, which is not comparable to ours). Their best model yields 44.5% one-to-one accuracy, compared to our best median 56.5% result. However, our full model takes advantage of word features not present in GracÂ¸a et al. (2009). Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming GracÂ¸a et al. (2009). Ablation Analysis We evaluate the impact of incorporating various linguistic features into our model in Table 3. A novel element of our model is the ability to capture type-level tag frequencies. For this experiment, we compare our model with the uniform tag assignment prior (1TW) with the learned prior (+PRIOR). Across all languages, +PRIOR consistently outperforms 1TW, reducing error on average by 9.1% and 5.9% on best and median settings respectively. Similar behavior is observed when adding features. The difference between the featureless model (+PRIOR) and our full model (+FEATS) is 13.6% and 7.7% average error reduction on best and median settings respectively. Overall, the difference between our most basic model (1TW) and our full model (+FEATS) is 21.2% and 13.1% for the best and median settings respectively. One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively. We observe similar trends when using another measure â type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 â â 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 â â 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 â â 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 â â 4 0 . 6 7 3 . 2 â â 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods. Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (GracÂ¸a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint. La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1. 1 1 0. 1 2 3. 8 1 2. 8 1 8. 4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type. The state-to-tag mapping is obtained from the best hyperparameter setting for 11 mapping shown in Table 3. our full model yields 39.3% average error reduction across languages when compared to the basic configuration (1TW). Table 5 provides insight into the behavior of different models in terms of the tagging lexicon they generate. The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard. 7 Conclusion and Future Work. We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model. This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model. The resulting model is compact, efficiently learnable and linguistically expressive. Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints. In this paper, we make a simplifying assumption of one-tag-per-word. This assumption, however, is not inherent to type-based tagging models. A promising direction for future work is to explicitly model a distribution over tags for each word type. We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.  The authors acknowledge the support of the NSF (CAREER grant IIS0448168, and grant IIS 0904684). We are especially grateful to Taylor Berg- Kirkpatrick for running additional experiments. We thank members of the MIT NLP group for their suggestions and comments. Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.
 Discovering Corpus-Specific Word Senses  This paper presents an unsupervised algorithm which automatically discovers word senses from text. The algorithm is based on a graph model representing words and relationships between them. Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word. Discrimination against previously extracted sense clusters enables us to discover new senses. We use the same data for both recognising and resolving ambiguity.  This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies. Automatic word sense discovery has applications of many kinds. It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones. The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998). This paper is organised as follows. In section 2, we present the graph model from which we discover word senses. Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering (van Dongen, 2000). The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph. In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning. We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity. Section 5 describes the experiment and presents a sample of the results. Finally, section 6 sketches applications of the algorithm and discusses future work.  The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech. Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. "genomic DNA from rat, mouse and dog". Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1. Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words. The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features. 1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words. Instead we link each word to its top n neighbors where n can be determined by the user (cf. section 4).. 41=0 441=P .4161. sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph of the word mouse  Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse. However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense. There are, of course, many more types of polysemy (cf. e.g. (Kilgarriff, 1992)). As can be seen in figure 2, wing "part of a bird" is closely related to tail, as is wing "part of a plane". Therefore, even after removal of the wing-node, the two areas of meaning are still linked via tail. The same happens with wing "part of a building" and wing "political group" which are linked via policy. However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning. To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000). The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters. The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000). Let G, denote the local graph around the ambiguous word w. The adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph of the word wing of a graph G, is defined by setting (111G) pq equal to the weight of the edge between nodes v and v q . Normalizing the columns of A/G results in the Markov Matrix Taw whose entries (Thi,)pq can be interpreted as transition probability from v q to vv . It can easily be shown that the k-th power of TG lists the probabilities (TL )pq of a path of length k starting at node vq and ending at node V. The MCL-algorithm simulates flow in Gw by iteratively recomputing the set of transition probabilities via two steps, expansion and inflation. The expansion step corresponds with taking the k-th power of TG as outlined above and allows nodes to see new neighbours. The inflation step takes each matrix entry to the r-th power and then rescales each column so that the entries sum to 1.Vi a inflation, popular neighbours are further supported at the expense of less popular ones. Flow within dense regions in the graph is concentrated by both expansion and inflation. Eventually, flow between dense regions will disappear, the matrix of transition probabilities TG will converge and the limiting matrix can be interpreted as a clustering of the graph.  The output of the MCL-algorithm strongly depends on the inflation and expansion parameters r and k as well as the size of the local graph which serves as input to MCL. An appropriate choice of the inflation param 80 eter r can depend on the ambiguous word w to be clustered. In case of homonymy, a small inflation parameter r would be appropriate. However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another. In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together. Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus. If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus. On the other hand, if the local graph is too big, we will get a lot of noise. Below, we outline an algorithm which circumvents the problem of choosing the right parameters. In contrast to pure Markov clustering, we don't try to find a complete clustering of G into senses at once. Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only. We then recompute the local graph Gw by discriminating against c's features. This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words. The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold. Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty. The algorithm consists of the following steps: 1. Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6. 2. Recursively remove all nodes of degree one. Then remove the node corresponding with w from G. 3. Apply MCL to Gw with a fairly big inflation parameter r which is fixed. 4. Take the "best" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5. Go back to 1 with the reduced/devalued set of features F. 6. Go through the final list of clusters L and assign a name to each cluster using a broad-coverage taxonomy (see below). Merge semantically close clusters using a taxonomy-based semantic distance measure (Budanitsky and Hirst, 2001) and assign a class-label to the newly formed cluster. 7. Output the list of class-labels which best represent the different senses of w in the corpus. The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the "best" cluster, it suffices to build a relatively small graph in 1. Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them. In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted. The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose. The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label. The family of such algorithms is described in (Widdows, 2003).  In this section, we describe an initial evaluation experiment and present the results. We will soon carry out and report on a more thorough analysis of our algorithm. We used the simple graph model based on co-occurrences of nouns in lists (cf. section 2) for our experiment. We gathered a list of nouns with varying degree of ambiguity, from homonymy (e.g. arms) to systematic polysemy (e.g. cherry). Our algorithm was applied to each word in the list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) in order to extract the top two sense clusters only. We then determined the WordNet synsets which most adequately characterized the sense clusters. An extract of the results is listed in table 1. Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output of word sense clustering.  The benefits of automatic, data-driven word sense discovery for natural language processing and lexicography would be very great. Here we only mention a few direct results of our work. Our algorithm does not only recognise ambiguity, but can also be used to resolve it, because the features shared by the members of each sense cluster provide strong indication of which reading of an ambiguous word is appropriate given a certain context. This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated. The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning. This approach to disambiguation combines the benefits of both Yarowsky's (1995) and Schtitze's (1998) approaches. Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used. Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted. They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora. The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus. We prepare an evaluation of our algorithm as applied to the collocation relationships (cf. section 2), and we plan to evaluate the uses of our clustering algorithm for unsupervised disambiguation more thoroughly.
 Foma: a finite-state compiler and library  Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses. It has specific support for many natural language processing applications such as producing morphological and phonological analyzers. Foma is largely compatible with the Xerox/PARC finite-state toolkit. It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the âMathematical Operatorsâ Unicode block.  Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications. The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Tâs fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005). One of Fomaâs design goals has been compatibility with the Xerox/PARC toolkit. Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions. Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples. The compiler and library are implemented in C and an API is available. The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions. However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms. These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions. The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton. This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately. Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma. It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort.  Retaining backwards compatibility with Xerox/PARC and at the same time extending the formalism means that one is often able to construct finite-state networks in equivalent various ways, either through ASCII-based operators or through the Unicode-based extensions. For example, one can either say: ContainsX = Î£* X Î£*; MyWords = {cat}|{dog}|{mouse}; MyRule = n -> m || p; ShortWords = [MyLex1]1 â© Î£Ë<6; or: Proceedings of the EACL 2009 Demonstrations Session, pages 29â32, Athens, Greece, 3 April 2009. Qc 2009 Association for Computational Linguistics Operators Compatibility variant Function [ ] () [ ] () grouping parentheses, optionality â â N/A quantifiers \ â term negation, substitution/homomorphism : : cross-product + â + â Kleene closures Ë<n Ë>n Ë{m,n} Ë<n Ë>n Ë{m,n} iterations 1 2 .1 .2 .u .l domain & range .f N/A eliminate all unification flags $ $. Ë $ $. complement, containment operators / ./. N/A N/A âignoresâ, left quotient, right quotient, âinsideâ quotient â â/ = /= N/A language membership, position equivalence âº < > precedes, follows â¨ âª â§ â© - .P. .p. | & â .P. .p. union, intersection, set minus, priority unions => -> (->) @-> => -> (->) @-> context restriction, replacement rules <> shuffle (asynchronous product) Ã â¦ .x. .o. cross-product, composition Table 1: The regular expressions available in Foma from highest to lower precedence. Horizontal lines separate precedence classes. define ContainsX ?* X ?*; define MyWords {cat}|{dog}|{mouse}; define MyRule n -> m || _ p; define ShortWords Mylex.i.l & ?Ë<6; In addition to the basic regular expression operators shown in table 1, the formalism is extended in various ways. One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions (Hulden, 2008). For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (âx)(x â L â§ (ây)(y â L â§ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to â and â§, and a kind of concatenative meaning to the predicate S(t1, t2). Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators. In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions.  As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes. This practice stems back from the earliest two-level compilers (Karttunen et al., 1987). Below is a simple example of the format: Multichar_Symbols +Pl +Sing LEXICON Root Nouns; LEXICON Nouns cat Plural; church Plural; LEXICON Plural +Pl:%Ës #; +Sing #;  The Foma API gives access to basic functions, such as constructing a finite-state machine from a regular expression provided as a string, performing a transduction, and exhaustively matching against a given string starting from every position. The following basic snippet illustrates how to use the C API instead of the main interface of Foma to construct a finite-state machine encoding the language a+b+ and check whether a string matches it: 1. void check_word(char *s) { 2. fsm_t *network; 3. fsm_match_result *result; 4. 5. network = fsm_regex("a+ b+"); 6. result = fsm_match(fsm, s); 7. if (result->num_matches > 0) 8. printf("Regex matches"); 9. 10 } Here, instead of calling the fsm regex() function to construct the machine from a regular expressions, we could instead have accessed the beforementioned low-level routines and built the network entirely without regular expressions by combining low-level primitives, as follows, replacing line 5 in the above: network = fsm_concat( fsm_kleene_plus( fsm_symbol("a")), fsm_kleene_plus( fsm_symbol("b"))); The API is currently under active development and future functionality is likely to include conversion of networks to 8-bit letter transducers/automata for maximum speed in regular expression matching and transduction.  educational use Foma has support for visualization of the machines it builds through the AT&T Graphviz library. For educational purposes and to illustrate automata construction methods, there is some support for changing the behavior of the algorithms. For instance, by default, for efficiency reasons, Foma determinizes and minimizes automata between nearly every incremental operation. Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata. However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possibleânon-deterministic automata naturally being easier to inspect and analyze.  Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers. With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms. Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers. One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines. For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).  The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata. Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma. Foma is free software and will remain under the GNU General Public License. As the source code is available, collaboration is encouraged. GNU AT&T Foma xfst flex fsm 4 Î£âaÎ£15 0.216s 16.23s 17.17s 1.884s Î£âaÎ£20 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits. The first and second entries are short regular expressions that exhibit exponential behavior. The second results in a FSM with 221 states and 222 arcs. The others are scripts that can be run on both Xerox/PARC and Foma. The file lexicon.lex is a LEXC format English dictionary with 38418 entries. North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no.
 Using Random Walks for Question-focused Sentence Retrieval  We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.How many victims have been found?)Judges found sentences providing an answer to each question. To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentenceto the input question via IDFweightedword overlap. In our experiments, themethod achieves a TRDR score that is significantly higher than that of the baseline.  Recent work has motivated the need for systemsthat support Information Synthesis tasks, in whicha user seeks a global understanding of a topic orstory (Amigo et al., 2004). In contrast to the classical question answering setting (e.g. TREC-style Q&A (Voorhees and Tice, 2000)), in which the userpresents a single question and the system returns acorresponding answer (or a set of likely answers), inthis case the user has a more complex informationneed. Similarly, when reading about a complex newsstory, such as an emergency situation, users mightseek answers to a set of questions in order to understand it better. For example, Figure 1 showsthe interface to our Web-based news summarizationsystem, which a user has queried for informationabout Hurricane Isabel. Understanding such storiesis challenging for a number of reasons. In particular,complex stories contain many sub-events (e.g. thedevastation of the hurricane, the relief effort, etc.) Inaddition, while some facts surrounding the situationdo not change (such as Which area did the hurricane first hit?), others may change with time (Howmany people have been left homeless?). Therefore, we are working towards developing a systemfor question answering from clusters of complex stories published over time. As can be seen at the bottom of Figure 1, we plan to add a component to ourcurrent system that allows users to ask questions asthey read a story. They may then choose to receiveeither a precise answer or a question-focused summary. Currently, we address the question-focused sentence retrieval task. While passage retrieval (PR) isclearly not a new problem (e.g. (Robertson et al.,1992; Salton et al., 1993)), it remains important andyet often overlooked. As noted by (Gaizauskas et al.,2004), while PR is the crucial first step for questionanswering, Q&A research has typically not empha915 Hurricane Isabel's outer bands moving onshoreproduced on 09/18, 6:18 AM 2% SummaryThe North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as faraway as Pennsylvania prepared for possibly ruinous flooding. (2:3) A hurricane warning was in effect from CapeFear in southern North Carolina to the VirginiaMaryland line, and tropical storm warnings extended from South Carolinato New Jersey. (2:14) While the outer edge of the hurricane approached the North Carolina coast Wednesday, the center of the storm was still400 miles south-southeast of Cape Hatteras, N.C., late Wednesday morning. (3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states have been affected by the hurricane so far? Around 200,000 people in coastal areas of North Carolina and Virginia were ordered to evacuate or risk getting trappedby flooding from storm surges up to 11 feet. (5:8) The storm was expected to hit with its full fury today, slamming intothe North Carolina coast with 105mph winds and 45-foot wave crests, before moving through Virginia and bashing thecapital with gusts of about 60 mph. (7:6) Figure 1: Question tracking interface to a summarization system. sized it. The specific problem we consider differsfrom the classic task of PR for a Q&A system ininteresting ways, due to the time-sensitive nature ofthe stories in our corpus. For example, one challengeis that the answer to a users question may be updated and reworded over time by journalists in orderto keep a running story fresh, or because the factsthemselves change. Therefore, there is often morethan one correct answer to a question.We aim to develop a method for sentence retrieval that goes beyond finding sentences that aresimilar to a single query. To this end, we propose to use a stochastic, graph-based method. Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004). In (Erkan and Radev, 2004), we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization. Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system. We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap). Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.  Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method. In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers. In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine). Our system does not rank the input documents, nor is it restricted in terms of the number ofsentences that may be selected from the same document. The output of our system, a ranked list of sentences relevant to the users question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences. Alternatively, the sentences canbe returned to the user as a question-focused summary. This is similar to snippet retrieval (Wu etal., 2004). However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis.  3.1 The LexRank method. In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries. To apply LexRank, a similarity graph is producedfor the sentences in an input document set. In thegraph, each node represents a sentence. There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold. The degree of a given node isan indication of how much information the respective sentence has in common with other sentences. Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15. Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality. As previously mentioned, the original LexRank method performed wellin the context of generic summarization. Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem. In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences. 3.2 Relevance to the question. In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in. We also stem the question and remove the stop words from it. Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively. This model hasproven to be successful in query-based sentence retrieval (Allan et al., 2003), and is used as our competitive baseline in this study (e.g. Tables 4, 5 and7). 3.3 The mixture model. The baseline system explained above does not makeuse of any inter-sentence information in a cluster.We hypothesize that a sentence that is similar tothe high scoring sentences in the cluster should alsohave a high score. For instance, if a sentence thatgets a high score in our baseline model is likely tocontain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer.This idea is captured by the following mixture model, where p(s|q), the score of a sentence s givena question q, is determined as the sum of its relevance to the question (using the same measure asthe baseline described above) and the similarity tothe other sentences in the document cluster: p(s|q) = d rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) where C is the set of all sentences in the cluster. Thevalue of d, which we will also refer to as the question bias, is a trade-off between two terms in the Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence  1 0.03614457831325301 At least two people are dead, inclu... 0 0.28454242157110576 Officials said the plane was carryin... 2 0.1973852892722677 Italian police said the plane was car.. 3 0.28454242157110576 Rescue officials said that at least th... Graph Figure 2: LexRank example: sentence similaritygraph with a cosine threshold of 0.15. equation and is determined empirically. For highervalues of d, we give more importance to the relevance to the question compared to the similarity tothe other sentences in the cluster. The denominatorsin both terms are for normalization, which are described below. We use the cosine measure weightedby word IDFs as the similarity between two sentences in a cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 can be written in matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) A is the square matrix such that for a given index i,all the elements in the ith column are proportionalto rel(i|q). B is also a square matrix such that eachentry B(i, j) is proportional to sim(i, j). Both matrices are normalized so that row sums add up to 1.Note that as a result of this normalization, all rowsof the resulting square matrixQ = [dA+(1-d)B]also add up to 1. Such a matrix is called stochasticand defines a Markov chain. If we view each sentence as a state in a Markov chain, thenQ(i, j) specifies the transition probability from state i to state jin the corresponding Markov chain. The vector pwe are looking for in Equation 5 is the stationarydistribution of the Markov chain. An intuitive interpretation of the stationary distribution can be under- 917 stood by the concept of a random walk on the graphrepresentation of the Markov chain.With probability d, a transition is made from the current node (sentence) to the nodes that are similar to the query. With probability (1-d), a transitionis made to the nodes that are lexically similar to thecurrent node. Every transition is weighted accordingto the similarity distributions. Each element of thevector p gives the asymptotic probability of endingup at the corresponding state in the long run regardless of the starting state. The stationary distributionof a Markov chain can be computed by a simple iterative algorithm, called power method.1 A simpler version of Equation 5, where A is auniform matrix andB is a normalized binary matrix,is known as PageRank (Brin and Page, 1998; Pageet al., 1998) and used to rank the web pages by theGoogle search engine. It was also the model used torank sentences in (Erkan and Radev, 2004). 3.4 Experiments with topic-sensitive LexRank. We experimented with different values of d on ourtraining data. We also considered several thresholdvalues for inter-sentence cosine similarities, wherewe ignored the similarities between the sentencesthat are below the threshold. In the training phaseof the experiment, we evaluated all combinationsof LexRank with d in the range of [0, 1] (in increments of 0.10) and with a similarity threshold ranging from [0, 0.9] (in increments of 0.05). We thenfound all configurations that outperformed the baseline. These configurations were then applied to ourdevelopment/test set. Finally, our best sentence retrieval system was applied to our test data set andevaluated against the baseline. The remainder of thepaper will explain this process and the results in detail. 4 Experimental setup. 4.1 Corpus. We built a corpus of 20 multi-document clusters ofcomplex news stories, such as plane crashes, political controversies and natural disasters. The data 1The stationary distribution is unique and the power methodis guaranteed to converge provided that the Markov chain isergodic (Seneta, 1981). A non-ergodic Markov chain can bemade ergodic by reserving a small probability for jumping toany other state from the current state (Page et al., 1998). clusters and their characteristics are shown in Table 1. The news articles were collected from varioussources. Newstracker clusters were collected automatically by our Web-based news summarization system. The number of clusters randomly assignedto the training, development/test and test data setswere 11, 3 and 6, respectively.Next, we assigned each cluster of articles to an annotator, who was asked to read all articles in thecluster. He or she then generated a list of factualquestions key to understanding the story. Once wecollected the questions for each cluster, two judgesindependently annotated nine of the training clusters. For each sentence and question pair in a givencluster, the judges were asked to indicate whetheror not the sentence contained a complete answerto the question. Once an acceptable rate of inter-judge agreement was verified on the first nine clusters (Kappa (Carletta, 1996) of 0.68), the remaining11 clusters were annotated by one judge each.In some cases, the judges did not find any sentences containing the answer for a given question.Such questions were removed from the corpus. Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1. 4.2 Evaluation metrics and methods. To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sentences deemed to be relevant to the question, for thesystem and from human judgment. To compare different configurations of our system to the baselinesystem, we produced extracts at a fixed length of 20sentences. While evaluations of question answeringsystems are often based on a shorter list of rankedsentences, we chose to generate longer lists for several reasons. One is that we are developing a PRsystem, of which the output can then be input to ananswer extraction system for further processing. Insuch a setting, we would most likely want to generate a relatively longer list of candidate sentences. Aspreviously mentioned, in our corpus the questionsoften have more than one relevant answer, so ideally,our PR system would find many of the relevant sentences, sending them on to the answer componentto decide which answer(s) should be returned to theuser. Each systems extract file lists the document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train What is the condition under whichthreat GIA will take its action?Milan plane MSNBC, CNN, ABC, 9 15 train How many people were in thecrash Fox, USAToday building at the time of the crash?Turkish plane BBC, ABC, 10 12 train To where was the plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train How many people were killed inattack the most recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train Who was to blame forclub fire Fox, BBC, Ananova the fire?FBI most AFP, UPI 3 14 train How much is the State Department offeringwanted for information leading to bin Ladens arrest?Russia bombing AP, AFP 2 11 train What was the cause of the blast?Bali terror CNN, FoxNews, ABC, 10 30 train What were the motivationsattack BBC, Ananova of the attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train What kinds of equipment or weaponssniper BBC, Washington Times, CBS were used in the killings?GSPC terror Newstracker 8 29 train What are the charges againstgroup the GSPC suspects?China Novelty 43 25 18 train What was the magnitude of theearthquake earthquake in Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test How many people FoxNews, Washington Post were on board?David Beckham AFP 20 28 dev/test How long had Beckham been playing fortrade MU before he moved to RM?Miami airport Newstracker 12 15 dev/test How many concourses doesevacuation the airport have?US hurricane DUC d04a 14 14 test In which places had the hurricane landed?EgyptAir crash Novelty 4 25 29 test How many people were killed?Kursk submarine Novelty 33 25 30 test When did the Kursk sink?Hebrew University bombing Newstracker 11 27 test How many people were injured?Finland mall bombing Newstracker 9 15 test How many people were in the mall at the time of the bombing?Putin visits Newstracker 12 20 test What issue concerned BritishEngland human rights groups? Table 1: Corpus of complex news stories. and sentence numbers of the top 20 sentences. Thegold standard extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question. This measuregives us an idea of how far down we must look in theranked list in order to find a correct answer. To contrast, TRDR is the total of the reciprocal ranks of allanswers found by the system. In the context of answering questions from complex stories, where thereis often more than one correct answer to a question,and where answers are typically time-dependent, weshould focus on maximizing TRDR, which gives us 2For clusters annotated by two judges, all sentences chosenby at least one judge were included. a measure of how many of the relevant sentenceswere identified by the system. However, we reportboth the average MRR and TRDR over all questionsin a given data set.  In the training phase, we searched the parameterspace for the values of d (the question bias) and thesimilarity threshold in order to optimize the resultingTRDR scores. For our problem, we expected that arelatively low similarity threshold pair with a highquestion bias would achieve the best results. Table 2shows the effect of varying the similarity threshold.3 The notation LR[a, d] is used, where a is the similarity threshold and d is the question bias. The optimal range for the parameter a was between 0.14 and0.20. This is intuitive because if the threshold is toohigh, such that only the most lexically similar sentences are represented in the graph, the method doesnot find sentences that are related but are more lex3A threshold of -1 means that no threshold was used suchthat all sentences were included in the graph. 919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect of similarity threshold (a) on Ave. MRR and TRDR. System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect of question bias (d)on Ave. MRR and TRDR. ically diverse (e.g. paraphrases). Table 3 shows theeffect of varying the question bias at two differentsimilarity thresholds (0.02 and 0.20). It is clear that ahigh question bias is needed. However, a small probability for jumping to a node that is lexically similar to the given sentence (rather than the questionitself) is needed. Table 4 shows the configurationsof LexRank that performed better than the baselinesystem on the training data, based on mean TRDRscores over the 184 training questions. We appliedall four of these configurations to our unseen development/test data, in order to see if we could furtherdifferentiate their performances. 5.1 Development/testing phase. The scores for the four LexRank systems and thebaseline on the development/test data are shown in System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline in terms of TRDR score. System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation. Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores by cluster: baseline versusLR[0.20,0.95]. Table 5. This time, all four LexRank systems outperformed the baseline, both in terms of average MRRand TRDR scores. An analysis of the average scoresover the 72 questions within each of the three clusters for the best system, LR[0.20,0.95], is shownin Table 6. While LexRank outperforms the baseline system on the first two clusters both in termsof MRR and TRDR, their performances are not substantially different on the third cluster. Therefore,we examined properties of the questions within eachcluster in order to see what effect they might have onsystem performance.We hypothesized that the baseline system, which compares the similarity of each sentence to the question using IDF-weighted word overlap, should perform well on questions that provide many contentwords. To contrast, LexRank might perform better when the question provides fewer content words,since it considers both similarity to the query andinter-sentence similarity. Out of the 72 questions inthe development/test set, the baseline system outperformed LexRank on 22 of the questions. In fact, theaverage number of content words among these 22questions was slightly, but not significantly, higherthan the average on the remaining questions (3.63words per question versus 3.46). Given this observation, we experimented with two mixed strategies,in which the number of content words in a questiondetermined whether LexRank or the baseline systemwas used for sentence retrieval. We tried thresholdvalues of 4 and 6 content words, however, this didnot improve the performance over the pure strategyof system LR[0.20,0.95]. Therefore, we applied this 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95]. system versus the baseline to our unseen test set of134 questions. 5.2 Testing phase. As shown in Table 7, LR[0.20,0.95] outperformedthe baseline system on the test data both in termsof average MRR and TRDR scores. The improvement in average TRDR score was statistically significant with a p-value of 0.0619. Since we are interested in a passage retrieval mechanism that findssentences relevant to a given question, providing input to the question answering component of our system, the improvement in average TRDR score isvery promising. While we saw in Section 5.1 thatLR[0.20,0.95] may perform better on some questionor cluster types than others, we conclude that it beatsthe competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions. However, in future work, we will continueto improve the performance, perhaps by developing mixed strategies using different configurationsof LexRank.  The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some important sentences. When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of desired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties. An example of such a scenario is illustrated inTables 8 and 9, which show the top ranked sentencesby the baseline and LexRank, respectively for thequestion What caused the Kursk to sink? from theKursk submarine cluster. It can be seen that all topfive sentences chosen by the baseline system have Rank Sentence Score Relevant? 1 The Russian governmental commission on the 4.2282 Naccident of the submarine Kursk sinking inthe Barents Sea on August 12 has rejected11 original explanations for the disaster,but still cannot conclude what caused the. tragedy indeed, Russian Deputy Premier IlyaKlebanov said here Friday. 2 There has been no final word on what caused 4.2282 Nthe submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory. that Kursk may have collided with anotherobject is receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y said Thursday that collision with a bigobject caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea. 4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday that collision with a big. object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea. 5 President Clintons national security adviser, 4.2282 NSamuel Berger, has provided his Russian. counterpart with a written summary of whatU.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials said Wednesday. Table 8: Top ranked sentences using baseline systemon the question What caused the Kursk to sink?. the same sentence score (similarity to the query), yetthe top ranking two sentences are not actually relevant according to the judges. To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them. It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher.  We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval. In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary. As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question. While thefirst two sentences provide the same answer (a collision caused the Kursk to sink), the third sentenceprovides a different answer (an explosion caused thedisaster). While the last two sentences do not provide answers according to our judges, they do provide context information about the situation. Alternatively, the user might prefer to see the extracted 921 Rank Sentence Score Relevant? 1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big. object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea. 2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big. object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea. 3 The Russian navy refused to confirm this, 0.0125 Ybut officers have said an explosion in thetorpedo compartment at the front of the. submarine apparently caused the Kursk to sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, has provided his Russiancounterpart with a written summary of whatU.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials said Wednesday.5 There has been no final word on what caused 0.0123 N the submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory that Kursk may have collided with anotherobject is receiving increasingly concrete confirmation. Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question What causedthe Kursk to sink? answers from the retrieved sentences. In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing. As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003). In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores.  We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project.
 Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging  The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text. The model has the advantage that a pre-tagged training corpus is not required. Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model. State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models. The structure of the state chains is based on both an analysis of errors and linguistic knowledge. Examples show how word dependency across phrases can be modeled.  The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed "category"). Application areas include speech recognition/synthesis and information retrieval. Several workers have addressed the problem of tagging text. Methods have ranged from locally-operating rules (Greene and Rubin, 1971), to statistical methods (Church, 1989; DeRose, 1988; Garside, Leech and Sampson, 1987; Jelinek, 1985) and back-propagation (Benello, Mackie and Anderson, 1989; Nakamura and Shikano, 1989). The statistical methods can be described in terms of Markov models. States in a model represent categories {cl...c=} (n is the number of different categories used). In a first order model, Ci and Ci_l are random variables denoting the categories of the words at position i and (i - 1) in a text. The transition probability P(Ci = cz ] Ci_~ = %) linking two states cz and cy, represents the probability of category cx following category %. A word at position i is represented by the random variable Wi, which ranges over the vocabulary {w~ ...wv} (v is the number of words in the vocabulary). State-dependent probabilities of the form P(Wi = Wa ] Ci = cz) represent the probability that word Wa is seen, given category c~. For instance, the word "dog" can be seen in the states noun and verb, and only has a nonzero probability in those states. A word sequence is considered as being generated from an underlying sequence of categories. Of all the possible category sequences from which a given word sequence can be generated, the one which maximizes the probability of the words is used. The Viterbi algorithm (Viterbi, 1967) will find this category sequence. The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation. The Brown Corpus (Francis and Kucera, 1982) is a notable example of such a corpus, and is used by many of the systems cited above. An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a "hidden" Markov model: that is, only the words of the training text are available, their corresponding categories are not known. In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters. This has the great advantage of eliminating the pre-tagged corpus. It minimizes the resources required, facilitates experimentation with different word categories, and is easily adapted for use with other languages. The work described here also makes use of a hidden Markov model. One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions. In this regard, word equivalence classes were used (Kupiec, 1989). There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly. Thus the words "play" and "touch" are considered to behave identically, as members of the class noun-or-verb, and "clay" and "zinc"are members of the class noun. This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data. Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci). In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary. In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model. Obviously, a trade-off is involved. For example, "dog" is more likely to be a noun than a verb and "see" is more likely to be a verb than a noun. However they are both members of the equivalence class noun-or-verb, and so are considered to behave identically. It is then local word context (embodied in the transition probabilities) which must aid disambiguation of the word. In practice, word context provides significant constraint, so the trade-off appears to be a remarkably favorable one. The Basic Model The development of the model was guided by evaluation against a simple basic model (much of the development of the model was prompted by an analysis of the errors in its hehaviour). The basic model contained states representing the following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating and subordinating Preposition Adjective Including comparative and superlative Verb Uninflected Verb 3rd Pers. Sing. Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could not be found in dictionary. Lisp Used to tag common symbols in the the Lisp programming language (see below:) To-inf. "To" acting as an infinitive marker Sentence Boundary The above states were arranged in a first-order, fully connected network, each state having a transition to every other state, allowing all possible sequences of categories. The training corpus was a collection of electronic mail messages concerning the design of the Common-Lisp programming language -a somewhat less than ideal representation of English. Many Lisp-specific words were not in the vocabulary, and thus tagged as unknown, however the lisp category was nevertheless created for frequently occurring Lisp symbols in an attempt to reduce bias in the estimation. It is interesting to note that the model performs very well, despite such "noisy" training data. The training was sentence-based, and the model was trained using 6,000 sentences from the corpus. Eight iterations of the Baum-Welch algorithm were used. The implementation of the hidden Markov model is based on that of Rabiner, Levinson and Sondhi (1983). By exploiting the fact that the matrix of probabilities P(Eqvi I Ci) is sparse, a considerable improvement can be gained over the basic training algorithm in which iterations are made over all states. The initial values of the model parameters are calculated from word occurrence probabilities, such that words are initially assumed to function equally probably as any of their possible categories. Superlative and comparative adjectives were collapsed into a single adjective category, to economize on the overall number of categories. (If desired, after tagging the finer category can be replaced). In the basic model all punctuation except sentence boundaries was ignored. An interesting observation is worth noting with regard to words that can act both as auxiliary and main verbs. Modal auxiliaries were consistently tagged as auxiliary whereas the tagging for other auxiliaries (e.g. "is .... have" etc.) was more variable. This indicates that modal auxiliaries can be recognized as a natural class via their pattern of usage. Extending the Basic Model The basic model was used as a benchmark for successive improvements. The first addition was the correct treatment of all non-words in a text. This includes hyphenation, punctuation, numbers and abbreviations. New categories were added for number, abbreviation, and comma. All other punctuation was collapsed into the single new punctuation category. Refinement of Basic Categories The verb states of the basic model were found to be too coarse. For example, many noun/verb ambiguities in front of past participles were incorrectly tagged as verbs. The replacement of the auxiliary category by the following categories greatly improved this: Category Name Words included in Category Be be Been been Being being Have have Have* has, have, had, having be* is, am, are, was, were do* do, does, did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably. In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably. The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories. This leaves 50% of the corpus for training all the other equivalence classes. Editing the Transition Structure A common error in the basic model was the assignment of the word "to" to the to-infcategory ("to" acting as an infinitive marker) instead of preposition before noun phrases. This is not surprising, because "to" is the only member of the to-inf category, P(Wi = "to" [ Ci = to-in]) = 1.0. In contrast, P(Wi = "to" I Ci = preposition) = 0.086, because many other words share the preposition state. Unless transition probabilities are highly constraining, the higher probability paths will tend to go through the to-infstate. This situation may be addressed in several ways, the simplest being to initially assign zero transition probabilities from the to-infstate to states other than verbs and the adverb state. ADJECTIVE DETERMINER To all states NOUN in Basic Network "Transitions to  To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network. The lexical context available for modeling a word's category is solely the category of the preceding word (expressed via the transition probabilities P(Ci [ Ci1). Such limited context does not adequately model the constraint present in local word context. A straightforward method of extending the context is to use second-order conditioning which takes account of the previous two word categories. Transition probabilities are then of the form P(Ci [ Ci1, Ci2). For an n category model this requires n 3 transition probabilities. Increasing the order of the conditioning requires exponentially more parameters. In practice, models have been limited to second-order, and smoothing methods are normally required to deal with the problem of estimation with limited data. The conditioning just described is uniform- all possible two-category contexts are modeled. Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner). An alternative to uniformly increasing the order of the conditioning is to extend it selectively. Mixed higher- order context can be modeled by introducing explicit state sequences. In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency. The basic network is then augmented with the extra state sequences which model certain category sequences in more detail. The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network. As an example, we may consider a systematic error made by the basic model. It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner. The error is exemplified by the sentence fragment "The period of...", where "period" is tagged as an adjective. To model the context necessary to correct the error, two extra states are used, as shown in Figure 1. The "augmented network" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}). Training a hidden Markov model having this topology corrected all nine instances of the error in the test data. An important point to note is that improving the model detail in this manner does not forcibly correct the error. The actual patterns of category usage must be distinct in the language. 95 To complete the description of the augmented model it is necessary to mention tying of the model states (Jelinek and Mercer, 1980). Whenever a transition is made to a state, the state-dependent probability distribution P(Eqvi I Ci) is used to obtain the probability of the observed equivalence class. A state is generally used in several places (E.g. in Figure 1. there are two noun states, and two adjective states: one of each in the augmented network, and in the basic network). The distributions P(Eqvi I Ci) are considered to be the same for every instance of the same state. Their estimates are pooled and reassigned identically after each iteration of the Baum-Welch algorithm. Modeling Dependencies across Phrases Linguistic considerations can be used to correct errors made by the model. In this section two illustrations are given, concerning simple subject/verb agreement across an intermediate prepositional phrase. These are exemplified by the following sentence fragments: 1. "Temperatures in the upper mantle range apparently from....". 2. "The velocity of the seismic waves rises to...". The basic model tagged these sentences correctly, except for- "range" and "rises" which were tagged as noun and plural-noun respectively 1. The basic network cannot model the dependency of the number of the verb on its subject, which precedes it by a prepositional phrase. To model such dependency across the phrase, the networks shown in Figure 2 can be used. It can be seen that only simple forms of prepositional phrase are modeled in the networks; a single noun may be optionally preceded by a single adjective and/or determiner. The final transitions in the networks serve to discriminate between the correct and incorrect category assignment given the selected preceding context. As in the previous section, the corrections are not programmed into the model. Only context has been supplied to aid the training procedure, and the latter is responsible for deciding which alternative is more likely, based on the training data. (Approximately 19,000 sentences were used to train the networks used in this example). Discussion and Results In Figure 2, the two copies of the prepositional phrase are trained in separate contexts (preceding singu- lax/plural nouns). This has the disadvantage that they cannot share training data. This problem could be resolved by tying corresponding transitions together. Alternatively, investigation of a trainable grammar (Baker, 1979; Fujisaki et al., 1989) may be a fruitful way to further develop the model in terms of grammatical components. A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words). A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary. In the document, 142 words were tagged as unknown (their possible categories were not known). A total of 1,526 words had ambiguous categories (i.e. 40% of the document). Critical examination of the tagging provided by the augmented model showed 168 word tagging errors, whereas the basic model gave 215 erroneous word tags. The former represents 95.6% correct word tagging on the text as a whole (ignoring unknown words), and 89% on the ambiguous words. The performance of a tagging program depends on the choice and number of categories used, and the correct tag assignment for words is not always obvious. In cases where the choice of tag was unclear (as often occurs in idioms), the tag was ruled as incorrect. For example, 9 errors are from 3 instances of "... as well as ..." that arise in the text. It would be appropriate to deal with idioms separately, as done by Gaxside, Leech and Sampson (1987). Typical errors beyond the scope of the model described here are exemplified by incorrect adverbial and prepositional assignment. 1 It is easy to construct counterexamples to the sentences presented here, where the tagging would be correct. However, the training procedure affirms that counterexamples occur less frequently in the corpus than the cases shown here.. 96 NOUN PREPOSITION ADJECTIVE NO UN~ PLURAL NOUN PLURAL NOUN PREPOSITION A E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD. SINGULAR ALL STATES IN BASIC NETWORK NOT SHOWN Figure 2: Augmented Networks for Example of Subject/Verb Agreement For example, consider the word "up" in the following sentences: "He ran up a big bill". "He ran up a big hill". Extra information is required to assign the correct tagging. In these examples it is worth noting that even if a model was based on individual words, and trained on a pre-tagged corpus, the association of "up" (as adverb) with "bill" would not be captured by trigrams. (Work on phrasal verbs, using mutual information estimates (Church et ai., 1989b) is directly relevant to this problem). The tagger could be extended by further category refinements (e.g. inclusion of a gerund category), and the single pronoun category currently causes erroneous tags for adjacent words. With respect to the problem of unknown words, alternative category assignments for them could be made by using the context embodied in transition probabilities. A stochastic method for assigning part-of-speech categories to unrestricted English text has been described. It minimizes the resources required for high performance automatic tagging. A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text. It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.  I would like to thank Meg Withgott and Lanri Karttunen of Xerox PARC, for their helpful contributions to this work. I am also indebted to Sheldon Nicholl of the Univ. of Illinois, for his comments and valuable insight. This work was sponsored in part by the Defense Advanced Research Projects Agency (DOD), under the Information Science and Technology Office, contract #N0014086-C-8996.
 Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs  Automatic paraphrase discovery is an important but challenging task. We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue. We focus on phrases which connect two Named Entities (NEs), and proceed in two stages. The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets. The second stage links sets which involve the same pairs of individual NEs. A total of 13,976 phrases were grouped. The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%.  One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event. If the expression is a word or a short phrase (like âcorporationâ and âcompanyâ), it is called a âsynonymâ. There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet. If the expression is longer or complicated (like âA buys Bâ and âAâs purchase of Bâ), it is called âparaphraseâ, i.e. a set of phrases which express the same thing or event. Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications. For example, in Information Retrieval (IR), we have to match a userâs query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâs question even if the formulation of the answer in the document is different from the question. Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently. We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge. For example, we can easily imagine that the number of paraphrases for âA buys Bâ is enormous and it is not possible to create a comprehensive inventory by hand. Also, we donât know how many such paraphrase sets are necessary to cover even some everyday things or events. Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks. So, there is a limitation that IE can only be performed for a predefined task, like âcorporate mergersâ or âmanagement successionâ. In order to create an IE system for a new domain, one has to spend a long time to create the knowledge. So, it is too costly to make IE technology âopen- domainâ or âon-demandâ like IR or QA. In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.  2.1 Overview. Before explaining our method in detail, we present a brief overview in this subsection. First, from a large corpus, we extract all the NE instance pairs. Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, âIBM plans to acquire Lotusâ. For each pair we also record the context, i.e. the phrase between the two NEs (Step1). Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair. We use a simple TF/IDF method to measure the topicality of words. Hereafter, each pair of NE categories will be called a domain; e.g. the âCompany â Companyâ domain, which we will call CC- domain (Step 2). For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3). Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link âIBMâ and âLotusâ) (Step 4). As we shall see, most of the linked sets are paraphrases. This overview is illustrated in Figure 1. Corpus Step 1 NE pair instances Step 2 Step 1. Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus. The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger. The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004]. These 140 NE categories are designed by extending MUCâs 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object). All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the âcontextâ). Figure 2 shows examples of extracted NE pair instances and their contexts. The data is sorted based on the frequency of the context (âa unit ofâ appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. âNBCâ and âGeneral Electric Co.â appeared 10 times with the context âa unit ofâ). Step 2. Find keywords for each NE pair When we look at the contexts for each domain, we noticed that there is one or a few important words which indicate the relation between the NEs (for example, the word âunitâ for the phrase âa unit ofâ). Once we figure out the important word (e.g. keyword), we believe we can capture the meaning of the phrase by the keyword. We used the TF/ITF metric to identify keywords. keywords Step 3 Sets of phrases based on keywords Step 4 Links between sets of phrases All the contexts collected for a given domain are gathered in a bag and the TF/ITF scores are calculated for all the words except stopwords in the bag. Here, the term frequency (TF) is the frequency of a word in the bag and the inverse term frequency (ITF) is the inverse of the log of the frequency in the entire corpus. Figure 3 Figure 1. Overview of the method 2.2 Step by Step Algorithm. In this section, we will explain the algorithm step by step with examples. Because of their size, the examples (Figures 2 to 4) appear at the end of the paper. shows some keywords with their scores. Step 3. Gather phrases using keywords Next, we select a keyword for each phrase â the top-ranked word based on the TF/IDF metric. (If the TF/IDF score of that word is below a threshold, the phrase is discarded.) We then gather all phrases with the same keyword. Figure 4 shows some such phrase sets based on keywords in the CC-domain. Step 4. Cluster phrases based on Links We now have a set of phrases which share a keyword. However, there are phrases which express the same meanings even though they do not share the same keyword. For example, in Figure 3, we can see that the phrases in the âbuyâ, âacquireâ and âpurchaseâ sets are mostly paraphrases. At this step, we will try to link those sets, and put them into a single cluster. Our clue is the NE instance pairs. If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases. For example, the two NEs âEastern Group Plcâ and âHanson Plcâ have the following contexts. Here, âEGâ represents âEastern Group Plcâ. and âHâ represents âHanson Plcâ. x EG, has agreed to be bought by H x EG, now owned by H x H to acquire EG x Hâs agreement to buy EG Three of those phrases are actually paraphrases, but sometime there could be some noise; such as the second phrase above. So, we set a threshold that at least two examples are required to build a link. More examples are shown in Figure 5. Notice that the CC-domain is a special case. As the two NE categories are the same, we canât differentiate phrases with different orders of par ticipants â whether the buying company or the to-be-bought company comes first. The links can solve the problem. As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation. In figure 4, reverse relations are indicated by `*â next to the frequency. Now we have sets of phrases which share a keyword and we have links between those sets.  3.1 Corpora. For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995. They contain about 200M words (25M, 110M, 40M and 19M words, respectively). All the sentences have been analyzed by our chunker and NE tag- ger. The procedure using the tagged sentences to discover paraphrases takes about one hour on a 2GHz Pentium 4 PC with 1GB of memory. 3.2 Results. In this subsection, we will report the results of the experiment, in terms of the number of words, phrases or clusters. We will report the evaluation results in the next subsection. Step 1. Extract NE pair instances with contexts From the four years of newspaper corpus, we extracted 1.9 million pairs of NE instances. The most frequent NE category pairs are âPerson - Person (209,236), followed by âCountry - Coun- tryâ (95,123) and âPerson - Countryâ (75,509). The frequency of the Company â Company domain ranks 11th with 35,567 examples. As lower frequency examples include noise, we set a threshold that an NE category pair should appear at least 5 times to be considered and an NE instance pair should appear at least twice to be considered. This limits the number of NE category pairs to 2,000 and the number of NE pair instances to 0.63 million. Step 2. Find keywords for each NE pair The keywords are found for each NE category pair. For example, in the CC-domain, 96 keywords are found which have TF/ITF scores above a threshold; some of them are shown in Figure 3. It is natural that the larger the data in the domain, the more keywords are found. In the âPerson â Personâ domain, 618 keywords are found, and in the âCountry â Countryâ domain, 303 keywords are found. In total, for the 2,000 NE category pairs, 5,184 keywords are found. Step 3. Gather phrases using keywords Now, the keyword with the top TF/ITF score is selected for each phrase. If a phrase does not contain any keywords, the phrase is discarded. For example, out of 905 phrases in the CC- domain, 211 phrases contain keywords found in step 2. In total, across all domains, we kept 13,976 phrases with keywords. Step 4. Link phrases based on instance pairs Using NE instance pairs as a clue, we find links between sets of phrases. In the CC-domain, there are 32 sets of phrases which contain more than 2 phrases. We concentrate on those sets. Among these 32 sets, we found the following pairs of sets which have two or more links. Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances. buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct. We will describe the evaluation of such clusters in the next subsection. 3.3 Evaluation Results. We evaluated the results based on two metrics. One is the accuracy within a set of phrases which share the same keyword; the other is the accuracy of links. We picked two domains, the CC-domain and the âPerson â Companyâ domain (PC-domain), for the evaluation, as the entire system output was too large to evaluate. It is not easy to make a clear definition of âparaphraseâ. Sometimes extracted phrases by themselves are not meaningful to consider without context, but we set the following criteria. If two phrases can be used to express the same relationship within an information extraction application (âscenarioâ), these two phrases are paraphrases. Although this is not a precise criterion, most cases we evaluated were relatively clear-cut. In general, different modalities (âplanned to buyâ, âagreed to buyâ, âboughtâ) were considered to express the same relationship within an extraction setting. We did have a problem classifying some modified noun phrases where the modified phrase does not represent a qualified or restricted form of the head, like âchairmanâ and âvice chairmanâ, as these are both represented by the keyword âchairmanâ. In this specific case, as these two titles could fill the same column of an IE table, we regarded them as paraphrases for the evaluation. Evaluation within a set The evaluation of paraphrases within a set of phrases which share a keyword is illustrated in Figure 4. For each set, the phrases with bracketed frequencies are considered not paraphrases in the set. For example, the phrase â's New York-based trust unit,â is not a paraphrase of the other phrases in the âunitâ set. As you can see in the figure, the accuracy for the domain is quite high except for the âagreeâ set, which contains various expressions representing different relationships for an IE application. The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set. The results, along with the total number of phrases, are shown in Table 1. D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1. Evaluation results within sets Table 1 shows the evaluation result based on the number of phrases in a set. The larger sets are more accurate than the small sets. We can make several observations on the cause of errors. One is that smaller sets sometime have meaningless keywords, like âstrengthâ or âaddâ in the CC-domain, or âcompareâ in the PC-domain. Eight out of the thirteen errors in the high frequency phrases in the CC-domain are the phrases in âagreeâ. As can be seen in Figure 3, the phrases in the âagreeâ set include completely different relationships, which are not paraphrases. Other errors include NE tagging errors and errors due to a phrase which includes other NEs. For example, in the phrase âCompany-A last week purchased rival Marshalls from Company-Bâ, the purchased company is Marshalls, not Company-B. Also there are cases where one of the two NEs belong to a phrase outside of the relation. For example, from the sentence âMr. Smith estimates Lotus will make a profit this quarterâ¦â, our system extracts âSmith esti mates Lotusâ as an instance. Obviously âLotusâ is part of the following clause rather than being the object of âestimatesâ and the extracted instance makes no sense. We will return to these issues in the discussion section. Evaluation of links A link between two sets is considered correct if the majority of phrases in both sets have the same meaning, i.e. if the link indicates paraphrase. All the links in the âCC-domain are shown in Step 4 in subsection 3.2. Out of those 15 links, 4 are errors, namely âbuy - payâ, âacquire - payâ, âpurchase - stakeâ âacquisition - stakeâ. When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event. The similar explanation applies to the link to the âstakeâ set. We checked whether the discovered links are listed in WordNet. Only 2 link in the CC- domain (buy-purchase, acquire-acquisition) and 2 links (trader-dealer and head-chief) in the PC- domain are found in the same synset of Word- Net 2.1 (http://wordnet.princeton.edu/). This result suggests the benefit of using the automatic discovery method. D o m ai n Li n k ac cu ra cy W N c o v e r a g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2. Evaluation results for links  The work reported here is closely related to [Ha- segawa et al. 04]. First, we will describe their method and compare it with our method. They first collect the NE instance pairs and contexts, just like our method. However, the next step is clearly different. They cluster NE instance pairs based on the words in the contexts using a bag- of-words method. In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30. Because of this threshold, very few NE instance pairs could be used and hence the variety of phrases was also limited. Instead, we focused on phrases and set the frequency threshold to 2, and so were able to utilize a lot of phrases while minimizing noise. [Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results. The number of NE instance pairs used in their experiment is less than half of our method. There have been other kinds of efforts to discover paraphrase automatically from corpora. One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01]. The availability of comparable corpora is limited, which is a significant limitation on the approach. Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01]. This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited. There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02]. The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs. This can be repeated several times to collect a list of author / book title pairs and expressions. However, those methods need initial seeds, so the relation between entities has to be known in advance. This limitation is the obstacle to making the technology âopen domainâ.  Keywords with more than one word In the evaluation, we explained that âchairmanâ and âvice chairmanâ are considered paraphrases. However, it is desirable if we can separate them. This problem arises because our keywords consist of only one word. Sometime, multiple words are needed, like âvice chairmanâ, âprime ministerâ or âpay forâ (âpayâ and âpay forâ are different senses in the CC-domain). One possibility is to use n-grams based on mutual information. If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate. Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain. As was explained in the results section, âstrengthâ or âaddâ are not desirable keywords in the CC-domain. In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords. Also, âagreeâ in the CC-domain is not a desirable keyword. It is a relatively frequent word in the domain, but it can be used in different extraction scenarios. In this domain the major scenarios involve the things they agreed on, rather than the mere fact that they agreed. âAgreeâ is a subject control verb, which dominates another verb whose subject is the same as that of âagreeâ; the latter verb is generally the one of interest for extraction. We have checked if there are similar verbs in other major domains, but this was the only one. Using structural information As was explained in the results section, we extracted examples like âSmith estimates Lotusâ, from a sentence like âMr. Smith estimates Lotus will make profit this quarterâ¦â. In order to solve this problem, a parse tree is needed to understand that âLotusâ is not the object of âestimatesâ. Chunking is not enough to find such relationships. This remains as future work. Limitations There are several limitations in the methods. The phrases have to be the expressions of length less than 5 chunks, appear between two NEs. Also, the method of using keywords rules out phrases which donât contain popular words in the domain. We are not claiming that this method is almighty. Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases. Applications The discovered paraphrases have multiple applications. One obvious application is information extraction. In IE, creating the patterns which express the requested scenario, e.g. âmanagement successionâ or âcorporate merger and acquisitionâ is regarded as the hardest task. The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set. Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically. While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.  We proposed an unsupervised method to discover paraphrases from a large untagged corpus. We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications. After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue. In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets. The accuracies for link were 73% and 86% on two evaluated domains. These results are promising and there are several avenues for improving on these results.  This research was supported in part by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection, Extraction and Summarization (TIDES) program, under Grant N66001001-18917 from the Space and Naval Warfare Systems Center, San Diego, and by the National Science Foundation under Grant IIS00325657. This paper does not necessarily reflect the position of the U.S. Government. We would like to thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa and Mr. Yusuke Shinyama for useful comments, discussion and evaluation.
 Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech  We describe a statistical approach for modeling dialogue acts in conversational speech, i.e., speech- act-like units such as STATEMENT, QUESTION, BACKCHANNEL, AGREEMENT, DISAGREEMENT, and APOLOGY. Our model detects and predicts dialogue acts based on lexical, collocational, and prosodic cues, as well as on the discourse coherence of the dialogue act sequence. The dialogue model is based on treating the discourse structure of a conversation as a hidden Markov model and the individual dialogue acts as observations emanating from the model states. Constraints on the likely sequence of dialogue acts are modeled via a dialogue act n-gram. The statistical dialogue grammar is combined with word n-grams, decision trees, and neural networks modeling the idiosyncratic lexical and prosodic manifestations of each dialogue act. We develop a probabilistic integration of speech recognition with dialogue modeling, to improve both speech recognition and dialogue act classification accuracy. Models are trained and evaluated using a large hand-labeled database of 1,155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We achieved good dialogue act labeling accuracy (65% based on errorful, automatically recognized words and prosody, and 71% based on word transcripts, compared to a chance baseline accuracy of 35% and human accuracy of 84%) and a small reduction in word recognition error.  Speech Technology and Research Laboratory, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025, 1650-8592544. Email: stolcke@speech.sri.com. @ 2000 Association for Computational Linguistics Table 1 Fragment of a labeled conversation (from the Switchboard corpus). Speaker Dialogue Act Utterance A YEs-No-QuESTION So do you go to college right now? A ABANDONED Are yo-, B YES- ANSWER Yeah, B STATEMENT it's my last year [laughter]. A DECLARATIVE-QUESTION You're a, so you're a senior now. B YEs-ANSWER Yeah, B STATEMENT I'm working on my projects trying to graduate [laughter]. A APPRECIATION Oh, good for you. B BACKCHANNEL Yeah. A APPRECIATION That's great, A YEs-No-QUESTION um, is, is N C University is that, uh, State, B STATEMENT N C State. A SIGNAL-NoN-UNDERSTANDING What did you say? B STATEMENT N C State.  The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialogue. While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs). A DA represents the meaning of an utterance at the level of illocutionary force (Austin 1962). Thus, a DA is approximately the equivalent of the speech act of Searle (1969), the conversational game move of Power (1979), or the adjacency pair part of Schegloff (1968) and Saks, Schegloff, and Jefferson (1974). Table 1 shows a sample of the kind of discourse structure in which we are interested. Each utterance is assigned a unique DA label (shown in column 2), drawn from a well-defined set (shown in Table 2). Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria. The computational community has usually defined these DA categories so as to be relevant to a particular application, although efforts are under way to develop DA labeling systems that are domain-independent, such as the Discourse Resource Initiative's DAMSL architecture (Core and Allen 1997). While not constituting dialogue understanding in any deep sense, DA tagging seems clearly useful to a range of applications. For example, a meeting summarizer needs to keep track of who said what to whom, and a conversational agent needs to know whether it was asked a question or ordered to do something. In related work DAs are used as a first processing step to infer dialogue games (Carlson 1983; Levin and Moore 1977; Levin et al. 1999), a slightly higher level unit that comprises a small number of DAs. Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand. In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words. Another important role of DA information could be feedback to lower-level processing. For example, a speech recognizer could be constrained by expectations of likely DAs in a given context, constraining the potential recognition hypotheses so as to improve accuracy. Table 2 The 42 dialogue act labels. DA frequencies are given as percentages of the total number of utterances in the overall corpus. Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I'm in the legal department. 36% Uh-huh. 19% I think it's great 13% So, -/ 6% That's exactly it. 5% I can imagine. 2% Do you have to have any special training? 2% <Laughter>, < Throat_clearing> 2% Yes. 1% Well, it's been nice talking to you. 1% What did you wear to work today? 1% No. 1% Oh, okay. 1% I don't know if I'm making any sense or not. 1% So you can afford to get a house? 1% Well give me a break, you know. 1% Is that right? 1% You can't be pregnant and have cats .5% Oh, you mean you switched schools for the kids. .5% It is. .4% Why don't you go first .4% Who aren't contributing. .4% Oh, fajitas .3% How about you ? .3% Who would steal a newspaper? .2% I'm drawing a blank. .3% Well, no .2% Uh, not a whole lot. .1% Excuse me? .1% I don't know .1% How are you? .1% or is it more of a company? .1% Well, not so much that. .1% My goodness, Diane, get down from there. .1% I'I1 have to check that out .1% What's the word I'm looking for .1% That's all right. .1% Something like that <.1% Right? <.1% You are what kind of buff? <.1% I'm sorry. <.1% Hey thanks a lot <.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods. In doing so, we will pull together previous approaches as well as new ideas. For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7). However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence. For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity. We will present methods in a domain-independent framework that for the most part treats DA labels as an arbitrary formal tag set. Throughout the presentation, we will highlight the simplifications and assumptions made to achieve tractable models, and point out how they might fall short of reality. Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech. These results, besides validating the methods described, are of interest for several reasons. For example, unlike in most previous work on DA labeling, the corpus is not task-oriented in nature, and the amount of data used (198,000 utterances) exceeds that in previous studies by at least an order of magnitude (see Table 14). To keep the presentation interesting and concrete, we will alternate between the description of general methods and empirical results. Section 2 describes the task and our data in detail. Section 3 presents the probabilistic modeling framework; a central component of this framework, the discourse grammar, is further discussed in Section 4. In Section 5 we describe experiments for DA classification. Section 6 shows how DA models can be used to benefit speech recognition. Prior and related work is summarized in Section 7. Further issues and open problems are addressed in Section 8, followed by concluding remarks in Section 9.  The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium. Each conversation involved two randomly selected strangers who had been charged with talking informally about one of several, self- selected general-interest topics. To train our statistical models on this corpus, we combined an extensive effort in human hand-coding of DAs for each utterance, with a variety of automatic and semiautomatic tools. Our data consisted of a substantial portion of the Switchboard waveforms and corresponding transcripts, totaling 1,155 conversations. 2.1 Utterance Segmentation. Before hand-labeling each utterance in the corpus with a DA, we needed to choose an utterance segmentation, as the raw Switchboard data is not segmented in a linguistically consistent way. To expedite the DA labeling task and remain consistent with other Switchboard-based research efforts, we made use of a version of the corpus that had been hand-segmented into sentence-level units prior to our own work and independently of our DA labeling system (Meteer et al. 1995). We refer to the units of this segmentation as utterances. The relation between utterances and speaker turns is not one-to-one: a single turn can contain multiple utterances, and utterances can span more than one turn (e.g., in the case of backchanneling by the other speaker in midutterance). Each utterance unit was identified with one DA, and was annotated with a single DA label. The DA labeling system had special provisions for rare cases where utterances seemed to combine aspects of several DA types. Automatic segmentation of spontaneous speech is an open research problem in its own right (Mast et al. 1996; Stolcke and Shriberg 1996). A rough idea of the difficulty of the segmentation problem on this corpus and using the same definition of utterance units can be derived from a recent study (Shriberg et al. 2000). In an automatic labeling of word boundaries as either utterance or nonboundaries using a combination of lexical and prosodic cues, we obtained 96% accuracy based on correct word transcripts, and 78% accuracy with automatically recognized words. The fact that the segmentation and labeling tasks are interdependent (Warnke et al. 1997; Finke et al. 1998) further complicates the problem. Based on these considerations, we decided not to confound the DA classification task with the additional problems introduced by automatic segmentation and assumed the utterance-level segmentations as given. An important consequence of this decision is that we can expect utterance length and acoustic properties at utterance boundaries to be accurate, both of which turn out to be important features of DAs (Shriberg et al. 1998; see also Section 5.2.1). 2.2 Tag Set. We chose to follow a recent standard for shallow discourse structure annotation, the Dialog Act Markup in Several Layers (DAMSL) tag set, which was designed by the natural language processing community under the auspices of the Discourse Resource Initiative (Core and Allen 1997). We began with the DAMSL markup system, but modified it in several ways to make it more relevant to our corpus and task. DAMSL aims to provide a domain-independent framework for dialogue annotation, as reflected by the fact that our tag set can be mapped back to DAMSL categories (Jurafsky, Shriberg, and Biasca 1997). However, our labeling effort also showed that content- and task-related distinctions will always play an important role in effective DA labeling. The Switchboard domain itself is essentially "task-free," thus giving few external constraints on the definition of DA categories. Our primary purpose in adapting the tag set was to enable computational DA modeling for conversational speech, with possible improvements to conversational speech recognition. Because of the lack of a specific task, we decided to label categories that seemed inherently interesting linguistically and that could be identified reliably. Also, the focus on conversational speech recognition led to a certain bias toward categories that were lexically or syntactically distinct (recognition accuracy is traditionally measured including all lexical elements in an utterance). While the modeling techniques described in this paper are formally independent of the corpus and the choice of tag set, their success on any particular task will of course crucially depend on these factors. For different tasks, not all the techniques used in this study might prove useful and others could be of greater importance. However, we believe that this study represents a fairly comprehensive application of technology in this area and can serve as a point of departure and reference for other work. The resulting SWBDDAMSL tag set was multidimensional; approximately 50 basic tags (e.g., QUESTION, STATEMENT) could each be combined with diacritics indicating orthogonal information, for example, about whether or not the dialogue function of the utterance was related to Task-Management and Communication-Management. Approximately 220 of the many possible unique combinations of these codes were used by the coders (Jurafsky, Shriberg, and Biasca 1997). To obtain a system with somewhat higher interlabeler agreement, as well as enough data per class for statistical modeling purposes, a less fine-grained tag set was devised. This tag set distinguishes 42 mutually exclusive utterance types and was used for the experiments reported here. Table 2 shows the 42 categories with examples and relative frequencies. 1 While some 1 For the study focusing on prosodic modeling of DAs reported elsewhere (Shriberg et al. 1998), the tag set was further reduced to six categories.. of the original infrequent classes were collapsed, the resulting DA type distribution is still highly skewed. This occurs largely because there was no basis for subdividing the dominant DA categories according to task-independent and reliable criteria. The tag set incorporates both traditional sociolinguistic and discourse-theoretic notions, such as rhetorical relations and adjacency pairs, as well as some more form- based labels. Furthermore, the tag set is structured so as to allow labelers to annotate a Switchboard conversation from transcripts alone (i.e., without listening) in about 30 minutes. Without these constraints the DA labels might have included some finer distinctions, but we felt that this drawback was balanced by the ability to cover a large amount of data. 2 Labeling was carried out in a three-month period in 1997 by eight linguistics graduate students at CU Boulder. Interlabeler agreement for the 421abel tag set used here was 84%, resulting in a Kappa statistic of 0.80. The Kappa statistic measures agreement normalized for chance (Siegel and Castellan, Jr. 1988). As argued in Carletta (1996), Kappa values of 0.8 or higher are desirable for detecting associations between several coded variables; we were thus satisfied with the level of agreement achieved. (Note that, even though only a single variable, DA type, was coded for the present study, our goal is, among other things, to model associations between several instances of that variable, e.g., between adjacent DAs.) A total of 1,155 Switchboard conversations were labeled, comprising 205,000 utterances and 1.4 million words. The data was partitioned into a training set of 1,115 conversations (1.4M words, 198K utterances), used for estimating the various components of our model, and a test set of 19 conversations (29K words, 4K utterances). Remaining conversations were set aside for future use (e.g., as a test set uncompromised of tuning effects). 2.3 Major Dialogue Act Types. The more frequent DA types are briefly characterized below. As discussed above, the focus of this paper is not on the nature of DAs, but on the computational framework for their recognition; full details of the DA tag set and numerous motivating examples can be found in a separate report (Jurafsky, Shriberg, and Biasca 1997). Statements and Opinions. The most common types of utterances were STATEMENTS and OPINIONS. This split distinguishes "descriptive, narrative, or personal" statements (STATEMENT)from "other-directed opinion statements" (OPINION). The distinction was designed to capture the different kinds of responses we saw to opinions (which are often countered or disagreed with via further opinions) and to statements (which more often elicit continuers or backchannels): Dialogue Act Example Utterance STATEMENT Well, we have a cat, um, STATEMENT He's probably, oh, a good two years old, big, old, fat and sassy tabby. STATEMENT He's about five months old OPINION Well, rabbits are darling. OPINION I think it would be kind of stressful. 2 The effect of lacking acoustic information on labeling accuracy was assessed by relabeling a subset of the data with listening, and was found to be fairly small (Shriberg et al. 1998). A conservative estimate based on the relabeling study is that, for most DA types, at most 2% of the labels might have changed based on listening. The only DA types with higher uncertainty were BACKCHANNELS and AGREEMENTS, which are easily confused with each other without acoustic cues; here the rate of change was no more than 10%.. OPINIONS often include such hedges as I think, I believe, it seems, and I mean. We combined the STATEMENT and OPINION classes for other studies on dimensions in which they did not differ (Shriberg et al. 1998). Questions. Questions were of several types. The YES-No-QUESTION label includes only utterances having both the pragmatic force of a yes-no-question and the syntactic markings of a yes-no-question (i.e., subject-inversion or sentence-final tags). DECLARATIVE- QUESTIONS are utterances that function pragmatically as questions but do not have "question form." By this we mean that declarative questions normally have no wh-word as the argument of the verb (except in "echo-question" format), and have "declarative" word order in which the subject precedes the verb. See Weber (1993) for a survey of declarative questions and their various realizations. Dialogue Act Example Utterance YEs-No-QUESTION Do you have to have any special training? YEs-No-QUESTION But that doesn't eliminate it, does it? YEs-No-QuESTION Uh, I guess a year ago you're probably watching C N N a lot, right? DECLARATIVE- QUESTION So you're taking a government course? WH-QUESTION Well, how old are you? Backchannels. A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking. These are usually referred to in the conversation analysis literature as "continuers" and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970). We expect recognition of backchannels to be useful because of their discourse-structuring role (knowing that the hearer expects the speaker to go on talking tells us something about the course of the narrative) and because they seem to occur at certain kinds of syntactic boundaries; detecting a backchannel may thus help in predicting utterance boundaries and surrounding lexical material. For an intuition about what backchannels look like, Table 3 shows the most common realizations of the approximately 300 types (35,827 tokens) of backchannel in our Switchboard subset. The following table shows examples of backchannels in the context of a Switchboard conversation: Speaker Dialogue Act Utterance B STATEMENT but, uh, we're to the point now where our financial income is enough that we can consider putting some away - A BACKCHANNEL Uh-huh. / B STATEMENT -for college, / B STATEMENT so we are going to be starting a regular payroll deduction - A BACKCHANNEL Urn. / B STATEMENT -- in the fall / B STATEMENT and then the money that I will be making this summer we'll be putting away for the college fund. A APPRECIATION Urn. Sounds good. Turn Exits and Abandoned Utterances. Abandoned utterances are those that the speaker breaks off without finishing, and are followed by a restart. Turn exits resemble abandoned utterances in that they are often syntactically broken off, but they are used Table 3 Most common realizations of backchannels in Switchboard. Frequency Form Frequency Form Frequency Form 38% uh-huh 2% yes 1% sure 34% yeah 2% okay 1.% um 9% right 2% oh yeah 1% huh-uh 3% oh 1% huh 1% uh mainly as a way of passing speakership to the other speaker. Turn exits tend to be single words, often so or or. Speaker Dialogue Act Utterance A STATEMENT we're from, uh, I'm from Ohio / A STATEMENT and my wife's from Florida / A TURN-ExIT SO, -/ B BACKCHANNEL Uh-huh./ A HEDGE so, I don't know, / A ABANDONED it's Klipsmack>, -/ A STATEMENT I'm glad it's not the kind of problem I have to come up with an answer to because it's not - Answers and Agreements. YES-ANSWERS include yes, yeah, yep, uh-huh, and other variations on yes, when they are acting as an answer to a YES-NO-QUESTION or DECLARATWE-0UESTION. Similarly, we also coded NO-ANSWERS. Detecting ANSWERS can help tell us that the previous utterance was a YES-NO-QUESTION. Answers are also semantically significant since they are likely to contain new information. AGREEMENT/ACCEPT, REJECT, and MAYBE/ACCEPT-PARTall mark the degree to which a speaker accepts some previous proposal, plan, opinion, or statement. The most common of these are the AGREEMENT/AccEPTS. These are very often yes or yeah, so they look a lot like ANSWERS. But where ANSWERS follow questions, AGREEMENTS often follow opinions or proposals, so distinguishing these can be important for the discourse.  We will now describe the mathematical and computational framework used in our study. Our goal is to perform DA classification and other tasks using a probabilistic formulation, giving us a principled approach for combining multiple knowledge sources (using the laws of probability), as well as the ability to derive model parameters automatically from a corpus, using statistical inference techniques. Given all available evidence E about a conversation, the goal is to find the DA sequence U that has the highest posterior probability P(UIE ) given that evidence. Applying Bayes' rule we get U* = argmaxP(UIE ) U P(U)P(ElU) = argmax u P(E) = argmaxP(U)P(ElU) (1) U Here P(U) represents the prior probability of a DA sequence, and P(EIU ) is the like- Table 4 Summary of random variables used in dialogue modeling. (Speaker labels are introduced in Section 4.) Symbol Meaning U sequence of DA labels E evidence (complete speech signal) F prosodic evidence A acoustic evidence (spectral features used in ASR) W sequence of words T speakers labels lihood of U given the evidence. The likelihood is usually much more straightforward to model than the posterior itself. This has to do with the fact that our models are generative or causal in nature, i.e., they describe how the evidence is produced by the underlying DA sequence U. Estimating P (U) requires building a probabilistic discourse grammar, i.e., a statistical model of DA sequences. This can be done using familiar techniques from language modeling for speech recognition, although the sequenced objects in this case are DA labels rather than words; discourse grammars will be discussed in detail in Section 4. 3.1 Dialogue Act Likelihoods. The computation of likelihoods P(EIU ) depends on the types of evidence used. In our experiments we used the following sources of evidence, either alone or in combination: Transcribed words: The likelihoods used in Equation 1 are P(WIU ), where W refers to the true (hand-transcribed) words spoken in a conversation. Recognized words: The evidence consists of recognizer acoustics A, and we seek to compute P(A I U). As described later, this involves considering multiple alternative recognized word sequences. Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U). For ease of reference, all random variables used here are summarized in Table 4. The same variables are used with subscripts to refer to individual utterances. For example, Wi is the word transcription of the ith utterance within a conversation (not the ith word). To make both the modeling and the search for the best DA sequence feasible, we further require that our likelihood models are decomposable by utterance. This means that the likelihood given a complete conversation can be factored into likelihoods given the individual utterances. We use Ui for the ith DA label in the sequence U, i.e., U = (U1 ..... Ui,..., Un), where n is the number of utterances in a conversation. In addition, we use Ei for that portion of the evidence that corresponds to the ith utterance, e.g., the words or the prosody of the ith utterance. Decomposability of the likelihood means that P(EIU) = P(E11 U1)..... P(En [Un) (2) Applied separately to the three types of evidence Ai, Wi, and Fi mentioned above, it is clear that this assumption is not strictly true. For example, speakers tend to reuse E1 Ei E. T T T <start> , U1 , ... ~ Ui ) ...---* Un <end> Figure 1 The discourse HMM as Bayes network. words found earlier in the conversation (Fowler and Housum 1987) and an answer might actually be relevant to the question before it, violating the independence of the P(WilUi). Similarly, speakers adjust their pitch or volume over time, e.g., to the conversation partner or because of the structure of the discourse (Menn and Boyce 1982), violating the independence of the P(FilUi). As in other areas of statistical modeling, we count on the fact that these violations are small compared to the properties actually modeled, namely, the dependence of Ei on Ui. 3.2 Markov Modeling. Returning to the prior distribution of DA sequences P(U), it is convenient to make certain independence assumptions here, too. In particular, we assume that the prior distribution of U is Markovian, i.e., that each Ui depends only on a fixed number k of preceding DA labels: P(UilUl, . . ., Ui1) ~-P(UilUi-k ..... Ui1) (3) (k is the order of the Markov process describing U). The n-gram-based discourse grammars we used have this property. As described later, k = 1 is a very good choice, i.e., conditioning on the DA types more than one removed from the current one does not improve the quality of the model by much, at least with the amount of data available in our experiments. The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986). The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui). We can represent the dependency structure (as well as the implied conditional independences) as a special case of Bayesian belief network (Pearl 1988). Figure 1 shows the variables in the resulting HMM with directed edges representing conditional dependence. To keep things simple, a first-order HMM (bigram discourse grammar) is assumed. 3.3 Dialogue Act Decoding. The HMM representation allows us to use efficient dynamic programming algorithms to compute relevant aspects of the model, such as  the most probable DA sequence (the Viterbi algorithm)  the posterior probability of various DAs for a given utterance, after considering all the evidence (the forward-backward algorithm) The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probable state sequence. When applied to a discourse model with locally decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA sequence with the highest posterior probability: U* = argmaxP(UIE ) (4) u The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995). To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(UilE) for each i = 1 ..... n. We can compute the per-utterance posterior DA probabilities by summing: P(u[E) = E P(UIE) (5) U: Ui=u where the summation is over all sequences U whose ith element matches the label in question. The summation is efficiently carried out by the forward-backward algorithm for HMMs (Baum et al. 1970). 3 For zeroth-order (unigram) discourse grammars, Viterbi decoding and forward- backward decoding necessarily yield the same results. However, for higher-order discourse grammars we found that forward-backward decoding consistently gives slightly (up to 1% absolute) better accuracies, as expected. Therefore, we used this method throughout. The formulation presented here, as well as all our experiments, uses the entire conversation as evidence for DA classification. Obviously, this is possible only during off-line processing, when the full conversation is available. Our paradigm thus follows historical practice in the Switchboard domain, where the goal is typically the off-line processing (e.g., automatic transcription, speaker identification, indexing, archival) of entire previously recorded conversations. However, the HMM formulation used here also supports computing posterior DA probabilities based on partial evidence, e.g., using only the utterances preceding the current one, as would be required for online processing.  The statistical discourse grammar models the prior probabilities P(U) of DA sequences. In the case of conversations for which the identities of the speakers are known (as in Switchboard), the discourse grammar should also model turn-taking behavior. A straightforward approach is to model sequences of pairs (Ui, Ti) where Ui is the DA label and Ti represents the speaker. We are not trying to model speaker idiosyncrasies, so conversants are arbitrarily identified as A or B, and the model is made symmetric with respect to the choice of sides (e.g., by replicating the training sequences with sides switched). Our discourse grammars thus had a vocabulary of 42 x 2 = 84 labels, plus tags for the beginning and end of conversations. For example, the second DA tag in Table 1 would be predicted by a trigram discourse grammar using the fact that the same speaker previously uttered a YES-NO-QUESTION, which in turn was preceded by the start-of-conversation. 3 We note in passing that the Viterbi and Baum algorithms have equivalent formulations in the Bayes network framework (Pearl 1988). The HMM terminology was chosen here mainly for historical reasons.. Table 5 Perplexities of DAs with and without turn information. Discourse Grammar P(U) P(U, T) P(UIT ) None 42 84 42 Unigram 11.0 18.5 9.0 Bigram 7.9 10.4 5.1 Trigram 7.5 9.8 4.8 4.1 N-gram Discourse Models A computationally convenient type of discourse grammar is an n-gram model based on DA tags, as it allows efficient decoding in the HMM framework. We trained standard backoff n-gram models (Katz 1987), using the frequency smoothing approach of Witten and Bell (1991). Models of various orders were compared by their perplexities, i.e., the average number of choices the model predicts for each tag, conditioned on the preceding tags. Table 5 shows perplexities for three types of models: P(U), the DAs alone; P(U, T), the combined DA/speaker ID sequence; and P(UIT ), the DAs conditioned on known speaker IDs (appropriate for the Switchboard task). As expected, we see an improvement (decreasing perplexities) for increasing n-gram order. However, the incremental gain of a trigram is small, and higher-order models did not prove useful. (This observation, initially based on perplexity, is confirmed by the DA tagging experiments reported in Section 5.) Comparing P(U) and P(U[T),we see that speaker identity adds substantial information, especially for higher-order models. The relatively small improvements from higher-order models could be a result of lack of training data, or of an inherent independence of DAs from DAs further removed. The near-optimality of the bigram discourse grammar is plausible given conversation analysis accounts of discourse structure in terms of adjacency pairs (Schegloff 1968; Sacks, Schegloff, and Jefferson 1974). Inspection of bigram probabilities estimated from our data revealed that conventional adjacency pairs receive high probabilities, as expected. For example, 30% of YES-NO-QUESTIONS are followed by YES-ANSWERS, 14% by NO-ANSWERS (confirming that the latter are dispreferred). COMMANDS are followed by AGREEMENTS in 23% of the cases, and STATEMENTS elicit BACKCHANNELS in 26% of all cases. 4.2 Other Discourse Models. We also investigated non-n-gram discourse models, based on various language modeling techniques known from speech recognition. One motivation for alternative models is that n-grams enforce a one-dimensional representation on DA sequences, whereas we saw above that the event space is really multidimensional (DA label and speaker labels). Another motivation is that n-grams fail to model long-distance dependencies, such as the fact that speakers may tend to repeat certain DAs or patterns throughout the conversation. The first alternative approach was a standard cache model (Kuhn and de Mori 1990), which boosts the probabilities of previously observed unigrams and bigrams, on the theory that tokens tend to repeat themselves over longer distances. However, this does not seem to be true for DA sequences in our corpus, as the cache model showed no improvement over the standard N-gram. This result is somewhat surprising since unigram dialogue grammars are able to detect speaker gender with 63% accuracy (over a 50% baseline) on Switchboard (Ries 1999b), indicating that there are global variables in the DA distribution that could potentially be exploited by a cache dialogue grammar. Clearly, dialogue grammar adaptation needs further research. Second, we built a discourse grammar that incorporated constraints on DA sequences in a nonhierarchical way, using maximum entropy (ME) estimation (Berger, Della Pietra, and Della Pietra 1996). The choice of features was informed by similar ones commonly used in statistical language models, as well our general intuitions about potentially information-bearing elements in the discourse context. Thus, the model was designed so that the current DA label was constrained by features such as unigram statistics, the previous DA and the DA once removed, DAs occurring within a window in the past, and whether the previous utterance was by the same speaker. We found, however, that an ME model using n-gram constraints performed only slightly better than a corresponding backoff n-gram. Additional constraints such as DA triggers, distance-1 bigrams, separate encoding of speaker change and bigrams to the last DA on the same/other channel did not improve relative to the trigram model. The ME model thus confirms the adequacy of the backoff n-gram approach, and leads us to conclude that DA sequences, at least in the Switchboard domain, are mostly characterized by local interactions, and thus modeled well by low-order n-gram statistics for this task. For more structured tasks this situation might be different. However, we have found no further exploitable structure.  We now describe in more detail how the knowledge sources of words and prosody are modeled, and what automatic DA labeling results were obtained using each of the knowledge sources in turn. Finally, we present results for a combination of all knowledge sources. DA labeling accuracy results should be compared to a baseline (chance) accuracy of 35%, the relative frequency of the most frequent DA type (STATEMENT)in our test set. 4 5.1 Dialogue Act Classification Using Words. DA classification using words is based on the observation that different DAs use distinctive word strings. It is known that certain cue words and phrases (Hirschberg and Litman 1993) can serve as explicit indicators of discourse structure. Similarly, we find distinctive correlations between certain phrases and DA types. For example, 92.4% of the uh-huh's occur in BACKCHANNELS,and 88.4% of the trigrams "<start> do you" occur in YES-NO-QUESTIONS. To leverage this information source, without hand-coding knowledge about which words are indicative of which DAs, we will use statistical language models that model the full word sequences associated with each DA type. 5.1.1 Classification from True Words. Assuming that the true (hand-transcribed) words of utterances are given as evidence, we can compute word-based likelihoods P(WIU ) in a straightforward way, by building a statistical language model for each of the 42 DAs. All DAs of a particular type found in the training corpus were pooled, and a DA-specific trigram model was estimated using standard techniques (Katz backoff [Katz 1987] with Witten-Bell discounting [Witten and Bell 1991]). 4 The frequency of STATEMENTS across all labeled data was slightly different, cf. Table 2.. A1 T wl Ai T wi An w. <start> ~ T U1 ~ .... ~ T Ui ~ .... T Un ~ <end> Figure 2 Modified Bayes network including word hypotheses and recognizer acoustics. 5.1.2 Classification from Recognized Words. For fully automatic DA classification, the above approach is only a partial solution, since we are not yet able to recognize words in spontaneous speech with perfect accuracy. A standard approach is to use the 1-best hypothesis from the speech recognizer in place of the true word transcripts. While conceptually simple and convenient, this method will not make optimal use of all the information in the recognizer, which in fact maintains multiple hypotheses as well as their relative plausibilities. A more thorough use of recognized speech can be derived as follows. The classification framework is modified such that the recognizer's acoustic information (spectral features) A appear as the evidence. We compute P(A[U) by decomposing it into an acoustic likelihood P(A]W) and a word-based likelihood P(W[ U), and summing over all word sequences: P(AlU) = ~-~P(AIW, U)P(WIU) w = ~P(AIW)P(W[U ) (6) w The second line is justified under the assumption that the recognizer acoustics (typically, cepstral coefficients) are invariant to DA type once the words are fixed. Note that this is another approximation in our modeling. For example, different DAs with common words may be realized by different word pronunciations. Figure 2 shows the Bayes network resulting from modeling recognizer acoustics through word hypotheses under this independence assumption; note the added Wi variables (that have to be summed over) in comparison to Figure 1. The acoustic likelihoods P(A[W) correspond to the acoustic scores the recognizer outputs for every hypothesized word sequence W. The summation over all W must be approximated; in our experiments we summed over the (up to) 2,500 best hypotheses generated by the recognizer for each utterance. Care must be taken to scale the recognizer acoustic scores properly, i.e., to exponentiate the recognizer acoustic scores by 1/~, where A is the language model weight of the recognizer, s 5 In a standard recognizer the total log score of a hypothesis Wi is computed as. logP(AdWi ) + )~ logP(Wi) - I~]Wi], where [Wi]is the number of words in the hypothesis, and both A and/~ are parameters optimized to minimize the word error rate. The word insertion penalty/~ represents a correction to the language model that allows balancing insertion and deletion errors. The language model weight ,~ compensates for acoustic score variances that are effectively too large due to severe independence assumptions in the recognizer acoustic model. According to this rationale, it is more appropriate to divideall score components by ),. Thus, in all our experiments, we computed a summand in Equation 6 whose Table 6 DA classification accuracies (in %) from transcribed and recognized words (chance = 35%). Discourse Grammar True Recognized Relative Error Increase None 54.3 42.8 25.2% Unigram 68.2 61.8 20.1% Bigram 70.6 64.3 21.4% Trigram 71.0 64.8 21.4% 5.1.3 Results. Table 6 shows DA classification accuracies obtained by combining the word- and recognizer-based likelihoods with the n-gram discourse grammars described earlier. The best accuracy obtained from transcribed words, 71%, is encouraging given a comparable human performance of 84% (the interlabeler agreement, see Section 2.2). We observe about a 21% relative increase in classification error when using recognizer words; this is remarkably small considering that the speech recognizer used had a word error rate of 41% on the test set. We also compared the n-best DA classification approach to the more straightforward 1-best approach. In this experiment, only the single best recognizer hypothesis is used, effectively treating it as the true word string. The 1-best method increased classification error by about 7% relative to the n-best algorithm (61.5% accuracy with a bigram discourse grammar). 5.2 Dialogue Act Classification Using Prosody. We also investigated prosodic information, i.e., information independent of the words as well as the standard recognizer acoustics. Prosody is important for DA recognition for two reasons. First, as we saw earlier, word-based classification suffers from recognition errors. Second, some utterances are inherently ambiguous based on words alone. For example, some YES-NO-QUESTiONS have word sequences identical to those of STATEMENTS, but can often be distinguished by their final F0 rise. A detailed study aimed at automatic prosodic classification of DAs in the Switchboard domain is available in a companion paper (Shriberg et al. 1998). Here we investigate the interaction of prosodic models with the dialogue grammar and the word-based DA models discussed above. We also touch briefly on alternative machine learning models for prosodic features. 5.2.1 Prosodic Features. Prosodic DA classification was based on a large set of features computed automatically from the waveform, without reference to word or phone information. The features can be broadly grouped as referring to duration (e.g., utterance duration, with and without pauses), pauses (e.g., total and mean of nonspeech regions exceeding 100 ms), pitch (e.g., mean and range of F0 over utterance, slope of F0 regression line), energy (e.g., mean and range of RMS energy, same for signal-to- logarithm was -d1 logP(Ai]Wi) + logP(WilUi) -~lWil. We found this approach to give better results than the standard multiplication of logP(W) by ,L Note that for selecting the best hypothesis in a recognizer only the relative magnitudes of the score weights matter; however, for the summation in Equation 6 the absolute values become important. The parameter values for )~ and # were those used by the standard recognizer; they were not specifically optimized for the DA classification task. ~ 23.403 an utt < 0.3"/~U >= 0.3"/17 ~ Figure 3 Decision tree for the classification of BACKCHANNELS (B) and AGREEMENTS (A). Each node is labeled with the majority class for that node, as well as the posterior probabilities of the two classes. The following features are queried in the tree: number of frames in continuous (> 1 s) speech regions (cont_speech_frames), total utterance duration (ling_dir), utterance duration excluding pauses > 100 ms (ling_dur_minus_minlOpause), and mean signal-to-noise ratio (snr_mean_utt ). noise ratio [SNR]), speaking rate (based on the "enrate" measure of Morgan, Fosler, and Mirghafori [1997]), and gender (of both speaker and listener). In the case of utterance duration, the measure correlates both with length in words and with overall speaking rate. The gender feature that classified speakers as either male or female was used to test for potential inadequacies in F0 normalizations. Where appropriate, we included both raw features and values normalized by utterance and/or conversation. We also included features that are the output of the pitch accent and boundary tone event detector of Taylor (2000) (e.g., the number of pitch accents in the utterance). A complete description of prosodic features and an analysis of their usage in our models can be found in Shriberg et al. (1998). 5.2.2 Prosodic Decision Trees. For our Prosodic classifiers, we used CART-style decision trees (Breiman et al. 1984). Decision trees allow the combination of discrete and continuous features, and can be inspected to help in understanding the role of different features and feature combinations. To illustrate one area in which prosody could aid our classification task, we applied trees to DA classifications known to be ambiguous from words alone. One frequent example in our corpus was the distinction between BACKCHANNELS and AGREEMENTS (see Table 2), which share terms such as right and yeah. As shown in Figure 3, a prosodic tree trained on this task revealed that agreements have consistently longer durations and greater energy (as reflected by the SNR measure) than do backchannels. Table 7 DA classification using prosodic decision trees (chance = 35%). Discourse Grammar Accuracy (%) None 38.9 Unigram 48.3 Bigram 49.7 The HMM framework requires that we compute prosodic likelihoods of the form P(FilUi) for each utterance Ui and associated prosodic feature values Fi. We have the apparent difficulty that decision trees (as well as other classifiers, such as neural networks) give estimates for the posterior probabilities, P(Ui[Fi). The problem can be overcome by applying Bayes' rule locally: (7) true) P(Ui) Note that P(Fi) does not depend on Ui and can be treated as a constant for the purpose of DA classification. A quantity proportional to the required likelihood can therefore be obtained either by dividing the posterior tree probability by the prior P(Ui), 6 or by training the tree on a uniform prior distribution of DA types. We chose the second approach, downsampling our training data to equate DA proportions. This also counteracts a common problem with tree classifiers trained on very skewed distributions of target classes, i.e., that low-frequency classes are not modeled in sufficient detail because the majority class dominates the tree-growing objective hznction. 5.2.3 Results with Decision Trees. As a preliminary experiment to test the integration of prosody with other knowledge sources, we trained a single tree to discriminate among the five most frequent DA types (STATEMENT, BACKCHANNEL, OPINION, ABANDONED, and AGREEMENT,totaling 79% of the data) and an Other category comprising all remaining DA types. The decision tree was trained on a downsampled training subset containing equal proportions of these six DA classes. The tree achieved a classification accuracy of 45.4% on an independent test set with the same uniform six-class distribution. The chance accuracy on this set is 16.6%, so the tree clearly extracts useful information from the prosodic features. We then used the decision tree posteriors as scaled DA likelihoods in the dialogue model HMM, combining it with various n-gram dialogue grammars for testing on our full standard test set. For the purpose of model integration, the likelihoods of the Other class were assigned to all DA types comprised by that class. As shown in Table 7, the tree with dialogue grammar performs significantly better than chance on the raw DA distribution, although not as well as the word-based methods (cf. Table 6). 5.2.4 Neural Network Classifiers. Although we chose to use decision trees as prosodic classifiers for their relative ease of inspection, we might have used any suitable probabilistic classifier, i.e., any model that estimates the posterior probabilities of DAs given the prosodic features. We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here. Neural networks are worth investigating since they offer potential advantages over decision trees. They can learn decision surfaces that lie at an angle to the axes of the input feature space, unlike standard CART trees, which always split continuous features on one dimension at a time. The response function of neural networks is continuous (smooth) at the decision boundaries, allowing them to avoid hard decisions and the complete fragmentation of data associated with decision tree questions. Most important, however, related work (Ries 1999a) indicated that similarly structured networks are superior classifiers if the input features are words and are therefore a plugin replacement for the language model classifiers described in this paper. Neural networks are therefore a good candidate for a jointly optimized classifier of prosodic and word-level information since one can show that they are a generalization of the integration approach used here. We tested various neural network models on the same six-class downsampled data used for decision tree training, using a variety of network architectures and output layer functions. The results are summarized in Table 8, along with the baseline result obtained with the decision tree model. Based on these experiments, a softmax network (Bridle 1990) without hidden units resulted in only a slight improvement over the decision tree. A network with hidden units did not afford any additional advantage, even after we optimized the number of hidden units, indicating that complex combinations of features (as far as the network could learn them) do not predict DAs better than linear combinations of input features. While we believe alternative classifier architectures should be investigated further as prosodic models, the results so far seem to confirm our choice of decision trees as a model class that gives close to optimal performance for this task. 5.2.5 Intonation Event Likelihoods. An alternative way to compute prosodically based DA likelihoods uses pitch accents and boundary phrases (Taylor et al. 1997). The approach relies on the intuition that different utterance types are characterized by different intonational "tunes" (Kowtko 1996), and has been successfully applied to the classification of move types in the DCIEM Map Task corpus (Wright and Taylor 1997). The system detects sequences of distinctive pitch patterns by training one continuous- density HMM for each DA type. Unfortunately, the event classification accuracy on the Switchboard corpus was considerably poorer than in the Map Task domain, and DA recognition results when coupled with a discourse grammar were substantially worse than with decision trees. The approach could prove valuable in the future, however, if the intonation event detector can be made more robust to corpora like OURS. A1 Ai An T 1 1 Wl Wi W,, T T t <start> -, 0"1 , ...---* U/ , ... ~ Un , <end> 1 1 ,t F1 Fi G Figure 4 Bayes network for discourse HMM incorporating both word recognition and prosodic features. 5.3 Using Multiple Knowledge Sources. As mentioned earlier, we expect improved performance from combining word and prosodic information. Combining these knowledge sources requires estimating a combined likelihood P(Ai, Fi[Ui) for each utterance. The simplest approach is to assume that the two types of acoustic observations (recognizer acoustics and prosodic features) are approximately conditionally independent once Ui is given: P(ai, w,,Fdui) = P(A~, Wdui)e(Fifai, W~, Ui) ~, P(ai, Wi[Ui)P(FilUi) (8) Since the recognizer acoustics are modeled by way of their dependence on words, it is particularly important to avoid using prosodic features that are directly correlated with word identities, or features that are also modeled by the discourse grammars, such as utterance position relative to turn changes. Figure 4 depicts the Bayes network incorporating evidence from both word recognition and prosodic features. One important respect in which the independence assumption is violated is in the modeling of utterance length. While utterance length itself is not a prosodic feature, it is an important feature to condition on when examining prosodic characteristics of utterances, and is thus best included in the decision tree. Utterance length is captured directly by the tree using various duration measures, while the DA-specific LMs encode the average number of words per utterance indirectly through n-gram parameters, but still accurately enough to violate independence in a significant way (Finke et al. 1998). As discussed in Section 8, this problem is best addressed by joint lexical-prosodic models. We need to allow for the fact that the models combined in Equation 8 give estimates of differing qualities. Therefore, we introduce an exponential weight a on P(Fi[Ui) that controls the contribution of the prosodic likelihood to the overall likelihood. Finally, a second exponential weight fl on the combined likelihood controls its dynamic range relative to the discourse grammar scores, partially compensating for any correlation between the two likelihoods. The revised combined likelihood estimate thus becomes: P(Ai, Wi, FilUi) ~, {P(Ai, WilUi)P(Fi[Ui)~} ~ (9) In our experiments, the parameters a and fl were optimized using twofold jackknifing. The test data was split roughly in half (without speaker overlap), each half was used to separately optimize the parameters, and the best values were then tested on the respective other half. The reported results are from the aggregate outcome on the two test set halves. Table 9 Combined utterance classification accuracies (chance = 35%). The first two columns correspond to Tables 7 and 6, respectively. Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%). Classification Task True Words Recognized Words Knowledge Source QUESTIONS/STATEMENTS prosody only 76.0 76.0 words only 85.9 75.4 words+prosody 87.6 79.8 AGREEMENTS / BACKCHANNELS prosody only 72.9 72.9 words only 81.0 78.2 words+prosody 84.7 81.7 5.3.1 Results. In this experiment we combined the acoustic n-best likelihoods based on recognized words with the Top-5 tree classifier mentioned in Section 5.2.3. Results are summarized in Table 9. As shown, the combined classifier presents a slight improvement over the recognizer-based classifier, The experiment without discourse grammar indicates that the combined evidence is considerably stronger than either knowledge source alone, yet this improvement seems to be made largely redundant by the use of priors and the discourse grammar. For example, by definition DECLARATIVE-QUESTIONS are not marked by syntax (e.g., by subject-auxiliary inversion) and are thus confusable with STATEMENTS and OPINIONS. While prosody is expected to help disambiguate these cases, the ambiguity can also be removed by examining the context of the utterance, e.g., by noticing that the following utterance is a YEs-ANswER or NO-ANSWER. 5.3.2 Focused Classifications. To gain a better understanding of the potential for prosodic DA classification independent of the effects of discourse grammar and the skewed DA distribution in Switchboard, we examined several binary DA classification tasks. The choice of tasks was motivated by an analysis of confusions committed by a purely word-based DA detector, which tends to mistake QUESTIONS for STATEMENTS, and BACKCHANNELS for AGREEMENTS (and vice versa). We tested a prosodic classifier, a word-based classifier (with both transcribed and recognized words), and a combined classifier on these two tasks, downsampling the DA distribution to equate the class sizes in each case. Chance performance in all experiments is therefore 50%. Results are summarized in Table 10. As shown, the combined classifier was consistently more accurate than the classifier using words alone. Although the gain in accuracy was not statistically significant for the small recognizer test set because of a lack of power, replication for a larger hand-transcribed test set showed the gain to be highly significant for both subtasks by a Sign test, p < .001 and p < .0001 (one-tailed), respectively. Across these, as well as additional subtasks, the relative advantage of adding prosody was larger for recognized than for true words, suggesting that prosody is particularly helpful when word information is not perfect.  We now consider ways to use DA modeling to enhance automatic speech recognition (ASR). The intuition behind this approach is that discourse context constrains the choice of DAs for a given utterance, and the DA type in turn constrains the choice of words. The latter can then be leveraged for more accurate speech recognition. 6.1 Integrating DA Modeling and ASR. Constraints on the word sequences hypothesized by a recognizer are expressed probabilistically in the recognizer language model (LM). It provides the prior distribution P(Wi) for finding the a posteriori most probable hypothesized words for an utterance, given the acoustic evidence Ai (Bahl, Jelinek, and Mercer 1983): 7 W 7 = argmaxP(WilAi) wi P(Wi)P(AilWi) = argmax wi P(Ai) = argmaxP(Wi)P(AilWi) (10) wi The likelihoods P(AilWi) are estimated by the recognizer's acoustic model. In a standard recognizer the language model P(Wi) is the same for all utterances; the idea here is to obtain better-quality LMs by conditioning on the DA type Ui, since presumably the word distributions differ depending on DA type. W7 --argmaxP(WilAi, Ui) wi P( WilUi)P(AilWi, Ui) = argmax Wi P(AiIUi) argmaxP(WilUi)P(AirWi) (11) wi As before in the DA classification model, we tacitly assume that the words Wi depend only on the DA of the current utterance, and also that the acoustics are independent of the DA type if the words are fixed. The DA-conditioned language models P(Wil Ui) are readily trained from DA-specific training data, much as we did for DA classification from words. 8 7 Note the similarity of Equations 10 and 1. They are identical except for the fact that we are now. operating at the level of an individual utterance, the evidence is given by the acoustics, and the targets are word hypotheses instead of DA hypotheses. 8 In Equation 11 and elsewhere in this section we gloss over the issue of proper weighting of model. probabilities, which is extremely important in practice. The approach explained in detail in footnote 5 applies here as well. The problem with applying Equation 11, of course, is that the DA type Ui is generally not known (except maybe in applications where the user interface can be engineered to allow only one kind of DA for a given utterance). Therefore, we need to infer the likely DA types for each utterance, using available evidence E from the entire conversation. This leads to the following formulation: W~ = argmaxP(WilAi, E) wi ---- argmax ~-~ P(WilAi, Ui, E)P(UilE) Wi Ui argmax ~[] P( WiiAi, Ui)P( Ui[E) (12) W~ U~ The last step in Equation 12 is justified because, as shown in Figures 1 and 4, the evidence E (acoustics, prosody, words) pertaining to utterances other than i can affect the current utterance only through its DA type Ui. We call this the mixture-of-posteriorsapproach, because it amounts to a mixture of the posterior distributions obtained from DA-specific speech recognizers (Equation 11), using the DA posteriors as weights. This approach is quite expensive, however, as it requires multiple full recognizer or rescoring passes of the input, one for each DA type. A more efficient, though mathematically less accurate, solution can be obtained by combining guesses about the correct DA types directly at the level of the LM. We estimate the distribution of likely DA types for a given utterance using the entire conversation E as evidence, and then use a sentence-level mixture (Iyer, Ostendorf, and Rohlicek 1994) of DA-specific LMs in a single recognizer run. In other words, we replace P(WilUi) in Equation 11 with ~_~ P(WilUi)P(Ui]E), ui a weighted mixture of all DA-specific LMs. We call this the mixture-of-LMs approach. In practice, we would first estimate DA posteriors for each utterance, using the forward-backward algorithm and the models described in Section 5, and then rerecognize the conversation or rescore the recognizer output, using the new posterior- weighted mixture LM. Fortunately, as shown in the next section, the mixture-of-LMs approach seems to give results that are almost identical to (and as good as) the mixture- of-posteriors approach. 6.2 Computational Structure of Mixture Modeling. It is instructive to compare the expanded scoring formulas for the two DA mixture modeling approaches for ASK The mixture-of-posteriors approach yields (13) P(WilAi, E) = ~ P(ailui) ui whereas the mixture-of-LMs approach gives ) P(A,Iw,) P(WilAi'E) ~ P(WiIUi)P(UilE) P(Ai) (14) Table 11 Switchboard word recognition error rates and LM perplexities. Model WER (%) Perplexity Baseline 41.2 76.8 1-best LM 41.0 69.3 Mixture-of-posteriors 41.0 n/a Mixture-of-LMs 40.9 66.9 Oracle LM 40.3 66.8 We see that the second equation reduces to the first under the crude approximation P(Ai] Ui) ~ P(Ai). In practice, the denominators are computed by summing the numerators over a finite number of word hypotheses Wi, so this difference translates into normalizing either after or before summing over DAs. When the normalization takes place as the final step it can be omitted for score maximization purposes; this shows why the mixture-of-LMs approach is less computationally expensive. 6.3 Experiments and Results. We tested both the mixture-of-posteriors and the mixture-of-LMs approaches on our Switchboard test set of 19 conversations. Instead of decoding the data from scratch using the modified models, we manipulated n-best lists consisting of up to 2,500 best hypotheses for each utterance. This approach is also convenient since both approaches require access to the full word string for hypothesis scoring; the overall model is no longer Markovian, and is therefore inconvenient to use in the first decoding stage, or even in lattice rescoring. The baseline for our experiments was obtained with a standard backoff trigram language model estimated from all available training data. The DA-specific language models were trained on word transcripts of all the training utterances of a given type, and then smoothed further by interpolating them with the baseline LM. Each DA- specific LM used its own interpolation weight, obtained by minimizing the perplexity of the interpolated model on held-out DA-specific training data. Note that this smoothing step is helpful when using the DA-specific LMs for word recognition, but not for DA classification, since it renders the DA-specific LMs less discriminative. 9 Table 11 summarizes both the word error rates achieved with the various models and the perplexities of the corresponding LMs used in the rescoring (note that perplexity is not meaningful in the mixture-of-posteriors approach). For comparison, we also included two additional models: the 'q-best LM" refers to always using the DA- specific LM corresponding to the most probable DA type for each utterance. It is thus an approximation to both mixture approaches where only the top DA is considered. Second, we included an "oracle LM," i.e., always using the LM that corresponds to the hand-labeled DA for each utterance. The purpose of this experiment was to give us an upper bound on the effectiveness of the mixture approaches, by assuming perfect DA recognition. It was somewhat disappointing that the word error rate (WER) improvement in the oracle experiment was small (2.2% relative), even though statistically highly significant (p < .0001, one-tailed, according to a Sign test on matched utterance pairs). 9 Indeed, during our DA classification experiments, we had observed that smoothed DA-specific LMs yield lower classification accuracy.. the DA-specific LMs. A more detailed analysis of the effect of DA modeling on speech recognition errors can be found elsewhere (Van EssDykema and Ries 1998). In summary, our experiments confirmed that DA modeling can improve word recognition accuracy quite substantially in principle, at least for certain DA types, but that the skewed distribution of DAs (especially in terms of number of words per type) limits the usefulness of the approach on the Switchboard corpus. The benefits of DA modeling might therefore be more pronounced on corpora with more even DA distribution, as is typically the case for task-oriented dialogues. Task-oriented dialogues might also feature specific subtypes of general DA categories that might be constrained by discourse. Prior research on task-oriented dialogues summarized in the next section, however, has also found only small reductions in WER (on the order of 1%). This suggests that even in task-oriented domains more research is needed to realize the potential of DA modeling for ASR.  As indicated in the introduction, our work builds on a number of previous efforts in computational discourse modeling and automatic discourse processing, most of which occurred over the last half-decade. It is generally not possible to directly compare quantitative results because of vast differences in methodology, tag set, type and amount of training data, and, principally, assumptions made about what information is available for "free" (e.g., hand-transcribed versus automatically recognized words, or segmented versus unsegmented utterances). Thus, we will focus on the conceptual aspects of previous research efforts, and while we do offer a summary of previous quantitative results, these should be interpreted as informative datapoints only, and not as fair comparisons between algorithms. Previous research on DA modeling has generally focused on task-oriented dialogue, with three tasks in particular garnering much of the research effort. The Map Task corpus (Anderson et al. 1991; Bard et al. 1995) consists of conversations between two speakers with slightly different maps of an imaginary territory. Their task is to help one speaker reproduce a route drawn only on the other speaker's map, all without being able to see each other's maps. Of the DA modeling algorithms described below, Taylor et al. (1998) and Wright (1998) were based on Map Task. The VERBMOBIL corpus consists of two-party scheduling dialogues. A number of the DA m6deling algorithms described below were developed for VERBMOBIL, including those of Mast et al. (1996), Warnke et al. (1997), Reithinger et al. (1996), Reithinger and Klesen (1997), and Samuel, Carberry, and VijayShanker (1998). The ATR Conference corpus is a subset of a larger ATR Dialogue database consisting of simulated dialogues between a secretary and a questioner at international conferences. Researchers using this corpus include Nagata (1992), Nagata and Morimoto (1993, 1994), and Kita et al. (1996). Table 13 shows the most commonly used versions of the tag sets from those three tasks. As discussed earlier, these domains differ from the Switchboard corpus in being task-oriented. Their tag sets are also generally smaller, but some of the same problems of balance occur. For example, in the Map Task domain, 33% of the words occur in 1 of the 12 DAs 0NSTRUCT). Table 14 shows the approximate size of the corpora, the tag set, and tag estimation accuracy rates for various recent models of DA prediction. The results summarized in the table also illustrate the differences in inherent difficulty of the tasks. For example, the task of Warnke et al. (1997) was to simultaneously segment and tag DAs, whereas the other results rely on a prior manual segmentation. Similarly, the task in Wright (1998) and in our study was to determine DA types from speech input, whereas work by others is based on hand-transcribed textual input. Table 13 Dialogue act tag sets used in three other extensively studied corpora. VERBMOBIL. These 18 high-level DAs used in VERBMOBIL1 are abstracted over a total of 43 more specific DAs; most experiments on VERBMOBIL DAs use the set of 18 rather than 43. Examples are from Jekat et al. (1995). Tag Example THANK Thanks GREET Hello Dan INTRODUCE It's me again BYE Alright bye REQUEST~COMMENT How does that look? SUGGEST from thirteenth through seventeenth June REJECT No Friday I'm booked all day ACCEPT Saturday sounds fine, REQUEST-SUGGEST What is a good day of the week for you? INIT I wanted to make an appointment with you GIVE_REASON Because I have meetings all afternoon FEEDBACK Okay DELIBERATE Let me check my calendar here CONFIRM Okay, that would be wonderful CLARIFY Okay, do you mean Tuesday the 23rd? DIGRESS [we could meet for lunch] and eat lots of ice cream MOTIVATE We should go to visit our subsidiary in Munich GARBAGE Oops, I- Maptask. The 12 DAs or "move types" used in Map Task. Examples are from Taylor et al. (1998). Tag Example INSTRUCT Go round, ehm horizontally underneath diamond mine EXPLAIN I don't have a ravine ALIGN Okay? CHECK So going down to Indian Country? QUERY-YN Have you got the graveyard written down ? QUERY-W In where? ACKNOWLEDGE Okay CLARIFY {you want to go... diagonally} Diagonally down REPLY-Y I do. REPLY-N No, I don't REPLY-W {And across to?} The pyramid. READY Okay ATR. The 9 DAs ("illocutionary force types") used in the ATR Dialogue database task; some later models used an extended set of 15 DAs. Examples are from the English translations given by Nagata (1992). Tag Example PHATIC Hello EXPRESSIVE Thank you RESPONSE That's right PROMISE I will send you a registration form REQUEST Please go to Kitaooji station by subway INFORM We are not giving any discount this time QUESTIONIP Do you have the announcement of the conference ? QUESTIONREF What should I do? QUESTIONCONF You have already transferred the registration fee, right ? C ~'~ % .~ o > to .~~ ~ ~ °°ll ~.~ ~~ g,..l m m ~ c~8,~.~ c ~ ~ Z v ~m .~ K r--~ ~ , O', ¢~ ,-~ ,.0 NS~ The use of n-grams to model the probabilities of DA sequences, or to predict upcoming DAs online, has been proposed by many authors. It seems to have been first employed by Nagata (1992), and in follow-up papers by Nagata and Morimoto (1993, 1994) on the ATR Dialogue database. The model predicted upcoming DAs by using bigrams and trigrams conditioned on preceding DAs, trained on a corpus of 2,722 DAs. Many others subsequently relied on and enhanced this n-grams-of-DAs approach, often by applying standard techniques from statistical language modeling. Reithinger et al. (1996), for example, used deleted interpolation to smooth the dialogue n-grams. ChuCarroll (1998) uses knowledge of subdialogue structure to selectively skip previous DAs in choosing conditioning for DA prediction. Nagata and Morimoto (1993, 1994) may also have been the first to use word n-grams as a miniature grammar for DAs, to be used in improving speech recognition. The idea caught on very quickly: Suhm and Waibel (1994), Mast et aL (1996), Warnke et al. (1997), Reithinger and Klesen (1997), and Taylor et al. (1998) all use variants of backoff, interpolated, or class n-gram language models to estimate DA likelihoods. Any kind of sufficiently powerful, trainable language model could perform this function, of course, and indeed Alexandersson and Reithinger (1997) propose using automatically learned stochastic context-free grammars. Jurafsky, Shriberg, Fox, and Curl (1998) show that the grammar of some DAs, such as appreciations, can be captured by finite-state automata over part-of-speech tags. N-gram models are likelihood models for DAs, i.e., they compute the conditional probabilities of the word sequence given the DA type. Word-based posterior probability estimators are also possible, although less common. Mast et al. (1996) propose the use of semantic classification trees, a kind of decision tree conditioned on word patterns as features. Finally, Ries (1999a) shows that neural networks using only unigram features can be superior to higher-order n-gram DA models. Warnke et al. (1999) and Ohler, Harbeck, and Niemann (1999) use related discriminative training algorithms for language models. Woszczyna and Waibel (1994) and Suhm and Waibel (1994), followed by ChuCarroll (1998), seem to have been the first to note that such a combination of word and dialogue n-grams could be viewed as a dialogue HMM with word strings as the observations. (Indeed, with the exception of Samuel, Carberry, and VijayShanker (1998), all models listed in Table 14 rely on some version of this HMM metaphor.) Some researchers explicitly used HMM induction techniques to infer dialogue grammars. Woszczyna and Waibel (1994), for example, trained an ergodic HMM using expectation-maximization to model speech act sequencing. Kita et al. (1996) made one of the few attempts at unsupervised discovery of dialogue structure, where a finite-state grammar induction algorithm is used to find the topology of the dialogue grammar. Computational approaches to prosodic modeling of DAs have aimed to automatically extract various prosodic parameters--such as duration, pitch, and energy patterns--from the speech signal (Yoshimura et al. [1996]; Taylor et al. [1997]; Kompe [1997], among others). Some approaches model F0 patterns with techniques such as vector quantization and Gaussian classifiers to help disambiguate utterance types. An extensive comparison of the prosodic DA modeling literature with our work can be found in Shriberg et al. (1998). DA modeling has mostly been geared toward automatic DA classification, and much less work has been done on applying DA models to automatic speech recognition. Nagata and Morimoto (1994) suggest conditioning word language models on DAs to lower perplexity. Suhm and Waibel (1994) and Eckert, Gallwitz, and Niemann (1996) each condition a recognizer LM on left-to-right DA predictions and are able to show reductions in word error rate of 1% on task-oriented corpora. Most similar to our own work, but still in a task-oriented domain, the work by Taylor et al. (1998) combines DA likelihoods from prosodic models with those from 1-best recognition output to condition the recognizer LM, again achieving an absolute reduction in word error rate of 1%, as disappointing as the 0.3% improvement in our experiments. Related computational tasks beyond DA classification and speech recognition have received even less attention to date. We already mentioned Warnke et al. (1997) and Finke et al. (1998), who both showed that utterance segmentation and classification can be integrated into a single search process. Fukada et al. (1998) investigate augmenting DA tagging with more detailed semantic "concept" tags, as a preliminary step toward an interlingua-based dialogue translation system. Levin et al. (1999) couple DA classification with dialogue game classification; dialogue games are units above the DA level, i.e., short DA sequences such as question-answer pairs. All the work mentioned so far uses statistical models of various kinds. As we have shown here, such models offer some fundamental advantages, such as modularity and composability (e.g., of discourse grammars with DA models) and the ability to deal with noisy input (e.g., from a speech recognizer) in a principled way. However, many other classifier architectures are applicable to the tasks discussed, in particular to DA classification. A nonprobabilistic approach for DA labeling proposed by Samuel, Car- berry, and VijayShanker (1998) is transformation-based learning (Brill 1993). Finally it should be noted that there are other tasks with a mathematical structure similar to that of DA tagging, such as shallow parsing for natural language processing (Munk 1999) and DNA classification tasks (Ohler, Harbeck, and Niemann 1999), from which further techniques could be borrowed. How does the approach presented here differ from these various earlier models, particularly those based on HMMs? Apart from corpus and tag set differences, our approach differs primarily in that it generalizes the simple HMM approach to cope with new kinds of problems, based on the Bayes network representations depicted in Figures 2 and 4. For the DA classification task, our framework allows us to do classification given unreliable words (by marginalizing over the possible word strings corresponding to the acoustic input) and given nonlexical (e.g., prosodic) evidence. For the speech recognition task, the generalized model gives a clean probabilistic framework for conditioning word probabilities on the conversation context via the underlying DA structure. Unlike previous models that did not address speech recognition or relied only on an intuitive 1-best approximation, our model allows computation of the optimum word sequence by effectively summing over all possible DA sequences as well as all recognition hypotheses throughout the conversation, using evidence from both past and future.  Our approach to dialogue modeling has two major components: statistical dialogue grammars modeling the sequencing of DAs, and DA likelihood models expressing the local cues (both lexical and prosodic) for DAs. We made a number of significant simplifications to arrive at a computationally and statistically tractable formulation. In this formulation, DAs serve as the hinges that join the various model components, but also decouple these components through statistical independence assumptions. Conditional on the DAs, the observations across utterances are assumed to be independent, and evidence of different kinds from the same utterance (e.g., lexical and prosodic) is assumed to be independent. Finally, DA types themselves are assumed to be independent beyond a short span (corresponding to the order of the dialogue n-gram). Further research within this framework can be characterized by which of these simplifications are addressed. Dialogue grammars for conversational speech need to be made more aware of the temporal properties of utterances. For example, we are currently not modeling the fact that utterances by the conversants may actually overlap (e.g., backchannels interrupting an ongoing utterance). In addition, we should model more of the nonlocal aspects of discourse structure, despite our negative results so far. For example, a context-free discourse grammar could potentially account for the nested structures proposed in Grosz and Sidner (1986). 1° The standard n-gram models for DA discrimination with lexical cues are probably suboptimal for this task, simply because they are trained in the maximum likelihood framework, without explicitly optimizing discrimination between DA types. This may be overcome by using discriminative training procedures (Warnke et al. 1999; Ohler, Harbeck, and Niemann 1999). Training neural networks directly with posterior probability (Ries 1999a) seems to be a more principled approach and it also offers much easier integration with other knowledge sources. Prosodic features, for example, can simply be added to the lexical features, allowing the model to capture dependencies and redundancies across knowledge sources. Keyword-based techniques from the field of message classification should also be applicable here (Rose, Chang, and Lippmann 1991). Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodic cues into a single model, e.g., one that predicts the next DA based on DA history and all the local evidence. The study of automatically extracted prosodic features for DA modeling is likewise only in its infancy. Our preliminary experiments with neural networks have shown that small gains are obtainable with improved statistical modeling techniques. However, we believe that more progress can be made by improving the underlying features themselves, in terms of both better understanding of how speakers use them, and ways to reliably extract them from data. Regarding the data itself, we saw that the distribution of DAs in our corpus limits the benefit of DA modeling for lower-level processing, in particular speech recognition. The reason for the skewed distribution was in the nature of the task (or lack thereof) in Switchboard. It remains to be seen if more fine-grained DA distinctions can be made reliably in this corpus. However, it should be noted that the DA definitions are really arbitrary as far as tasks other than DA labeling are concerned. This suggests using unsupervised, self-organizing learning schemes that choose their own DA definitions in the process of optimizing the primary task, whatever it may be. Hand-labeled DA categories may still serve an important role in initializing such an algorithm. We believe that dialogue-related tasks have much to benefit from corpus-driven, automatic learning techniques. To enable such research, we need fairly large, standardized corpora that allow comparisons over time and across approaches. Despite its shortcomings, the Switchboard domain could serve this purpose.  We have developed an integrated probabilistic approach to dialogue act modeling for conversational speech, and tested it on a large speech corpus. The approach combines models for lexical and prosodic realizations of DAs, as well as a statistical discourse 10 The inadequacy of n-gram models for nested discourse structures is pointed out by ChuCarroll (1998), although the suggested solution is a modified n-gram approach.. grammar. All components of the model are automatically trained, and are thus applicable to other domains for which labeled data is available. Classification accuracies achieved so far are highly encouraging, relative to the inherent difficulty of the task as measured by human labeler performance. We investigated several modeling alternatives for the components of the model (backoff n-grams and maximum entropy models for discourse grammars, decision trees and neural networks for prosodic classification) and found performance largely independent of these choices. Finally, we developed a principled way of incorporating DA modeling into the probability model of a continuous speech recognizer, by constraining word hypotheses using the discourse context. However, the approach gives only a small reduction in word error on our corpus, which can be attributed to a preponderance of a single dialogue act type (statements). Note The research described here is based on a project at the 1997 Workshop on Innovative Techniques in LVCSR at the Center for Speech and Language Processing at Johns Hopkins University (Jurafsky et al. 1997; Jurafsky et al. 1998). The DA-labeled Switchboard transcripts as well as other project-related publications are available at http://www.colorado. edu/ling/jurafsky/ws97/.  We thank the funders, researchers, and support staff of the 1997 Johns Hopkins Summer Workshop, especially Bill Byrne, Fred Jelinek, Harriet Nock, Joe Picone, Kimberly Shiring, and Chuck Wooters. Additional support came from the NSF via grants IRI9619921 and IRI9314967, and from the UK Engineering and Physical Science Research Council (grant GR/J55106). Thanks to Mitch Weintraub, to Susann LuperFoy, Nigel Ward, James Allen, Julia Hirschberg, and Marilyn Walker for advice on the design of the SWBDDAMSL tag set, to the discourse labelers at CU Boulder (Debra Biasca, Marion Bond, Traci Curl, Anu Erringer, Michelle Gregory, Lori Heintzelman, Taimi Metzler, and Amma Oduro) and the intonation labelers at the University of Edinburgh (Helen Wright, Kurt Dusterhoff, Rob Clark, Cassie Mayo, and Matthew Bull). We also thank Andy Kehler and the anonymous reviewers for valuable comments on a draft of this paper.
 A Stochastic Finite-State Word-Segmentation Algorithm for Chinese  The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words. For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation. In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information. In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer. The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words. We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.  Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS). An initial step of any textÂ­ analysis task is the tokenization of the input into words. For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation. Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces. A moment's reflection will reveal that things are not quite that simple. There are clearly eight orthographic words in the example given, but if one were doing syntactic analysis one would probably want to consider I'm to consist of two syntactic words, namely I and am. If one is interested in translation, one would probably want to consider show up as a single dictionary word since its semantic interpretation is not trivially derivable from the meanings of show and up. And if one is interested in TIS, one would probably consider the single orthographic word ACL to consist of three phonological words-lei s'i d/-corresponding to the pronunciation of each of the letters in the acronym. Space- or punctuation-delimited * 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA. Email: rlls@bell-labs. com t 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA. Email: cls@bell-labs. com t 600 Mountain Avenue, 2c278, Murray Hill, NJ 07974, USA. Email: gale@research. att. com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries. In (b) is a plausible segmentation for this sentence; in (c) is an implausible segmentation. orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words. Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal. Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words. In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984). Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view. All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia.. 2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji.. 3 Throughout this paper we shall give Chinese examples in traditional orthography, followed. immediately by a Romanization into the pinyin transliteration scheme; numerals following each pinyin syllable represent tones. Examples will usually be accompanied by a translation, plus a morpheme-by-morpheme gloss given in parentheses whenever the translation does not adequately serve this purpose. In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest. raphy: A ren2 'person' is a fairly uncontroversial case of a monographemic word, and rplil zhong1guo2 (middle country) 'China' a fairly uncontroversial case of a diÂ­ graphernic word. The relevance of the distinction between, say, phonological words and, say, dictionary words is shown by an example like rpftl_A :;!:Hfllil zhong1hua2 ren2min2 gong4he2-guo2 (China people republic) 'People's Republic of China.' Arguably this consists of about three phonological words. On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English. Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide. For example, suppose one is building a ITS system for Mandarin Chinese. For that application, at a minimum, one would want to know the phonological word boundaries. Now, for this application one might be tempted to simply bypass the segmentation problem and pronounce the text character-by-character. However, there are several reasons why this approach will not in general work: 1. Many hanzi have more than one pronunciation, where the correct. pronunciation depends upon word affiliation: tfJ is pronounced deO when it is a prenominal modification marker, but di4 in the word Â§tfJ mu4di4 'goal'; fl; is normally ganl 'dry,' but qian2 in a person's given name.  including Third Tone Sandhi (Shih 1986), which changes a 3 (low) tone into a 2 (rising) tone before another 3 tone: 'j";gil, xiao3 [lao3 shu3] 'little rat,' becomes xiao3 { lao2shu3 ], rather than xiao2 { lao2shu3 ], because the rule first applies within the word lao3shu3 'rat,' blocking its phrasal application. 3. In various dialects of Mandarin certain phonetic rules apply at the word. level. For example, in Northern dialects (such as Beijing), a full tone (1, 2, 3, or 4) is changed to a neutral tone (0) in the final syllable of many words: Jll donglgual 'winter melon' is often pronounced donglguaO. The high 1 tone of J1l would not normally neutralize in this fashion if it were functioning as a word on its own. 4. TIS systems in general need to do more than simply compute the. pronunciations of individual words; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances. It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks. Given that part-of-speech labels are properties of words rather than morphemes, it follows that one cannot do part-of-speech assignment without having access to word-boundary information. Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation. The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval. There are thus some very good reasons why segmentation into words is an important task. A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented. For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words. Among these are words derived by various productive processes, including: 1. Morphologically derived words such as, xue2shengl+men0. (student+plural) 'students,' which is derived by the affixation of the plural affix f, menD to the nounxue2shengl. 2. Personal names such as 00, 3R; zhoulenl-lai2 'Zhou Enlai.' Of course, we. can expect famous names like Zhou Enlai's to be in many dictionaries, but names such as :fi lf;f; shi2jil-lin2, the name of the second author of this paper, will not be found in any dictionary.  'Malaysia.' Again, famous place names will most likely be found in the dictionary, but less well-known names, such as 1PMÂ± R; bu4lang3-shi4wei2-ke4 'Brunswick' (as in the New Jersey town name 'New Brunswick') will not generally be found. In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes. The segmenter handles the grouping of hanzi into words and outputs word pronunciations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system's ability to segment text appropriately (rather than on its pronunciation abilities). The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers. It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen conÂ­ structions, including morphological derivatives and personal names. We will evaluate various specific aspects of the segmentation, as well as the overall segmentation perÂ­ formance. This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text. Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context. 2. A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper. The first point we need to address is what type of linguistic object a hanzi repreÂ­ sents. Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas. The most accurate characterization of Chinese writing is that it is morphosyllabic (DeFrancis 1984): each hanzi represents one morpheme lexically and semantically, and one syllable phonologiÂ­ cally. Thus in a two-hanzi word like lflli?J zhong1guo2 (middle country) 'China' there are two syllables, and at the same time two morphemes. Of course, since the number of attested (phonemic) Mandarin syllables (roughly 1400, including tonal distinctions) is far smaller than the number of morphemes, it follows that a given syllable could in principle be written with any of several different hanzi, depending upon which morpheme is intended: the syllable zhongl could be lfl 'middle,''clock,''end,' or ,'loyal.' A morpheme, on the other hand, usually corresponds to a unique hanzi, though there are a few cases where variant forms are found. Finally, quite a few hanzi are homographs, meaning that they may be pronounced in several different ways, and in extreme cases apparently represent different morphemes: The prenominal modifiÂ­ cation marker eg deO is presumably a different morpheme from the second morpheme of Â§eg mu4di4, even though they are written the same way.4 The second point, which will be relevant in the discussion of personal names in Section 4.4, relates to the internal structure of hanzi. Following the system devised under the Qing emperor Kang Xi, hanzi have traditionally been classified according to a set of approximately 200 semantic radicals; members of a radical class share a particular structural component, and often also share a common meaning (hence the term 'semantic'). For example, hanzi containing the INSECT radical !R tend to denote insects and other crawling animals; examples include tr wal 'frog,' feng1 'wasp,' and !Itt she2 'snake.' Similarly, hanzi sharing the GHOST radical _m tend to denote spirits and demons, such as _m gui3 'ghost' itself, II: mo2 'demon,' and yan3 'nightmare.' While the semantic aspect of radicals is by no means completely predictive, the semantic homogeneity of many classes is quite striking: for example 254 out of the 263 examples (97%) of the INSECT class listed by Wieger (1965, 77376) denote crawling or invertebrate animals; similarly 21 out of the 22 examples (95%) of the GHOST class (page 808) denote ghosts or spirits. As we shall argue, the semantic class affiliation of a hanzi constitutes useful information in predicting its properties. 3. Previous Work. There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993). Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ­ cal rule-based approaches, and approaches that combine lexical information with staÂ­ tistical information. The present proposal falls into the last group. Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach. In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words. Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary. A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf. 4 To be sure, it is not always true that a hanzi represents a syllable or that it represents a morpheme. For. example, in Northern Mandarin dialects there is a morpheme -r that attaches mostly to nouns, and which is phonologically incorporated into the syllable to which it attaches: thus men2+r (door+R) 'door' is realized as mer2. This is orthographically represented as 7C. so that 'door' would be and in this case the hanzi 7C, does not represent a syllable. Similarly, there is no compelling evidence that either of the syllables of f.ifflll binllang2 'betelnut' represents a morpheme, since neither can occur in any context without the other: more likely fjfflll binllang2 is a disyllabic morpheme. (See Sproat and Shih 1995.) However, the characterization given in the main body of the text is correct sufficiently often to be useful. Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary. Nonstochastic lexical-knowledge-based approaches have been much more numerÂ­ ous. Two issues distinguish the various proposals. The first concerns how to deal with ambiguities in segmentation. The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based. The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics. This method, one instance of which we term the "greedy algorithm" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ­ ning) of the sentence is reached. Papers that use this method or minor variants thereof include Liang (1986), Li et al. (1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994). The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation. Methods that allow multiple segmentations must provide criteria for choosing the best segmentation. Some approaches depend upon some form of conÂ­ straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach). Others depend upon various lexical heurisÂ­ tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word. Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994). Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism. Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process. The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993). More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}. Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach. Several papers report the use of part-of-speech information to rank segmentations (Lin, Chiang, and Su 1993; Peng and Chang 1993; Chang and Chen 1993); typically, the probability of a segmentation is multiplied by the probability of the tagging(s) for that segmentation to yield an estimate of the total probability for the analysis. Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÂ­ quence of hanzi might be a name, but that it is likely to be a name with some probabilÂ­ ity. Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993). Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression. This is not ideal for some applications, however. For instance, for TTS it is necessary to know that a particular sequence of hanzi is of a particular category because that knowlÂ­ edge could affect the pronunciation; consider, for example the issues surrounding the pronunciation of ganl I qian2 discussed in Section 1. Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range. However, it is almost universally the case that no clear definition of what constitutes a "correct" segmentation is given, so these performance measures are hard to evaluate. Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures. In a few cases, the criteria for correctness are made more explicit. For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary. Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult. The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words. The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation. Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.  Chinese word segmentation can be viewed as a stochastic transduction problem. More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994). Let H be the set of hanzi, p be the set of pinyin syllables with tone marks, and P be the set of grammatical part-of-speech labels. Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word. Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994). We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items. selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected. then define the best segmentation to be the cheapest or best path in Id(I) o D* (i.e., Id(I) composed with the transitive closure of 0).6 Consider the abstract example illustrated in Figure 2. In this example there are four "input characters," A, B, C and D, and these map respectively to four "pronunciations" a, b, c and d. Furthermore, there are four "words" represented in the dictionary. These are shown, with their associated costs, as follows: ABj nc 4.0 AB C/jj 6.0 CD /vb 5. 0 D/ nc 5.0 The minimal dictionary encoding this information is represented by the WFST in Figure 2(a). An input ABCD can be represented as an FSA as shown in Figure 2(b). This FSA I can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d). This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels. Since the segmentation corresponds to the sequence of words that has the lowest summed unigram cost, the segmenter under discussion here is a zeroth-order model. It is important to bear in mind, though, that this is not an inherent limitation of the model. For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.) In Section 6 we disÂ­ cuss other issues relating to how higher-order language models could be incorporated into the model. 4.1 Dictionary Representation. As we have seen, the lexicon of basic words and stems is represented as a WFST; most arcs in this WFST represent mappings between hanzi and pronunciations, and are costless. Each word is terminated by an arc that represents the transduction between f and the part of speech of that word, weighted with an estimated cost for that word. The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols. Note that hanzi that are not grouped into dictionary words (and are not identified as singleÂ­ hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words. Other strategies could readily 6 As a reviewer has pointed out, it should be made clear that the function for computing the best path is. an instance of the Viterbi algorithm. 7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong. It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China. (a) IDictionary D I D:d/0.000 B:b/0.000 B:b/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cps:nd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm. The transitive closure of the dictionary in (a) is composed with Id(input) (b) to form the WFST (c). The segmentation chosen is the best path through the WFST, shown in (d). (In this figure eps is c) be implemented, though, such as a maximal-grouping strategy (as suggested by one reviewer of this paper); or a pairwise-grouping strategy, whereby long sequences of unattached hanzi are grouped into two-hanzi words (which may have some prosodic motivation). We have not to date explored these various options. Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of. newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material. This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used. The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges. Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer. In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates. Note also that the costs currently used in the system are actually string costs, rather than word costs. This is because our corpus is not annotated, and hence does not distinguish between the various words represented by homographs, such as, which could be /adv jiangl 'be about to' orInc jiang4 '(military) general'-as in 1j\xiao3jiang4 'little general.' In such cases we assign all of the estimated probability mass to the form with the most likely pronunciation (determined by inspection), and assign a very small probability (a very high cost, arbitrarily chosen to be 40) to all other variants. In the case of, the most common usage is as an adverb with the pronunciation jiangl, so that variant is assigned the estimated cost of 5.98, and a high cost is assigned to nominal usage with the pronunciation jiang4. The less favored reading may be selected in certain contexts, however; in the case of , for example, the nominal reading jiang4 will be selected if there is morphological information, such as a following plural affix ir, menD that renders the nominal reading likely, as we shall see in Section 4.3. Figure 3 shows a small fragment of the WFST encoding the dictionary, containing both entries forjust discussed, g:tÂ¥ zhonglhua2 min2guo2 (China Republic) 'Republic of China,' and iÂ¥inl. nan2gual 'pumpkin.' 4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1. As noted, this sentence consists of four words, namely B X ri4wen2 'Japanese,' :Â¥, zhanglyu2 'octopus/ :&P:l zen3me0 'how,' and IDt shuol 'say.' As indicated in Figure 1(c), apart from this correct analysis, there is also the analysis taking B ri4 as a word (e.g., a common abbreviation for Japan), along with X:Â¥ wen2zhangl 'essay/ and f!!. yu2 'fish.' Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen. 4.3 Morphological Analysis. The method just described segments dictionary words, but as noted in Section 1, there are several classes of words that should be handled that are not found in a standard dictionary. One class comprises words derived by productive morphologiÂ­ cal processes, such as plural noun formation using the suffix ir, menD. (Other classes handled by the current system are discussed in Section 5.) The morphological analÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up. each word in the lexicon whether or not each string is actually an instance of the word in question. Â£ : _ADV: 5.88 If:! :zhong1 : 0.0 tjl :huo2 :0.0 (R:spub:/ic of Ch:ina) + .,_,...I : jlong4 :0.0 (mUifaty genG181) 0 Â£: _NC: 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I â¢=- :il: .;ss:;zhangt â¢ '-:. I â¢ JAPANS :rl4 .Â·Â·Â·Â·Â·Â·Â·Â·Â·"\)Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·"oÂ·'Â·Â·Â·Â·Â·Â·Â·"\:JÂ·Â·Â·Â·Â·Â·Â·Â·Â· Â·Â·Â·Â·Â·Â·Â·Â·Â·'\; . '.:: ..........0 6.51 9.51 : jj / JAPANESE OCTOPUS 10Â·28iÂ£ :_nc HOW SAY f B :rl4 :il: :wen2 t '- â¢ :zhang! !!:\ :yu2 e:_nc [::!!:zen3 l!f :moO t:_adv il!:shuot ,:_vb i i i 1 â¢ 10.03 13... 7.96 5.55 1 l...................................................................................................................................................................................................J.. Figure 4 Input lattice (top) and two segmentations (bottom) of the sentence 'How do you say octopus in Japanese?'. A non-optimal analysis is shown with dotted lines in the bottom frame. ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t:-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,. However, for our purposes it is not sufficient to repreÂ­ sent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word. For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry. So, 1: f, xue2shengl+men0 (student+PL) 'students' occurs and we estimate its cost at 11.43; similarly we estimate the cost of f, jiang4+men0 (general+PL) 'generals' (as in 'J' f, xiao3jiang4+men0 'little generals'), at 15.02. But we also need an estimate of the probability for a non-occurring though possible plural form like iÂ¥JJ1l.f, nan2gua1-men0 'pumpkins.' 10 Here we use the Good-Turing estimate (Baayen 1989; Church and Gale 1991), whereby the aggregate probability of previously unseen instances of a construction is estimated as ni/N, where N is the total number of observed tokens and n1 is the number of types observed only once. Let us notate the set of previously unseen, or novel, members of a category X as unseen(X); thus, novel members of the set of words derived in f, menO will be deÂ­ noted unseen(f,). For irt the Good-Turing estimate just discussed gives us an estimate of p(unseen(f,) I f,)-the probability of observing a previously unseen instance of a construction in ft given that we know that we have a construction in f,. This GoodÂ­ Turing estimate of p(unseen(f,) If,) can then be used in the normal way to define the probability of finding a novel instance of a construction in ir, in a text: p(unseen(f,)) = p(unseen(f,) I f,) p(fn Here p(ir,) is just the probability of any construction in ft as estimated from the frequency of such constructions in the corpus. Finally, asÂ­ suming a simple bigram backoff model, we can derive the probability estimate for the particular unseen word iÂ¥1J1l. irL as the product of the probability estimate for iÂ¥JJ1l., and the probability estimate just derived for unseen plurals in ir,: p(iÂ¥1J1l.ir,) p(iÂ¥1J1l.)p(unseen(f,)). The cost estimate, cost(iÂ¥JJ1l.fn is computed in the obvious way by summing the negative log probabilities of iÂ¥JJ1l. and f,. Figure 5 shows how this model is implemented as part of the dictionary WFST. There is a (costless) transition between the NC node and f,. The transition from f, to a final state transduces c to the grammatical tag PL with cost cost(unseen(f,)): cost(iÂ¥JJ1l.ir,) == cost(iÂ¥JJ1l.) + cost(unseen(fm, as desired. For the seen word ir, 'genÂ­ erals,' there is an c:NC transduction from to the node preceding ir,; this arc has cost cost( f,) - cost(unseen(f,)), so that the cost of the whole path is the desired cost( f,). This representation gives ir, an appropriate morphological decomposition, preÂ­ serving information that would be lost by simply listing ir, as an unanalyzed form. Note that the backoff model assumes that there is a positive correlation between the frequency of a singular noun and its plural. An analysis of nouns that occur in both the singular and the plural in our database reveals that there is indeed a slight but significant positive correlation-R2 = 0.20, p < 0.005; see Figure 6. This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form. 10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to. attaching to terms denoting human beings. However, it is possible to personify any noun, so in children's stories or fables, iÂ¥JJ1l. f, nan2gual+men0 'pumpkins' is by no means impossible. J:j:l :zhongl :0.0 ;m,Jlong4 :0.0 (mHHaryg9tltHBI) Â£: _ADV: 5.98 Â¥ :hua2:o.o E :_NC: 4.41 :mln2:o.o mm : guo2 : 0.0 (RopubllcofChlna) .....,. 0 Figure 5 An example of affixation: the plural affix. 4.4 Chinese Personal Names. Full Chinese personal names are in one respect simple: they are always of the form family+given. The family name set is restricted: there are a few hundred single-hanzi family names, and about ten double-hanzi ones. Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1. wo rd => na m e 2. na me =>1 ha nzi fa mi ly 2 ha nzi gi ve n 3. na me =>1 ha nzi fa mi ly 1 ha nzi gi ve n 4. na me =>2 ha nzi fa mi ly 2 ha nzi gi ve n 5. na me =>2 ha nzi fa mi ly 1 ha nzi gi ve n 6.1 ha nzi fa mi ly => ha nz ii 7.2 ha nzi fa mi ly => ha nzi i ha nz ij 8.1 ha nzi gi ve n => ha nz ii 9.2 ha nzi giv en => ha nzi i ha nz ij The difficulty is that given names can consist, in principle, of any hanzi or pair of hanzi, so the possible given names are limited only by the total number of hanzi, though some hanzi are certainly far more likely than others. For a sequence of hanzi that is a possible name, we wish to assign a probability to that sequence qua name. We can model this probability straightforwardly enough with a probabilistic version of the grammar just given, which would assign probabilities to the individual rules. For example, given a sequence F1G1G2, where F1 is a legal single-hanzi family name, and Plural Nouns X g 0 g "' X X 0 T!i c"'. 0 X u} "' o; .2 X X><X X XX X X X X X X x X X X X X x X V X X X X .;t'*- XXX:OX X X X X X X 9 x X X XX XX X X X X X X X XXX:< X X>O<XX>!KXX XI<>< Â»C X X XX :X: X X "' X X XX >OO<X>D<XIK X X X X X X --XXÂ»: XXX X XÂ»C X XÂ«X...C:XXX X Xll< X X ><XX>IIC:liiC:oiiiiCI--8!X:liiOC!I!S8K X X X 10 100 1000 10000 log(F)_base: R"2=0.20 (p < 0.005) X 100000 Figure 6 Plot of log frequency of base noun, against log frequency of plural nouns. G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of: â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al. (1992). The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on. This model is easily incorporated into the segmenter by building a WFST restrictÂ­ ing the names to the four licit types, with costs on the arcs for any particular name summing to an estimate of the cost of that name. This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model. There are two weaknesses in Chang et al.'s model, which we improve upon. First, the model assumes independence between the first and second hanzi of a double given name. Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model. As a partial solution, for pairs of hanzi that co-occur sufficiently often in our namelists, we use the estimated bigram cost, rather than the independence-based cost. The second weakness is purely conceptual, and probably does not affect the perÂ­ formance of the model. For previously unseen hanzi in given names, Chang et al. assign a uniform small cost; but we know that some unseen hanzi are merely acciÂ­ dentally missing, whereas others are missing for a reason-for example, because they have a bad connotation. As we have noted in Section 2, the general semantic class to which a hanzi belongs is often predictable from its semantic radical. Not surprisingly some semantic classes are better for names than others: in our corpora, many names are picked from the GRASS class but very few from the SICKNESS class. Other good classes include JADE and GOLD; other bad classes are DEATH and RAT. We can better predict the probability of an unseen hanzi occurring in a name by computing a within-class Good-Turing estimate for each radical class. Assuming unseen objects within each class are equiprobable, their probabilities are given by the Good-Turing theorem as: cis E( n'J.ls) Po oc N * E(N8ls) (2) where p815 is the probability of one unseen hanzi in class cls, E(n'J.15 ) is the expected number of hanzi in cls seen once, N is the total number of hanzi, and E(N(/ 5 ) is the expected number of unseen hanzi in class cls. The use of the Good-Turing equation presumes suitable estimates of the unknown expectations it requires. In the denomi 11 We have two such lists, one containing about 17,000 full names, and another containing frequencies of. hanzi in the various name positions, derived from a million names. 12 One class of full personal names that this characterization does not cover are married women's names. where the husband's family name is optionally prepended to the woman's full name; thus ;f:*lf#i xu3lin2-yan2hai3 would represent the name that Ms. Lin Yanhai would take if she married someone named Xu. This style of naming is never required and seems to be losing currency. It is formally straightforward to extend the grammar to include these names, though it does increase the likelihood of overgeneration and we are unaware of any working systems that incorporate this type of name. We of course also fail to identify, by the methods just described, given names used without their associated family name. This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name. Table 1 The cost as a novel given name (second position) for hanzi from various radical classes. JA DE G O L D G R AS S SI C K NE SS DE AT H R A T 14. 98 15. 52 15. 76 16. 25 16. 30 16. 42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation. In the numerator, however, the counts of ni1s are quite irregular, inÂ­ cluding several zeros (e.g., RAT, none of whose members were seen). However, there is a strong relationship between ni1s and the number of hanzi in the class. For E(ni1s), then, we substitute a smooth S against the number of class elements. This smooth guarantees that there are no zeroes estimated. The final estimating equation is then: (3) Since the total of all these class estimates was about 10% off from the Turing estimate n1/N for the probability of all unseen hanzi, we renormalized the estimates so that they would sum to n 1jN. This class-based model gives reasonable results: for six radical classes, Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name. Note that the good classes JADE, GOLD and GRASS have lower costs than the bad classes SICKNESS, DEATH and RAT, as desired, so the trend observed for the results of this method is in the right direction. 4.5 Transliterations of Foreign Words. Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name. Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identiÂ­ fication of such names is tricky. Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM. As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate. As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN). Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects. For instance, the common "suffixes," -nia (e.g.,. Virginia) and -sia are normally transliterated as fbSi! ni2ya3 and @5:2 xilya3, respectively. The interdependence between fb or 1/!i, and 5:2 is not captured by our model, but this could easily be remedied. logical rules, and personal names; the transitive closure of the resulting machine is then computed.  In this section we present a partial evaluation of the current system, in three parts. The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis. To date we have not done a separate evaluation of foreign-name recognition. Evaluation of the Segmentation as a Whole. Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair. The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text. Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects. To this end, we picked 100 sentences at random containing 4,372 total hanzi from a test corpus.14 (There were 487 marks of punctuation in the test sentences, including the sentence-final periods, meaning that the average inter-punctuation distance was about 9 hanzi.) We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus. Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud. An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units. (See also Wu and Fung [1994].) Various segmentation approaches were then compared with human performance: 1. A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point. 2. An anti-greedy algorithm, AG: instead of the longest match, take the. shortest match at each point. 3. The method being described-henceforth ST.. Two measures that can be used to compare judgments are: 1. Precision. For each pair of judges consider one judge as the standard,. computing the precision of the other's judgments relative to this standard. 2. Recall. For each pair of judges, consider one judge as the standard,. computing the recall of the other's judgments relative to this standard. Clearly, for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard, and computing for h, 14 All evaluation materials, with the exception of those used for evaluating personal names were drawn. from the subset of the United Informatics corpus not used in the training of the models. Table 2 Similarity matrix for segmentation judgments. Jud ges A G G R ST M 1 M 2 M 3 T1 T2 T3 AG 0.7 0 0.7 0 0 . 4 3 0.4 2 0.6 0 0.6 0 0.6 2 0.5 9 GR 0.9 9 0 . 6 2 0.6 4 0.7 9 0.8 2 0.8 1 0.7 2 ST 0 . 6 4 0.6 7 0.8 0 0.8 4 0.8 2 0.7 4 M1 0.7 7 0.6 9 0.7 1 0.6 9 0.7 0 M2 0.7 2 0.7 3 0.7 1 0.7 0 M3 0.8 9 0.8 7 0.8 0 T1 0.8 8 0.8 2 T2 0.7 8 respectively, the recall and precision. We therefore used the arithmetic mean of each interjudge precision-recall pair as a single measure of interjudge similarity. Table 2 shows these similarity measures. The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions. The result of this is shown in Figure 7. The horizontal axis in this plot represents the most significant dimension, which explains 62% of the variation. In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names). This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based. As can be seen, GR and this "pared-down" statistical method perform quite similarly, though the statistical method is still slightly better.16 AG clearly performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods. It can also be seen clearly in this plot that two of the Taiwan speakers cluster very closely together, and the third TaiÂ­ wan speaker is also close in the most significant dimension (the x axis). Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers. The breakdown of the different types of words found by ST in the test corpus is given in Table 3. Clearly the percentage of productively formed words is quite small (for this particular corpus), meaning that dictionary entries are covering most of the 15 GR is .73 or 96%.. 16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words. That is, given a choice between segmenting a sequence abc into abc and ab, c, the former will always be picked so long as its cost does not exceed the summed costs of ab and c: while; it is possible for abc to be so costly as to preclude the larger grouping, this will certainly not usually be the case. In this way, the method reported on here will necessarily be similar to a greedy method, though of course not identical. As the reviewer also points out, this is a problem that is shared by, e.g., probabilistic context-free parsers, which tend to pick trees with fewer nodes. The question is how to normalize the probabilities in such a way that smaller groupings have a better shot at winning. This is an issue that we have not addressed at the current stage of our research. i..f,.. "c' 0 + 0 "0 ' â¢ + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y â¢ Taiwan 0 Â·;; 0 c CD E i5 0"' 9 9 â¢ Mainland â¢ â¢ â¢ â¢ -0.30.20.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 7 Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions. The percentage scores on the axis labels represent the amount of variation in the data explained by the dimension in question. Table 3 Classes of words found by ST for the test corpus. Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases. Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words. It may seem surprising to some readers that the interhuman agreement scores reported here are so low. However, this result is consistent with the results of exÂ­ periments discussed in Wu and Fung (1994). Wu and Fung introduce an evaluation method they call nk-blind. Under this scheme, n human judges are asked independently to segment a text. Their results are then compared with the results of an automatic segmenter. For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct. For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed. Proper-Name Identification. To evaluate proper-name identification, we randomly seÂ­ lected 186 sentences containing 12,000 hanzi from our test corpus and segmented the text automatically, tagging personal names; note that for names, there is always a sinÂ­ gle unambiguous answer, unlike the more general question of which segmentation is correct. The performance was 80.99% recall and 61.83% precision. Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits. However, we have reason to doubt Chang et al.'s performance claims. Without using the same test corpus, direct comparison is obviously difficult; fortunately, Chang et al. include a list of about 60 sentence fragments that exemplify various categories of performance for their system. The performance of our system on those sentences apÂ­ peared rather better than theirs. On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision. However, they list two sets, one consisting of 28 fragments and the other of 22 fragments, in which they had 0% recall and precision. On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision. Note that it is in precision that our overÂ­ all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise. Thus we have some confidence that our own performance is at least as good as that of Chang et al. (1992). In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system. Fortunately, we were able to obtain a copy of the full set of sentences from Chang et al. on which Wang, Li, and Chang tested their system, along with the output of their system.18 In what follows we will discuss all cases from this set where our performance on names differs from that of Wang, Li, and Chang. Examples are given in Table 4. In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ­ senting differences in the capabilities of the model per se. The first issue relates to the completeness of the base lexicon. The Wang, Li, and Chang system fails on fragment (b) because their system lacks the word youlyoul 'soberly' and misinterpreted the thus isolated first youl as being the final hanzi of the preceding name; similarly our system failed in fragment (h) since it is missing the abbreviation i:lJI! tai2du2 'Taiwan Independence.' This is a rather important source of errors in name identifiÂ­ cation, and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used. 17 They also provide a set of title-driven rules to identify names when they occur before titles such as $t. 1: xianlshengl 'Mr.' or i:l:itr!J tai2bei3 shi4zhang3 'Taipei Mayor.' Obviously, the presence of a title after a potential name N increases the probability that N is in fact a name. Our system does not currently make use of titles, but it would be straightforward to do so within the finite-state framework that we propose. 18 We are grateful to ChaoHuang Chang for providing us with this set. Note that Wang, Li, and Chang's. set was based on an earlier version of the Chang et a!. paper, and is missing 6 examples from the A set. 19 We note that it is not always clear in Wang, Li, and Chang's examples which segmented words. constitute names, since we have only their segmentation, not the actual classification of the segmented words. Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical. Table 4 Differences in performance between our system and Wang, Li, and Chang (1992). Our System Wang, Li, and Chang a. 1\!f!IP Eflltii /1\!f!J:P $1til I b. agm: I a m: c. 5 Bf is Bf 1 d. "*:t: w _t ff 1 "* :t: w_tff 1 g., , Transliteration/Translation chen2zhongl-shenl qu3 'music by Chen Zhongshen ' huang2rong2 youlyoul de dao4 'Huang Rong said soberly' zhangl qun2 Zhang Qun xian4zhang3 you2qingl shang4ren2 hou4 'after the county president You Qing had assumed the position' lin2 quan2 'Lin Quan' wang2jian4 'Wang Jian' oulyang2-ke4 'Ouyang Ke' yinl qi2 bu4 ke2neng2 rong2xu3 tai2du2 er2 'because it cannot permit Taiwan Independence so' silfa3-yuan4zhang3 lin2yang2-gang3 'president of the Judicial Yuan, Lin Yanggang' lin2zhangl-hu2 jiangl zuo4 xian4chang3 jie3shuol 'Lin Zhanghu will give an exÂ­ planation live' jin4/iang3 nian2 nei4 sa3 xia4 de jinlqian2 hui4 ting2zhi3 'in two years the distributed money will stop' gaoltangl da4chi2 ye1zi0 fen3 'chicken stock, a tablespoon of coconut flakes' you2qingl ru4zhu3 xian4fu3 lwu4 'after You Qing headed the county government' Table 5 Performance on morphological analysis. Affix Pron Base category N found N missed (recall) N correct (precision) t,-,7 The second issue is that rare family names can be responsible for overgeneration, especially if these names are otherwise common as single-hanzi words. For example, the Wang, Li, and Chang system fails on the sequence 1:f:p:]nian2 nei4 sa3 in (k) since 1F nian2 is a possible, but rare, family name, which also happens to be written the same as the very common word meaning 'year.' Our system fails in (a) because of$ shenl, a rare family name; the system identifies it as a family name, whereas it should be analyzed as part of the given name. Finally, the statistical method fails to correctly group hanzi in cases where the individual hanzi comprising the name are listed in the dictionary as being relatively high-frequency single-hanzi words. An example is in (i), where the system fails to group t;,f;?"$?t!: lin2yang2gang3 as a name, because all three hanzi can in principle be separate words (t;,f; lin2 'wood';?"$ yang2 'ocean'; ?t!; gang3 'harbor'). In many cases these failures in recall would be fixed by having better estimates of the actual probÂ­ abilities of single-hanzi words, since our estimates are often inflated. A totally nonÂ­ stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended. Evaluation of Morphological Analysis. In Table 5 we present results from small test corÂ­ pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong. The first four affixes are so-called resultative affixes: they denote some propÂ­ erty of the resultant state of a verb, as in E7 wang4bu4-liao3 (forget-not-attain) 'cannot forget.' The last affix in the list is the nominal plural f, men0.20 In the table are the (typical) classes of words to which the affix attaches, the number found in the test corpus by the method, the number correct (with a precision measure), and the number missed (with a recall measure).  In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers. This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration. Other kinds of productive word classes, such as company names, abbreviations (termed fijsuolxie3 in Mandarin), and place names can easily be 20 Note that 7 in E 7 is normally pronounced as leO, but as part of a resultative it is liao3.. handled given appropriate models. (For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].) We have argued that the proposed method performs well. However, some caveats are in order in comparing this method (or any method) with other approaches to segÂ­ mentation reported in the literature. First of all, most previous articles report perforÂ­ mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall. What both of these approaches presume is that there is a sinÂ­ gle correct segmentation for a sentence, against which an automatic algorithm can be compared. We have shown that, at least given independent human judgments, this is not the case, and that therefore such simplistic measures should be mistrusted. This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca. May 1995). However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion. Second, comparisons of different methods are not meaningful unless one can evalÂ­ uate them on the same corpus. Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods. One hopes that such a corpus will be forthÂ­ coming. Finally, we wish to reiterate an important point. The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]). We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted. However, there will remain a large number of words that are not readily adduced to any producÂ­ tive pattern and that would simply have to be added to the dictionary. This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used. The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation. However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework. For example, as Gan (1994) has noted, one can construct examples where the segmenÂ­ tation is locally ambiguous but can be determined on the basis of sentential or even discourse context. Two sets of examples from Gan are given in (1) and (2) (:::::: Gan's Appendix B, exx. lla/llb and 14a/14b respectively). In (1) the sequencema3lu4 cannot be resolved locally, but depends instead upon broader context; similarly in (2), the sequence :::tcai2neng2 cannot be resolved locally: 1. (a) 1 Â§ . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1Â§: . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' :$ chel jinglguo4 car pass by 2. (a) I f f fi * fi :1 }'l ij 1Â§: {1M m m s h e n 3 m e 0 shi2 ho u4 wo 3 cai2 ne ng 2 ke4 fu 2 zh e4 ge 4 ku n4 w h a t ti m e I just be abl e ov er co m e thi s C L dif fic 'When will I be able to overcome this difficulty?' (b) 89 :1 t& tal de cai2neng2 hen3 he DE talent very 'He has great talent' f.b ga ol hig h While the current algorithm correctly handles the (b) sentences, it fails to handle the (a) sentences, since it does not have enough information to know not to group the sequences.ma3lu4 and?]cai2neng2 respectively. Gan's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic, semantic, and lexical relations between objects of various linguistic types (hanzi, words, phrases). An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL). A high-level relation is agent, which relates an animate nominal to a predicate. Particular instances of relations are associated with goodness scores. Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence, and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are "popular" or not. While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable. Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper. For the examples given in (1) and (2) this certainly seems possible. Consider first the examples in (2). The segmenter will give both analyses :1 cai2 neng2 'just be able,' and ?]cai2neng2 'talent,' but the latter analysis is preferred since splitting these two morphemes is generally more costly than grouping them. In (2a), we want to split the two morphemes since the correct analysis is that we have the adverb :1 cai2 'just,' the modal verb neng2 'be able' and the main verb R: Hke4fu2 'overcome'; the competing analysis is, of course, that we have the noun :1 cai2neng2 'talent,' followed by }'lijke4fu2 'overcome.' Clearly it is possible to write a rule that states that if an analysis Modal+ Verb is available, then that is to be preferred over Noun+ Verb: such a rule could be stated in terms of (finite-state) local grammars in the sense of Mohri (1993). Turning now to (1), we have the similar problem that splitting.into.ma3 'horse' andlu4 'way' is more costly than retaining this as one word .ma3lu4 'road.' However, there is again local grammatical information that should favor the split in the case of (1a): both .ma3 'horse' and .ma3 lu4 are nouns, but only .ma3 is consistent with the classifier pil, the classifier for horses.21 By a similar argument, the preference for not splitting , lm could be strengthened in (lb) by the observation that the classifier 1'1* tiao2 is consistent with long or winding objects like , lm ma3lu4 'road' but not with,ma3 'horse.' Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules. Thus, we feel fairly confident that for the examples we have considered from Gan's study a solution can be incorporated, or at least approximated, within a finite-state framework. With regard to purely morphological phenomena, certain processes are not hanÂ­ dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required. Mandarin exhibits several such processes, including A-not-A question formation, ilÂ­ lustrated in (3a), and adverbial reduplication, illustrated in (3b): 3. (a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?' JI! gaolxing4 'happy' => F.i'JF.i'J Jl! gaolbu4-gaolxing4 (hap-not-happy) 'happy?' (b) F.i'JJI! gaolxing4 'happy'=> F.i'JF.i'JJI!JI! gaolgaolxing4xing4 'happily' In the particular form of A-not-A reduplication illustrated in (3a), the first syllable of the verb is copied, and the negative markerbu4 'not' is inserted between the copy and the full verb. In the case of adverbial reduplication illustrated in (3b) an adjective of the form AB is reduplicated as AABB. The only way to handle such phenomena within the framework described here is simply to expand out the reduplicated forms beforehand, and incorporate the expanded forms into the lexical transducer.  Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages. The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way. The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind. As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis. Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994). Since the transducers are built from human-readable descriptions using a lexical toolkit (Sproat 1995), the system is easily maintained and extended. While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf. 21 In Chinese, numerals and demonstratives cannot modify nouns directly, and must be accompanied by. a classifier. The particular classifier used depends upon the noun. Mohri [1995]) shows promise for improving this situation. The model described here thus demonstrates great potential for use in widespread applications. This flexibility, along with the simplicity of implementation and expansion, makes this framework an attractive base for continued research.  We thank United Informatics for providing us with our corpus of Chinese text, and BDC for the 'Behavior ChineseEnglish Electronic Dictionary.' We further thank Dr. J.-S. Chang of Tsinghua University, Taiwan, R.O.C., for kindly providing us with the name corpora. We also thank ChaoHuang Chang, reviewers for the 1994 ACL conference, and four anonymous reviewers for Computational Linguistics for useful comments.
 Estimation of Probabilistic Context-Free Grammars  The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one. The condition for proper assignment is rather subtle. Production probabilities can be estimated from parsed or unparsed sentences, and the question arises as to whether or not an estimated system is automatically proper. We show here that estimated production probabilities always yield proper distributions.  Context-free grammars (CFG's) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms. Furthermore, CFG's are readily fit with a probability distribution (to make probabilistic CFG's--or PCFG's), rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse. For each nonterminal symbol, a (normalized) probability is placed on the set of all productions from that symbol. Unfortunately, this simple procedure runs into an unexpected complication: the language generated by the grammar may have probability less than one. The reason is that the derivation tree may have probability greater than zero of never terminating--some mass can be lost to infinity. This phenomenon is well known and well understood, and there are tests for "tightness" (by which we mean total probability mass equal to one) involving a matrix derived from the expected growth in numbers of symbols generated by the probabilistic rules (see for example Booth and Thompson [1973], Grenander [1976], and Harris [1963]). What if the production probabilities are estimated from data? Suppose, for example, that we have a parsed corpus that we treat as a collection of (independent) samples from a grammar. It is reasonable to hope that if the trees in the sample are finite, then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees. For example, there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees (see Section 2), resulting in a PCFG. Is it tight? If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG's assign some mass to infinite trees? We will show that in both cases the estimated probability is tight. 2 * Division of Applied Mathematics, Brown University, Providence, RI 02912 USA 1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recently appeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened([1997]). 2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions; see Section 2.. Computational Linguistics Volume 24, Number 2 Wetherell (1980) has asked a similar question: a scheme (different from maximum likelihood) is introduced for estimating production probabilities from an unparsed corpus, and it is conjectured that the resulting system is tight. (Wetherell and others use the designation "consistent" instead of "tight," but in statistics, consistency refers to the asymptotic correctness of an estimator.) A trivial example is the CFG with one nonterminal and one terminal symbol, in Chomsky normal form: A ~ AA a ~ a where a is the only terminal symbol. Assign probability p to the first production (A ~ AA) and q = 1 -p to the second (A ~ a). Let Sh be the total probability of all trees with depth less than or equal to h. For example, $2 = q corresponding to A ~ a, and $3 = q + pq2 corresponding to {A ~ a} tO {A ~ AA, A --~ a,A --~ a}. In general, Sh+l = q + pSi. (Condition on the first production: with probability q the tree terminates and with probability p it produces two nonterminal symbols, each of which must now terminate with depth less than or equal to h.) It is not hard to show that Sh is nondecreasing and converges to min(1, I), meaning that a proper probability is obtained if and only if p < ~. a What if p is estimated from data? Given a set of finite parse trees wl, w2 ..... w,, the maximum-likelihood estimator for p (see Section 2) is, sensibly enough, the "relative frequency" estimator y'~nlf(A ~ AA; wi) ~i=1 [f(A ~ AA; wi) + f(A ~ a;wi)] where f(.;w) is the number of occurrences of the production "." in the tree w. The sentence a m, although ambiguous (there are multiple parses when m > 2), always involves m - 1 of the A ~ AA productions and m of the A ~ a productions. Hence f(A ~ AA; Odi) < f(A ~ a; odi) for each wi. Consequently: f(A ---+AA;wi) < l[f(A ~ AA;~i) + f(A ~ a;wi)] for each wi, and ~ < ½. The maximum-likelihood probability is tight. If only the yields (left-to-right sequence of terminals) Y(o;1), Y(w2)..... Y(wn) are available, the EM algorithm can be used to iteratively "climb" the likelihood surface (see Section 2). In the simple example here, the estimator converges in one step and is the same ~ as if we had observed the entire parse tree for each wi. Thus, ~ is again less than ½ and the distribution is again tight.  More generally, let G -- (V, T, R, S) denote a context-free grammar with finite variable set V, start symbol S E V, finite terminal set T, and finite production (or rule) set R. (We use R in place of the more typical P to avoid confusion with probabilities.) Each production in R has the form A ~ oL, where A E V and o~ E (VUT)*. In the usual way, probabilities are introduced through the productions: P : R --~ [0,1] such that VA E V: p(A -~ c~) = 1. (1) orE(rUT)* s.t. (A~c~)ER Chi and Geman Probabilistic Context-Free Grammars Given a set of finite parse trees cab ca2,..., can, drawn independently according to the distribution imposed by p, we wish to estimate p. In terms of the frequency function f, introduced in Section 1, the likelihood of the data is L = L(p;cal,ca2 ..... con) n = II II p(AY i=1 (A~)ER Recall the derivation of the maximum-likelihood estimator of p: The log of the likelihood is: n ~ ~f(A --+ a;cai)logp(A ~ a). (2) AEV a s.t. i=1 The function p : R ~ [0,1] subject to (1) that maximizes (2) satisfies: 6 AAp(A ~ a) + f(A ~ a;cai)logp(A ~ a) = 0 AEV   i=1 (A~o~)ER V(B ~/3) E R where {AA}AEV are Lagrange multipliers. Denote the maximum-likelihood estimator by fi: n B AB q- ~i=lf( --+ /3;ca;) = 0 V(S ~ /3) E R f,(B +/3) Since ~ fi(B+/3)=l) fl sA. (8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H <B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, "relative frequency," estimator. Suppose B E V is unobserved among the parse trees cabc02,-..,can. Then we can assign fi(B --+ fl) arbitrarily, requiring only that (1) be respected. Evidently the likelihood is unaffected by the particular assignment of fi(B --~ fl). Furthermore, it is not hard to see that any such B has probability zero of arising in any derivation that is based upon the maximum-likelihood probabilitiesg--~ence the issue of tightness is independent of this assignment. We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca) is the probability of ca ff fl under the maximum-likelihood production probabilities, then fi(f~) = 1. 3 Consider any sequence of productions that leads from S to B. If the parent (antecedent) of B arose in. the sample, then the last production has ~ probability zero and hence the sequence has probability zero. Otherwise, move "up" through the ancestors of B until finding the first variable in the S-to-B sequence represented in the sample (certainly S is represented). Apply the same reasoning to the production from that variable, and conclude that the given sequence has/3 probability zero. Computational Linguistics Volume 24, Number 2 2.1 The EM Algorithm. Usually the derivation trees are unobserved--the sample, or corpus, contains only the yields Y(wl), Y(w2) ..... Y(wn) (Y(wi) E T* for each 1 < i < n). The likelihood is substantially more complex, since p(Y(w)) is now a marginal probability; we need to sum over the set of w E f~ that yield Y(w): p(Y(w)) = E p(Y(w')). wlEU~ Y(w¢)=Y(oa) In the case where only yields are observed, the treatment is complicated considerably by the possibility of null productions (A --, 0) and unit productions (A ~ B E V). If, however, the language of the grammar does not include the null string, then there is an equivalent grammar (one with the same language) that has no null productions and no unit productions (cf. Hopcroft & Ullman [1979], Theorem 4.4). It is, then, perhaps best to simplify the treatment by assuming that there are no null or unit productions. Therefore, when the corpus consists of yields only, we shall assume a priori a model free of null and unit productions, and study tightness for probabilities estimated under such a model. Based upon the results of Stolcke [1995] it is likely that this restriction can be relaxed, but we have not pursued this. Letting ~y denote {w Efk Y(w) = Y}, the likelihood of the corpus becomes n H E H P(A--'~oL)f(A~;~)" i=1 ~OE~y(~i) (A---~o~)ER And the maximum-likelihood equation becomes + p(B fl) Ei=l EwEfly(wi,I-I(A--.)cR p(A -~ a)f(A-~";~) = 0 fT(B ~ /3) = ~iL1 Ep~f(B ~ fl;w)lw E ~y(~,)] (4) ,~s,,, ~Ei=IEpV(" a;w)lw E ~Y(o~,)] E(B_~,E B ~ where E~ is expectation under fi and where "]w E~-~y(wi)" means "conditioned on 0.2E ~-~Y(wi)'" There is no hope for a closed form solution, but (4) does suggest an iteration scheme, which, as it turns out, "climbs" the likelihood surface (though there are no guarantees about approaching a global maximum): Let P0 be an arbitrary assignment respecting (1). Define a sequence of probabilities, ~n, by the iteration ~n+i(B ~ fl) = ~i'=1E~,[f(B ~ fl;w)iw E fly(w,)] (5) ~ ~" ~7=, E~,[f(B ~ a;wliw E aY(o,D] (8~a)ER The right-hand side is manageable, as long as we can manageably compute all possible parses of a sentence (yield) Y(w). (More efficient approaches exist; see Baker [1979].) This iteration procedure is an instance of the EM Algorithm. Baum [1972] first introduced it for hidden Markov models (regular grammars) and Baker [1979] extended it to the problem addressed here (estimation for context-free grammars). Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization. The right-hand side of (5) is computed using the expectedfrequencies under j~,; pn+l is then the maximum-likelihood estimator, treating the expected frequencies as though they were observed frequencies. The issue of tightness comes up again. We will show that pn(f~) = 1 for each n > 0.  Given a context-free grammar G = (V, T, R, S), let f2 be the set of finite parse trees, let p : R ~ [0,1] be a system of production probabilities satisfying (1), and let wl, w2,. ., w, be a set (sample) of finite parse trees 0;k EfL For now, null and unit productions are permitted. Finally, let ~ be the maximum-likelihood estimator of p, as defined by (3). (See also the remarks following [3] concerning variables unobserved in wl, 0;2 .... , w,.) More generally, ~ will refer to the probability distribution on (possibly infinite) parse trees induced by the maximum-likelihood estimator. Theorem ~b(~) = 1 Proof Let qA = p (derivation tree rooted with A fails to terminate). We will show that qs = 0 (i.e., derivation trees rooted with S always terminate). For each A E V, let F(A; w) be the number of instances of A in w and let F(A; w) be the number of nonroot instances of A in w. Given oz E (V U T)*, let nA(cZ)be the number of instances of A in the string o~, and, finally, let ai be the ith component of the string o~. For any A E V: qA = p(UBEv U ..... t~,~ U i s,t. oti=B{Olifails to terminate}) -~ Z p(U a s.t. B~c, [-Ji s.t. o~i=B {Oli fails to terminate}) (A~c,)ERBEV = Z Z fi(A ~ c~)~(Ui s.t. ozi=B{Olifails to terminate}lA ~ ol) BEV a s.t. B~c, (A~c~)ER < ~ ~ ~(A~cz)nB(eOqB BEV a s.t. aEc~ { Y]~, .~, nB(a) Y'~i__,f(A ---* ol;wi) } .... n s, " a B*V E(A~,e a ~i=lf( ----r OGWi) { ~in=l E ..... ,~,~ nB(ol)f(A --'~ Ol;Wi) } qB n BEV Ei=I Z ,Z~'i~af(A ~ °Gwi) { ~in=l ~ ~.'. "E~ nn(ol)f(A "---~ Ol;Wi) } Z (A~cQER = qB n BEV Zi=I F(A;¢0i) yt qA Z F(A; wi) Z qB ~ Z nB(OOf(m ~ OGCdi) i=1 BEV i=1 a s.t. Be~ (A~o,)ER Computational Linguistics Volume 24, Number 2 Sum over A E V: E qA E F(A;wi) AEV i~-1 _< E qB ~ E E nB(ol)f(A ~ BEV i=1 AEV c~ s.t. B~a (A~c~)ER c~;wi) n = ~ qB ~l:(B;wi) BEV i=1 i.e./ H qA E(Ie(A;wi) -F(A;wi)) ~ 0 AEV i=1 Clearly, for every i = 1,2,...,n F(A;wi) = F(A;wi) whenever A # S and F(S;wi) < F(S; wi). Hence qs = 0, completing the proof of the theorem. [] Now let ~, be the system of probabilities produced by the nth iteration of the EM Algorithm (5): Corollary If R contains no null productions and no unit productions, then ~,(f~) = 1 Vn > 1. Proof Almost identical, except that we use (5) in place of (3) and end up with: n E qA EEG_1[F(A;wi) -F(A;wi)lw C fly(w,)] ~ 0. (6) AEV i=1 In the absence of unit productions and null productions, F(A;w) < 21w [ (twice the length of the string w). Hence the expectations in (6) are finite. Furthermore, F(A; w) and F(A; ~) satisfy the same conditions as before: I:(A; w) = F(A; w) except when A = S, in which case F(A; w) < F(A; w). Again, we conclude that qs = O. [] Acknowledgments statistical estimation of probabilistic We are indebted to Mark Johnson for functions of Markov processes. encouraging us to look at this problem in Inequalities, 3:18. the first place, and for much good advice Booth, T. L. and R. A. Thompson. 1973. along the way. This work was supported by Applying probability measures to abstract the Army Research Office languages. IEEE Trans. on Computers, (DAAL0392-G-0115), the National Science C-22:442450. Foundation (DMS9217655), and the Office Dempster, A., N. Laird, and D. Rubin. 1977. of Naval Research (N0001496-10647). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:138.
 A Decision Tree of Bigrams is an Accurate Predictor of Word Sense  This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby. This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise. It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.  Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs. For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined. For example, suppose bill has the following set of possible meanings: a piece of currency, pending legislation, or a bird jaw. When used in the context of The Senate bill is under consideration, a human reader immediately understands that bill is being used in the legislative sense. However, a computer program attempting to perform the same task faces a diÆcult problem since it does not have the bene?t of innate common{sense or linguistic knowledge. Rather than attempting to provide computer programs with real{world knowledge comparable to that of humans, natural language processing has turned to corpus{based methods. These approaches use techniques from statistics and machine learning to induce models of language usage from large samples of text. These models are trained to perform particular tasks, usually via supervised learning. This paper describes an approach where a decision tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense{tag that denotes the most appropriate sense for that context. Prior to learning, the sense{tagged corpus must be converted into a more regular form suitable for automatic processing. Each sense{tagged occurrence of an ambiguous word is converted into a feature vector, where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process. Given the exibility and complexity of human language, there is potentially an in?nite set of features that could be utilized. However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources. These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word. The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text. The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated. We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy. It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution. One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements. This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning. Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison. The experimental data is discussed, and then the empirical results are presented. We close with an analysis of our ?ndings and a discussion of related work.  We have developed an approach to word sense disambiguation that represents text entirely in terms of the occurrence of bigrams, which we de?ne to be two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation of Bigram Counts consecutive words that occur in a text. The distributional characteristics of bigrams are fairly consistent across corpora; a majority of them only occur one time. Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen. We explore two alternatives, the power divergence family of goodness of ?t statistics and the Dice CoeÆcient, an information theoretic measure related to point- wise Mutual Information. Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 ? 2 contingency table. The value of n 11 shows how many times the bigram big cat occurs in the corpus. The value of n 12 shows how often bigrams occur where big is the ?rst word and cat is not the second. The counts in n +1 and n 1+ indicate how often words big and cat occur as the ?rst and second words of any bigram in the corpus. The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family. (Cressie and Read, 1984) introduce the power divergence family of goodness of ?t statistics. A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic. These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ij is estimated based on the assumption that the component words in the bigram occur together strictly by chance. (Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions. However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other. In light of this, (Pedersen, 1996) presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio. Unfortunately it is usually not clear which test is most appropriate for a particular sample of data. We take the following approach, based on the observation that all tests should assign approximately the same measure of statistical signi?cance when the bi- gram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of ?t statistics. We perform tests using X 2 , G 2 , and Fisher's exact test for each bigram. If the resulting measures of statistical signi?cance di?er, then the distribution of the bigram counts is causing at least one of the tests to become unreliable. When this occurs we rely upon the value from Fisher's exact test since it makes fewer assumptions about the underlying distribution of data. For the experiments in this paper, we identi?ed the top 100 ranked bigrams that occur more than 5 times in the training corpus associated with a word. There were no cases where rankings produced by G 2 , X 2 , and Fisher's exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded. Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic. 2.2 Dice CoeÆcient. The Dice CoeÆcient is a descriptive statistic that provides a measure of association among two words in a corpus. It is similar to pointwise Mutual Information, a widely used measure that was ?rst introduced for identifying lexical relationships in (Church and Hanks, 1990). Pointwise Mutual Information can be de?ned as follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ where w 1 and w 2 represent the two words that make up the bigram. Pointwise Mutual Information quanti?es how often two words occur together in a bigram (the numerator) relative to how often they occur overall in the corpus (the denominator). However, there is a curious limitation to pointwise Mutual Information. A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases. Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bi- grams that occur one time, and whose component words never occur outside that bigram. These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information. The Dice CoeÆcient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- eÆcient are similar to those of Mutual Information. The relationship between pointwise Mutual Information and the Dice CoeÆcient is also discussed in (Smadja et al., 1996). We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests. This software is written in Perl and is freely available from www.d.umn.edu/~tpederse.  Decision trees are among the most widely used machine learning algorithms. They perform a general to speci?c search of a feature space, adding the most informative features to a tree structure as the search proceeds. The objective is to select a minimal set of features that eÆciently partitions the feature space into classes of observations and assemble them into a tree. In our case, the observations are manually sense{tagged examples of an ambiguous word in context and the partitions correspond to the di?erent possible senses. Each feature selected during the search process is represented by a node in the learned decision tree. Each node represents a choice point between a number of di?erent possible values for a feature. Learning continues until all the training examples are accounted for by the decision tree. In general, such a tree will be overly speci?c to the training data and not generalize well to new examples. Therefore learning is followed by a pruning step where some nodes are eliminated or reorganized to produce a tree that can generalize to new circumstances. Test instances are disambiguated by ?nding a path through the learned decision tree from the root to a leaf node that corresponds with the observed features. An instance of an ambiguous word is dis- ambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context. We also include three benchmark learning algorithms in this study: the majority classi?er, the decision stump, and the Naive Bayesian classi?er. The majority classi?er assigns the most common sense in the training data to every instance in the test data. A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree. The Naive Bayesian classi?er (Duda and Hart, 1973) is based on certain blanket assumptions about the interactions among features in a corpus. There is no search of the feature space performed to build a representative model as is the case with decision trees. Instead, all features are included in the classi- ?er and assumed to be relevant to the task at hand. There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word. It is most often used with a bag of words feature set, where every word in the training sample is represented by a binary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word. We use the Weka (Witten and Frank, 2000) implementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classi?er. Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml.  Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems. Ten teams participated in the supervised learning portion of this event. Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarri? and Palmer, 2000). We included all 36 tasks from SENSEVAL for which training and test data were provided. Each task requires that the occurrences of a particular word in the test data be disambiguated based on a model learned from the sense{tagged instances in the training data. Some words were used in multiple tasks as di?erent parts of speech. For example, there were two tasks associated with bet, one for its use as a noun and the other as a verb. Thus, there are 36 tasks involving the disambiguation of 29 di?erent words. The words and part of speech associated with each task are shown in Table 1 in column 1. Note that the parts of speech are encoded as n for noun, a for adjective, v for verb, and p for words where the part of speech was not provided. The number of test and training instances for each task are shown in columns 2 and 4. Each instance consists of the sentence in which the ambiguous word occurs as well as one or two surrounding sentences. In general the total context available for each ambiguous word is less than 100 surrounding words. The number of distinct senses in the test data for each task is shown in column 3.  The following process is repeated for each task. Capitalization and punctuation are removed from the training and test data. Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice CoeÆcient. The bigram must have occurred 5 or more times to be included as a feature. This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process. The training and test data are converted to feature vectors where each feature represents the occurrence of one of the bigrams that belong in the feature set. This representation of the training data is the actual input to the learning algorithms. Decision tree and decision stump learning is performed twice, once using the feature set determined by the power divergence statistic and again using the feature set identi?ed by the Dice CoeÆcient. The majority classi?er simply determines the most frequent sense in the training data and assigns that to all instances in the test data. The Naive Bayesian classi?er is based on a feature set where every word that occurs 5 or more times in the training data is included as a feature. All of these learned models are used to disambiguate the test data. The test data is kept separate until this stage. We employ a ?ne grained scoring method, where a word is counted as correctly disambiguated only when the assigned sense tag exactly matches the true sense tag. No partial credit is assigned for near misses.  The accuracy attained by each of the learning algorithms is shown in Table 1. Column 5 reports the accuracy of the majority classi?er, columns 6 and 7 show the best and average accuracy reported by the 10 participating SENSEVAL teams. The evaluation at SENSEVAL was based on precision and recall, so we converted those scores to accuracy by taking their product. However, the best precision and recall may have come from di?erent teams, so the best accuracy shown in column 6 may actually be higher than that of any single participating SENSEVAL system. The average accuracy in column 7 is the product of the average precision and recall reported for the participating SENSEVAL teams. Column 8 shows the accuracy of the decision tree using the J48 learning algorithm and the features identi?ed by a power divergence statistic. Column 10 shows the accuracy of the decision tree when the Dice CoeÆcient selects the features. Columns 9 and 11 show the accuracy of the decision stump based on the power divergence statistic and the Dice CoeÆcient respectively. Finally, column 13 shows the accuracy of the Naive Bayesian classi?er based on a bag of words feature set. The most accurate method is the decision tree based on a feature set determined by the power divergence statistic. The last line of Table 1 shows the win-tie-loss score of the decision tree/power divergence method relative to every other method. A win shows it was more accurate than the method in the column, a loss means it was less accurate, and a tie means it was equally accurate. The decision tree/power divergence method was more accurate than the best reported SENSEVAL results for 19 of the 36 tasks, and more accurate for 30 of the 36 tasks when compared to the average reported accuracy. The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks. In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice CoeÆcient. The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither. The Dice CoeÆcient is based strictly on the event where w 1 and w 2 occur together in a bigram. There are 6 tasks where the decision tree / power divergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, and sanction-p. The most dramatic difference occurred with amaze-v, where the SENSE- VAL average was 92.4% and the decision tree accuracy was 58.6%. However, this was an unusual task where every instance in the test data belonged to a single sense that was a minority sense in the training data.  The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2. Column 1 shows the word and part of speech. Columns 2, 3, and 4 are based on the feature set selected by the power divergence statistic while columns 5, 6, and 7 are based on the Dice CoeÆ- cient. Columns 2 and 5 show the node selected to serve as the decision stump. Columns 3 and 6 show the number of leaf nodes in the learned decision tree relative to the number of total nodes. Columns 4 and 7 show the number of bigram features selected Table 1: Experimental Results. This table shows that there is little di?erence in the decision stump nodes selected from feature sets determined by the power divergence statistics versus the Dice CoeÆcient. This is to be expected since the top ranked bigrams for each measure are consistent, and the decision stump node is generally chosen from among those. However, there are di?erences between the feature sets selected by the power divergence statistics and the Dice CoeÆcient. These are re ected in the different sized trees that are learned based on these feature sets. The number of leaf nodes and the total number of nodes for each learned tree is shown in columns 3 and 6. The number of internal nodes is simply the di?erence between the total nodes and the leaf nodes. Each leaf node represents the end of a path through the decision tree that makes a sense distinction. Since a bigram feature can only appear once in the decision tree, the number of inter- Table 2: Decision Tree and Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n by accident 8/15 101 by accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n in excess 13/25 104 in excess 11/21 102 oat-n the oat 7/13 13 the oat 7/13 13 giant-n the giants 16/31 103 the giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n in the 1/1 7 in the 1/1 7 promise-n promise of 95/189 100 a promising 49/97 107 sack-n the sack 5/9 31 the sack 5/9 31 scrap-n scrap of 7/13 8 scrap of 7/13 8 shirt-n shirt and 38/75 101 shirt and 55/109 101 amaze-v amazed at 11/21 102 amazed at 11/21 102 bet-v i bet 4/7 10 i bet 4/7 10 bother-v be bothered 19/37 101 be bothered 20/39 106 bury-v buried in 28/55 103 buried in 32/63 103 calculate-v calculated to 5/9 103 calculated to 5/9 103 consume-v on the 4/7 20 on the 4/7 20 derive-v derived from 10/19 104 derived from 10/19 104 oat-v oated on 24/47 80 oated on 24/47 80 invade-v to invade 55/109 107 to invade 66/127 108 promise-v promise to 3/5 100 promise you 5/9 106 sack-v return to 1/1 91 return to 1/1 91 scrap-v of the 1/1 7 of the 1/1 7 seize-v to seize 26/51 104 to seize 57/113 104 brilliant-a a brilliant 26/51 101 a brilliant 42/83 103 oating-a in the 7/13 10 in the 7/13 10 generous-a a generous 57/113 103 a generous 56/111 102 giant-a the giant 2/3 102 a giant 1/1 101 modest-a a modest 14/27 101 a modest 10/19 105 slight-a the slightest 2/3 105 the slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band of 14/27 100 the band 21/41 117 bitter-p a bitter 22/43 54 a bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p his head 90/179 100 his head 81/161 105 nal nodes represents the number of bigram features selected by the decision tree learner. One of our original hypotheses was that accurate decision trees of bigrams will include a relatively small number of features. This was motivated by the success of decision stumps in performing disambiguation based on a single bigram feature. In these experiments, there were no decision trees that used all of the bigram features identi?ed by the ?ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features. This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7. 1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice CoeÆcient. This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner. If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set. suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses. In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the overall Mutual Information between the bigram and a particular word sense. Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods. A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classi?er. A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump.  One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies. We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. While the accuracy of this approach was as good as any previously published results, the learned models were complex and diÆcult to interpret, in e?ect acting as very accurate black boxes. Our experience has been that variations in learning algorithms are far less signi?cant contributors to disambiguation accuracy than are variations in the feature set. In other words, an informative feature set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features. Therefore, our focus is on developing and discovering feature sets that make distinctions among word senses. Our learning algorithms must not only produce accurate models, but they should also shed new light on the relationships among features and allow us to continue re?ning and understanding our feature sets. We believe that decision trees meet these criteria. A wide range of implementations are available, and they are known to be robust and accurate across a range of domains. Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation.  Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)). While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word. We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed. The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)). Rather than building and traversing a tree to perform disambiguation, a list is employed. In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over{trained. However, we believe that fragmentation also re ects on the feature set used for learning. Ours consists of at most approximately 100 binary features. This results in a relatively small feature space that is not as likely to su?er from fragmentation as are larger spaces.  There are a number of immediate extensions to this work. The ?rst is to ease the requirement that bi- grams be made up of two consecutive words. Rather, we will search for bigrams where the component words may be separated by other words in the text. The second is to eliminate the ?ltering step by which candidate bigrams are selected by a power divergence statistic. Instead, the decision tree learner would consider all possible bigrams. Despite increasing the danger of fragmentation, this is an interesting issue since the bigrams judged most informative by the decision tree learner are not always ranked highly in the ?ltering step. In particular, we will determine if the ?ltering process ever eliminates bi- grams that could be signi?cant sources of disambiguation information. In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text. We are optimistic that this is viable, since bigram features are easy to identify in raw text.  This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation. The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words.  The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the OÆce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota. We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available. The comments of three anonymous reviewers were very helpful in preparing the ?nal version of this paper. A preliminary version of this paper appears in (Pedersen, 2001).
 Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution  We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor. BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning. These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible. BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources. Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.  The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information. The focus of our work is on the use of contextual role knowledge for coreference resolution. A contextual role represents the role that a noun phrase plays in an event or relationship. Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase. Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place. (b) After they were released... (c) After they blindfolded the men... In (b) âtheyâ refers to the kidnapping victims, but in (c) âtheyâ refers to the armed men. The role that each noun phrase plays in the kidnapping event is key to distinguishing these cases. The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released. The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims. We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions. BABAR employs information extraction techniques to represent and learn role relationships. Each pattern represents the role that a noun phrase plays in the surrounding context. BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data. Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics. BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions. In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned. Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features. Our coreference resolver also incorporates an existential noun phrase recognizer and a DempsterShafer probabilistic model to make resolution decisions. Section 4 presents experimen tal results on two corpora: the MUC4 terrorism corpus, and Reuters texts about natural disasters. Our results show that BABAR achieves good performance in both domains, and that the contextual role knowledge improves performance, especially on pronouns. Finally, Section 5 explains how BABAR relates to previous work, and Section 6 summarizes our conclusions.  In this section, we describe how contextual role knowledge is represented and learned. Section 2.1 describes how BABAR generates training examples to use in the learning process. We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents. Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples. 2.1 Reliable Case Resolutions. The first step in the learning process is to generate training examples consisting of anaphor/antecedent resolutions. BABAR uses two methods to identify anaphors that can be easily and reliably resolved with their antecedent: lexical seeding and syntactic seeding. 2.1.1 Lexical Seeding It is generally not safe to assume that multiple occurrences of a noun phrase refer to the same entity. For example, the company may refer to Company X in one paragraph and Company Y in another. However, lexically similar NPs usually refer to the same entity in two cases: proper names and existential noun phrases. BABAR uses a named entity recognizer to identify proper names that refer to people and companies. Proper names are assumed to be coreferent if they match exactly, or if they closely match based on a few heuristics. For example, a personâs full name will match with just their last name (e.g., âGeorge Bushâ and âBushâ), and a company name will match with and without a corporate suffix (e.g., âIBM Corp.â and âIBMâ). Proper names that match are resolved with each other. The second case involves existential noun phrases (Allen, 1995), which are noun phrases that uniquely specify an object or concept and therefore do not need a prior referent in the discourse. In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood. For example, a story can mention âthe FBIâ, âthe White Houseâ, or âthe weatherâ without any prior referent in the story. Although these existential NPs do not need a prior referent, they may occur multiple times in a document. By definition, each existential NP uniquely specifies an object or concept, so we can infer that all instances of the same existential NP are coreferent (e.g., âthe FBIâ always refers to the same entity). Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved. Table 1 briefly describes the seven syntactic heuristics used by BABAR to resolve noun phrases. Words and punctuation that appear in brackets are considered optional. The anaphor and antecedent appear in boldface. 1. Reflexive pronouns with only 1 NP in scope.. Ex: The regime gives itself the right... 2. Relative pronouns with only 1 NP in scope.. Ex: The brigade, which attacked ...  Ex: Mr. Cristiani is the president ...  Ex: The government said it ...  Ex: He was found in San Jose, where ...  Ex: Mr. Cristiani, president of the country ...  Ex: Mr. Bush disclosed the policy by reading it... Table 1: Syntactic Seeding Heuristics BABARâs reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge. For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding. For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding. 2.2 Contextual Role Knowledge. Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes. First, we describe how the caseframes are represented and learned. Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes. 2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too. an event. For IE, the system must be able to distinguish between semantically similar noun phrases that play different roles in an event. For example, management succession systems must distinguish between a person who is fired and a person who is hired. Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime. We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain. Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found. For example, kidnapping victims should be extracted from the subject of the verb âkidnappedâ when it occurs in the passive voice (the shorthand representation of this pattern would be â<subject> were kidnappedâ). The types of patterns produced by AutoSlog are outlined in (Riloff, 1996). Ideally weâd like to know the thematic role of each extracted noun phrase, but AutoSlog does not generate thematic roles. As a (crude) approximation, we normalize the extraction patterns with respect to active and passive voice and label those extractions as agents or patients. For example, the passive voice pattern â<subject> were kidnappedâ and the active voice pattern âkidnapped <direct object>â are merged into a single normalized pattern âkidnapped <patient>â.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, â<agent> kidnappedâ or âkidnapped <patient>â), and (2) predicate-argument relations associated with both verbs and nouns (e.g., âkidnapped for <np>â or âvehicle with <np>â). We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus. The learned patterns are then normalized and applied to the corpus. This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted. The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data. 2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions. Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice. 3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot. conceptual relationship in the discourse. For example, co-occurring caseframes may reflect synonymy (e.g., â<patient> kidnappedâ and â<patient> abductedâ) or related events (e.g., â<patient> kidnappedâ and â<patient> releasedâ). We do not attempt to identify the types of relationships that are found. BABAR merely identifies caseframes that frequently co-occur in coreference resolutions. Te rro ris m Na tur al Dis ast ers mu rde r of < NP > kill ed <p atie nt > <a ge nt > da ma ged wa s inj ure d in < NP > <a ge nt > rep ort ed <a ge nt > add ed <a ge nt > occ urr ed cau se of < NP > <a ge nt > stat ed <a ge nt > add ed <a ge nt > wr eak ed <a ge nt > cro sse d per pet rat ed <p atie nt > con de mn ed <p atie nt > dri ver of < NP > <a ge nt > car ryi ng Figure 1: Caseframe Network Examples Figure 1 shows examples of caseframes that co-occur in resolutions, both in the terrorism and natural disaster domains. The terrorism examples reflect fairly obvious relationships: people who are murdered are killed; agents that âreportâ things also âaddâ and âstateâ things; crimes that are âperpetratedâ are often later âcondemnedâ. In the natural disasters domain, agents are often forces of nature, such as hurricanes or wildfires. Figure 1 reveals that an event that âdamagedâ objects may also cause injuries; a disaster that âoccurredâ may be investigated to find its âcauseâ; a disaster may âwreakâ havoc as it âcrossesâ geographic regions; and vehicles that have a âdriverâ may also âcarryâ items. During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent. Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence. For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphorâs caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions. If so, the CF Network reports that the anaphor and candidate may be coreferent. 2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source. For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus. For each resolution in the training data, BABAR also associates the co-referring expression of an NP with the NPâs caseframe. For example, if X and Y are coreferent, then both X and Y are considered to co-occur with the caseframe that extracts X as well as the caseframe that extracts Y. We will refer to the set of nouns that co-occur with a caseframe as the lexical expectations of the case- frame. Figure 2 shows examples of lexical expectations that were learned for both domains. collected too. We will refer to the semantic classes that co-occur with a caseframe as the semantic expectations of the caseframe. Figure 3 shows examples of semantic expectations that were learned. For example, BABAR learned that agents that âassassinateâ or âinvestigate a causeâ are usually humans or groups (i.e., organizations). T e r r o r i s m Ca sef ra me Semantic Classes <a ge nt > ass ass ina ted group, human inv esti gat ion int o < N P> event exp lod ed out sid e < N P> building N a t u r a l D i s a s t e r s Ca sef ra me Semantic Classes <a ge nt > inv esti gat ing cau se group, human sur viv or of < N P> event, natphenom hit wit h < N P> attribute, natphenom Figure 3: Semantic Caseframe Expectations Figure 2: Lexical Caseframe Expectations To illustrate how lexical expectations are used, suppose we want to determine whether noun phrase X is the antecedent for noun phrase Y. If they are coreferent, then X and Y should be substitutable for one another in the story.4 Consider these sentences: (S1) Fred was killed by a masked man with a revolver. (S2) The burglar fired the gun three times and fled. âThe gunâ will be extracted by the caseframe âfired <patient>â. Its correct antecedent is âa revolverâ, which is extracted by the caseframe âkilled with <NP>â. If âgunâ and ârevolverâ refer to the same object, then it should also be acceptable to say that Fred was âkilled with a gunâ and that the burglar âfireda revolverâ. During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor. If either case is true, then CFLex reports that the anaphor and candidate might be coreferent. 2.2.4 Semantic Caseframe Expectations The third type of contextual role knowledge learned by BABAR is Semantic Caseframe Expectations. Semantic expectations are analogous to lexical expectations except that they represent semantic classes rather than nouns. For each caseframe, BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe. As with lexical expections, the semantic classes of co-referring expressions are 4 They may not be perfectly substitutable, for example one NP may be more specific (e.g., âheâ vs. âJohn F. Kennedyâ). But in most cases they can be used interchangably. For each domain, we created a semantic dictionary by doing two things. First, we parsed the training corpus, collected all the noun phrases, and looked up each head noun in WordNet (Miller, 1990). We tagged each noun with the top-level semantic classes assigned to it in Word- Net. Second, we identified the 100 most frequent nouns in the training corpus and manually labeled them with semantic tags. This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain. Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected. However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse. For example, both a chair and a truck would be labeled as artifacts, but this does not at all suggest that they are coreferent. So we decided to use semantic class information only to rule out candidates. If two nouns have mutually exclusive semantic classes, then they cannot be coreferent. This solution also obviates the need to perform word sense disambiguation. Each word is simply tagged with the semantic classes corresponding to all of its senses. If these sets do not overlap, then the words cannot be coreferent. The semantic caseframe expectations are used in two ways. One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves. Given an anaphor and candidate, BABAR checks (1) whether the semantic classes of the anaphor intersect with the semantic expectations of the caseframe that extracts the candidate, and (2) whether the semantic classes of the candidate intersect with the semantic ex pectations of the caseframe that extracts the anaphor. If one of these checks fails then this knowledge source reports that the candidate is not a viable antecedent for the anaphor. A different knowledge source, called CFSemCFSem, compares the semantic expectations of the caseframe that extracts the anaphor with the semantic expectations of the caseframe that extracts the candidate. If the semantic expectations do not intersect, then we know that the case- frames extract mutually exclusive types of noun phrases. In this case, this knowledge source reports that the candidate is not a viable antecedent for the anaphor. 2.3 Assigning Evidence Values. Contextual role knowledge provides evidence as to whether a candidate is a plausible antecedent for an anaphor. The two knowledge sources that use semantic expectations, WordSemCFSem and CFSemCFSem, always return values of -1 or 0. -1 means that an NP should be ruled out as a possible antecedent, and 0 means that the knowledge source remains neutral (i.e., it has no reason to believe that they cannot be coreferent). The CFLex and CFNet knowledge sources provide positive evidence that a candidate NP and anaphor might be coreferent. They return a value in the range [0,1], where 0 indicates neutrality and 1 indicates the strongest belief that the candidate and anaphor are coreferent. BABAR uses the log-likelihood statistic (Dunning, 1993) to evaluate the strength of a co-occurrence relationship. For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the Ï2 table to obtain a confidence level. The confidence level is then used as the belief value for the knowledge source. For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent. 3 The Coreference Resolution Model. Given a document to process, BABAR uses four modules to perform coreference resolution. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1. Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources. Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision. In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions. 3.1 General Knowledge Sources. Figure 4 shows the seven general knowledge sources (KSs) that represent features commonly used for coreference resolution. The gender, number, and scoping KSs eliminate candidates from consideration. The scoping heuristics are based on the anaphor type: for reflexive pronouns the scope is the current clause, for relative pronouns it is the prior clause following its VP, for personal pronouns it is the anaphorâs sentence and two preceding sentences, and for definite NPs it is the anaphorâs sentence and eight preceding sentences. The semantic agreement KS eliminates some candidates, but also provides positive evidence in one case: if the candidate and anaphor both have semantic tags human, company, date, or location that were assigned via NER or the manually labeled dictionary entries. The rationale for treating these semantic labels differently is that they are specific and reliable (as opposed to the WordNet classes, which are more coarse and more noisy due to polysemy). KS Function Ge nde r filters candidate if gender doesnât agree. Nu mb er filters candidate if number doesnât agree. Sc opi ng filters candidate if outside the anaphorâs scope. Se ma nti c (a) filters candidate if its semantic tags d o n â t i n t e r s e c t w i t h t h o s e o f t h e a n a p h o r . ( b ) s u p p o r t s c a n d i d a t e i f s e l e c t e d s e m a n t i c t a g s m a t c h t h o s e o f t h e a n a p h o r . Le xic al computes degree of lexical overlap b e t w e e n t h e c a n d i d a t e a n d t h e a n a p h o r . Re cen cy computes the relative distance between the c a n d i d a t e a n d t h e a n a p h o r . Sy nR ole computes relative frequency with which the c a n d i d a t e â s s y n t a c t i c r o l e o c c u r s i n r e s o l u t i o n s . Figure 4: General Knowledge Sources The Lexical KS returns 1 if the candidate and anaphor are identical, 0.5 if their head nouns match, and 0 otherwise. The Recency KS computes the distance between the candidate and the anaphor relative to its scope. The SynRole KS computes the relative frequency with which the candidatesâ syntactic role (subject, direct object, PP object) appeared in resolutions in the training set. During development, we sensed that the Recency and Syn- role KSs did not deserve to be on equal footing with the other KSs because their knowledge was so general. Consequently, we cut their evidence values in half to lessen their influence. 3.2 The DempsterShafer Decision Model. BABAR uses a DempsterShafer decision model (Stefik, 1995) to combine the evidence provided by the knowledge sources. Our motivation for using DempsterShafer is that it provides a well-principled framework for combining evidence from multiple sources with respect to competing hypotheses. In our situation, the competing hypotheses are the possible antecedents for an anaphor. An important aspect of the DempsterShafer model is that it operates on sets of hypotheses. If evidence indicates that hypotheses C and D are less likely than hypotheses A and B, then probabilities are redistributed to reflect the fact that {A, B} is more likely to contain the answer than {C, D}. The ability to redistribute belief values across sets rather than individual hypotheses is key. The evidence may not say anything about whether A is more likely than B, only that C and D are not likely. Each set is assigned two values: belief and plausibility. Initially, the DempsterShafer model assumes that all hypotheses are equally likely, so it creates a set called Î¸ that includes all hypotheses. Î¸ has a belief value of 1.0, indicating complete certainty that the correct hypothesis is included in the set, and a plausibility value of 1.0, indicating that there is no evidence for competing hypotheses.5 As evidence is collected and the likely hypotheses are whittled down, belief is redistributed to subsets of Î¸. Formally, the DempsterShafer theory defines a probability density function m(S), where S is a set of hypotheses. m(S) represents the belief that the correct hypothesis is included in S. The model assumes that evidence also arrives as a probability density function (pdf) over sets of hypotheses.6 Integrating new evidence into the existing model is therefore simply a matter of defining a function to merge pdfs, one representing the current belief system and one representing the beliefs of the new evidence. The DempsterShafer rule for combining pdfs is: to {C}, meaning that it is 70% sure the correct hypothesis is C. The intersection of these sets is the null set because these beliefs are contradictory. The belief value that would have been assigned to the intersection of these sets is .60*.70=.42, but this belief has nowhere to go because the null set is not permissible in the model.7 So this probability mass (.42) has to be redistributed. DempsterShafer handles this by re-normalizing all the belief values with respect to only the non-null sets (this is the purpose of the denominator in Equation 1). In our coreference resolver, we define Î¸ to be the set of all candidate antecedents for an anaphor. Each knowledge source then assigns a probability estimate to each candidate, which represents its belief that the candidate is the antecedent for the anaphor. The probabilities are incorporated into the DempsterShafer model using Equation 1. To resolve the anaphor, we survey the final belief values assigned to each candidateâs singleton set. If a candidate has a belief value â¥ .50, then we select that candidate as the antecedent for the anaphor. If no candidate satisfies this condition (which is often the case), then the anaphor is left unresolved. One of the strengths of the DempsterShafer model is its natural ability to recognize when several credible hypotheses are still in play. In this situation, BABAR takes the conservative approach and declines to make a resolution. 4 Evaluation Results. 4.1 Corpora. We evaluated BABAR on two domains: terrorism and natural disasters. We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuterâs text collection8 that had a subject code corresponding to natural disasters. For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X â©Y =S 1 â ) m1 (X ) â m2 (Y ) m1 (X ) â m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)). In the terrorism domain, 1600 texts were used for training and the 40 test docu X â©Y =â All sets of hypotheses (and their corresponding belief values) in the current model are crossed with the sets of hypotheses (and belief values) provided by the new evidence. Sometimes, however, these beliefs can be contradictory. For example, suppose the current model assigns a belief value of .60 to {A, B}, meaning that it is 60% sure that the correct hypothesis is either A or B. Then new evidence arrives with a belief value of .70 assigned 5 Initially there are no competing hypotheses because all hypotheses are included in Î¸ by definition. 6 Our knowledge sources return some sort of probability estimate, although in some cases this estimate is not especially well-principled (e.g., the Recency KS). ments contained 322 anaphoric links. For the disasters domain, 8245 texts were used for training and the 40 test documents contained 447 anaphoric links. In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998). We considered using the MUC6 and MUC7 data sets, but their training sets were far too small to learn reliable co-occurrence statistics for a large set of contextual role relationships. Therefore we opted to use the much 7 The DempsterShafer theory assumes that one of the hypotheses in Î¸ is correct, so eliminating all of the hypotheses violates this assumption. 8 Volume 1, English language, 19961997, Format version 1, correction level 0 An ap ho r T e r r o r i s m R e c Pr F D i s a s t e r s R e c Pr F De f. NP s Pro no uns .43 .79 .55 .50 .72 .59 .42 .91 .58 .42 .82 .56 Tot al .46 .76 .57 .42 .87 .57 Table 2: General Knowledge Sources Table 4: Individual Performance of KSs for Terrorism Table 3: General + Contextual Role Knowledge Sources larger MUC4 and Reuters corpora.9 4.2 Experiments. We adopted the MUC6 guidelines for evaluating coreference relationships based on transitivity in anaphoric chains. For example, if {N P1, N P2, N P3} are all coreferent, then each NP must be linked to one of the other two NPs. First, we evaluated BABAR using only the seven general knowledge sources. Table 2 shows BABARâs performance. We measured recall (Rec), precision (Pr), and the F-measure (F) with recall and precision equally weighted. BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters. We suspect that the higher precision in the disasters domain may be due to its substantially larger training corpus. Table 3 shows BABARâs performance when the four contextual role knowledge sources are added. The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision. The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters. The difference in performance between pronouns and definite noun phrases surprised us. Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs. Since pronouns carry little semantics of their own, resolving them depends almost entirely on context. In contrast, even though context can be helpful for resolving definite NPs, context can be trumped by the semantics of the nouns themselves. For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections. Table 5: Individual Performance of KSs for Disasters (e.g., âthe mayorâ vs. âthe journalistâ). We also performed experiments to evaluate the impact of each type of contextual role knowledge separately. Tables 4 and 5 show BABARâs performance when just one contextual role knowledge source is used at a time. For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision. For pronouns, however, all of the knowledge sources increased recall, often substantially, and with little if any decrease in precision. This result suggests that all of contextual role KSs can provide useful information for resolving anaphora. Tables 4 and 5 also show that putting all of the contextual role KSs in play at the same time produces the greatest performance gain. There are two possible reasons: (1) the knowledge sources are resolving different cases of anaphora, and (2) the knowledge sources provide multiple pieces of evidence in support of (or against) a candidate, thereby acting synergistically to push the DempsterShafer model over the belief threshold in favor of a single candidate. 5 Related Work. Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR. Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations. Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate. However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver. The learned information was recycled back into the resolver to improve its performance. This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions. (Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference. Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001). These systems rely on a training corpus that has been manually annotated with coreference links. 6 Conclusions. The goal of our research was to explore the use of contextual role knowledge for coreference resolution. We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability. We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model. Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information. We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases. This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics. In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used. 7 Acknowledgements. This work was supported in part by the National Science Foundation under grant IRI9704240. The inventions disclosed herein are the subject of a patent application owned by the University of Utah and licensed on an exclusive basis to Attensity Corporation.
 Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation  We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.  The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as âBâ if it is the first character of a multiple-character word, or âOâ if the character functions as an independent word, or âIâ otherwise.â For example, â (whole) (Beijing city)â is labeled as â (whole)/O (north)/B (capital)/I (city)/Iâ. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, â (whole) (Beijing city)â is labeled as â (whole)/O (Beijing)/B (city)/Iâ in the subword-based tagging, where â (Beijing)/Bâ is labeled as one unit. We will give a detailed description of this approach in Section 2. â Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness. By this approach we can change R-oovs and R-ivs and find an optimal tradeoff. This approach will be described in Section 2.2. In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method. Section 3 presents our experimental results. Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared. Section 5 provides the concluding remarks.  Our word segmentation process is illustrated in Fig. 1. It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging. An example exhibiting each stepâs results is also given in the figure. Since the dictionary-based approach is a well-known method, we skip its technical descriptions. However, keep in mind that the dictionary-based approach can produce a higher R-iv rate. We will use this advantage in the confidence measure approach. 2.1 Subword-based IOB tagging using CRFs. There are several steps to train a subword-based IOB tag- ger. First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193â196, New York, June 2006. Qc 2006 Association for Computational Linguistics input åã£á¯¹Ô£à³¼à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation å ã£ á¯¹ Ô£ à³¼ à£«Òá +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation å/% ã£/, á¯¹/, Ô£/2 à³¼/2 à£«Ò/% á/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output åã£á¯¹ Ô£ à³¼ à£«Òá +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data. We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging. If the subset consists of Chinese characters only, it is a character-based IOB tagger. We regard the words in the subset as the subwords for the IOB tagging. Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them. For a character-based IOB tagger, there is only one possibility of re-segmentation. However, there are multiple choices for a subword-based IOB tagger. For example, â (Beijing-city)â can be segmented as â (Beijing-city)/O,â or â (Beijing)/B (city)/I,â or â (north)/B (capital)/I (city)/I.â In this work we used forward maximal match (FMM) for disambiguation. Of course, backward maximal match (BMM) or other approaches are also applicable. We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach. In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data. We downloaded and used the package âCRF++â from the site âhttp://www.chasen.org/Ëtaku/software.â According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively. The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method. In order to overcome overfitting, a gaussian prior was imposed in the training. The types of unigram features used in our experiments included the following types: w0 , wâ1 , w1 , wâ2 , w2 , w0 wâ1 , w0 w1 , wâ1 w1 , wâ2 wâ1 , w2 w0 where w stands for word. The subscripts are position indicators. 0 means the current word; â1, â2, the first or second word to the left; 1, 2, the first or second word to the right. For the bigram features, we only used the previous and the current observations, tâ1 t0 . As to feature selection, we simply used absolute counts for each feature in the training data. We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff. A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding. 2.2 Confidence-dependent word segmentation. Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging. However, neither was perfect. The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results. In this section we introduce a confidence measure approach to combine the two results. We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation. The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation. Its calculation is defined as: C M(tiob |w) = Î±C Miob (tiob |w) + (1 â Î±)Î´(tw , tiob )ng (2) where tiob is the word wâs IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation. After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging. Each subword is given a prior IOB tag, tw . C Miob (t|w), a ï£« M ï£«ï£¶ï£¶ confidence probability derived in the process of IOB tag exp ï£¬)' ï£¬)' Î»k fk (tiâ1 , ti , W ) + )' Âµk gk (ti , W )ï£·ï£· /Z, ï£¬ï£­ i=1 ï£¬ï£­ k k ï£·ï£¸ ï£·ï£¸ (1) ging, is defined as Z = )' T =t0 t1 Â·Â·Â·tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 Â·Â·Â·tM ,ti =t P(T |W, wi ) T =t 0 t1 Â·Â·Â· tM P ( T | W ) where we call fk (tiâ1 , ti , W ) bigram feature functions because the features trigger the previous observation tiâ1 where the numerator is a sum of all the observation sequences with word wi labeled as t. Î´(tw , tiob )ng denotes the contribution of the dictionary- based segmentation. It is a Kronecker delta function defined as Î´(tw , tiob )ng = { 1 if tw = tiob 0 otherwise In Eq. 2, Î± is a weighting between the IOB tagging and the dictionary-based word segmentation. We found the value 0.7 for Î±, empirically. By Eq. 2 the results of IOB tagging were reevaluated. A confidence measure threshold, t, was defined for making a decision based on the value. If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used. A new OOV was thus created. For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach. In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold. In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.  We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections. The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR). Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only. Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv). For detailed info. of the corpora and these scores, refer to (Emerson, 2005). For the dictionary-based approach, we extracted a word list from the training data as the vocabulary. Tri- gram LMs were generated using the SRI LM toolkit for disambiguation. Table 1 shows the performance of the dictionary-based segmentation. Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment. In fact, there were no OOV recognition. Hence, this approach produced lower F-scores. However, the R-ivs were very high. 3.1 Effects of the Character-based and the. subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation. For the character-based tagging, we used all the Chinese characters. For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging. The segmentation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied. R P FR oo vR iv A S 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0. 67 8 0. 64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0. 70 0 0. 73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0. 78 3 0. 75 4 0.9 49 0.9 55 M S R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0. 71 0 0. 71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging. The upper numbers are of the character- based and the lower ones, the subword-based. using the FMM, and then labeled with âIOBâ tags by the CRFs. The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based. We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus. There were no F-score changes for AS and PKU corpora, but the recall rates were improved. Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach. However, the R-iv rates were getting worse in return for higher R-oov rates. We will tackle this problem by the confidence measure approach. 3.2 Effect of the confidence measure. In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation. The effect of the confidence measure is shown in Table 3, where we used Î± = 0.7 and confidence threshold t = 0.8. In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based. We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach. The act of confidence measure made a tradeoff between R-ivs and R- oovs, yielding higher R-oovs than Table 1 and higher R R P FR oo vR iv A S 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0. 60 7 0. 64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0. 68 2 0. 74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0. 77 5 0. 74 8 0.9 52 0.9 59 M S R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0. 67 4 0. 71 2 0.9 67 0.9 76 Table 3: Effects of combination using the confidence measure. The upper numbers and the lower numbers are of the character-based and the subword-based, respectively A S CI T Y U M SR P K U Ba ke off be st 0. 95 2 0. 9 4 3 0. 96 4 0. 95 0 O u r s 0. 95 1 0. 9 5 1 0. 97 1 0. 95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2. Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging. It proves the proposed word-based IOB tagging was very effective.  The IOB tagging approach adopted in this work is not a new idea. It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used. Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001). Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based. We proved the new approach enhanced the word segmentation significantly. Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores. We achieved the highest F-scores in CITYU, PKU and MSR corpora. We think our proposed subword- based tagging played an important role for the good results. Since it was a closed test, some information such as Arabic and Chinese number and alphabetical letters cannot be used. We could yield a better results than those shown in Table 4 using such information. For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known. For AS corpus, âAdam Smithâ are two words in the training but become a one- word in the test, âAdamSmithâ. Our approaches produced wrong segmentations for labeling inconsistency. Another advantage of the word-based IOB tagging over the character-based is its speed. The subword-based approach is faster because fewer words than characters were labeled. We found a speed up both in training and test. The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more delicately. By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.  In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation. Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches. We also successfully employed the confidence measure to make a confidence-dependent word segmentation. This approach is effective for performing desired segmentation based on usersâ requirements to R-oov and R-iv.  The authors appreciate the reviewersâ effort and good advice for improving the paper.
  We supplement WordNet entries with information on the subjectivity of its word senses. Supervised classifiers that operate on word sense definitions in the same way that text classifiers operate on web or newspaper texts need large amounts of training data. The resulting data sparseness problem is aggravated by the fact that dictionary definitions are very short. We propose a semi-supervised minimum cut framework that makes use of both WordNet definitions and its relation structure. The experimental results show that it outperforms supervised minimum cut as well as standard supervised, non-graph classification, reducing the error rate by 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data.  There is considerable academic and commercial interest in processing subjective content in text, where subjective content refers to any expression of a private state such as an opinion or belief (Wiebe et al., 2005). Important strands of work include the identification of subjective content and the determination of its polarity, i.e. whether a favourable or unfavourable opinion is expressed. Automatic identification of subjective content often relies on word indicators, such as unigrams (Pang et al., 2002) or predetermined sentiment lexica (Wilson et al., 2005). Thus, the word positive the sentence contains a favourable opinion. However, such word-based indicators can be misleading for two reasons. First, contextual indicators such as irony and negation can reverse subjectivity or polarity indications (Polanyi and Zaenen, 2004). Second, different word senses of a single word can actually be of different subjectivity or polarity. A typical subjectivity-ambiguous word, i.e. a word that has at least one subjective and at least one objective sense, is positive, as shown by the two example senses given below.1 (1) positive, electropositiveâhaving a positive electric charge;âprotons are positiveâ (objective) (2) plus, positiveâinvolving advantage or good; âa plus (or positive) factorâ (subjective) We concentrate on this latter problem by automatically creating lists of subjective senses, instead of subjective words, via adding subjectivity labels for senses to electronic lexica, using the example of WordNet. This is important as the problem of subjectivity-ambiguity is frequent: We (Su and Markert, 2008) find that over 30% of words in our dataset are subjectivity-ambiguous. Information on subjectivity of senses can also improve other tasks such as word sense disambiguation (Wiebe and Mihalcea, 2006). Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. in the sentence âThis deal is a positive development for our company.â gives a strong indication that 1 All examples in this paper are from WordNet 2.0.. 1 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 1â9, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics We propose a semi-supervised approach based on minimum cut in a lexical relation graph to assign subjectivity (subjective/objective) labels to word senses.2 Our algorithm outperforms supervised minimum cuts and standard supervised, non-graph classification algorithms (like SVM), reducing the error rate by up to 40%. In addition, the semi-supervised approach achieves the same results as the supervised framework with less than 20% of the training data. Our approach also outperforms prior approaches to the subjectivity recognition of word senses and performs well across two different data sets. The remainder of this paper is organized as follows. Section 2 discusses previous work. Section 3 describes our proposed semi-supervised minimum cut framework in detail. Section 4 presents the experimental results and evaluation, followed by conclusions and future work in Section 5.  There has been a large and diverse body of research in opinion mining, with most research at the text (Pang et al., 2002; Pang and Lee, 2004; Popescu and Etzioni, 2005; Ounis et al., 2006), sentence (Kim and Hovy, 2005; Kudo and Matsumoto, 2004; Riloff et al., 2003; Yu and Hatzivassiloglou, 2003) or word (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005; Andreevskaia and Bergler, 2006; Kaji and Kitsuregawa, 2007) level. An up-to-date overview is given in Pang and Lee (2008). Graph-based algorithms for classification into subjective/objective or positive/negative language units have been mostly used at the sentence and document level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005; Thomas et al., 2006), instead of aiming at dictionary annotation as we do. We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. At the word level Takamura et al. (2005) use a semi-supervised spin model for word polarity determination, where the graph 2 It can be argued that subjectivity labels are maybe rather more graded than the clear-cut binary distinction we assign. However, in Su and Markert (2008a) as well as Wiebe and Mi- halcea (2006) we find that human can assign the binary distinction to word senses with a high level of reliability. is constructed using a variety of information such as gloss co-occurrences and WordNet links. Apart from using a different graph-based model from ours, they assume that subjectivity recognition has already been achieved prior to polarity recognition and test against word lists containing subjective words only. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, we deal with classification at the word sense level, treating also subjectivity-ambiguous words, which goes beyond the work in Takamura et al. (2005). Word Sense Level: There are three prior approaches addressing word sense subjectivity or polarity classification. Esuli and Sebastiani (2006) determine the polarity (positive/negative/objective) of word senses in WordNet. However, there is no evaluation as to the accuracy of their approach. They then extend their work (Esuli and Sebastiani, 2007) by applying the Page Rank algorithm to rank the WordNet senses in terms of how strongly a sense possesses a given semantic property (e.g., positive or negative). Apart from us tackling subjectivity instead of polarity, their Page Rank graph is also constructed focusing on WordNet glosses (linking glosses containing the same words), whereas we concentrate on the use of WordNet relations. Both Wiebe and Mihalcea (2006) and our prior work (Su and Markert, 2008) present an annotation scheme for word sense subjectivity and algorithms for automatic classification. Wiebe and Mi- halcea (2006) use an algorithm relying on distributional similarity and an independent, large manually annotated opinion corpus (MPQA) (Wiebe et al., 2005). One of the disadvantages of their algorithm is that it is restricted to senses that have distributionally similar words in the MPQA corpus, excluding 23% of their test data from automatic classification. Su and Markert (2008) present supervised classifiers, which rely mostly on WordNet glosses and do not effectively exploit WordNetâs relation structure.  3.1 Minimum Cuts: The Main Idea. Binary classification with minimum cuts (Mincuts) in graphs is based on the idea that similar items should be grouped in the same cut. All items in the training/test data are seen as vertices in a graph with undirected weighted edges between them specifying how strong the similarity/association between two vertices is. We use minimum s-t cuts: the graph contains two particular vertices s (source, corresponds to subjective) and t (sink, corresponds to objective) and each vertex u is connected to s and t via a weighted edge that can express how likely u is to be classified as s or t in isolation. Binary classification of the vertices is equivalent to splitting the graph into two disconnected subsets of all vertices, S and T with s â S and t â T . This corresponds to removing a set of edges from the graph. As similar items should be in the same part of the split, the best split is one which removes edges with low weights. In other words, a minimum cut problem is to find a partition of the graph which minimizes the following formula, where w(u, v) expresses the weight of an edge between two vertices. subjective or both objective.3 An example here is the antonym relation, where two antonyms such as goodâmorally admirable and evil, wickedâmorally bad or wrong are both subjective. Second, Mincuts can be easily expanded into a semi-supervised framework (Blum and Chawla, 2001). This is essential as the existing labeled datasets for our problem are small. In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations. Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework. Semi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, as the unlabeled data can be chosen to be related to the labeled and test data, they might help pull test data to the right cuts (categories). 3.3 Formulation of Semi-supervised Mincuts. W (S, T ) = ) uâS,vâT w(u, v)The formulation of our semi supervised Mincut for sense subjectivity classification involves the follow Globally optimal minimum cuts can be found in polynomial time and near-linear running time in practice, using the maximum flow algorithm (Pang and Lee, 2004; Cormen et al., 2002). 3.2 Why might Semi-supervised Minimum. Cuts Work? We propose semi-supervised mincuts for subjectivity recognition on senses for several reasons. First, our problem satisfies two major conditions necessary for using minimum cuts. It is a binary classification problem (subjective vs. objective senses) as is needed to divide the graph into two components. Our dataset also lends itself naturally to s-t Mincuts as we have two different views on the data. Thus, the edges of a vertex (=sense) to the source/sink can be seen as the probability of a sense being subjective or objective without taking similarity to other senses into account, for example via considering only the sense gloss. In contrast, the edges between two senses can incorporate the WordNet relation hierarchy, which is a good source of similarity for our problem as many WordNet relations are subjectivity-preserving, i.e. if two senses are connected via such a relation they are likely to be both ing steps, which we later describe in more detail. 1. We define two vertices s (source) and t (sink),. which correspond to the âsubjectiveâ and âobjectiveâ category, respectively. Following the definition in Blum and Chawla (2001), we call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices. Each example vertex corresponds to one WordNet sense and is connected to both s and t via a weighted edge. The latter guarantees that the graph is connected. 2. For the test and unlabeled examples, we see. the edges to the classification vertices as the probability of them being subjective/objective disregarding other example vertices. We use a supervised classifier to set these edge weights. For the labeled training examples, they are connected by edges with a high constant weight to the classification vertices that they belong to. 3. WordNet relations are used to construct the. edges between two example vertices. Such 3 See Kamps et al. (2004) for an early indication of such properties for some WordNet relations. edges can exist between any pair of example vertices, for example between two unlabeled examples.  maximum-flow algorithm to find the minimum s-t cuts of the graph. The cut in which the source vertex s lies is classified as âsubjectiveâ, and the cut in which the sink vertex t lies is âobjectiveâ. to reflect the degree to which they are subjectivity- preserving. Therefore, we experiment with two methods of weight assignment. Method 1 (NoSL) assigns the same constant weight of 1.0 to all Word- Net relations. Method 2 (SL) reflects different degrees of preserving subjectivity. To do this, we adapt an unsupervised method of generating a large noisy set of subjective and objective senses from our previous work (Su and Markert, 2008). This method 5 We now describe the above steps in more detail. uses a list of subjective words (SL) to classify each Selection of unlabeled data: Random selection of unlabeled data might hurt the performance of Mincuts, as they might not be related to any sense in our training/test data (denoted by A). Thus a basic principle is that the selected unlabeled senses should be related to the training/test data by WordNet relations. We therefore simply scan each sense in A, and collect all senses related to it via one of the WordNet relations in Table 1. All such senses that are not in A are collected in the unlabeled data set. Weighting of edges to the classification vertices: The edge weight to s and t represents how likely it is that an example vertex is initially put in the cut in which s (subjective) or t (objective) lies. For unlabeled and test vertices, we use a supervised classifier (SVM4) with the labeled data as training data to assign the edge weights. The SVM is also used as a baseline and its features are described in Section 4.3. As we do not wish the Mincut to reverse labels of the labeled training data, we assign a high constant weight of 5 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.01 to the edge to the other classification vertex. Assigning weights to WordNet relations: We connect two vertices that are linked by one of the ten WordNet relations in Table 1 via an edge. Not all WordNet relations we use are subjectivity- preserving to the same degree: for example, hyponyms (such as simpleton) of objective senses (such as person) do not have to be objective. However, we aim for high graph connectivity and we can assign different weights to different relations 4 We employ LIBSVM, available at http://www.csie.. WordNet sense with at least two subjective words in its gloss as subjective and all other senses as objective. We then count how often two senses related via a given relation have the same or a different subjectivity label. The weight is computed by #same/(#same+#different). Results are listed in Table 1. Table 1: Relation weights (Method 2) M et ho d #S a m e #D iff er en t W ei gh t An to ny m 2, 80 8 30 9 0. 90 Si milar to 6, 88 7 1, 61 4 0. 81 De riv ed fro m 4, 63 0 94 7 0. 83 Dir ect Hy pe rn y m 71 ,9 15 8, 60 0 0. 89 Dir ect Hy po ny m 71 ,9 15 8, 60 0 0. 89 Att rib ut e 35 0 10 9 0. 76 Al so se e 1, 03 7 33 7 0. 75 Ex ten ded An ton ym 6, 91 7 1, 65 1 0. 81 Do m ai n 4, 38 7 89 2 0. 83 Do m ain m e m be r 4, 38 7 89 2 0. 83 Example graph: An example graph is shown in Figure 1. The three example vertices correspond to the senses religiousâextremely scrupulous and conscientious, scrupulousâhaving scruples; arising from a sense of right and wrong; principled; and flicker, spark, glintâa momentary flash of light respectively. The vertex âscrupulousâ is unlabeled data derived from the vertex âreligiousâ(a test item) by the relation âsimilar-toâ. 4 Experiments and Evaluation. 4.1 Datasets. We conduct the experiments on two different gold standard datasets. One is the MicroWNOp corpus, ntu.edu.tw/Ëcjlin/libsvm/. Linear kernel and probability estimates are used in this work.  http://www.cs.pitt.edu/mpq a subjective 0.24 0.83 religio us similar-to 0.81 scrupulo us 0.76 0.17 objective baseline.8 Three different feature types are used. Lexic al Feature s (L): a bag-of words represen tation of the sense glosses with stop word filtering. Relati on Feature s (R): First, we use two features for each of the ten WordNet relations in Table 1, describing how many relations of that type the sense has to senses in the subjectiv e or objective part of the training set, respectiv ely. This provides a non graph 0.16 0.84 flicker Figure 1: Graph of Word Senses which is representative of the part-of-speech distribution in WordNet 6. It includes 298 words with 703 objective and 358 subjective WordNet senses. The second one is the dataset created by Wiebe and Mihalcea (2006).7 It only contains noun and verb senses, and includes 60 words with 236 objective and 92 subjective WordNet senses. As the MicroWNOp set is larger and also contains adjective and adverb senses, we describe our results in more detail on that corpus in the Section 4.3 and 4.4. In Section 4.5, we shortly discuss results on. Wiebe&Mihalceaâs dataset. 4.2 Baseline and Evaluation. We compare to a baseline that assigns the most frequent category objective to all senses, which achieves an accuracy of 66.3% and 72.0% on MicroWNOp and Wiebe&Mihalceaâs dataset respectively. We use the McNemar test at the significance level of 5% for significance statements. All evaluations are carried out by 10-fold cross-validation. 4.3 Standard Supervised Learning. We use an SVM classifier to compare our proposed semi-supervised Mincut approach to a reasonable  summary of subjectivity-preserving links. Second, we manually collected a small set (denoted by SubjSet) of seven subjective verb and noun senses which are close to the root in WordNetâs hypernym tree. A typical example element of SubjSet is psychological feature âa feature of the mental life of a living organism, which indicates subjectivity for its hyponyms such as hope â the general feeling that some desire will be fulfilled. A binary feature describes whether a noun/verb sense is a hyponym of an element of SubjSet. Monosemous Feature (M): for each sense, we scan if a monosemous word is part of its synset. If so, we further check if the monosemous word is collected in the subjective word list (SL). The intuition is that if a monosemous word is subjective, obviously its (single) sense is subjective. For example, the sense uncompromising, inflexibleânot making concessions is subjective, as âuncompromisingâ is a monosemous word and also in SL. We experiment with different combinations of features and the results are listed in Table 2, prefixed by âSVMâ. All combinations perform significantly better than the more frequent category baseline and similarly to the supervised Naive Bayes classifier (see S&M in Table 2) we used in Su and Mark- ert (2008). However, improvements by adding more features remain small. In addition, we compare to a supervised classifier (see Lesk in Table 2) that just assigns each sense the subjectivity label of its most similar sense in the training data, using Leskâs similarity measure from Pedersenâs WordNet similarity package9. We use Lesk as it is one of the few measures applicable across all parts-of-speech. markert/data. This dataset was first used with a different annotation scheme in Esuli and Sebastiani (2007) and we also used it in Su and Markert (2008).  pubs/papers/goldstandard.total.acl06. classification vertices in the Mincut approach. 9 Available at http://www.d.umn.edu/Ëtpederse/. similarity.html. Table 2: Results of SVM and Mincuts with different settings of feature M et ho d S u b j e c t i v e O b j e c t i v e Ac cu ra cy Pr eci sio n Re cal lF sc or e Pr eci sio n Re cal lF sc or e Ba sel in e N/ A 0 N/ A 66 .3 % 10 0 % 79 .7 % 66 .3 % S & M 66 .2 % 64 .5 % 65 .3 % 82 .2 % 83 .2 % 82 .7 % 76 .9 % Le sk 65 .6 % 50 .3 % 56 .9 % 77 .5 % 86 .6 % 81 .8 % 74 .4 % S VM L 69 .6 % 37 .7 % 48 .9 % 74 .3 % 91 .6 % 82 .0 % 73 .4 %L SL 82 .0 % 43 .3 % 56 .7 % 76 .7 % 95 .2 % 85 .0 % 77 .7 %L No SL 80 .8 % 43 .6 % 56 .6 % 76 .7 % 94 .7 % 84 .8 % 77 .5 % S VM L M 68 .9 % 42 .2 % 52 .3 % 75 .4 % 90 .3 % 82 .2 % 74 .1 % LM SL 83 .2 % 44 .4 % 57 .9 % 77 .1 % 95 .4 % 85 .3 % 78 .2 % LM No SL 83 .6 % 44 .1 % 57 .8 % 77 .1 % 95 .6 % 85 .3 % 78 .2 % S VM LR 68 .4 % 45 .3 % 54 .5 % 76 .2 % 89 .3 % 82 .3 % 74 .5 % LR SL 82 .7 % 65 .4 % 73 .0 % 84 .1 % 93 .0 % 88 .3 % 83 .7 % LR No SL 82 .4 % 65 .4 % 72 .9 % 84 .0 % 92 .9 % 88 .2 % 83 .6 % S VM LR M 69 .8 % 47 .2 % 56 .3 % 76 .9 % 89 .6 % 82 .8 % 75 .3 % LRM SL 85 .5 % 65 .6 % 74 .2 % 84 .4 % 94 .3 % 89 .1 % 84 .6 % LRM No SL 84 .6 % 65 .9 % 74 .1 % 84 .4 % 93 .9 % 88 .9 % 84 .4 % 1 L, R and M correspond to the lexical, relation and monosemous features respectively. 2 SVM-L corresponds to using lexical features only for the SVM classifier. Likewise, SVM-. LRM corresponds to using a combination for lexical, relation, and monosemous features for the SVM classifier. 3 L-SL corresponds to the Mincut that uses only lexical features for the SVM classifier, and subjective list (SL) to infer the weight of WordNet relations. Likewise, LMNoSL corresponds to the Mincut algorithm that uses lexical and monosemous features for the SVM, and predefined constants for WordNet relations (without subjective list). 4.4 Semi-supervised Graph Mincuts. Using our formulation in Section 3.3, we import 3,220 senses linked by the ten WordNet relations to any senses in MicroWNOp as unlabeled data. We construct edge weights to classification vertices using the SVM discussed above and use WordNet relations for links between example vertices, weighted by either constants (NoSL) or via the method illustrated in Table 1 (SL). The results are also summarized in Table 2. Semi-supervised Mincuts always significantly outperform the corresponding SVM classifiers, regardless of whether the subjectivity list is used for setting edge weights. We can also see that we achieve good results without using any other knowledge sources (setting LRNoSL). The example in Figure 1 explains why semi- supervised Mincuts outperforms the supervised approach. The vertex âreligiousâ is initially assigned the subjective/objective probabilities 0.24/0.76 by the SVM classifier, leading to a wrong classification. However, in our graph-based Mincut framework, the vertex âreligiousâ might link to other vertices (for example, it links to the vertex âscrupulousâ in the unlabeled data by the relation âsimilar-toâ). The mincut algorithm will put vertices âreligiousâ and âscrupulousâ in the same cut (subjective category) as this results in the least cost 0.93 (ignoring the cost of assigning the unrelated sense of âflickerâ). In other words, the edges between the vertices are likely to correct some initially wrong classification and pull the vertices into the right cuts. In the following we will analyze the best minimum cut algorithm LRMSL in more detail. We measure its accuracy for each part-of-speech in the MicroWNOp dataset. The number of noun, adjective, adverb and verb senses in MicroWNOp is 484, 265, 31 and 281, respectively. The result is listed in Table 3. The significantly better performance of semi-supervised mincuts holds across all parts-of- speech but the small set of adverbs, where there is no significant difference between the baseline, SVM and the Mincut algorithm. Mincuts SVM with different sizes of labeled and unlabeled data. All learning curves are generated via averaging 10 learning curves from 10-fold cross-validation. Performance with different sizes of labeled data: we randomly generate subsets of labeled data A1, A2... An, and guarantee that A1 â A2... â An.Results for the best SVM (LRM) and the best min imum cut (LRMSL) are listed in Table 4, and the corresponding learning curve is shown in Figure 2. As can be seen, the semi-supervised Mincuts is consistently better than SVM. Moreover, the semi- supervised Mincut with only 200 labeled data items performs even better than SVM with 954 training items (78.9% vs 75.3%), showing that our semi- supervised framework allows for a training data reduction of more than 80%. Table 4: Accuracy with different sizes of labeled data 71 68 100 200 300 400 500 600 700 800 900 1000 Size of Labeled Data Figure 2: Learning curve with different sizes of labeled data The results are listed in Table 5 and Table 6 respectively. The corresponding learning curves are shown in Figure 3. We see that performance improves with the increase of unlabeled data. In addition, the curves seem to converge when the size of unlabeled data is larger than 3,000. From the results in Tabel 5 one can also see that hyponymy is the relation accounting for the largest increase. Table 6: Accuracy with different sizes of unlabeled data (random selection) # unl ab ele d da ta Ac cu ra cy 0 75 .9 % 20 0 76 .5 % 50 0 78 .6 % 10 00 80 .2 % 20 00 82 .8 % 30 00 84 .0 % 32 20 84 .6 % Performance with different sizes of unlabeled data: We propose two different settings. Option1: Use a subset of the ten relations to generate the unlabeled data (and edges between example vertices). For example, we first use {antonym, similar-to} only to obtain a unlabeled dataset U1, then use a larger subset of the relations like {antonym, similar-to, direct-hyponym, direct- hypernym} to generate another unlabeled dataset U2, and so forth. Obviously, Ui is a subset of Ui+1. Option2: Use all the ten relations to generate the unlabeled data U . We then randomly select subsets of U , such as subset U1, U2 and U3, and guarantee that U1 â U2 â U3 â . . . U . Furthermore, these results also show that a supervised mincut without unlabeled data performs only on a par with other supervised classifiers (75.9%). The reason is that if we exclude the unlabeled data, there are only 67 WordNet relations/edges between senses in the small MicroWNOp dataset. In contrast, the use of unlabeled data adds more edges (4,586) to the graph, which strongly affects the graph cut partition (see also Figure 1). 4.5 Comparison to Prior Approaches. In our previous work (Su and Markert, 2008), we report 76.9% as the best accuracy on the same Micro Table 5: Accuracy with different sizes of unlabeled data from WordNet relation Re lati on # unl ab ele d da ta Ac cu ra cy {â } 0 75 .3 % {si milar to } 41 8 79 .1 % {si milar to, ant on ym } 51 4 79 .5 % {si milarto, antonym, direct-hypernym, direct hy po ny m } 2, 72 1 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, also se e, ext en ded ant on ym } 3, 00 4 84 .4 % {si milarto, antonym, direct-hypernym, direct hy po ny m, al so se e, ex te nd ed an to ny m, d eri ved fr o m , at tri bu te , d o m ai n, d o m ain m e m be r} 3, 22 0 84 .6 % 89 Option1 87 Option2. 85 83 81 79 77 75 0 500 1000 1500 2000 2500 3000 3500 Size of Unlabeled Data Figure 3: Learning curve with different sizes of unlabeled data WNOp dataset used in the previous sections, using a supervised Naive Bayes (S&M in Tabel 2). Our best result from Mincuts is significantly better at 84.6% (see LRMSL in Table 2). For comparison to Wiebe and Mihalcea (2006), we use their dataset for testing, henceforth called Wiebe (see Section 4.1 for a description). Wiebe and Mihalcea (2006) report their results in precision and recall curves for subjective senses, such as a precision of about 55% at a recall of 50% for subjective senses. Their F-score for subjective senses seems to remain relatively static at 0.52 throughout their precision/recall curve. We run our best Mincut LRMSL algorithm with two different settings on Wiebe. Using MicroWNOp as training set and Wiebe as test set, we achieve an accuracy of 83.2%, which is similar to the results on the MicroWNOp dataset. At the recall of50% we achieve a precision of 83.6% (in compari son to their precision of 55% at the same recall). Our F-score is 0.63 (vs. 0.52). To check whether the high performance is just due to our larger training set, we also conduct 10-fold cross-validation on Wiebe. The accuracy achieved is 81.1% and the F-score 0.56 (vs. 0.52), suggesting that our algorithm performs better. Our algorithm can be used on all WordNet senses whereas theirs is restricted to senses that have distributionally similar words in the MPQA corpus (see Section 2). However, they use an unsupervised algorithm i.e. they do not need labeled word senses, although they do need a large, manually annotated opinion corpus. 5 Conclusion and Future Work. We propose a semi-supervised minimum cut algorithm for subjectivity recognition on word senses. The experimental results show that our proposed approach is significantly better than a standard supervised classification framework as well as a supervised Mincut. Overall, we achieve a 40% reduction in error rates (from an error rate of about 25% to an error rate of 15%). To achieve the results of standard supervised approaches with our model, we need less than 20% of their training data. In addition, we compare our algorithm to previous state-of-the-art approaches, showing that our model performs better on the same datasets. Future work will explore other graph construction methods, such as the use of morphological relations as well as thesaurus and distributional similarity measures. We will also explore other semi- supervised algorithms.
 11,001 New Features for Statistical Machine Translation  We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrase- based translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 Bï¬ï¥ïµ and+1.1 Bï¬ï¥ïµ, respectively. We analyze the impact of the new features and the performance of the learning algorithm.  What linguistic features can improve statistical machine translation (MT)? This is a fundamental question for the discipline, particularly as it pertains to improving the best systems we have. Further: â¢ Do syntax-based translation systems have unique and effective levers to pull when designing new features? â¢ Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntax- based MT systemâalready the highest-scoring single system in the NIST 2008 ChineseEnglish common-data trackâby +1.1 Bï¬ï¥ïµ. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 Bï¬ï¥ïµ improvement. âThis research was supported in part by DARPA contract HR001106-C-0022 under subcontract to BBN Technologies. Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntax- based models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks.  The work of Och et al (2004) is perhaps the best- known study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-of- speech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntax- based decoders. A third difficulty with Och et al.âs study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scal- ability: to train many features, we need many train 218 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218â226, Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: count(rule)  P(rule) =count(LHS root(rule)) 3.1 Hiero. Hiero (Chiang, 2005) is a hierarchical, string-to- string translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X â X1 de X2, X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system. Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0:NPB PP(IN(of x1:NPB)) â x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectation- maximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training. We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: â¢ Select a batch of input sentences f1, . . . , fm and decode each fi to obtain a forest of translations. â¢ For each i, select from the forest a set of hypothesis translations ei1, . . . , ein, which are the 10-best translations according to each of: h(e) Â· w Bï¬ï¥ïµ(e) + h(e) Â· w âBï¬ï¥ïµ(e) + h(e) Â· w â¢ For each i, select an oracle translation: (1) 4.1 Target-side. features String-to-tree MT offers some unique levers to pull, in terms of target-side features. Because the system outputs English trees, we can analyze output trees on the tuning set and design new features to encourage the decoder to produce more grammatical trees. eâ = arg max (Bï¬ï¥ïµ(e) + h(e) Â· w) (2) e Let âhi j = h(eâ) â h(ei j). â¢ For each ei j, compute the loss fi j = Bï¬ï¥ïµ(eâ) â Bï¬ï¥ïµ(ei j) (3) â¢ Update w to the value of wt that minimizes: m Rule overlap features While individual rules observed in decoder output are often quite reasonable, two adjacent rules can create problems. For example, a rule that has a variable of type IN (preposition) needs another rule rooted with IN to fill the position. If the second rule supplies the wrong preposition, a bad translation results. The IN node here is an overlap point between rules. Considering that 1 2 2 \lw â w\l + C i=1 max (fi j â âhi j Â· wt) (4) 1â¤ jâ¤n certain nonterminal symbols may be more reliable overlap points than others, we create a binary fea where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence Bï¬ï¥ïµ scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simpleâfor each sentence, the decoder sends the trainer a forest, and the trainer returns a weight updateâit is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder.  In this section, we describe the new features introduced on top of our baseline systems. Discount features Both of our systems calculate several features based on observed counts of rules in the training data. Though the syntax-based system uses Good-Turing discounting when computing the P(e, c) feature, we find, as noted above, that it uses quite a few one-count rules, suggesting that their probabilities have been overestimated. We can directly attack this problem by adding features counti that reward or punish rules seen i times, or features count[i, j] for rules seen between i and j times. ture for each nonterminal. A rule like: IN(at) â zai will have feature rule-root-IN set to 1 and all other rule-root features set to 0. Our rule root features range over the original (non-split) nonterminal set; we have 105 in total. Even though the rule root features are locally attached to individual rulesâand therefore cause no additional problems for the decoder searchâthey are aimed at problematic rule/rule interactions. Bad single-level rewrites Sometimes the decoder uses questionable rules, for example: PP(x0:VBN x1:NP-C) â x0 x1 This rule is learned from 62 cases in our training data, where the VBN is almost always the word given. However, the decoder misuses this rule with other VBNs. So we can add a feature that penalizes any rule in which a PP dominates a VBN and NP-C. The feature class bad-rewrite comprises penalties for the following configurations based on our analysis of the tuning set: PP â VBN NP-C PP-BAR â NP-C IN VP â NP-C PP CONJP â RB IN Node count features It is possible that the decoder creates English trees with too many or too few nodes of a particular syntactic category. For example, there may be an tendency to generate too many determiners or past-tense verbs. We therefore add a count feature for each of the 109 (non-split) English nonterminal symbols. For a rule like NPB(NNP(us) NNP(president) x0:NNP) â meiguo zongtong x0 the feature node-count-NPB gets value 1, node- count-NNP gets value 2, and all others get 0. Insertion features Among the rules we extract from bilingual corpora are target-language insertion rules, which have a word on the English side, but no words on the source Chinese side. Sample syntax- based insertion rules are: NPB(DT(the) x0:NN) â x0 S(x0:NP-C VP(VBZ(is) x1:VP-C)) â x0 x1 We notice that our decoder, however, frequently fails to insert words like is and are, which often have no equivalent in the Chinese source. We also notice that the-insertion rules sometimes have a good effect, as in the translation âin the bloom of youth,â but other times have a bad effect, as in âpeople seek areas of the conspiracy.â Each time the decoder uses (or fails to use) an insertion rule, it incurs some risk. There is no guarantee that the interaction of the rule probabilities and the language model provides the best way to manage this risk. We therefore provide MIRA with a feature for each of the most common English words appearing in insertion rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features. We now turn to features that make use of source-side context. Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature. Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separately- tunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither systemâs basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance re- orderings from short-distance reorderings. To remedy this problem, Chiang et al. (2008) introduce a structural distortion model, which we include in our experiment. Our syntax-based baseline includes the generative version of this model already. Word context During rule extraction, we retain word alignments from the training data in the extracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f , e, f+1), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f , e, fâ1) with fâ1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model.  For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For Sy st e m Training Fe at ur es # Tu ne Test Hi er o MERT M I R A b a s el in e 1 1 s y nt a x, di st o rt io n 5 6 s y nt a x, di st or ti o n, di s c o u n t 6 1 al l s o ur ce si d e, di s c o u n t 1 0 9 9 0 35 .4 36.1 35 .9 36.9â 36.  38. 4 37.6ââ Sy nt ax MERT M I R A b a s el in e 2 5 b a s el in e 2 5 o v e rl a p 1 3 2 n o d e c o u n t 1 3 6 al l ta rg et si d e, di s c o u n t 2 8 3 38 .6 39.5 38 .5 39.8â 38 .7 39.9â 38.  39. 6 40.6ââ Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM Bï¬ï¥ïµ scores. â or ââ = significantly better than MERT baseline ( p < 0.05 or 0.01, respectively). the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data. We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used by Hiero. Modified KneserNey smoothing (Chen and Goodman, 1998) was applied to all language models. The language models are represented using randomized data structures similar to those of Talbot et al. (2007). Our tuning set (2010 sentences) and test set (1994 sentences) were drawn from newswire data from the NIST 2004 and 2005 evaluations and the GALE program (with no overlap at either the segment or document level). For the source-side syntax features, we used the Berkeley parser (Petrov et al., 2006) to parse the Chinese side of both sets. We implemented the source-side context features for Hiero and the target-side syntax features for the syntax-based system, and the discount features for both. We then ran MIRA on the tuning set with 20 parallel learners for Hiero and 73 parallel learners for the syntax-based system. We chose a stopping iteration based on the Bï¬ï¥ïµ score on the tuning set, and used the averaged feature weights from all iter Syntax-based Hiero count weight count weight 1 +1.28 1 +2.23 2 +0.35 2 +0.77 3â5 â0.73 3 +0.54 6â10 â0.64 4 +0.29 5+ â0.02 Table 2: Weights learned for discount features. Negative weights indicate bonuses; positive weights indicate penalties. ations of all learners to decode the test set. The results (Table 1) show significant improvements in both systems ( p < 0.01) over already very strong MERT baselines. Adding the source-side and discount features to Hiero yields a +1.5 Bï¬ï¥ïµ improvement, and adding the target-side syntax and discount features to the syntax-based system yields a +1.1 Bï¬ï¥ïµ improvement. The results also show that for Hiero, the various classes of features contributed roughly equally; for the syntax-based system, we see that two of the feature classes make small contributions but time constraints unfortunately did not permit isolated testing of all feature classes. 6 Analysis. How did the various new features improve the translation quality of our two systems? We begin by examining the discount features. For these features, we used slightly different schemes for the two systems, shown in Table 2 with their learned feature weights. We see in both cases that one-count rules are strongly penalized, as expected. Reward â0.42 a â0.13 are â0.09 at â0.09 on â0.05 was â0.05 from â0.04 âs â0.04 by â0.04 is â0.03 it â0.03 its . Penalty +0.67 of +0.56 the +0.47 comma +0.13 period +0.11 in +0.08 for +0.06 to +0.05 will +0.04 and +0.02 as +0.02 have . Bonus â0.50 period â0.39 VP-C â0.36 VB â0.31 SG-C â0.30 MD â0.26 VBG â0.25 ADJPâ0.22LRB â0.21 VP-BAR â0.20 NPB-BAR â0.16 FRAG P en alt y +0.93 IN +0.57 NNP +0.44 NN +0.41 DT +0.34 JJ +0.24 right double quote +0.20 VBZ +0.19 NP +0.16 TO +0.15 ADJP-BAR +0.14 PRN-BAR Table 3: Weights learned for inserting target English words with rules that lack Chinese words. 6.1 Syntax features. Table 3 shows word-insertion feature weights. The system rewards insertion of forms of be; examples â0.16 PRN â0.15 NPB â0.13 RB â0.12 SBAR-C â0.12 VP-C-BARâ0.11RRB . +0.14 NML +0.13 comma +0.12 VBD +0.12 NNPS +0.12 PRP +0.11 SG . 1â3 in Figure 1 show typical improved translations that result. Among determiners, inserting a is rewarded, while inserting the is punished. This seems to be because the is often part of a fixed phrase, such as the White House, and therefore comes naturally as part of larger phrasal rules. Inserting the outside Table 4: Weights learned for employing rules whose English sides are rooted at particular syntactic categories. these fixed phrases is a risk that the generative model is too inclined to take. We also note that the system learns to punish unmotivated insertions of commas and periods, which get into our grammar via quirks in the MT training data. Table 4 shows weights for rule-overlap features. MIRA punishes the case where rules overlap with an IN (preposition) node. This makes sense: if a rule has a variable that can be filled by any English preposition, there is a risk that an incorrect preposition will fill it. On the other hand, splitting at a period is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S. Table 5 shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished. The combined effect of all weights is subtle. To interpret them further, it helps to look at gross Bonus â0.73 SBAR-C â0.54 VBZ â0.54 IN â0.52 NN â0.51 PP-C â0.47 right double quote â0.39 ADJP â0.34 POS â0.31 ADVP â0.30 RP â0.29 PRT â0.27 SG-C â0.22 S-C â0.21 NNPS â0.21 VP-BAR â0.20 PRP â0.20 NPB-BAR . Penalty +1.30 comma +0.80 DT +0.58 PP +0.44 TO +0.33 NNP +0.30 NNS +0.30 NML +0.22 CD +0.18 PRN +0.16 SYM +0.15 ADJP-BAR +0.15 NP +0.15 MD +0.15 HYPH +0.14 PRN-BAR +0.14 NP-C +0.11 ADJP-C . changes in the systemâs behavior. For example, a major error in the baseline system is to move âX saidâ or âX askedâ from the beginning of the Chinese input to the middle or end of the English trans Table 5: Weights learned for generating syntactic nodes of various types anywhere in the English translation. lation. The error occurs with many speaking verbs, and each time, we trace it to a different rule. The problematic rules can even be non-lexical, e.g.: S(x0:NP-C x1:VP x2:, x3:NP-C x4:VP x5:.) â x3 x4 x2 x0 x1 x5 It is therefore difficult to come up with a straightforward feature to address the problem. However, when we apply MIRA with the features already listed, these translation errors all disappear, as demon 38.5 38 37.5 37 36.5 36 35.5 35 Tune Test 0 5 10 15 20 25 Epoch strated by examples 4â5 in Figure 1. Why does this happen? It turns out that in translation hypotheses that move âX saidâ or âX askedâ away from the beginning of the sentence, more commas appear, and fewer S-C and SBAR-C nodes appear. Therefore, the new features work to discourage these hypotheses. Example 6 shows additionally that commas next to speaking verbs are now correctly deleted. Examples 7â8 in Figure 1 show other kinds of unanticipated improvements. We do not have space for a fuller analysis, but we note that the specific effects we describe above account for only part of the overall Bï¬ï¥ïµ improvement. 6.2 Word context features. In Table 6 are shown feature weights learned for the word-context features. A surprising number of the highest-weighted features have to do with translations of dates and bylines. Many of the penalties seem to discourage spurious insertion or deletion of frequent words (for, âs, said, parentheses, and quotes). Finally, we note that several of the features (the third- and eighth-ranked reward and twelfth- ranked penalty) shape the translation of shuo âsaidâ, preferring translations with an overt complementizer that and without a comma. Thus these features work together to attack a frequent problem that our target- syntax features also addressed. Figure 2 shows the performance of Hiero with all of its features on the tuning and test sets over time. The scores on the tuning set rise rapidly, and the scores on the test set also rise, but much more slowly, and there appears to be slight degradation after the 18th pass through the tuning data. This seems in line with the finding of Watanabe et al. (2007) that with on the order of 10,000 features, overfitting is possible, but we can still improve accuracy on new data. Figure 2: Using over 10,000 word-context features leads to overfitting, but its detrimental effects are modest. Scores on the tuning set were obtained from the 1-best output of the online learning algorithm, whereas scores on the test set were obtained using averaged weights. Early stopping would have given +0.2 Bï¬ï¥ïµ over the results reported in Table 1.1 7 Conclusion. We have described a variety of features for statistical machine translation and applied them to syntax- based and hierarchical systems. We saw that these features, discriminatively trained using MIRA, led to significant improvements, and took a closer look at the results to see how the new features qualitatively improved translation quality. We draw three conclusions from this study. First, we have shown that these new features can improve the performance even of top-scoring MT systems. Second, these results add to a growing body of evidence that MIRA is preferable to MERT for discriminative training. When training over 10,000 features on a modest amount of data, we, like Watanabe et al. (2007), did observe overfitting, yet saw improvements on new data. Third, we have shown that syntax-based machine translation offers possibilities for features not available in other models, making syntax-based MT and MIRA an especially strong combination for future work. 1 It was this iteration, in fact, which was used to derive the combined feature count used in the title of this paper. 1 MERT: the united states pending israeli clarification on golan settlement plan. MIRA: the united states is waiting for israeli clarification on golan settlement plan 2 MERT: . . . the average life expectancy of only 18 months , canada âs minority goverment will . . .. MIRA: . . . the average life expectancy of canadaâs previous minority government is only 18 months . . . 3 MERT: . . . since un inspectors expelled by north korea . . .. MIRA: . . . since un inspectors were expelled by north korea . . . 4 MERT: another thing is . . . , " he said , " obviously , the first thing we need to do . . . MIRA: he said : " obviously , the first thing we need to do . . . , and another thing is . . . " 5 MERT: the actual timing . . . reopened in january , yoon said .. MIRA: yoon said the issue of the timing . . . 6 MERT: . . . us - led coalition forces , said today that the crash . . .. MIRA: . . . us - led coalition forces said today that a us military . . . 7 MERT: . . . and others will feel the danger .. MIRA: . . . and others will not feel the danger . 8 MERT: in residential or public activities within 200 meters of the region , . . .. MIRA: within 200 m of residential or public activities area , . . . Figure 1: Improved syntax-based translations due to MIRA-trained weights. Bonus f e context â1.19 <unk> <unk> fâ1 = ri âdayâ â1.01 <unk> <unk> fâ1 = ( â0.84 , that fâ1 = shuo âsayâ â0.82 yue âmonthâ <unk> f+1 = <unk> â0.78 " " fâ1 = <unk> â0.76 " " f+1 = <unk> â0.66 <unk> <unk> f+1 = nian âyearâ â0.65 , that f+1 = <unk> . P e n al ty f e context +1.12 <unk> ) f+1 = <unk> +0.83 jiang âshallâ be f+1 = <unk> +0.83 zhengfu âgovernmentâ the fâ1 = <unk> +0.73 <unk> ) fâ1 = <unk> +0.73 <unk> ( f+1 = <unk> +0.72 <unk> ) fâ1 = ri âdayâ +0.70 <unk> ( fâ1 = ri âdayâ +0.69 <unk> ( fâ1 = <unk> +0.66 <unk> for fâ1 = <unk> . +0.66 <unk> âs fâ1 = , +0.65 <unk> said fâ1 = <unk> +0.60 , , fâ1 = shuo âsayâ . Table 6: Weights learned for word-context features, which fire when English word e is generated aligned to Chinese word f , with Chinese word fâ1 to the left or f+1 to the right. Glosses for Chinese words are not part of features.
 Finite-State Non-Concatenative Morphotactics  Finite-state morphology in the general tradition of the Two-Level and Xerox implementations has proved very successful in the production of robust morphological analyzer-generators, including many large-scale commercial systems. However, it has long been recognized that these implementations have serious limitations in handling non-concatenative phenomena. We describe a new technique for constructing finite- state transducers that involves reapplying the regular-expression compiler to its own output. Implemented in an algorithm called compile- replace, this technique has proved useful for handling non-concatenative phenomena; and we demonstrate it on Malay full-stem reduplication and Arabic stem interdigitation.  Most natural languages construct words by concatenating morphemes together in strict orders. Such âconcatenative morphotacticsâ can be impressively productive, especially in agglutinative languages like Aymara (Figure 11) or Turkish, and in agglutinative/polysynthetic languages like Inuktitut (Figure 2)(Mallon, 1999, 2). In such languages a single word may contain as many morphemes as an average-length English sentence. Finite-state morphology in the tradition of the Two-Level (Koskenniemi, 1983) and Xerox implementations (Karttunen, 1991; Karttunen, 1994; Beesley and Karttunen, 2000) has been very successful in implementing large-scale, robust and efficient morphological analyzergenerators for concatenative languages, includ ing the commercially important European languages and non-Indo-European examples like 1 I wish to thank Stuart Newton for this example. Finnish, Turkish and Hungarian. However, Koskenniemi himself understood that his initial implementation had significant limitations in handling non-concatenative morphotactic processes: âOnly restricted infixation and reduplication can be handled adequately with the present system. Some extensions or revisions will be necessary for an adequate description of languages possessing extensive infixation or reduplicationâ (Koskenniemi, 1983, 27). This limitation has of course not escaped the notice of various reviewers, e.g. Sproat(1992). We shall argue that the morphotactic limitations of the traditional implementations are the direct result of relying solely on the concatenation operation in morphotactic description. We describe a technique, within the Xerox implementation of finite-state morphology, that corrects the limitations at the source, going beyond concatenation to allow the full range of finite-state operations to be used in morphotac- tic description. Regular-expression descriptions are compiled into finite-state automata or transducers (collectively called networks) as usual, and then the compiler is reapplied to its own output, producing a modified but still finite- state network. This technique, implemented in an algorithm called compile-replace, has already proved useful for handling Malay full- stem reduplication and Arabic stem interdigitation, which will be described below. Before illustrating these applications, we will first outline our general approach to finite-state morphology.  Lexical: uta+ma+naka+p+xa+samacha-i+wa Surface: uta ma n ka p xa samach i wa uta = house (root) +ma = 2nd person possessive +na = in -ka = (locative, verbalizer) +p = plural +xa = perfect aspect +samacha = "apparently" -i = 3rd person +wa = topic marker Figure 1: Aymara: utamankapxasamachiwa = âit appears that they are in your houseâ Lexical: Paris+mut+nngau+juma+niraq+lauq+sima+nngit+junga Surface: Pari mu nngau juma nira lauq sima nngit tunga Paris = (root = Paris) +mut = terminalis case ending +nngau = go (verbalizer) +juma = want +niraq = declare (that) +lauq = past +sima = (added to -lauq- indicates "distant past") +nngit = negative +junga = 1st person sing. present indic (nonspecific) Figure 2: Inuktitut: Parimunngaujumaniralauqsimanngittunga = âI never said I wanted to go to Parisâ 2.1 Analysis and Generation. In the most theory- and implementation-neutral form, morphological analysis and generation of written words can be modeled as a relation between the words themselves and analyses of those words. Computationally, as shown in Figure 3, a black-box module maps from words to analyses to effect Analysis, and from analyses to words to effect Generation. ANALYSES ANALYZER/ GENERATOR WORDS Figure 3: Morphological Analysis/Generation as a Relation between Analyses and Words The basic claim or hope of the finite-state approach to natural-language morphology is that relations like that represented in Figure 3 are in fact regular relations, i.e. relations between two regular languages. The surface language consists of strings (= words = sequences of symbols) written according to some defined orthography. In a commercial application for a natural language, the surface language to be modeled is usually a given, e.g. the set of valid French words as written according to standard French orthography. The lexical language again consists of strings, but strings designed according to the needs and taste of the linguist, representing analyses of the surface words. It is sometimes convenient to design these lexical strings to show all the constituent morphemes in their morphophonemic form, separated and identified as in Figures 1 and 2. In other applications, it may be useful to design the lexical strings to contain the traditional dictionary citation form, together with linguist-selected âtagâ sym Analysis Strings Regular Expression Compiler F S T Word Strings Figure 4: Compilation of a Regular Expression into an fst that Maps between Two Regular Languages bols like +Noun, +Verb, +SG, +PL, that convey category, person, number, tense, mood, case, etc. Thus the lexical string representing paie, the first-person singular, present indicative form of the French verb payer (âto payâ), might be spelled payer+IndP+SG+P1+Verb. The tag symbols are stored and manipulated just like alphabetic symbols, but they have multicharacter print names. If the relation is finite-state, then it can be defined using the metalanguage of regular expressions; and, with a suitable compiler, the regular expression source code can be compiled into a finite-state transducer (fst), as shown in Figure 4, that implements the relation computationally. Following convention, we will often refer to the upper projection of the fst, representing analyses, as the lexical language, a set of lexical strings; and we will refer to the lower projection as the surface language, consisting of surface strings. There are compelling advantages to computing with such finite-state machines, including mathematical elegance, flexibility, and for most natural-language applications, high efficiency and data-compaction. One computes with fsts by applying them, in either direction, to an input string. When one such fst that was written for French is applied in an upward direction to the surface word maisons (âhousesâ), it returns the related string maison+Fem+PL+Noun, consisting of the citation form and tag symbols chosen by a linguist to convey that the surface form is a feminine noun in the plural form. A single surface string can be related to multiple lexical strings, e.g. applying this fst in an upward direction to surface string suis produces the four related lexical strings shown in Figure 5. Such ambiguity of surface strings is very common. ^etre+IndP+SG+P1+Verb suivre+IndP+SG+P2+Verb suivre+IndP+SG+P1+Verb suivre+Imp+SG+P2+Verb Figure 5: Multiple Analyses for suis Conversely, the very same fst can be applied in a downward direction to a lexical string like ^etre+IndP+SG+P1+Verb to return the related surface string suis ; such transducers are inherently bidirectional. Ambiguity in the downward direction is also possible, as in the relation of the lexical string payer+IndP+SG+P1+Verb (âI payâ) to the surface strings paie and paye, which are in fact valid alternate spellings in standard French orthography. 2.2 Morphotactics and Alternations. There are two challenges in modeling natural language morphology: â¢ Morphotactics â¢ Phonological/Orthographical Alternations Finite-state morphology models both using regular expressions. The source descriptions may also be written in higher-level notations (e.g. lexc (Karttunen, 1993), twolc (Karttunen and Beesley, 1992) and Replace Rules (Karttunen, 1995; Karttunen, 1996; Kempe and Karttunen, 1996)) that are simply helpful short- hands for regular expressions and that compile, using their dedicated compilers, into finite-state networks. In practice, the most commonly separated modules are a lexicon fst, containing lexical strings, and a separately written set of Lexicon Regular Expression Rule Regular Expression Compiler Lexicon FST .o. Rule FST Lexical Transducer (a single FST) Figure 6: Creation of a Lexical Transducer rule fsts that map from the strings in the lexicon to properly spelled surface strings. The lexicon description defines the morphotactics of the language, and the rules define the alternations. The separately compiled lexicon and rule fsts can subsequently be composed together as in Figure 6 to form a single âlexical transducerâ (Karttunen et al., 1992) that could have been defined equivalently, but perhaps less perspicuously and less efficiently, with a single regular expression. In the lexical transducers built at Xerox, the strings on the lower side of the transducer are inflected surface forms of the language. The strings on upper side of the transducer contain the citation forms of each morpheme and any number of tag symbols that indicate the inflections and derivations of the corresponding surface form. For example, the information that the comparative of the adjective big is bigger might be represented in the English lexical transducer by the path (= sequence of states and arcs) in Figure 7 where the zeros represent epsilon symbols.2 The gemination of g and Lexical side: For the sake of clarity, Figure 7 represents the upper (= lexical) and the lower (= surface) side of the arc label separately on the opposite sides of the arc. In the remaining diagrams, we use a more compact notation: the upper and the lower symbol are combined into a single label of the form upper:lower if the symbols are distinct. A single symbol is used for an identity pair. In the standard notation, the path in Figure 7 is labeled as b i g 0:g +Adj:0 0:e +Comp:r. Lexical transducers are more efficient for analysis and generation than the classical two- level systems (Koskenniemi, 1983) because the morphotactics and the morphological alternations have been precompiled and need not be consulted at runtime. But it would be possible in principle, and perhaps advantageous for some purposes, to view the regular expressions defining the morphology of a language as an un- compiled âvirtual networkâ. All the finite-state operations (concatenation, union, intersection, composition, etc.) can be simulated by an apply routine at runtime. Most languages build words by simply stringing morphemes (prefixes, roots and suffixes) b i g b i g 0 +Adj g 0 0 +Comp e r together in strict orders. The morphotactic (word building) processes of prefixation and suffixation can be straightforwardly Surface side: Figure 7: A Path in a Transducer for English the epenthetical e in the surface form bigger result from the composition of the original lexicon fst with the rule fst representing the regular morphological alternations in English. 2 The epsilon symbols and their placement in the string are not significant. We will ignore them whenever it is convenient. modeled in finite state terms as concatenation. But some natural languages also exhibit non-concatenative morphotactics. Some times the languages themselves are called ânon- concatenative languagesâ, but most employ significant concatenation as well, so the term ânot completely concatenativeâ (Lavie et al., 1988) is usually more appropriate. In Arabic, for example, prefixes and suffixes attach to stems in the usual concatenative way, but stems themselves are formed by a process known informally as interdigitation; while in Malay, noun plurals are formed by a process known as full-stem reduplication. Although Arabic and Malay also include prefixation and suffixation that are modeled straightforwardly by concatenation, a complete lexicon cannot be a a:0 *:a *:0 *:0 0:a obtained without non-concatenative processes. We will proceed with descriptions of how Malay reduplication and Semitic stem interdigitation are handled in finite-state morphology using the new compile-replace algorithm.  The central idea in our approach to the modeling of non-concatenative processes is to define networks using regular expressions, as before; but we now define the strings of an intermediate network so that they contain appropriate substrings that are themselves in the format of regular expressions. The compile- replace algorithm then reapplies the regular- expression compiler to its own output, compiling the regular-expression substrings in the intermediate network and replacing them with the result of the compilation. To take a simple non-linguistic example, Figure 8 represents a network that maps the regular expression a* into ^[a*^]; that is, the same expression enclosed between two special delimiters, ^[ and ^], that mark it as a regular- expression substring.Figure 9: After the Application of Compile Replace lower) of the network. Until an opening delimiter ^[ is encountered, the algorithm constructs a copy of the path it is following. If the network contains no regular-expression substrings, the result will be a copy of the original network. When a ^[ is encountered, the algorithm looks for a closing ^] and extracts the path between the delimiters to be handled in a special way: 1. The symbols along the indicated side of the. path are concatenated into a string and eliminated from the path leaving just the symbols on the opposite side. 2. A separate network is created that contains the modified path. 3. The extracted string is compiled into a. second network with the standard regular- expression compiler.  gle one using the crossproduct operation.  resenting the origin and the destination of 0:^[ a * 0:^] the regular-expression path. Figure 8: A Network with a Regular-Expression Substring on the Lower Side The application of the compile-replace algorithm to the lower side of the network eliminates the markers, compiles the regular expression a* and maps the upper side of the path to the language resulting from the compilation. The network created by the operation is shown in Figure 9. When applied in the âupwardâ direction, the transducer in Figure 9 maps any string of the infinite a* language into the regular expression from which the language was compiled. The compile-replace algorithm is essentially a variant of a simple recursive-descent copyingroutine. It expects to find delimited regular expression substrings on a given side (upper or After the special treatment of the regular- expression path is finished, normal processing is resumed in the destination state of the closing ^] arc. For example, the result shown in Figure 9 represents the crossproduct of the two networks shown in Figure 10. a * a Figure 10: Networks Illustrating Steps 2 and 3 of the Compile-Replace Algorithm In this simple example, the upper language of the original network in Figure 8 is identical to the regular expression that is compiled and replaced. In the linguistic applications presented Lexical: b a g i +Noun +Plural Surface: ^[ { b a g i } ^ 2 ^] Lexical: p e l a b u h a n +Noun +Plural Surface: ^[ { p e l a b u h a n } ^ 2 ^] Figure 11: Two Paths in the Initial Malay Transducer Defined via Concatenation in the next sections, the two sides of a regular- expression path contain different strings. The upper side contains morphological information; the regular-expression operators appear only on the lower side and are not present in the final result. 3.1 Reduplication. Traditional Two-Level implementations are already capable of describing some limited reduplication and infixation as in Tagalog (Antworth, 1990, 156â162). The more challenging phenomenon is variable-length redupli- cation, as found in Malay and the closely related Indonesian language. An example of variable-length full-stem reduplication occurs with the Malay stem bagi, which means âbagâ or âsuitcaseâ; this form is in fact number-neutral and can translate as the plural. Its overt plural is phonologically bagibagi,3 formed by repeating the stem twice in a row. Although this pluralization process may appear concatenative, it does not involve concatenating a predictable pluralizing morpheme, but rather copying the preceding stem, whatever it may be and however long it may be. Thus the overt plural of pelabuhan (âportâ), itself a derived form, is phonologically pelabuhanpelabuhan. Productive reduplication cannot be described by finite-state or even context-free formalisms. It is well known that the copy language, {ww | w Ç« L}, where each word contains two copies of the same string, is a context-sensitive language. However, if the âbaseâ language L is finite, we can construct a finite-state network that encodes L and the reduplications of all the strings in L. On the assumption that there are only a finite number of words subject to reduplication (no free compounding), it is possible to construct a lexical transducer for languages 3 In the standard orthography, such reduplicated words are written with a hyphen, e.g. bagibagi, that we will ignore for this example. such as Malay. We will show a simple and elegant way to do this with strictly finite-state operations. To understand the general solution to full- stem reduplication using the compile-replace algorithm requires a bit of background. In the regular expression calculus there are several operators that involve concatenation. For example, if A is a regular expression denoting a language or a relation, A* denotes zero or more and A+ denotes one or more concatenations of A with itself. There are also operators that express a fixed number of concatenations. In the Xerox calculus, expressions of the form A^n, where n is an integer, denote n concatenations of A. {abc} denotes the concatenation of symbols a, b, and c. We also employ ^[ and ^] as delimiter symbols around regular-expression substrings. The reduplication of any string w can then be notated as {w}^2, and we start by defining a network where the lower-side strings are built by simple concatenation of a prefix ^[, a root enclosed in braces, and an overt-plural suffix ^2 followed by the closing ^]. Figure 11 shows the paths for two Malay plurals in the initial network. The compile-replace algorithm, applied to the lower-side of this network, recognizes each individual delimited regular-expression substring like ^[{bagi}^2^], compiles it, and replaces it with the result of the compilation, here bagibagi. The same process applies to the entire lower-side language, resulting in a network that relates pairs of strings such as the ones in Figure 12. This provides the desired solution, still finite-state, for analyzing and generating full- stem reduplication in Malay.4 4 It is well-known (McCarthy and Prince, 1995) that reduplication can be a more complex phenomenon than it is in Malay. In some languages only a part of the stem is reduplicated and there may be systematic differences between the reduplicate and the base form. We believe that our approach to reduplication can account for these complex phenomena as well but we cannot discuss the Lexical: b a g i +Noun +Plural Surface: b a g i b a g i Lexical: p e l a b u h a n +Noun +Plural Surface: p e l a b u h a n p e l a b u h a n Figure 12: The Malay fst After the Application of Compile-Replace to the Lower-Side Language The special delimiters ^[ and ^] can be used to surround any appropriate regular- expression substring, using any necessary regular-expression operators, and compile- replace may be applied to the lower-side and/or upper-side of the network as desired. There is nothing to stop the linguist from inserting delimiters multiple times, including via composition, and reapplying compile-replace multiple times (see the Appendix). The technique implemented in compile-replace is a general way of allowing the regular-expression compiler to reapply to and modify its own output. 3.2 Semitic Stem Interdigitation. 3.2.1 Review of Earlier Work Much of the work in non-concatenative finite- state morphotactics has been dedicated to handling Semitic stem interdigitation. An example of interdigitation occurs with the Arabic stem katab, which means âwroteâ. According to an influential autosegmental analysis (McCarthy, 1981), this stem consists of an all-consonant root ktb whose general meaning has to do with writing, an abstract consonant-vowel template CVCVC, and a voweling or vocalization that he symbolized simply as a, signifying perfect aspect and active voice. The root consonants are associated with the C slots of the template and the vowel or vowels with the V slots, producing a complete stem katab. If the root and the vocalization are thought of as morphemes, neither morpheme occurs continuously in the stem. The same root ktb can combine with the template CVCVC and a different vocalization ui, signifying perfect aspect and passive voice, producing the stem kutib, which means âwas writtenâ. Similarly, the root ktb can combine with template CVVCVC and ui to produce kuutib, the root drs can combine with CVCVC and ui to form duris, and so forth. tiers of McCarthy (1981) as projections of a multi-level transducer and wrote a small Prolog- based prototype that handled the interdigitation of roots, CV-templates and vocalizations into abstract Arabic stems; this general approach, with multi-tape transducers, has been explored and extended by Kiraz in several papers (1994a; 1996; 1994b; 2000) with respect to Syriac and Arabic. The implementation is described in Kiraz and GrimleyEvans (1999). In work more directly related to the current solution, it was Kataja and Koskenniemi (1988) who first demonstrated that Semitic (Akkadian) roots and patterns5 could be formalized as regular languages, and that the non-concatenative interdigitation of stems could be elegantly formalized as the intersection of those regular languages. Thus Akkadian words were formalized as consisting of morphemes, some of which were combined together by intersection and others of which were combined via concatenation. This was the key insight: morphotactic description could employ various finite-state operations, not just concatenation; and languages that required only concatenation were just special cases. By extension, the widely noticed limitations of early finite-state implementations in dealing with non-concatenative morphotactics could be traced to their dependence on the concatenation operation in morphotactic descriptions. This insight of Kataja and Koskenniemi was applied by Beesley in a large-scale morphological analyzer for Arabic, first using an implementation that simulated the intersection of stems in code at runtime (Beesley, 1989; Beesley et al., 1989; Beesley, 1990; Beesley, 1991), and ran rather slowly; and later, using Xerox finite-state technology (Beesley, 1996; Beesley, 1998a), a new implementation that intersected the stems at compile time and performed well at runtime. Kay (1987) reformalized the autosegmental 5 These patterns combine what McCarthy (1981). issue here due to lack of space. would call templates and vocalizations. The 1996 algorithm that intersected roots and patterns into stems, and substituted the original roots and patterns on just the lower side with the intersected stem, was admittedly rather ad hoc and computationally intensive, taking over two hours to handle about 90,000 stems on a SUN Ultra workstation. The compile-replace algorithm is a vast improvement in both generality and efficiency, producing the same result in a few minutes. Following the lines of Kataja and Koskenniemi (1988), we could define intermediate networks with regular-expression substrings that indicate the intersection of suitably encoded roots, templates, and vocalizations (for a formal description of what such regular-expression substrings would look like, see Beesley (1998c; 1998b)). However, the general-purpose intersection algorithm would be expensive in any nontrivial application, and the interdigitation of stems represents a special case of intersection that we achieve in practice by a much more efficient finite-state algorithm called merge. 3.2.2 Merge The merge algorithm is a pattern-filling operation that combines two regular languages, a template and a filler, into a single one. The strings of the filler language consist of ordinary symbols such as d, r, s, u, i. The template expressions may contain special class symbols such as C (= consonant) or V (= vowel) that represent a predefined set of ordinary symbols. The objective of the merge operation is to align the template strings with the filler strings and to instantiate the class symbols of the template as the matching filler symbols. Like intersection, the merge algorithm operates by following two paths, one in the template network, the other in the filler network, and it constructs the corresponding single path in the result network. Every state in the result corresponds to two original states, one in template, the other in the filler. If the original states are both final, the resulting state is also final; otherwise it is non-final. In other words, in order to construct a successful path, the algorithm must reach a final state in both of the original networks. If the new path terminates in a non-final state, it represents a failure and will eventually be pruned out. The operation starts in the initial state of the original networks. At each point, the algorithm tries to find all the successful matches between the template arcs and filler arcs. A match is successful if the filler arc symbol is included in the class designated by the template arc symbol. The main difference between merge and classical intersection is in Conditions 1 and 2 below: 1. If a successful match is found, a new arc is. added to the current result state. The arc is labeled with the filler arc symbol; its destination is the result state that corresponds to the two original destinations. 2. If no successful match is found for a given. template arc, the arc is copied into the current result state. Its destination is the result state that corresponds to the destination of the template arc and the current filler state. In effect, Condition 2 preserves any template arc that does not find a match. In that case, the path in the template network advances to a new state while the path in the filler network stays at the current state. We use the networks in Figure 13 to illustrate the effect of the merge algorithm. Figure 13 shows a linear template network and two filler networks, one of which is cyclic. C V V C V C d r s i u Figure 13: A Template Network and Two Filler Networks It is easy to see that the merge of the drs network with the template network yields the result shown in Figure 14. The three symbols of the filler string are instantiated in the three consonant slots in the CVVCVC template. d V V r V s Figure 14: Intermediate Result. Figure 15 presents the final result in which the second filler network in Figure 13 is merged with the intermediate result shown in Figure 14. Lexical: k t b =Root C V C V C =Template a + =Voc Surface: ^[ k t b .m>. C V C V C .<m. a + ^] Lexical: k t b =Root C V C V C =Template u * i =Voc Surface: ^[ k t b .m>. C V C V C .<m. u * i ^] Lexical: d r s =Root C V V C V C =Template u * i =Voc Surface: ^[ d r s .m>. C V V C V C .<m. u * i ^] Figure 16: Initial paths d u u r i s Figure 15: Final Result In this case, the filler language contains an infinite set of strings, but only one successful path can be constructed. Because the filler string ends with a single i, the first two V symbols can be instantiated only as u. Note that ordinary symbols in the partially filled template are treated like the class symbols that do not find a match. That is, they are copied into the result in their current position without consuming a filler symbol. To introduce the merge operation into the Xerox regular expression calculus we need to choose an operator symbol. Because merge, like subtraction, is a non-commutative operation, we also must distinguish between the template and the filler. For example, we could choose .m. as the operator and decide by convention which of the two operands plays which role in expressions such as [A .m. B]. What we actually have done, perhaps without a sufficiently good motivation, is to introduce two variants of the merge operator, .<m. and .m>., that differ only with respect to whether the template is to the left (.<m.) or to the right (.m>.) of the filler. The expression [A .<m. B] represents the same merge operation as [B .m>. A]. In both cases, A denotes the template, B denotes the filler, and the result is the same. With these new operators, the network in Figure 15 can be compiled from an expression such as d r s .m>. C V V C V C .<m. u* i As we have defined them, .<m. and .m>. are weakly binding left-associative operators. In this example, the first merge instantiates the filler consonants, the second operation fills the vowel slots. However, the order in which the merge operations are performed is irrelevant in this case because the two filler languages do not provide competing instantiations for the same class symbols. 3.2.3 Merging Roots and Vocalizations with Templates Following the tradition, we can represent the lexical forms of Arabic stems as consisting of three components, a consonantal root, a CV template and a vocalization, possibly preceded and followed by additional affixes. In contrast to McCarthy, Kay, and Kiraz, we combine the three components into a single projection. In a sense, McCarthyâs three tiers are conflated into a single one with three distinct parts. In our opinion, there is no substantive difference from a computational point of view. For example, the initial lexical representation of the surface forms katab, kutib, and duuris, may be represented as a concatenation of the three components shown in Figure 16. We use the symbols =Root, =Template, and =Voc to designate the three components of the lexical form. The corresponding initial surface form is a regular-expression substring, containing two merge operators, that will be compiled and replaced by the interdigitated surface form. The application of the compile-replace operation to the lower side of the initial lexicon yields a transducer that maps the Arabic interdigitated forms directly into their corresponding tripartite analyses and vice versa, as illustrated in Figure 17. Alternation rules are subsequently composed on the lower side of the result to map the in- terdigitated, but still morphophonemic, strings into real surface strings. Although many Arabic templates are widely considered to be pure CV-patterns, it has been argued that certain templates also contain Lexical: k t b =Root C V C V C =Template a + =Voc Surface: k a t a b Lexical: k t b =Root C V C V C =Template u * i =Voc Surface: k u t i b Lexical: d r s =Root C V V C V C =Template u * i =Voc Surface: d u u r i s Figure 17: After Applying Compile-Replace to the Lower Side âhard-wiredâ specific vowels and consonants.6 For example, the so-called âFormVIIIâ template is considered, by some linguists, to contain an embedded t: CtVCVC. The presence of ordinary symbols in the template does not pose any problem for the analysis adopted here. As we already mentioned in discussing the intermediate representation in Figure 14, the merge operation treats ordinary symbols in a partially filled template in the same manner as it treats unmatched class symbols. The merge of a root such as ktb with the presumed FormVIII template and the a+ vocal- ism, k t b .m>. C t V C V C .<m. a+ produces the desired result, ktatab, without any additional mechanism. 4 Status of the Implementations. 4.1 Malay Morphological. Analyzer/Generator Malay and Indonesian are closely-related languages characterized by rich derivation and little or nothing that could be called inflection. The Malay morphological analyzer prototype, written using lexc, Replace Rules, and compile-replace, implements approximately 50 different derivational processes, including pre- fixation, suffixation, prefix-suffix pairs (circum- fixation), reduplication, some infixation, and combinations of these processes. Each root is marked manually in the source dictionary to indicate the idiosyncratic subset of derivational processes that it undergoes. The small prototype dictionary, stored in an XML format, contains approximately 1000 roots, with about 1500 derivational subentries (i.e. an average of 1.5 derivational processes per root). At compile time, the XML dictionary is parsed and âdowntranslatedâ into the source format required for the lexc compiler. The XML dictionary could be expanded by any competent Malay lexicographer. 4.2 Arabic Morphological. Analyzer/Generator The current Arabic system has been described in some detail in previous publications (Beesley, 1996; Beesley, 1998a; Beesley, 1998b) and is available for testing on the Internet.7 The modification of the system to use the compile-replace algorithm has not changed the size or the behavior of the system in any way, but it has reduced the compilation time from hours to minutes. 5 Conclusion. The well-founded criticism of traditional implementations of finite-state morphology, that they are limited to handling concatenative morpho- tactics, is a direct result of their dependence on the concatenation operation in morphotactic description. The technique described here, implemented in the compile-replace algorithm, allows the regular-expression compiler to reapply to and modify its own output, effectively freeing morphotactic description to use any finite-state operation. Significant experiments with Malay and a much larger application in Arabic have shown the value of this technique in handling two classic examples of non-concatenative morphotactics: full-stem reduplication and Semitic stem interdigitation. Work remains to be done in applying the technique to other known varieties of non-concatenative morphotactics. The compile-replace algorithm and the merge operator introduced in this paper are general techniques not limited to handling the specific  versial issue. 7 http://www.x rce.xerox.com /research/mlt t/arabic/ morphotactic problems we have discussed. We expect that they will have many other useful applications. One illustration is given in the Appendix. 6 Appendix: Palindrome Extraction. To demonstrate the power of the compile- replace method, let us show how it can be applied to solve another âhardâ problem: identifying and extracting all the palindromes from a lexicon. Like reduplication, palindrome identification appears at first to require more powerful tools than a finite-state calculus. But this task can be accomplished, in fact quite efficiently, by using the compile-replace technique. Let us assume that L is a simple network constructed from an English wordlist. We start by extracting from L all the words with a property that is necessary but not sufficient for being a palindrome, namely, the words whose inverse is also an English word. This step can be accomplished by redefining L as [L & L.r] where & represents intersection and .r is the reverse operator. The resulting network contains palindromes such as madam as well non-palindromes such as dog and god. The remaining task is to eliminate all the words like dog that are not identical to their own inverse. This can be done in three steps. We first apply the technique used for Malay reduplication. That is, we redefine L as "^[" "[" L XX "]" "^" 2 "^]", and apply the compile-replace operation. At this point the lower-side of L contains strings such as dogXXdogXX and madamXXmadamXX where XX is a specially introduced symbol to mark the middle (and the end) of each string. The next, and somewhat delicate, step is to replace the XX markers by the desired operators, intersection and reverse, and to wrap the special regular expression delimiters ^[ and ^] around the whole lexicon. This can be done by composing L with one or several replace transducers to yield a network consisting of expressions such as ^[ d o g & [d o g].r ^] and ^[ m a d a m & [m a d a m].r ^] In the third and final step, the application of compile-replace eliminates words like dog because the intersection of dog with the inverted form god is empty. Only the palindromes survive the operation. The extrac tion of all the palindromes from the 25K Unix /usr/dict/words file by this method takes a couple of seconds.
 Supersense Tagging of Unknown Nouns using Semantic Similarity  The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET. Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples. We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger. We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.  Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). In particular, WORDNET (Fellbaum, 1998) has significantly influenced research in NLP. Unfortunately, these resource are extremely time- consuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise. Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular. Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary. Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap. Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics. Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses. Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus. By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult. consistency when classifying similar words into categories. For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earthâs atmosphere. These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources. Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy. Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETâs hierarchical structure to create many annotated training instances from the synset glosses. This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences. Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun. The supersenses of these synonyms are then combined to determine the supersense. This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1. 26 Proceedings of the 43rd Annual Meeting of the ACL, pages 26â33, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET  There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lex- files). For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops. Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs. Lex-files form a set of coarse-grained sense distinctions within WORDNET. For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship. The names and descriptions of the noun lex-files are shown in Table 1. Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30). For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time. Ciaramita and Johnson (2003) call the noun lex-file classes supersenses. There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses. Ciaramita (2002) has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories. Ciaramita et al. (2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appear ing directly underneath. Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Rogetâs (Yarowsky, 1992). These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts. Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses. They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g. location and person) for labelling predominantly unseen terms. Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy. Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself. Clearly, this is the ultimate goal, to be able to insert new terms into lexical resources, extending the structure where necessary. Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.  A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORD- NET and the construction of new wordnets using the concept structure from English. For lexical FreeNet, Beefer- man (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information. The co-occurrence window was 500 words which was designed to approximate average document length. Caraballo and Charniak (1999) have explored determining noun specificity from raw text. They find that simple frequency counts are the most effective way of determining the parent-child ordering, achieving 83% accuracy over types of vehicle, food and occupation. The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun. Specificity ordering is a necessary step for building a noun hierarchy. However, this approach clearly cannot build a hierarchy alone. For instance, entity is less frequent than many concepts it subsumes. This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners. Hearst and SchuÂ¨ tze (1993) flatten WORDNET into 726 categories using an algorithm which attempts to minimise the variance in category size. These categories are used to label paragraphs with topics, effectively repeating Yarowskyâs (1992) experiments using the their categories rather than Rogetâs thesaurus. SchuÂ¨ tzeâs (1992) WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem). Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word. Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy. He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms. This same technique as is used in our approach to supersense tagging. Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems. Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy. They developed an efficient algorithm for estimating the model over hierarchical training data.  Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET. They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation. Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify. Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003). The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense. The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003). They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set. Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set. Some examples from the test sets are given in Table 2 with their supersenses.  We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search. The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortiumâs news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus. The components and their sizes including punctuation are given in Table 3. The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above. C O R P U S D O C S . S E N T S . WO R D S B N C 4 1 2 4 6 . 2 M 1 1 4 M R C V1 8 0 6 7 9 1 8 . 1 M 2 0 7 M C S R -I I I 4 9 1 3 4 9 9 . 3 M 2 2 6 M NA N T C 9 3 0 3 6 7 2 3. 2 M 5 5 9 M NA N T S 9 4 2 1 6 7 2 5. 2 M 5 0 7 M AC QU A I N T 1 03 3 46 1 2 1. 3 M 4 9 1 M Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997). Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise. The rest of the pipeline is described in the next section.  Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts. This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in. In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in. The key parameters are the context extraction method and the similarity measure used to compare context vectors. Our approach to vector-space similarity is based on the SEXTANT system described in Grefenstette (1994). Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results. SEXTANT extracts relation tuples (w, r, wt ) for each noun, where w is the headword, r is the relation type and wt is the other word. The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible. We describe the shallow pipeline in detail below. Curran and Moens (2002a) compared several different similarity measures and found that Grefenstetteâs weighted JACCARD measure performed the best: R E L AT I O N D E S C R I P T I O N adj nounâadjectival modifier relation dobj verbâdirect object relation iobj verbâindirect object relation nn nounânoun modifier relation nnprep nounâprepositional head relation subj verbâsubject relation Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001) â and is very efficient, analysing over 80 000 words per second. morpha often maintains sense distinctions between singular and plural nouns; for instance: spectacles is not reduced to spectacle, but fails to do so in other cases: glasses is converted to glass. This inconsis L min(wgt(w1 , âr , âwI ), wgt(w2 , âr , âwI )) L max(wgt(w1 , âr , âwI ), wgt(w2 , âr , âwI )) (1) tency is problematic when using morphological analysis to smooth vector-space models. However, morphological smoothing still produces better results in practice. where wgt(w, r, wt ) is the weight function for relation (w, r, wt ). Curran and Moens (2002a) introduced the TTEST weight function, which is used in collocation extraction. Here, the t-test compares the joint and product probability distributions of the headword and context: 6.3 Grammatical Relation Extraction. After the raw text has been POS tagged and chunked, the grammatical relation extraction algorithm is run over the chunks. This consists of five passes over each sentence that first identify noun and verb phrase heads and p(w, r, wt ) â p(â, r, wt )p(w, â, â) p(â, r, wt )p(w, â, â) (2) then collect grammatical relations between each common noun and its modifiers and verbs. A global list of grammatical relations generated by each pass is maintained where â indicates a global sum over that element of the relation tuple. JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moenâs configuration for our super- sense tagging experiments. 6.1 Part of Speech Tagging and Chunking. Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994). The only similar performing tool is the Trigrams ânâ Tags tagger (Brants, 2000) which uses a much simpler statistical model. Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script. Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstetteâs table-based results, i.e. the SEXTANT always prefers noun attachment. 6.2 Morphological Analysis. Our implementation uses morpha, the Sussex morphological analyser (Minnen et al., 2001), which is implemented using lex grammars for both affix splitting and generation. morpha has wide coverage â nearly 100% across the passes. The global list is used to determine if a word is already attached. Once all five passes have been completed this association list contains all of the noun- modifier/verb pairs which have been extracted from the sentence. The types of grammatical relation extracted by SEXTANT are shown in Table 4. For relations between nouns (nn and nnprep), we also create inverse relations (wt , rt , w) representing the fact that wt can modify w. The 5 passes are described below. Pass 1: Noun Pre-modifiers This pass scans NPs, left to right, creating adjectival (adj) and nominal (nn) pre-modifier grammatical relations (GRs) with every noun to the pre-modifierâs right, up to a preposition or the phrase end. This corresponds to assuming right-branching noun compounds. Within each NP only the NP and PP heads remain unattached. Pass 2: Noun Post-modifiers This pass scans NPs, right to left, creating post-modifier GRs between the unattached heads of NPs and PPs. If a preposition is encountered between the noun heads, a prepositional noun (nnprep) GR is created, otherwise an appositional noun (nn) GR is created. This corresponds to assuming right-branching PP attachment. After this phrase only the NP head remains unattached. Tense Determination The rightmost verb in each VP is considered the head. A VP is initially categorised as active. If the head verb is a form of be then the VP becomes attributive. Otherwise, the algorithm scans the VP from right to left: if an auxiliary verb form of be is encountered the VP becomes passive; if a progressive verb (except being) is encountered the VP becomes active. Only the noun heads on either side of VPs remain unattached. The remaining three passes attach these to the verb heads as either subjects or objects depending on the voice of the VP. Pass 3: Verb Pre-Attachment This pass scans sentences, right to left, associating the first NP head to the left of the VP with its head. If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created. For example, antigen is the subject of represent. Pass 4: Verb Post-Attachment This pass scans sentences, left to right, associating the first NP or PP head to the right of the VP with its head. If the VP was classed as active and the phrase is an NP then a direct object (dobj) relation is created. If the VP was classed as passive and the phrase is an NP then a subject (subj) relation is created. If the following phrase is a PP then an indirect object (iobj) relation is created. The interaction between the head verb and the preposition determine whether the noun is an indirect object of a ditransitive verb or alternatively the head of a PP that is modifying the verb. However, SEXTANT always attaches the PP to the previous phrase. Pass 5: Verb Progressive Participles The final step of the process is to attach progressive verbs to subjects and objects (without concern for whether they are already attached). Progressive verbs can function as nouns, verbs and adjectives and once again a naÂ¨Ä±ve approximation to the correct attachment is made. Any progressive verb which appears after a determiner or quantifier is considered a noun. Otherwise, it is a verb and passes 3 and 4 are repeated to attach subjects and objects. Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation. Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences.  Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns. This technique is similar to Hearst and SchuÂ¨ tze (1993) and Widdows (2003). However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms. In these cases, our SUFFIX EXAMPLE SUPERSENSEness remoteness attribute -tion, -ment annulment act -ist, -man statesman person -ing, -ion bowling act -ity viscosity attribute -ics, -ism electronics cognition -ene, -ane, -ine arsine substance -er, -or, -ic, -ee, -an mariner person -gy entomology cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix. These rules were created by inspecting the suffixes of rare nouns in WORDNET 1.6. The supersense guessing rules are given in Table 5. If none of the rules match, then the default supersense artifact is assigned. The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection. Each extracted synonym votes for its one or more supersenses that appear in WORDNET 1.6. There are many parameters to consider: â¢ how many extracted synonyms to use; â¢ how to weight each synonymâs vote; â¢ whether unreliable synonyms should be filtered out; â¢ how to deal with polysemous synonyms. The experiments described below consider a range of options for these parameters. In fact, these experiments are so quick to run we have been able to exhaustively test many combinations of these parameters. We have experimented with up to 200 voting extracted synonyms. There are several ways to weight each synonymâs contribution. The simplest approach would be to give each synonym the same weight. Another approach is to use the scores returned by the similarity system. Alternatively, the weights can use the ranking of the extracted synonyms. Again these options have been considered below. A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable. The final issue is how to deal with polysemy. Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)? Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET. A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in S Y S T E M W N 1.6 W N 1.7 .1 Cia ra mit a an d Joh nso n bas eli ne 2 1 % 2 8 % Cia ra mit a an d Joh nso n per cep tro n 5 3 % 5 3 % Si mil arit y bas ed res ult s 6 8 % 6 3 % Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses. This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive.  We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003). The experiments were performed by considering all possible configurations of the parameters described above. The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym. The initial weight could then be divided by the number of supersenses to share out the weight (SHARED). The weight could also be divided by the rank (RANK) to penalise supersenses further down the list. The best performance on the 1.6 test set was achieved with the SCORE voting, without sharing or ranking penalties. The extracted synonyms are filtered before contributing to the vote with their supersense(s). This filtering involves checking that the synonymâs frequency and number of contexts are large enough to ensure it is reliable. We have experimented with a wide range of cutoffs and the best performance on the 1.6 test set was achieved using a minimum cutoff of 5 for the synonymâs frequency and the number of contexts it appears in. The next question is how many synonyms are considered. We considered using just the nearest unambiguous synonym, and the top 5, 10, 20, 50, 100 and 200 synonyms. All of the top performing configurations used 50 synonyms. We have also experimented with filtering out highly polysemous nouns by eliminating words with two, three or more synonyms. However, such a filter turned out to make little difference. Finally, we need to decide when to use the similarity measure and when to fall-back to the guessing rules. This is determined by looking at the frequency and number of attributes for the unknown word. Not surprisingly, the similarity system works better than the guessing rules if it has any information at all. The results are summarised in Table 6. The accuracy of the best-performing configurations was 68% on the Table 7: Breakdown of results by supersense WORDNET 1.6 test set with several other parameter combinations described above performing nearly as well. On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set. By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set. Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised. The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder. Table 7 shows the breakdown in performance for each supersense. The columns show the number of instances of each supersense with the precision, recall and f-score measures as percentages. The most frequent supersenses in both test sets were person, attribute and act. Of the frequent categories, person is the easiest supersense to get correct in both the 1.6 and 1.7.1 test sets, followed by food, artifact and substance. This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency. These factors are conducive for extracting reliable synonyms. These results also support Ciaramita and Johnsonâs view that abstract concepts like communication, cognition and state are much harder. We would expect the location supersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier. Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help.  An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words. This has the advantage of producing a much smaller number of vectors to compare against. In the current system, we must compare a word against the entire vocabulary (over 500 000 headwords), which is much less efficient than a comparison against only 26 supersense context vectors. The question now becomes how to construct vectors of supersenses. The most obvious solution is to sum the context vectors across the words which have each supersense. However, our early experiments suggest that this produces extremely large vectors which do not match well against the much smaller vectors of each unseen word. Also, the same questions arise in the construction of these vectors. How are words with multiple supersenses handled? Our preliminary experiments suggest that only combining the vectors for unambiguous words produces the best results. One solution would be to take the intersection between vectors across words for each supersense (i.e. to find the common contexts that these words appear in). However, given the sparseness of the data this may not leave very large context vectors. A final solution would be to consider a large set of the canonical attributes (Curran and Moens, 2002a) to represent each supersense. Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons. There are a number of problems our system does not currently handle. Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate). Further, our similarity system does not currently incorporate multi-word terms. We overcome this by using the synonyms of the last word in the multi-word term. However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term. Finally, we plan to implement a supervised machine learner to replace the fall- back method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set. We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results. We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis. Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.  Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003). To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6. We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability. Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor. Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003). This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus. Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag. This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available.  We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections. This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.
 Exploring Various Knowledge in Relation Extraction  Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.  With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text. Information Extraction (IE) systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format. According to the scope of the NIST Automatic Content Extraction (ACE) program, current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC). The EDT task entails the detection of entity mentions and chaining them together by identifying their coreference. In ACE vocabulary, entities are objects, mentions are references to them, and relations are semantic relationships between entities. Entities can be of five types: persons, organizations, locations, facilities and geopolitical entities (GPE: geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.). Mentions have three levels: names, nomial expressions or pronouns. The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task. For example, we want to determine whether a person is at a location, based on the evidence in the context. Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query âWho is the president of the United States?â. This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs). Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance. We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework. Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance. It also shows that our fea 1 In ACE (http://www.ldc.upenn.edu/Projects/ACE),. explicit relations occur in text with explicit evidence suggesting the relationships. Implicit relations need not have explicit supporting evidence in text, though they should be evident from a reading of the document. 427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005. Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 and Section 4 describe our approach and various features employed respectively. Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6.  The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. It achieves 52.8 F- measure on the 24 ACE relation subtypes. Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines. Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering. Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical, syntactic and semantic information. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. We also show how semantic information like WordNet and Name List can be equipped to further improve the performance. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.  Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998). Based on the structural risk minimization of the statistical learning theory, SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set. Basically, SVMs are binary classifiers. Therefore, we must extend SVMs to multi-class (e.g. K) such as the ACE RDC task. For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes. The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output. Moreover, we only apply the simple linear kernel, although other kernels can peform better. The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available. In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998). 2 Joachims has just released a new version of SVMLight. for multi-class classification. However, this paper only uses the binary-class version. For details about SVMLight, please see http://svmlight.joachims.org/  The semantic relation is determined between two mentions. In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1. For each pair of mentions3, we compute various lexical, syntactic and semantic features. 4.1 Words. According to their positions, four categories of words are considered: 1) the words of both the mentions, 2) the words between the two mentions, 3) the words before M1, and 4) the words after M2. For the words of both the mentions, we also differentiate the head word4 of a mention from other words since the head word is generally much more important. The words between the two mentions are classified into three bins: the first word in between, the last word in between and other words in between. Both the words before M1 and after M2 are classified into two bins: the first word next to the mention and the second word next to the mention. Since a pronominal mention (especially neutral pronoun such as âitâ and âitsâ) contains little information about the sense of the mention, the co- reference chain is used to decide its sense. This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features, which include: â¢ WM1: bag-of-words in M1 â¢ HM1: head word of M1 3 In ACE, each mention has a head annotation and an. extent annotation. In all our experimentation, we only consider the word string between the beginning point of the extent annotation and the end point of the head annotation. This has an effect of choosing the base phrase contained in the extent annotation. In addition, this also can reduce noises without losing much of information in the mention. For example, in the case where the noun phrase âthe former CEO of McDonaldâ has the head annotation of âCEOâ and the extent annotation of âthe former CEO of McDonaldâ, we only consider âthe former CEOâ in this paper. 4 In this paper, the head word of a mention is normally. set as the last word of the mention. However, when a preposition exists in the mention, its head word is set as the last word before the preposition. For example, the head word of the name mention âUniversity of Michiganâ is âUniversityâ. â¢ WM2: bag-of-words in M2 â¢ HM2: head word of M2 â¢ HM12: combination of HM1 and HM2 â¢ WBNULL: when no word in between â¢ WBFL: the only word in between when only one word in between â¢ WBF: first word in between when at least two words in between â¢ WBL: last word in between when at least two words in between â¢ WBO: other words in between except first and last words when at least three words in between â¢ BM1F: first word before M1 â¢ BM1L: second word before M1 â¢ AM2F: first word after M2 â¢ AM2L: second word after M2 4.2 Entity Type. This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE: â¢ ET12: combination of mention entity types 4.3 Mention Level. This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN: â¢ ML12: combination of mention levels 4.4 Overlap. This category of features includes: â¢ #MB: number of other mentions in between â¢ #WB: number of words in between â¢ M1>M2 or M1<M2: flag indicating whether M2/M1is included in M1/M2. Normally, the above overlap features are too general to be effective alone. Therefore, they are HM12+M1>M2; 4) HM12+M1<M2. 4.5 Base Phrase Chunking. It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference (MUC7 1998). The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees. In this paper, we separate the features of base phrase chunking from those of full parsing. In this way, we can separately evaluate the contributions of base phrase chunking and full parsing. Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing. Most of the chunking features concern about the head words of the phrases between the two mentions. Similar to word features, three categories of phrase heads are considered: 1) the phrase heads in between are also classified into three bins: the first phrase head in between, the last phrase head in between and other phrase heads in between; 2) the phrase heads before M1 are classified into two bins: the first phrase head before and the second phrase head before; 3) the phrase heads after M2 are classified into two bins: the first phrase head after and the second phrase head after. Moreover, we also consider the phrase path in between. â¢ CPHBNULL when no phrase in between â¢ CPHBFL: the only phrase head when only one phrase in between â¢ CPHBF: first phrase head in between when at least two phrases in between â¢ CPHBL: last phrase head in between when at least two phrase heads in between â¢ CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between â¢ CPHBM1F: first phrase head before M1 â¢ CPHBM1L: second phrase head before M1 â¢ CPHAM2F: first phrase head after M2 â¢ CPHAM2F: second phrase head after M2 â¢ CPP: path of phrase labels connecting the two mentions in the chunking â¢ CPPH: path of phrase labels connecting the two mentions in the chunking augmented with head words, if at most two phrases in between 4.6 Dependency Tree. This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree. The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head. It also includes flags indicating whether the two mentions are in the same NP/PP/VP. â¢ ET1DW1: combination of the entity type and the dependent word for M1 â¢ H1DW1: combination of the head word and the dependent word for M1 â¢ ET2DW2: combination of the entity type and the dependent word for M2 â¢ H2DW2: combination of the head word and the dependent word for M2 â¢ ET12SameNP: combination of ET12 and whether M1 and M2 included in the same NP â¢ ET12SamePP: combination of ET12 and whether M1 and M2 exist in the same PP â¢ ET12SameVP: combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree. This category of features concerns about the information inherent only in the full parse tree. â¢ PTP: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree â¢ PTPH: path of phrase labels (removing duplicates) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path. 4.8 Semantic Resources. Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships. Country Name List This is to differentiate the relation subtype âROLE.Citizen-Ofâ, which defines the relationship between a person and the country of the personâs citizenship, from other subtypes, especially âROLE.Residenceâ, where defines the relationship between a person and the location in which the person lives. Two features are defined to include this information: â¢ ET1Country: the entity type of M1 when M2 is a country name â¢ CountryET2: the entity type of M2 when M1 is a country name 5 http://ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE: Parent, Grandparent, Spouse, Sibling, Other-Relative and Other- Personal. This trigger word list is first gathered from WordNet by checking whether a word has the semantic class âperson|â¦|relativeâ. Then, all the trigger words are semi-automatically6 classified into different categories according to their related personal social relation subtypes. We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships. Two features are defined to include this information: â¢ ET1SC2: combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype. â¢ SC1ET2: combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype.  This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system. The ACE corpus is gathered from various newspapers, newswire and broadcasts. In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number. 5.1 Experimental Setting. We use the official ACE corpus from LDC. The training set consists of 674 annotated text documents (~300k words) and 9683 instances of relations. During development, 155 of 674 documents in the training set are set aside for fine-tuning the system. The testing set is held out only for final evaluation. It consists of 97 documents (~50k words) and 1386 instances of relations. Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization (RDC) task, along with their frequency of occurrence in the ACE training set. It shows that the  âGrandParentâ, âSpouseâ and âSiblingâ are automatically set with the same classes without change. However, The remaining words that do not have above four classes are manually classified. ACE corpus suffers from a small amount of annotated data for a few subtypes such as the subtype âFounderâ under the type âROLEâ. It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes âBased-Inâ, âLocatedâ and âResidenceâ under the type âATâ, which are difficult even for human experts to differentiate. Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1: Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved. For example, when comparing mentions m1 and m2, we distinguish between m1-ROLE.Citizen-Of-m2 and m2- ROLE.Citizen-Of-m1. Note that only 6 of these 24 relation subtypes are symmetric: âRelative- Locationâ, âAssociateâ, âOther-Relativeâ, âOther- Professionalâ, âSiblingâ, and âSpouseâ. In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a âNONEâ class for the case where the two mentions are not related. 5.2 Experimental Results. In this paper, we only measure the performance of relation extraction on âtrueâ mentions with âtrueâ chaining of coreference (i.e. as annotated by the corpus annotators) in the ACE corpus. Table 2 measures the performance of our relation extrac tion system over the 43 ACE relation subtypes on the testing set. It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features. Table 2 also measures the contributions of different features by gradually increasing the feature set. It shows that: Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2: Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure. â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase. â¢ The usefulness of mention level features is quite limited. It only improves the F-measure by 0.8 due to the recall increase. â¢ Incorporating the overlap features gives some balance between precision and recall. It increases the F-measure by 3.6 with a big precision decrease and a big recall increase. â¢ Chunking features are very useful. It increases the precision/recall/F-measure by 4.1%/5.6%/ 5.2 respectively. â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively. This may be due to the fact that most of relations in the ACE corpus are quite local. Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word. While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations. However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing. â¢ Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype âROLE.Citizen-Ofâ from âROLE. Residenceâ by distinguishing country GPEs from other GPEs. The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes. Table 4 separately measures the performance of different relation types and major subtypes. It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype. It is not surprising that the performance on the relation type âNEARâ is low because it occurs rarely in both the training and testing data. Others like âPART.Subsidaryâ and âSOCIAL. Other-Professionalâ also suffer from their low occurrences. It also shows that our system performs best on the subtype âSOCIAL.Parentâ and âROLE. Citizen-Ofâ. This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list. Table 4 also indicates the low performance on the relation type âATâ although it frequently occurs in both the training and testing data. This suggests the difficulty of detecting and classifying the relation type âATâ and its subtypes. Table 5 separates the performance of relation detection from overall performance on the testing set. It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection. It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall. It also shows that feature-based methods dramatically outperform kernel methods. This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction. The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task. Finally, Table 6 shows the distributions of errors. It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types. This suggests that relation detection is critical for relation extraction. # of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3: Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68. 1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79. 6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52. 4 5 4 . 8 5 3 . 5 Reside nce 6 6 1 9 9 67. 9 2 8 . 8 4 0 . 4 N EA R 3 5 8 1 88. 9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88. 9 2 2 . 9 3 6 . 4 P A R T 1 6 4 1 0 6 3 9 73. 1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70. 4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37. 8 5 1 . 9 4 3 . 8 R O LE 6 9 9 4 4 3 8 2 84. 4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75. 8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71. 1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59. 6 6 4 . 2 6 1 . 8 Memb er 2 2 4 1 0 4 3 6 74. 3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74. 1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33. 3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4: Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs: fea ture bas ed 8 4. 8 66 .7 74 .7 77 .2 60 .7 68 .0 6 3. 1 4 9. 5 55 .5 Ka mb hat la (20 04) :fe ature bas ed 6 3. 5 4 5. 2 52 .8 Cu lott a et al (20 04) :tre e ker nel 8 1. 2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5: Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first. Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6: Distribution of errors 6 Discussion and Conclusion. In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed. Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance. This may be due to three reasons: First, most of relations defined in ACE have two mentions being close to each other. While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations. Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance. Therefore, the state-of-art full parsing still needs to be further enhanced to provide accurate enough information, especially PP (Preposition Phrase) attachment. Last, effective ways need to be explored to incorporate information embedded in the full Collins M. (1999). Head-driven statistical models for natural language parsing. Ph.D. Dissertation, University of Pennsylvania. Collins M. and Duffy N. (2002). Covolution kernels for natural language. In Dietterich T.G., Becker S. and Ghahramani Z. editors. Advances in Neural Information Processing Systems 14. Cambridge, MA. Culotta A. and Sorensen J. (2004). Dependency tree th parse trees. Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus. Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted. Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data. The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types. In the future work, we will focus on exploring more semantic knowledge in relation extraction, which has not been covered by current research. Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done. Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.
 BiTAM: Bilingual Topic AdMixture Models forWord Alignment  We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation. Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model. Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels). These models enable word- alignment process to leverage topical contents of document-pairs. Efficient variational approximation algorithms are designed for inference and parameter estimation. With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects. Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.  Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models. Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001). Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices. For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations. Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics. For example, the word shot in âIt was a nice shot.â should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing. Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances. In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs. With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous. Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation. This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches. These approaches can be expensive, and they do not emphasize stochastic translation aspects. Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success. We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT. Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003). Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model. In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object. Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent. Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969â976, Sydney, July 2006. Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts. In this paper we investigate three instances of the BiTAM model, They are data-driven and do not need handcrafted knowledge engineering. The remainder of the paper is as follows: in section 2, we introduce notations and baselines; in section 3, we propose the topic admixture models; in section 4, we present the learning and inference algorithms; and in section 5 we show experiments of our models. We conclude with a brief discussion in section 6.  In statistical machine translation, one typically uses parallel data to identify entities such as âword-pairâ, âsentence-pairâ, and âdocument- pairâ. Formally, we define the following terms1: â¢ A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. â¢ A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.â¢ A document-pair (F, E) refers to two doc uments which are translations of each other. Assuming sentences are one-to-one correspondent, a document-pair has a sequence of N parallel sentence-pairs {(fn, en)}, where (fn, en) is the ntth parallel sentence-pair. â¢ A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}. 2.1 Baseline: IBM Model-1. The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level. The translation lexicon p(f |e) is the key component in this generative process. An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004). We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.  Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: Eâ = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity. In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair. A unique normalized and real-valued vector Î¸, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions. Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments. Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions. Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled. There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels. We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels. J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) Â· p(ei |e). (1) j=1 i=1 1 We follow the notations in (Brown et al., 1993) for. English-French, i.e., e â f , although our models are tested,in this paper, for EnglishChinese. We use the end-user ter minology for source and target languages. In the first BiTAM model, we assume that topics are sampled at the sentence-level. Each document- pair is represented as a random mixture of latent topics. Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I Î² e I a Î± Î¸ z f J B N M Î± Î¸ z a a f J B Î± Î¸ z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs. A node in the graph represents a random variable, and a hexagon denotes a parameter. Un-shaded nodes are hidden variables. All the plates represent replicates. The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair. (a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair. a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic. Given a specific topic-weight vector Î¸d for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics. This generative process, for a document-pair (Fd, Ed), is summarized as below: 1. Sample sentence-number N from a Poisson(Î³).. 2. Sample topic-weight vector Î¸d from a Dirichlet(Î±).. 3. For each sentence-pair (fn , en ) in the dtth doc-pair ,. (a) Sample sentence-length Jn from Poisson(Î´); (b) Sample a topic zdn from a Multinomial(Î¸d ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far. Note that, the sentence-pairs are now connected by the node Î¸d. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector Î¸d. Specifically, BiTAM1 assumes that each sentence-pair has one single topic. Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair. The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B). and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear. For each document-pair, a K -dimensional Dirichlet random variable Î¸d, referred to as the topic-weight vector of the document, can take values in the (K â1)-simplex following a probability density: to the proposed distributions. We simplify alignment model of a, as in IBM1, by assuming that aj is sampled uniformly at random. Given the parameters Î±, B, and the English part E, the joint conditional distribution of the topic-weight vector Î¸, the topic indicators z, the alignment vectors A, and the document F can be written as: Î( K Î±k ) p(Î¸|Î±) = k=1 Î¸Î±1 â1 Â· Â· Â· Î¸Î±K â1 , (3) p(F,A, Î¸, z|E, Î±, B) = k=1 Î(Î±k ) N (4) where the hyperparameter Î± is a K -dimension vector with each component Î±k >0, and Î(x) is the Gamma function. The alignment is represented by a J -dimension vector a = {a1, a2, Â· Â· Â· , aJ }; for each French word fj at the position j, an position variable aj maps it to anEnglish word eaj at the position aj in English sen p(Î¸ | Î±) n p(zn |Î¸)p(fn , an |en , Î±, Bzn), n=1 where N is the number of the sentence-pair. Marginalizing out Î¸ and z, we can obtain the marginal conditional probability of generating F from E for each document-pair: p(F, A|E, Î±, Bzn ) = tence. The word level translation lexicon probabil- r ( (5) ities are topic-specific, and they are parameterized by the matrix B = {Bk }. p(Î¸|Î±) n) p(zn |Î¸)p(fn , an |en , Bzn ) dÎ¸, n=1 zn For simplicity, in our current models we omit the modelings of the sentence-number N and the sentence-length Jn, and focus only on the bilingual translation model. Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model. For simplicity, we assume that the French words fj âs are conditionally independent of each other; the alignment variables aj âs are independent of other variables and are uniformly distributed a priori. Therefore, the distribution for each sentence-pair is: p(fn , an |en , Bzn) = p(fn |en , an , Bzn)p(an |en , Bzn) Jn âNullâ is attached to every target sentence to align the source words which miss their translations. Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ). (6) contains only one word: âNullâ, and the alignment link a is no longer a hidden variable. Thus, the conditional likelihood for the entire parallel corpus is given by taking the product of the marginal probabilities of each individual document-pair in Eqn. 5. 3.2 BiTAM2: Monolingual Admixture. In general, the monolingual model for English can also be a rich topic-mixture. This is realized by using the same topic-weight vector Î¸d and the same topic indicator zdn sampled according to Î¸d, as described in Â§3.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)). Now e is generated  Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and Î¸ is intractable. A variational inference is used to approximate the true posteriors of these hidden variables. The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted. 4.1 Variational Approximation. To approximate: p(Î¸, z, A|E, F, Î±, B), the joint posterior, we use the fully factorized distribution over the same set of hidden variables: q(Î¸,z, A) â q(Î¸|Î³, Î±)Â· from a topic-based language model Î², instead of a N Jn (7) uniform distribution in BiTAM1. We refer to this n q(zn |Ïn ) n q(anj , fnj |Ïnj , en , B), model as BiTAM2. n=1 j=1 Unlike BiTAM1, where the information observed in ei is indirectly passed to z via the node of fj and the hidden variable aj , in BiTAM2, the topics of corresponding English and French sentences are also strictly aligned so that the information observed in ei can be directly passed to z, in the hope of finding more accurate topics. The topics are inferred more directly from the observed bilingual data, and as a result, improve alignment. 3.3 BiTAM3: Word-level Admixture. where the Dirichlet parameter Î³, the multinomial parameters (Ï1, Â· Â· Â· , Ïn), and the parameters (Ïn1, Â· Â· Â· , ÏnJn ) are known as variational param eters, and can be optimized with respect to the KullbackLeibler divergence from q(Â·) to the original p(Â·) via an iterative fixed-point algorithm. It can be shown that the fixed-point equations for the variational parameters in BiTAM1 are as follows: Nd Î³k = Î±k + ) Ïdnk (8) n=1 K It is straightforward to extend the sentence-level BiTAM1 to a word-level admixture model, by Ïdnk â exp (Î¨(Î³k ) â Î¨( Jdn Idn ) kt =1 Î³kt ) Â· sampling topic indicator zn,j for each word-pair (fj , eaj ) in the ntth sentence-pair, rather than once for all (words) in the sentence (Figure 1 (c)). exp ( ) ) Ïdnji log Bf ,e ,k (9) j i j=1 i=1 K ( This gives rise to our BiTAM3. The conditional Ïdnji â exp ) Ïdnk log Bf ,e ,k , (10) k=1 likelihood functions can be obtained by extending where Î¨(Â·) is a digamma function. Note that inthe formulas in Â§3.1 to move the variable zn,j in side the same loop over each of the fn,j . the above formulas Ï dnkis the variational param 3.4 Incorporation of Word âNullâ. Similar to IBM models, âNullâ word is used for the source words which have no translation counterparts in the target language. For example, Chinese words âdeâ (ffl) , âbaâ (I\) and âbeiâ (%i) generally do not have translations in English. eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair. Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters Î± and B in an unsupervised fashion. Essentially, Eqs. (810) above constitute the E-step, where the posterior estimations of the latent variables are obtained. In the M-step, we update Î± and B so that they improve a lower bound of the log-likelihood defined bellow: L(Î³, Ï, Ï; Î±, B) = Eq [log p(Î¸|Î±)]+Eq [log p(z|Î¸)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]âEq [log q(Î¸)] âEq [log q(z)]âEq [log q(a)]. (11) The close-form iterative updating formula B is: BDA selects iteratively, for each f , the best aligned e, such that the word-pair (f, e) is the maximum of both row and column, or its neighbors have more aligned pairs than the other combpeting candidates.A close check of {Ïdnji} in Eqn. 10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k â ) ) ) ) Î´(f, fj )Î´(e, ei )Ïdnk Ïdnji (12) d n=1 j=1 i=1 For Î±, close-form update is not available, and we resort to gradient accent as in (SjoÂ¨ lander et al., 1996) with restarts to ensure each updated Î±k >0. 4.2 Data Sparseness and Smoothing. The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003). To reduce the data sparsity problem, we introduce two remedies in our models. First: Laplace smoothing. In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory. Second: interpolation smoothing. Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = Î»Bf,e,k +(1âÎ»)p(f |e). (13) As in Eqn. 1, p(f |e) is learned via IBM1; Î» is estimated via EM on held out data. 4.3 Retrieving Word Alignments. Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA). Both use the posterior mean of the alignment indicators adnji, captured by what we call the poste rior alignment matrix Ï â¡ {Ïdnji}. UDA uses a French word fdnj (at the jtth position of ntth sentence in the dtth document) to query Ï to get the best aligned English word (by taking the maximum point in a row of Ï): adnj = arg max Ïdnji . (14) iâ[1,Idn ] iconâs strength.  We evaluate BiTAM models on the word alignment accuracy and the translation quality. For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu (Papineni et al., 2002) and its variation of NIST scores are reported. Table 1: Training and Test Data Statistics Tra in #D oc. #S ent . #T ok en s En gli sh Ch ine se Tr ee b a n k F B IS . B J Si n or a m a Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes t 95 62 7 25, 50 0 19, 72 6 We have two training data settings with different sizes (see Table 1). The small one consists of 316 document-pairs from Tree- bank (LDC2002E17). For the large training data setting, we collected additional document- pairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document boundaries are kept in our sentence-aligner (Zhao and Vogel, 2002)). There are 27,940 document-pairs, containing 327K sentence-pairs or 12 million (12M) English tokens and 11M Chinese tokens. To evaluate word alignment, we hand-labeled 627 sentence-pairs from 95 document-pairs sampled from TIDESâ01 dryrun data. It contains 14,769 alignment-links. To evaluate translation quality, TIDESâ02 Eval. test is used as development set, and TIDESâ03 Eval. test is used as the unseen test data. 5.1 Model Settings. First, we explore the effects of Null word and smoothing strategies. Empirically, we find that adding âNullâ word is always beneficial to all models regardless of number of topics selected. To pics Le xic ons To pic1 To pic2 To pic3 Co oc. IBM 1 H M M IBM 4 p( Ch ao Xi an (Ji! $) |K ore an) 0. 06 12 0. 21 38 0. 22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li! ï¿½ )|K ore an) 0. 83 79 0. 61 16 0. 02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1. The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean). The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!ï¿½:South Korean). The two candidate translations may both fade out in the learned translation lexicons. Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h i n a u . s . dev elop men t trad e ente rpri ses tech nolo gy cou ntri es y e a r eco nom ic Topi c B. cho ngqi ng com pani es take over s co m pa ny cit y bi lli o n m o r e eco nom ic re a c h e d y u a n Topi c C. sp or ts dis abl ed te a m p e o p l e caus e w at e r na tio na l ga m es han dica ppe d me mb ers Table 3: Three most distinctive topics are displayed. The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {Ïdnk }. 33 functional words were removed to highlight the main content of each topic. Topic A is about Us-China economic relationships; Topic B relates to Chinese companiesâ merging; Topic C shows the sports of handicapped people.The interpolation smoothing in Â§4.2 is effec tive, and it gives slightly better performance than Laplace smoothing over different number of topics for BiTAM1. However, the interpolation leverages the competing baseline lexicon, and this can blur the evaluations of BiTAMâs contributions. Laplace smoothing is chosen to emphasize more on BiTAMâs strength. Without any smoothing, F- measure drops very quickly over two topics. In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models. We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish. Choosing the number of topics is a model selection problem. We performed a tenfold cross- validation, and a setting of three-topic is chosen for both the small and the large training data sets. The overall computation complexity of the BiTAM is linear to the number of hidden topics. 5.2 Variational Inference. Under a non-symmetric Dirichlet prior, hyperparameter Î± is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1. Better initialization of B can help to avoid local optimal as shown in Â§ 5.5. With the learned B and Î± fixed, the variational parameters to be computed in Eqn. (810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10â5. The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs. To estimate B, Î² (for BiTAM2) and Î±, at most eight variational EM iterations are run on the training data. Figure 2 shows absolute 2â¼3% better F-measure over iterations of variational EM using two and three topics of BiTAM1 comparing with IBM1. BiTam with Null and Laplace Smoothing Over Var. EM Iterations 41 40 39 38 37 36 35 BiTamâ1, Topic #=3 34 BiTamâ1, Topic #=2. IB M â1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number of EM/Variational EM Iterations for IBMâ1 and BiTamâ1 Figure 2: performances over eight Variational EM iterations of BiTAM1 using both the âNullâ word and the laplace smoothing; IBM1 is shown over eight EM iterations for comparison. 5.3 Topic-Specific Translation. Lexicons The topic-specific lexicons Bk are smaller in size than IBM1, and, typically, they contain topic trends. For example, in our training data, North Korean is usually related to politics and translated into âChaoXianâ (Ji! $); South Korean occurs more often with economics and is translated as âHanGuoâ(li! ï¿½). BiTAMs discriminate the two by considering the topics of the context. Table 2 shows the lexicon entries for âKoreanâ learned by a 3-topic BiTAM1. The values are relatively sharper, and each clearly favors one of the candidates. The co-occurrence count, however, only favors âHanGuoâ, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context. Monolingual topics learned by BiTAMs are, roughly speaking, fuzzy especially when the number of topics is small. With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3. 5.4 Evaluating Word. Alignments We evaluate word alignment accuracies in various settings. Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE). Additional heuristics are applied to further improve the accuracies. Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6. 45 8 15 .7 0 6. 82 2 17 .7 0 6. 92 6 18 .2 5 6. 93 7 6.954 17 .93 18.14 6. 90 4 6.976 18 .13 18.05 6. 96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1. For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality. Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments. As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1â¼3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1. A close look at the three BiTAMs does not yield significant difference. BiTAM3 is slightly better in most settings; BiTAM1 is slightly worse than the other two, because the topics sampled at the sentence level are not very concentrated. The BDA align ments of BiTAM1â¼3 yield 48.26%, 48.63% and 49.02%, which are even better than HMM and IBM4 â their best performances are at 44.26% and 45.96%, respectively. This is because BDA partially utilizes similar heuristics on the approximated posterior matrix {Ïdnji} instead of di rect operations on alignments of two directions in the heuristics of Refined. Practically, we also apply BDA together with heuristics for IBM1, HMM and IBM4, and the best achieved performances are at 40.56%, 46.52% and 49.18%, respectively. Overall, BiTAM models achieve performances close to or higher than HMM, using only a very simple IBM1 style alignment model. Similar improvements over IBM models and HMM are preserved after applying the three kinds of heuristics in the above. As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic. We also test BiTAM3 on large training data, and similar improvements are observed over those of the baseline models (see Table. 5). 5.5 Boosting BiTAM Models. The translation lexicons of Bf,e,k are initialized uniformly in our previous experiments. Better ini tializations can potentially lead to better performances because it can help to avoid the undesirable local optima in variational EM iterations. We use the lexicons from IBM Model-4 to initialize Bf,e,k to boost the BiTAM models. This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement. The boosted alignments are denoted as BUDA and BBDA in Table. 5, corresponding to the uni-direction and bi-direction alignments, respectively. We see an improvement in alignment quality. 5.6 Evaluating Translations. To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities. Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones. We use TIDES Evalâ02 CE test set as development data to tune the decoder parameters; the Evalâ03 data (919 sentences) is the unseen data. A trigram language model is built using 180 million English words. Across all the reported comparative settings, the key difference is the bilingual ngram-identity of the phrase-pair, which is collected directly from the underlying word alignment. Shown in Table 4 are results for the small- data track; the large-data track results are in Table 5. For the small-data track, the baseline Bleu scores for IBM1, HMM and IBM4 are 15.70, 17.70 and 18.25, respectively. The UDA alignment of BiTAM1 gives an improvement over the baseline IBM1 from 15.70 to 17.93, and it is close to HMMâs performance, even though BiTAM doesnât exploit any sequential structures of words. The proposed BiTAM2 and BiTAM 3 are slightly better than BiTAM1. Similar improvements are observed for the large-data track (see Table 5). Note that, the boosted BiTAM3 us SE T TI N G IBM 1 H M M IBM 4 B I T A M 3 U D A BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E D ( % ) U N I O N ( % ) I N T E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N I S T B L E U 7. 5 9 19 .1 9 7. 7 7 21 .9 9 7. 8 3 23 .1 8 7. 64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models, HMMs, and boosted BiTAMs using all the training data listed in Table. 1. Other experimental conditions are similar to Table. 4. ing IBM4 as the seed lexicon, outperform the Refined IBM4: from 23.18 to 24.07 on Bleu score, and from 7.83 to 8.23 on NIST. This result suggests a straightforward way to leverage BiTAMs to improve statistical machine translations.  In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models. Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models. The proposed models significantly improve the alignment accuracy and lead to better translation qualities. Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.
 Improved Word-Level System Combination for Machine Translation  Recently, confusion network decoding has been applied in machine translation system combination. Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs. This paper describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.  System combination has been shown to improve classification performance in various tasks. There are several approaches for combining classifiers. In ensemble learning, a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting (Schapire, 1990). Another approach is to combine outputs from a few highly specialized classifiers. The classifiers may 312 be based on the same basic modeling techniques but differ by, for example, alternative feature representations. Combination of speech recognition outputs is an example of this approach (Fiscus, 1997). In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312â319, Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size. Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005). A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering. Confusion networks are generated by choosing one hypothesis as the âskeletonâ, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007). The average TER score was computed between each systemâs -best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER. This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the -best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors. Also the word-level decoding may break coherent phrases produced by the individual systems. In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences. This allows a log-linear addition of arbitrary features such as language model (LM) scores. The LM scores should increase the total log-posterior of more grammatical hypotheses. Powellâs method (Brent, 1973) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set. Tuning is fully automatic, as opposed to (Matusov et al., 2006) where global system weights were set manually.This paper is organized as follows. Three evalu ation metrics used in weights tuning and reporting the test set results are reviewed in Section 2. Section 3 describes confusion network decoding for MT system combination. The extensions to add features log-linearly and improve the skeleton selection are presented in Sections 4 and 5, respectively. Section 6 details the weights optimization algorithm and the experimental results are reported in Section 7. Conclusions and future work are discussed in Section 8.  Currently, the most widely used automatic MT evaluation metric is the NIST BLEU4 (Papineni et al., 2002). It is computed as the geometric mean of - gram precisions up to -grams between the hypothesis and reference as follows (1) where is the brevity penalty and are the -gram precisions. When mul tiple references are provided, the -gram counts against all references are accumulated to compute the precisions. Similarly, full test set scores are obtained by accumulating counts over all hypothesis and reference pairs. The BLEU scores are between and , higher being better. Often BLEU scores are reported as percentages and âone BLEU point gainâ usually means a BLEU increase of . Other evaluation metrics have been proposed to replace BLEU. It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision (Banerjee and Lavie, 2005). METEOR is based on the weighted harmonic mean of the precision and recall measured on uni- gram matches as follows (2) where is the total number of unigram matches, is the hypothesis length, is the reference length and is the minimum number of -gram matches that covers the alignment. The second term is a fragmentation penalty which penalizes the harmonic mean by a factor of up to when ; i.e., there are no matching -grams higher than . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer. Additional matching modules including WordNet stemming and synonymy may also be used. When multiple references are provided, the lowest score is reported. Full test set scores are obtained by accumulating statistics over all test sentences. The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate (TER) (Snover et al., 2006)has been proposed as more intuitive evaluation met 1. Each arc represents an alternative word at that. position in the sentence and the number of votes for each word is marked in parentheses. Confusion network decoding usually requires finding the path with the highest confidence in the network. Based on vote counts, there are three alternatives in the example: âcat sat on the matâ, âcat on the matâ and âcat sitting on the matâ, each having accumulated 10 votes. The alignment procedure plays an important role, as by switching the position of the word âsatâ and the following NULL in the skeleton, there would be a single highest scoring path through the network; that is, âcat on the matâ. ric since it is based on the rate of edits required to transform the hypothesis into the reference. The cat (2) sat (1) on (2) the (2) mat (3) TER score is computed as follows 1 2 3 4 5 6 (3) hat (1) sitting (1) a (1) where is the reference length. The only difference to word error rate is that the TER allows shifts. A shift of a sequence of words is counted as a single edit. The minimum translation edit alignment is usually found through a beam search. When multiple references are provided, the edits from the closest reference are divided by the average reference length. Full test set scores are obtained by accumulating the edits and the average reference lengths. The perfect TER score is 0, and otherwise higher than zero. The TER score may also be higher than 1 due to insertions. Also TER is reported as a percentage in the results section.  Confusion network decoding in MT has to pick one hypothesis as the skeleton which determines the word order of the combination. The other hypotheses are aligned against the skeleton. Either votes or some form of confidences are assigned to each word in the network. For example using âcat sat the matâ as the skeleton, aligning âcat sitting on the matâ and âhat on a matâ against it might yield the following alignments: cat sat the mat cat sitting on the mat hat on a mat where represents a NULL word. In graphical form, the resulting confusion network is shown in Figure Figure 1: Example consensus network with votes on word arcs. Different alignment methods yield different confusion networks. The modified Levenshtein alignment as used in TER is more natural than simple edit distance such as word error rate since machine translation hypotheses may have different word orders while having the same meaning. As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton. Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score. The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems. This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the th- best hypothesis. Due to the computational burden of the TER alignment, only -best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work. System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systemsâ relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 2007), the total confidence of the th best confusion network hypothesis , including NULL words, given the th source sentence was given by (5) word-level decoding. For example, two synonymous words may be aligned to other words not already aligned, which may result in repetitive output. Second, the additive confidence scores in Equation 5 have no probabilistic meaning and cannot therefore be combined with language model scores. Language model expansion and re-scoring may help by increasing the probability of more grammatical hypotheses in decoding. Third, the system weights are independent of the skeleton selection. Therefore, a hypothesis from a system with a low or zero weight may be chosen as the skeleton.  Features To address the issue with ungrammatical hypotheses and allow language model expansion and re-scoring, the hypothesis confidence computation is modified. Instead of summing arbitrary confidence scores as in Equation 5, word posterior probabilities are used as follows (6) where is the number of nodes in the confusion network for the source sentence , is the number of translation systems, is the th system weight, is the accumulated confidence for word produced by system between nodes and , and is a weight for the number of NULL links along the hypothesis . The word confidences were increased by if the word aligns between nodes and in the network. If no word aligns between nodes and , the NULL word confidence at that position was increased by . The last term controls the number of NULL words generated in the output and may be viewed as an insertion penalty. Each arc in the confusion network carries the word label and scores . The decoder outputs the hypothesis with the highest given the current set of weights. 3.1 Discussion. There are several problems with the previous confusion network decoding approaches. First, the decoding can generate ungrammatical hypotheses due to alignment errors and phrases broken by the where is the language model weight, is the LM log-probability and is the number of words in the hypothesis . The word posteriors are estimated by scaling the confidences to sum to one for each system over all words in between nodes and . The system weights are also constrained to sum to one. Equation 6 may be viewed as a log-linear sum of sentence- level features. The first feature is the sum of word log-posteriors, the second is the LM log-probability, the third is the log-NULL score and the last is the log-length score. The last two terms are not completely independent but seem to help based on experimental results. The number of paths through a confusion network grows exponentially with the number of nodes. Therefore expanding a network with an -gram language model may result in huge lattices if is high. Instead of high order -grams with heavy pruning, a bi-gram may first be used to expand the lattice. After optimizing one set of weights for the expanded confusion network, a second set of weights for - best list re-scoring with a higher order -gram model may be optimized. On a test set, the first set of weights is used to generate an -best list from the bi-gram expanded lattice. This -best list is then re-scored with the higher order -gram. The second set of weights is used to find the final -best from the re-scored -best list.  As discussed in Section 3, there is a disconnect between the skeleton selection and confidence estimation. To prevent the -best from a system with a low or zero weight being selected as the skeleton, confusion networks are generated for each system and the average TER score in Equation 4 is used to estimate a prior probability for the corresponding network. All confusion networks are connected to a single start node with NULL arcs which contain the prior probability from the system used as the skeleton for that network. All confusion network are connected to a common end node with NULL arcs. The final arcs have a probability of one. The prior probabilities in the arcs leaving the first node will be multiplied by the corresponding system weights which guarantees that a path through a network generated around a -best from a system with a zero weight will not be chosen. The prior probabilities are estimated by viewing the negative average TER scores between the skeleton and other hypotheses as log-probabilities. These log-probabilities are scaled so that the priors sum to one. There is a concern that the prior probabilities estimated this way may be inaccurate. Therefore, the priors may have to be smoothed by a tunable exponent. However, the optimization experiments showed that the best performance was obtained by having a smoothing factor of 1 which is equivalent to the original priors. Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors , and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence 1 Na. Figure 2: Three confusion networks with prior probabilities. specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights.  The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991). A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be reordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows (7) The objective is to optimize the -best score on a development set given a set of reference translations. For example, estimating weights which minimize TER between a set of -best hypothesis and reference translations can be written as (8) This objective function is very complicated, so gradient-based optimization methods may not be used. In this work, modified Powellâs method as proposed by (Brent, 1973) is used. The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search. To improve the chances of finding a global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs. Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new -best list from the lattice for the next iteration. Similarly, weights which maximize BLEU or METEOR may be optimized. The same Powellâs method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in (Och, 2003). A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used.  The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrase- based similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task. The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration. The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks. All four reference translations available for the tuning and test sets were used. A first set of weights with the bi- gram LM was optimized with three iterations. A second set of weights was tuned for 5-gram -best list re-scoring. The bi-gram and 5-gram English language models were trained on about 7 billion words. The final combination outputs were detokenized and cased before scoring. The tuning set results on the Arabic to English NIST MT03+MT04 task are shown in Table 1. The Ar ab ic tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 44 .9 3 46 .4 1 46 .1 0 44 .3 6 45 .3 5 47 .1 0 45 .7 1 43 .0 7 46 .4 1 46 .8 3 45 .4 4 44 .5 2 66 .0 9 64 .7 9 65 .3 3 66 .9 1 65 .6 9 65 .2 8 no we ig hts ba sel in e 42 .3 5 42 .1 9 48 .9 1 49 .8 6 67 .7 6 68 .3 4 T E R t u n e d B L E U t u n e d M T R t u n e d 41 .8 8 42 .1 2 54 .0 8 51 .4 5 51 .7 2 38 .9 3 68 .6 2 68 .5 9 71 .4 2 Table 1: Mixed-case TER and BLEU, and lowercase METEOR scores on Arabic NIST MT03+MT04. Ar ab ic tes t T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 42 .9 8 43 .7 9 43 .9 2 40 .7 5 42 .1 9 44 .3 0 49 .5 8 47 .0 6 47 .8 7 52 .0 9 50 .8 6 50 .1 5 69 .8 6 68 .6 2 66 .9 7 71 .2 3 70 .0 2 69 .7 5 no we ig hts ba sel in e 39 .3 3 39 .2 9 53 .6 6 54 .5 1 71 .6 1 72 .2 0 T E R t u n e d B L E U t u n e d M T R t u n e d 39 .1 0 39 .1 3 51 .5 6 55 .3 0 55 .4 8 41 .7 3 72 .5 3 72 .8 1 74 .7 9 Table 2: Mixed-case TER and BLEU, and lowercase METEOR scores on Arabic NIST MT05. best score on each metric is shown in bold face fonts. The row labeled as no weights corresponds to Equation 5 with uniform system weights and zero NULL weight. The baseline corresponds to Equation 5 with TER tuned weights. The following three rows correspond to the improved confusion network decoding with different optimization metrics. As expected, the scores on the metric used in tuning are the best on that metric. Also, the combination results are better than any single system on all metrics in the case of TER and BLEU tuning. However, the METEOR tuning yields extremely high TER and low BLEU scores. This must be due to the higher weight on the recall compared to precision in the harmonic mean used to compute the METEOR Ch in es e tu ni ng T E R B L E U M T R s y s t e m A s y s t e m B s y s t e m C s y s t e m D s y s t e m E s y s t e m F 56 .5 6 55 .8 8 58 .3 5 57 .0 9 57 .6 9 56 .1 1 29 .3 9 30 .4 5 32 .8 8 36 .1 8 33 .8 5 36 .6 4 54 .5 4 54 .3 6 56 .7 2 57 .1 1 58 .2 8 58 .9 0 no we ig hts ba sel in e 53 .1 1 53 .4 0 37 .7 7 38 .5 2 59 .1 9 59 .5 6 T E R t u n e d B L E U t u n e d M T R t u n e d 52 .1 3 53 .0 3 70 .2 7 36 .8 7 39 .9 9 28 .6 0 57 .3 0 58 .9 7 63 .1 0 Table 3: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT03+MT04. score. Even though METEOR has been shown to be a good metric on a given MT output, tuning to optimize METEOR results in a high insertion rate and low precision. The Arabic test set results are shown in Table 2. The TER and BLEU optimized combination results beat all single system scores on all metrics. The best results on a given metric are again obtained by the combination optimized for the corresponding metric. It should be noted that the TER optimized combination has significantly higher BLEU score than the TER optimized baseline. Compared to the baseline system which is also optimized for TER, the BLEU score is improved by 0.97 points. Also, the METEOR score using the METEOR optimized weights is very high. However, the other scores are worse in common with the tuning set results. The tuning set results on the Chinese to English NIST MT03+MT04 task are shown in Table 3. The baseline combination weights were tuned to optimize BLEU. Again, the best scores on each metric are obtained by the combination tuned for that metric. Only the METEOR score of the TER tuned combination is worse than the METEOR scores of systems E and F - other combinations are better than any single system on all metrics apart from the METEOR tuned combinations. The test set results follow clearly the tuning results again - the TER tuned combination is the best in terms of TER, the BLEU tuned in terms of BLEU, and the METEOR tuned in Table 4: Mixed-case TER and BLEU, and lowercase METEOR scores on Chinese NIST MT05. terms of METEOR. Compared to the baseline, the BLEU score of the BLEU tuned combination is improved by 1.47 points. Again, the METEOR tuned weights hurt the other metrics significantly.  An improved confusion network decoding method combining the word posteriors with arbitrary features was presented. This allows the addition of language model scores by expanding the lattices or re-scoring -best lists. The LM integration should result in more grammatical combination outputs. Also, confusion networks generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese. It also seems like METEOR should not be used in tuning due to high insertion rate and low precision. It would be interesting to know which tuning metric results in the best translations in terms of human judgment. However, this would require time consuming evaluations such as human mediated TER post-editing (Snover et al., 2006). The improved confusion network decoding approach allows arbitrary features to be used in the combination. New features may be added in the future. Hypothesis alignment is also very important in confusion network generation. Better alignment methods which take synonymy into account should be investigated. This method could also benefit from more sophisticated word posterior estimation.  This work was supported by DARPA/IPTO Contract No. HR001106-C-0022 under the GALE program (approved for public release, distribution unlimited). The authors would like to thank ISI and University of Edinburgh for sharing their MT system outputs.
 Investigating regular sense extensions based on intersective Levin classes  In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We see verb classes as the key to making genÂ­ eralizations about regular extensions of meanÂ­ ing. Current approaches to English classificaÂ­ tion, Levin classes and WordNet, have limitaÂ­ tions in their applicability that impede their utility as general classification schemes. We present a refinement of Levin classes, intersecÂ­ tive sets, which are a more fine-grained clasÂ­ sification and have more coherent sets of synÂ­ tactic frames and associated semantic compoÂ­ nents. We have preliminary indications that the membership of our intersective sets will be more compatible with WordNet than the origÂ­ inal Levin classes. We also have begun to exÂ­ amine related classes in Portuguese, and find that these verbs demonstrate similarly coherent syntactic and semantic properties.  The difficulty of achieving adequate handÂ­ crafted semantic representations has limited the field of natural language processing to applicaÂ­ tions that can be contained within well-defined subdomains. The only escape from this limÂ­ itation will be through the use of automated or semi-automated methods of lexical acquisiÂ­ tion. However, the field has yet to develop a clear consensus on guidelines for a computaÂ­ tional lexicon that could provide a springboard for such methods, although attempts are being made (Pustejovsky, 1991), (Copestake and SanÂ­ filippo, 1993), (Lowe et al., 1997), (Dorr, 1997). The authors would like to acknowledge the supÂ­ port of DARPA grant N6600194C-6043, ARO grant DAAH0494G-0426, and CAPES grant 0914/952. One of the most controversial areas has to do with polysemy. What constitutes a clear sepaÂ­ ration into senses for any one verb, and how can these senses be computationally characterized and distinguished? The answer to this question is the key to breaking the bottleneck of semantic representation that is currently the single greatÂ­ est limitation on the general application of natÂ­ ural language processing techniques. In this paper we specifically address questions of polysemy with respect to verbs, and how regular extensions of meaning can be achieved through the adjunction of particular syntactic phrases. We base these regular extensions on a fine-grained variation on Levin classes, interÂ­ sective Levin classes, as a source of semantic components associated with specific adjuncts. We also examine similar classes in Portuguese, and the predictive powers of alternations in this language with respect to the same semantic components. The difficulty of determining a suitable lexical representation becomes multiÂ­ plied when more than one language is involved and attempts are made to map between them. Preliminary investigations have indicated that a straightforward translation of Levin classes into other languages is not feasible (Jones et al., 1994), (Nomura et al., 1994), (Saint-Dizier, 1996). However, we have found interesting parÂ­ allels in how Portuguese and English treat regÂ­ ular sense extensions.  Two current approaches to English verb classiÂ­ fications are WordNet (Miller et al., 1990) and Levin classes (Levin, 1993). WordNet is an onÂ­ line lexical database of English that currently contains approximately 120,000 sets of noun, verb, adjective, and adverb synonyms, each repÂ­ resenting a lexicalized concept. A synset (syn onym set) contains, besides all the word forms that can refer to a given concept, a definitional gloss and - in most cases - an example sentence. Words and synsets are interrelated by means of lexical and semantic-conceptual links, respecÂ­ tively. Antonymy or semantic opposition links individual words, while the super-/subordinate relation links entire synsets. WordNet was deÂ­ signed principally as a semantic network, and contains little syntactic information. Levin verb classes are based on the ability of a verb to occur or not occur in pairs of syntacÂ­ tic frames that are in some sense meaning preÂ­ serving (diathesis alternations) (Levin, 1993). The distribution of syntactic frames in which a verb can appear determines its class memberÂ­ ship. The fundamental assumption is that the syntactic frames are a direct reflection of the unÂ­ derlying semantics. Levin classes are supposed to provide specific sets of syntactic frames that are associated with the individual classes. The sets of syntactic frames associated with a particular Levin class are not intended to be arbitrary, and they are supposed to reflect unÂ­ derlying semantic components that constrain alÂ­ lowable arguments. For example, break verbs and cut verbs are similar in that they can all participate in the transitive and in the midÂ­ dle construction, John broke the window, Glass breaks easily, John cut the bread, This loaf cuts easily. However, only break verbs can also occur in the simple intransitive, The window broke, *The bread cut. In addition, cut verbs can ocÂ­ cur in the conative, John valiantly cut/hacked at the frozen loaf, but his knife was too dull to make a dent in it, whereas break verbs cannot, *John broke at the window. The explanation given is that cut describes a series of actions diÂ­ rected at achieving the goal of separating some object into pieces. It is possible for these acÂ­ tions to be performed without the end result being achieved, but where the cutting manner can still be recognized, i.e., John cut at the loaf. Where break is concerned, the only thing speciÂ­ fied is the resulting change of state where the object becomes separated into pieces. If the result is not achieved, there are no attempted breaking actions that can still be recognized. 2.1 Ambiguities in Levin classes. It is not clear how much WordNet synsets should be expected to overlap with Levin classes, and preliminary indications are that there is a wide discrepancy (Dorr and Jones, 1996), (Jones and Onyshkevych, 1997), (Dorr, 1997). However, it would be useful for the WordNet senses to have access to the detailed syntactic information that the Levin classes contain, and it would be equally useful to have more guidance as to when membership in a Levin class does in fact indicate shared semanÂ­ tic components. Of course, some Levin classes, such as braid (bob, braid, brush, clip, coldcream, comb, condition, crimp, crop, curl, etc.) are clearly not intended to be synonymous, which at least partly explains the lack of overlap beÂ­ tween Levin and WordNet. The association of sets of syntactic frames with individual verbs in each class is not as straightforward as one might suppose. For inÂ­ stance, carry verbs are described as not taking the conative, *The mother carried at the baby, and yet many of the verbs in the carry class {push, pull, tug, shove, kick) are also listed in the push/pull class, which does take the conaÂ­ tive. This listing of a verb in more than one class (many verbs are in three or even four classes) is left open to interpretation in Levin. Does it indicate that more than one sense of the verb is involved, or is one sense primary, and the alternations for that class should take precedence over the alternations for the other classes in which the verb is listed? The grounds for deciding that a verb belongs in a particular class because of the alternations that it does not take are elusive at best.  We augmented the existing database of Levin semantic classes with a set of intersective classes, which were created by grouping toÂ­ gether subsets of existing classes with overÂ­ lapping members. All subsets were included which shared a minimum of three members. If only one or two verbs were shared between two classes, we assumed this might be due to hoÂ­ mophony, an idiosyncrasy involving individual verbs rather than a systematic relationship inÂ­ volving coherent sets of verbs. This filter alÂ­ lowed us to reject the potential intersective class that would have resulted from combining the reÂ­ move verbs with the scribble verbs, for example. The sole member of this intersection is the verb draw. On the other hand, the scribble verbs do form an intersective class with the perforÂ­ mance verbs, since paint and write are also in both classes, in addition to draw. The algorithm we used is given in Figure 1. 1. Enumerate all sets S = {c1, ... , Cn} of se-. mantic classes such that let n ... n enI e, where e is a relevance cutoff. 2. For each such S = {ct, ... ,en}, define an. intersective class Is such that a verb v E Is iff v E c1 n ... n en, and there is no S' = {d1, ..â¢ ,c} such that S C S' and v E ci n ... n dm (subset criterion). Figure 1: Algorithm for identifying relevant semantic-class intersections We then reclassified the verbs in the database as follows. A verb was assigned membership in an intersective class if it was listed in each of the existing classes that were combined to form the new intersective class. Simultaneously, the verb was removed from the membership lists of those existing classes. 3.1 Using intersective Levin classes to. isolate semantic components Some of the large Levin classes comprise verbs that exhibit a wide range of possible semantic components, and could be divided into smaller subclasses. The split verbs (cut, draw, kick, knock, push, rip, roll, shove, slip, split, etc.) do not obviously form a homogeneous semanÂ­ tic class. Instead, in their use as split verbs, each verb manifests an extended sense that can be paraphrased as "separate by V-ing," where "V" is the basic meaning of that verb (Levin, 1993). Many of the verbs (e.g., draw, pull, push, shove, tug, yank) that do not have an inherent semantic component of "separating" belong to this class because of the component of force in their meaning. They are interpretable as verbs of splitting or separating only in particular synÂ­ tactic frames (I pulled the twig and the branch apart, I pulled the twig off {of) the branch, but not *I pulled the twig and the branch). The adÂ­ junction of the apart adverb adds a change of state semantic component with respect to the object which is not present otherwise. These fringe split verbs appear in several other interÂ­ sective classes that highlight the force aspect of their meaning. Figure 2 depicts the intersection of split, carry and push/pull. Figure 2: Intersective class formed from Levin carry, push/pull and split verbs- verbs in() are not listed by Levin in all the intersecting classes but participate in all the alternations The intersection between the push/pull verbs of exerting force, the carry verbs and the split verbs illustrates how the force semantic compoÂ­ nent of a verb can also be used to extend its meaning so that one can infer a causation of accompanied motion. Depending on the parÂ­ ticular syntactic frame in which they appear, members of this intersective class (pull, push, shove, tug, kick, draw, yank) * can be used to exemplify any one (or more) of the the compoÂ­ nent Levin classes. 1. Nora pushed the package to Pamela.. (carry verb implies causation of accompaÂ­ nied motion, no separation) 2. Nora pushed at/against the package.. â¢ Although kick is not listed as a verb of exerting force, it displays all the alternations that define this class. SimÂ­ ilarly, draw and yank can be viewed as carry verbs alÂ­ though they are not listed as such. The list of members for each Levin verb class is not always complete, so to check if a particular verb belongs to a class it is better to check that the verb exhibits all the alternations that deÂ­ fine the class. Since intersective classes were built using membership lists rather than the set of defining alternaÂ­ tions, they were similarly incomplete. This is an obvious shortcoming of the current implementation of intersecÂ­ tive classes, and might affect the choice of 3 as a relevance cutoff in later implementations. (verb of exerting force, no separation or causation of accompanied motion implied) 3. Nora pushed the branches apart.. (split verb implies separation, no causation of accompanied motion)  {verb of exerting force; no separation imÂ­ plied, but causation of accompanied motion possible) 5. *Nora pushed at the package to Pamela. Although the Levin classes that make up an intersective class may have conflicting alternaÂ­ tions {e.g., verbs of exerting force can take the conative alternation, while carry verbs cannot), this does not invalidate the semantic regularity of the intersective class. As a verb of exerting force, push can appear in the conative alternaÂ­ tion, which emphasizes its force semantic comÂ­ ponent and ability to express an "attempted" action where any result that might be associÂ­ ated- with the verb (e.g., motion) is not necÂ­ essarily achieved; as a carry verb (used with a goal or directional phrase), push cannot take the conative alternation, which would conflict with the core meaning of the carry verb class (i.e., causation of motion). The critical point is that, while the verb's meaning can be extended to either "attempted" action or directed motion, these two extensions cannot co-occur - they are mutually exclusive. However the simultaneous potential of mutually exclusive extensions is not a problem. It is exactly those verbs that are triple-listed in the split/push/carry intersective class (which have force exertion as a semantic component) that can take the conative. The carry verbs that are not in the intersective class (carry, drag, haul, heft, hoist, lug, tote, tow) are more "pure" examples of the carry class and always imply the achievement of causation of motion. Thus they cannot take the conative alÂ­ ternation. 3.2 Comparisons to WordNet. Even though the Levin verb classes are defined by their syntactic behavior, many reflect semanÂ­ tic distinctions made by WordNet, a classificaÂ­ tion hierarchy defined in terms of purely seÂ­ mantic word relations (synonyms, hypernyms, etc.). When examining in detail the intersecÂ­ tive classes just described, which emphasize not only the individual classes, but also their relaÂ­ tion to other classes, we see a rich semantic latÂ­ tice much like WordNet. This is exemplified by the Levin cut verbs and the intersective class formed by the cut verbs and split verbs. The original intersective class (cut, hack, hew, saw) exhibits alternations of both parent classes, and has been augmented with chip, clip, slash, snip since these cut verbs also display the syntactic properties of split verbs. WordNet distinguishes two subclasses of cut, differentiated by the type of result: 1. Manner of cutting that results in separaÂ­. tion into pieces (chip, clip, cut, hack, hew, saw, slash, snip), having cut, separate with an instrument as an immediate hypernym. 2. Manner of cutting that doesn't separate. completely (scrape, scratch), having cut into, incise as an immediate hypernym, which in turn has cut, separate with an inÂ­ strument as an immediate hypernym. This distinction appears in the second-order Levin classes as membership vs. nonmemberÂ­ ship in the intersective class with split. Levin verb classes are based on an underlying latÂ­ tice of partial semantic descriptions, which are manifested indirectly in diathesis alternations. Whereas high level semantic relations (synÂ­ onym, hypernym) are represented directly in WordNet, they can sometimes be inferred from the intersection between Levin verb classes, as with the cut/split class. However, other intersective classes, such as the split/push/carry class, are no more conÂ­ sistent with WordNet than the original Levin classes. The most specific hypernym common to all the verbs in this intersective class is move, displace, which is also a hypernym for other carry verbs not in the intersection. In addition, only one verb (pull) has a WordNet sense corÂ­ responding to the change of state - separation semantic component associated with the split class. The fact that the split sense for these verbs does not appear explicitly in WordNet is not surprising since it is only an extended sense of the verbs, and separation is inferred only when the verb occurs with an appropriate adjunct, such as apart. However, apart can also be used with other classes of verbs, including many verbs of motion. To explicitly list separa tion as a possible sense for all these verbs would be extravagant when this sense can be generÂ­ ated from the combination of the adjunct with the force (potential cause of change of physical state) or motion (itself a special kind of change of state, i.e., of position) semantic component of the verb. WordNet does not currently provide a consistent treatment of regular sense extenÂ­ sion (some are listed as separate senses, others are not mentioned at all). It would be straightÂ­ forward to augment it with pointers indicating which senses are basic to a class of verbs and which can be generated automatically, and inÂ­ clude corresponding syntactic information. 3.3 Sense extension for manner of. motion Figure 3 shows intersective classes involving two classes of verbs of manner of motion (run and roll verbs) and a class of verbs of existence (meÂ­ ander verbs). Roll and run verbs have semanÂ­ tic components describing a manner of motion that typically, though not necessarily, involves change of location. In the absence of a goal or path adjunct they do not specify any direction of motion, and in some cases (e.g., float, bounce) require the adjunct to explicitly specify any disÂ­ placement at all. The two classes differ in that roll verbs relate to manners of motion characÂ­ teristic of inanimate entities, while run verbs describe manners in which animate entities can move. Some manner of motion verbs allow a transitive alternation in addition to the basic inÂ­ transitive. When a roll verb occurs in the tranÂ­ sitive (Bill moved the box across the room), the subject physically causes the object to move, whereas the subject of a transitive run verb merely induces the object to move (the coach ran the athlete around the track). Some verbs can be used to describe motion of both animate and inanimate objects, and thus appear in both roll and run verb classes. The slide class partiÂ­ tions this roll/run intersection into verbs that can take the transitive alternation and verbs that cannot (drift and glide cannot be causative, because they are not typically externally conÂ­ trollable). Verbs in the slide/roll/run intersecÂ­ tion are also allowed to appear in the dative alternation (Carla slid the book to Dale, Carla slid Dale the book), in which the sense of change of location is extended to change of possession.When used intransitively with a path prepo sitional phrase, some of the manner of motion verbs can take on a sense of pseudo-motional existence, in which the subject does not actuÂ­ ally move, but has a shape that could describe a path for the verb (e.g., The stream twists through the valley). These verbs are listed in the intersective classes with meander verbs of existence. "Meander Verbs" Figure 3: Intersections between roll and run verbs of motion and meander verbs of existence 4 Cross-linguistic verb classes. The Portuguese verbs we examined behaved much more similarly to their English counterÂ­ parts than we expected. Many of the verbs participate in alternations that are direct transÂ­ lations of the English alternations. However, there are some interesting differences in which sense extensions are allowed. 4.1 Similar sense extensions. We have made a preliminary study of the PorÂ­ tuguese translation of the carry verb class. As in English, these verbs seem to take different alterÂ­ nations, and the ability of each to participate in an alternation is related to its semantic content. Table 1 shows how these Portuguese verbs natuÂ­ rally cluster into two different subclasses, based on their ability to take the conative and apart alternations as well as path prepositions. These subclasses correspond very well to the English subclasses created by the intersective class. The conative alternation in Portuguese is mainly contra (against), and the apart alternaÂ­ tion is mainly separando (separating). For exÂ­ ample, Eu puxei o ramo e o galho separandoos As in English, derivar and planar are not exterÂ­ nally controllable actions and thus don't take the causativejinchoative alternation common to other verbs in the roll class. Planar doesn't take a direct object in Portuguese, and it shows the induced action alternation the same way as fluÂ­ tuar (by using the light verb fazer). Derivar is usually said as "estar a deriva" ("to be adrift"), showing its non-controllable action more explicÂ­ itly. Table 1: Portuguese carry verbs with their alÂ­ ternations (I pulled the twig and the branch apart ) , and Ele empurrou contra a parede (He pushed against the walQ. 4.2 Changing class membership. We also investigated the Portuguese translation of some intersective classes of motion verbs. We selected the slide/roll/run, meander/roll and roll/run intersective classes. Most verbs have more than one translation into Portuguese, so we chose the translation that best described the meaning or that had the same type of arguments as described in Levin's verb classes. The elements of the slide/roll/run class are rebater (bounce), flutuar (float), rolar ( rolQ and deslizar (slide). The resultative in Portuguese cannot be expressed in the same way as in EnÂ­ glish. It takes a gerund plus a reflexive, as in A porta deslizou abrindose (The door slid opening itself). Transitivity is also not always preserved in the translations. For example, flutuar does not take a direct object, so some of the alternaÂ­ tions that are related to its transitive meaning are not present. For these verbs, we have the inÂ­ duced action alternation by using the light verb fazer (make) before the verb, as in Maria fez o barco flutuar (Mary floated the boat). As can be seen in Table 2 the alternations for the Portuguese translations of the verbs in this intersective class indicate that they share simiÂ­ lar properties with the English verbs, including the causative/inchoative. The exception to this, as just noted, is flutuar (float). The result of this is that ftutuar should move out of the slide class, which puts it with derivar (drift) and plaÂ­ nar (glide) in the closely related roll/run class.  We have presented a refinement of Levin classes, intersective classes, and discussed the potential for mapping them to WordNet senses. Whereas each WordNet synset is hierarchicalized accordÂ­ ing to only one aspect (e.g., Result, in the case of cut), Levin recognizes that verbs in a class may share many different semantic features, without designating one as primary. Intersective Levin sets partition these classes according to more coÂ­ herent subsets of features (force, force+motion, force+separation), in effect highlighting a lattice of semantic features that determine the sense of a verb. Given the incompleteness of the list of members of Levin classes, each verb must be examined to see whether it exhibits all the alÂ­ ternations of a class. This might be approxiÂ­ mated by automatically extracting the syntacÂ­ tic frames in which the verb occurs in corpus data, rather than manual analysis of each verb, as was done in this study. We have also examined a mapping between the English verbs that we have discussed and their Portuguese translations, which have sevÂ­ eral of the same properties as the corresponding verbs in English. Most of these verbs take the same alternations as in English and, by virtue of these alternations, achieve the same regular sense extensions. There are still many questions that require further investigation. First, since our experiÂ­ ment was based on a translation from English to Portuguese, we can expect that other verbs in Portuguese would share the same alternations, so the classes in Portuguese should by no means be considered complete. We will be using reÂ­ sources such as dictionaries and online corpora to investigate potential additional members of our classes. Second, since the translation mapÂ­ pings may often be many-to-many, the alterna re bat er (bo unc e) flut uar (flo at) rola r (rol l) desl izar (sli de) deri var (dr ift) pla nar (gli de) dati ve â¢ c o n a t i v e c a u s . / i n c h . m i d d l e acc ept. cor eÂ£. y e s n o y e s y e s y e s y e s n o y e s y e s y e s y e s n o y e s y e s y e s cau s:f mc h. res ulta tive adj ect. par t. y e s y e s y e s y e s y e s ye s ye s ye s y e s y e s y e s y e s y e s y e s y e s ind. acti on loc at. inv ers. me asu re â¢ad j. per f. â¢c og n. ob je ct ze ro no m. y e s y e s y e s n o n o y e s y e s y e s y e s n o n o y e s y e s y e s y e s n o n o n o y e s y e s y e s n o n o y e s n o y e s y e s n o n o y e s y e s y e s y e s n o n o y e s Table 2: Portuguese slide/roll/run and roll/run verbs with their alternations tions may depend on which translation is choÂ­ sen, potentially giving us different clusters, but it is uncertain to what extent this is a factor, and it also requires further investigation. In this experiment, we have tried to choose the Portuguese verb that is most closely related to the description of the English verb in the Levin class. We expect these cross-linguistic features to be useful for capturing translation generalizations between languages as discussed in the literaÂ­ ture (Palmer and Rosenzweig, 1996), (CopesÂ­ take and Sanfilippo, 1993), (Dorr, 1997). In pursuing this goal, we are currently implementÂ­ ing features for motion verbs in the English Tree-Adjoining Grammar, TAG (Bleam et al., 1998). TAGs have also been applied to PorÂ­ tuguese in previous work, resulting in a small Portuguese grammar (Kipper, 1994). We inÂ­ tend to extend this grammar, building a more robust TAG grammar for Portuguese, that will allow us to build an English/Portuguese transÂ­ fer lexicon using these features.
 Improving Data Driven Wordclass Tagging by System Combination  In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system. We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging. Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second stage classifiers. All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.  In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations. Traditionally, these models were categorized as either rule-based/symbolic or corpus-based/probabilistic. Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven). Data driven methods appear to be the more popular. This can be explained by the fact that, in general, hand crafting an explicit model is rather difficult, especially since what is being modelled, natural language, is not (yet) well- understood. When a data driven method is used, a model is automatically learned from the implicit structure of an annotated training corpus. This is much easier and can quickly lead to a model which produces results with a 'reasonably' good quality. Obviously, 'reasonably good quality' is not the ultimate goal. Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used. Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material. Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system. However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited. A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors. In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance. In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers. It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996). The underlying assumption is twofold. First, the combined votes will make the system more robust to the quirks of each learner's particular bias. Also, the use of information about each individual method's behaviour in principle even admits the possibility to fix collective errors. We will execute our investigation by means of an experiment. The NLP task used in the experiment is morpho-syntactic wordclass tagging. The reasons for this choice are several. First of all, tagging is a widely researched and well-understood task (cf. van Halteren (ed.) 1998). Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words. Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text. Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf. van Halteren 1996). Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited. Now there are more varied systems available, a variety which we hope will lead to better combination effects. For this experiment we have selected four systems, primarily on the basis of availability. Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model. The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies. The Viterbi algorithm is used to determine the most probable tag sequence. Since this model has no facilities for handling unknown words, a Memory-Based system (see below) is used to propose distributions of potential tags for words not in the lexicon. The second system is the Transformation Based Learning system as described by Brill (19941; henceforth tagger R, for Rules). This 1 Brill's system is available as a collection of C programs and Perl scripts at ftp ://ftp. cs. j hu. edu/pub/brill/Programs/ RULE_BASED_TAGGER_V. I. 14. tar. Z. system starts with a basic corpus annotation (each word is tagged with its most likely tag) and then searches through a space of transformation rules in order to reduce the discrepancy between its current annotation and the correct one (in our case 528 rules were learned). During tagging these rules are applied in sequence to new text. Of all the four systems, this one has access to the most information: contextual information (the words and tags in a window spanning three positions before and after the focus word) as well as lexical information (the existence of words formed by suffix/prefix addition/deletion). However, the actual use of this information is severely limited in that the individual information items can only be combined according to the patterns laid down in the rule templates. The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory). During the training phase, cases containing information about the word, the context and the correct tag are stored in memory. During tagging, the case most similar to that of the focus word is retrieved from the memory, which is indexed on the basis of the Information Gain of each feature, and the accompanying tag is selected. The system used here has access to information about the focus word and the two positions before and after, at least for known words. For unknown words, the single position before and after, three suffix letters, and information about capitalization and presence of a hyphen or a digit are used. The fourth and final system is the MXPOST system as described by Ratnaparkhi (19962; henceforth tagger E, for Entropy). It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features). A beam search is then used to find the highest probability tag sequence. Both this system and Brill's system are used with the default settings that are suggested in their documentation. 2Ratnaparkhi's Java implementation of this system is available at ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/  The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986). The corpus comprises about one million words, divided over 500 samples of 2000 words from 15 text types. Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate. Here we use a slight adaptation of the tagset. The changes are mainly cosmetic, e.g. non-alphabetic characters such as "$" in tag names have been replaced. However, there has also been some retokenization: genitive markers have been split off and the negative marker "n't" has been reattached. An example sentence tagged with the resulting tagset is: The ATI singular or plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense of verb an AT singular article invitation NN singular common noun to IN preposition all ABN pre-quantifier the ATI singular or plural article parliamentary JJ adjective candidates NNS plural common noun SPER period The tagset consists of 170 different tags (including ditto tags 3) and has an average ambiguity of 2.69 tags per wordform. The difficulty of the tagging task can be judged by the two baseline measurements in Table 2 below, representing a completely random choice from the potential tags for each token (Random) and selection of the lexically most likely tag (LexProb). For our experiment, we divide the corpus into three parts. The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if "as well as" is taken to be a coordination conjunction, it is tagged "as_CC1 well_CC2 as_CC3", using three related but different ditto tags. by taking the first eight utterances of every ten. This part is used to train the individual tag- gers. The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods. The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers. Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags. The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done. The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement.  In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune. As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances. The quality of the individual tuggers (cf. Table 2 below) certainly still leaves room for improvement, although tagger E surprises us with an accuracy well above any results reported so far and makes us less confident about the gain to be accomplished with combination. However, that there is room for improvement is not enough. As explained above, for combination to lead to improvement, the component taggers must differ in the errors that they make. That this is indeed the case can be seen in Table 1. It shows that for 99.22% of Tune, at least one tagger selects the correct tag. However, it is unlikely that we will be able to identify this 4This implies that it is impossible to note if errors counted against a tagger are in fact errors in the benchmark tagging. We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation. All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune. The patterns between the brackets give the distribution of correct/incorrect tags over the systems. tag in each case. We should rather aim for optimal selection in those cases where the correct tag is not outvoted, which would ideally lead to correct tagging of 98.21% of the words (in Tune). Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers. In this and the following sections we examine a number of them. The accuracy measurements for all of them are listed in Table 2. 5 The most straightforward selection method is an n-way vote. Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected. 6 The question is how large a vote we allow each tagger. The most democratic option is to give each tagger one vote (Majority). However, it appears more useful to give more weight to taggers which have proved their quality. This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision). The information about each tagger's quality is derived from an inspection of its results on Tune. 5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger. When abstracting away from individual tags, precision and recall are equal and measure how many tokens are tagged correctly; in this case we also use the more generic term accuracy. 6In our experiment, a random selection from among the winning tags is made whenever there is a tie. Table 2: Accuracy of individual taggers and combination methods. But we have even more information on how well the taggers perform. We not only know whether we should believe what they propose (precision) but also know how often they fail to recognize the correct tag (recall). This information can be used by forcing each tagger also to add to the vote for tags suggested by the opposition, by an amount equal to 1 minus the recall on the opposing tag (Precision-Recall). As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall. However, specific information is not always superior, for TotPrecision scores higher than TagPrecision. This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision). 7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0. Pairwise Voting So far, we have only used information on the performance of individual taggers. A next step is to examine them in pairs. We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx. When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers. If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz. the probability of each tag Tx given that the tagger suggested tag Ti. Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win. In principle, this could remove the restriction of gain only in 22 and 1111 cases. In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often. When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%). Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement. It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely. The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0. is usually called stacking (Wolpert 1992). The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern. The first choice for this is to use a Memory- Based second level learner. In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag. In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context). For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997). Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair. 9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test. This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags. To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material. 1° Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse. We conjecture that pruning is not beneficial when the interesting cases are very rare. To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem. 9Tags (Memory-Based) scores significantly worse than TagPair (p=0.0274) and not significantly better than Precision-Recall (p=0.2766). 1°Tags+Word could not be handled by C5.0 due to the huge number of feature values. Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination. The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3. The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components. Also of note is the improvement yielded by the best combination. The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz. the Maximum Entropy tagger (97.43%). A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11). After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone. A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608. 12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant. nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination. To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen. It turns out that the increase in the individual taggers is quite limited when compared to combination. The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction). Conclusion. Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems. Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf. Ratnaparkhi 1996 on such effects in the Penn Treebank corpus). Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages. But the investigation need not be limited to wordclass tagging, for we expect that there are many other NLP tasks where combination could lead to worthwhile improvements.  Our thanks go to the creators of the tagger generators used here for making their systems available.
 Robust pronoun resolution with limited knowledge  Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task. This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successÂ­ fully adapted for other languages with minimum modifications.  For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979). However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense. While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources. Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996). Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ­ ora resolution. It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing. Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).  With a view to avoiding complex syntactic, semanÂ­ tic and discourse analysis (which is vital for realÂ­ world applications), we developed a robust, knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors. It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as "antecedent indicators"). The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the reÂ­ maining candidates (see next section). The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for imÂ­ mediate reference. If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score. If this does not help, the candidate with the higher score for indicating verbs is preferred. If still no choice is possible, the most recent from the remaining candiÂ­ dates is selected as the antecedent. 2.1 Antecedent indicators. Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates. Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the anteÂ­ cedent. The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms. Whilst some of the indicators are more genre-specific (term preferÂ­ ence) and others are less genre-specific ("immediate reference"), the majority appear to be genreÂ­ independent. In the following we shall outline some the indicators used and shall illustrate them by exÂ­ amples. Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1). We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or possesÂ­ sive pronouns. This rule is ignored if there are no definite articles, possessive or demonstrative proÂ­ nouns in the paragraph (this exception is taken into account because some English user's guides tend to omit articles). Givenness Noun phrases in previous sentences representing the "given information" (theme) 1 are deemed good candidates for antecedents and score I (candidates not representing the theme score 0). In a coherent text (Firbas 1992), the given or known information, or theme, usually appears first, and thus forms a coÂ­ referential link with the preceding text. The new information, or rheme, provides some information about the theme. 1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence. Indicating verbs If a verb is a member of the Verb_set = {discuss, present, illustrate, identify, summarise, examine, describe, define, show, check, develop, review, reÂ­ port, outline, consider, investigate, explore, assess, analyse, synthesise, study, survey, deal, cover}, we consider the first NP following it as the preferred anÂ­ tecedent (scores 1 and 0). Empirical evidence sugÂ­ gests that because of the salience of the noun phrases which follow them, the verbs listed above are particularly good indicators. Lexical reiteration Lexically reiterated items are likely candidates for antecedent (a NP scores 2 if is repeated within the same paragraph twice or more, 1 if repeated once and 0 if not). Lexically reiterated items include reÂ­ peated synonymous noun phrases which may often be preceded by definite articles or demonstratives. Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. "toner bottle", "bottle of toner", "the bottle"). Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we conÂ­ sider it as the preferred candidate (1, 0). "Non-prepositional" noun phrases A "pure", "non-prepositional" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ). Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording. Here "the VCR" is penalised (-1) for being part of the prepositional phrase "into the VCR". This preference can be explained in terms of saliÂ­ ence from the point of view of the centering theory. The latter proposes the ranking "subject, direct obÂ­ ject, indirect object" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects. Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0). The collocation preference here is restricted to the patterns "noun phrase (pronoun), verb" and "verb, noun phrase (pronoun)". Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990). Example: Press the keyi down and turn the volume up... Press iti again. Immediate reference In technical manuals the "immediate reference" clue can often be useful in identifying the antecedent. The heuristics used is that in constructions of the form "...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)", where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun "it" immeÂ­ diately following V2 and is therefore given preference (scores 2 and 0). This preference can be viewed as a modification of the collocation preference. It is also quite freÂ­ quent with imperative constructions. Example: To print the paper, you can stand the printeri up or lay iti flat. To turn on the printer, press the Power buttoni and hold iti down for a moment. Unwrap the paperiâ¢ form iti and align itiâ¢ then load iti into the drawer. Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1). For anaphors in simple sentences, noun phrases in the previous senÂ­ tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1). Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not). 21dentification of clauses in complex sentences is do e heuristically. As already mentioned, each of the antecedent inÂ­ dicators assigns a score with a value {-1, 0, 1, 2}. These scores have been determined experimentally on an empirical basis and are constantly being upÂ­ dated. Top symptoms like "lexical reiteration" asÂ­ sign score "2" whereas "non-prepositional" noun phrases are given a negative score of "-1". We should point out that the antecedent indicators are preferences and not absolute factors. There might be cases where one or more of the antecedent indicators do not "point" to the correct antecedent. For inÂ­ stance, in the sentence "Insert the cassette into the VCRi making sure iti is turned on", the indicator "non-prepositional noun phrases" would penalise the correct antecedent. When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the "non-prepositional noun phrases" heuristics (penalty) would be overturned by the "collocational preference" heuristics. 2.2 Informal description of the algorithm. The algorithm for pronoun resolution can be deÂ­ scribed informally as follows: 1. Examine the current sentence and the two preÂ­. ceding sentences (if available). Look for noun phrases3 only to the left of the anaphor4 2. Select from the noun phrases identified only. those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates  tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test. For this purpose we have drawn up a compreÂ­ hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resoluÂ­ tion has addressed the problem of "agreement excepÂ­ tions". antecedent. If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent. If immediate reference does not hold, propose the candidate with higher score for collocational pattern. If collocational pattern suggests a tie or does not hold, select the candidate with higher score for indicating verbs. If this indicator does not hold again, go for the most recent candidate. 3. Evaluation. For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ­ istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences. The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking. Syntactic parallelism, useful in discrimiÂ­ nating between identical pronouns on the basis of their syntactic function, also has to be forgone. Lack of semantic knowledge rules out the use of verb seÂ­ mantics and semantic parallelism. Our evaluation, however, suggests that much less is lost than might be feared. In fact, our evaluation shows that the reÂ­ sults are comparable to syntax-based methods (Lappin & Leass I994). We believe that the good success rate is due to the fact that a number of anteÂ­ cedent indicators are taken into account and no facÂ­ tor is given absolute preference. In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism preferÂ­ ences (see below). 3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994). There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric. The resolution of anaphors was carried out with a sucÂ­ cess rate of 95.8%. The approach being robust (an attempt is made to resolve each anaphor and a proÂ­ posed antecedent is returned), this figure represents both "precision" and "recall" if we use the MUC terminology. To avoid any terminological confusion, we shall therefore use the more neutral term "success rate" while discussing the evaluation. In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor. The success rate of the "Baseline Subject" was 29.2%, whereas the success rate of "Baseline Most Recent NP" was 62.5%. Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon. Typically, our preference-based model proved superior to both baseline models when the anteceÂ­ dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number. Example: Identify the draweq by the lit paper port LED and add paper to itj. The aggregate score for "the drawer" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate referÂ­ ence 2 = 7), whereas aggregate score for the most recent matching noun phrase ("the lit paper port LED") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reiteraÂ­ tion 0 + section heading 0 + collocation 0 + referenÂ­ tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4). From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles. Usually knowledge-based apÂ­ proaches have difficulties in such a situation because they use preferences such as "syntactic parallelism" or "semantic parallelism". Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the synÂ­ tactic function/semantic role of each individual word. As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ plex syntactic structure. This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree. Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj. where "blank sheet of paper" scores only 2 as opÂ­ posed to the "the paper through key" which scores 6. 3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994). Out of 223 proÂ­ nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric "it"). The evaluation carried out was manual to ensure that no added error was genÂ­ erated (e.g. due to possible wrong sentence/clause detection or POS tagging). Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3). The evaluation indicated 83.6% success rate. The "Baseline subject" model tested on the same data scored 33.9% recall and 67.9% precision, whereas "Baseline most recent" scored 66.7%. Note that "Baseline subject" can be assessed both in terms of recall and precision because this "version" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many imÂ­ perative zero-subject sentences). In the second experiment we evaluated the apÂ­ proach from the point of view also of its "critical success rate". This measure (Mitkov 1998b) applies only to anaphors "ambiguous" from the point of view of number and gender (i.e. to those "tough" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators. Our evaluation estabÂ­ lished the critical success rate as 82%. A case where the system failed was when the anaphor and the antecedent were in the same senÂ­ tence and where preference was given to a candidate in the preceding sentence. This case and other cases suggest that it might be worthwhile reconsiderÂ­ ing/refining the weights for the indicator "referential distance". Similarly to the first evaluation, we found that the robust approach was not very successful on senÂ­ tences with too complicated syntax - a price we have to pay for the "convenience" of developing a knowlÂ­ edge-poor system. The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures. R ob ust aQ pr oa ch B a s el i n e s u b je ct B as eli ne m os t re ce nt Su cc es s rat e (= Pr ec isi on / Re ca ll) 8 9. 7 % 31. 55 % I 48 .5 5 % 6 5 . 9 5 % The lower figure in "Baseline subject" corresponds to "recall" and the higher figure- to "precision". If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%). The relaÂ­ tively low figures for the majority of indicators should not be regarded as a surprise: firstly, we should bear in mind that in most cases a candidate was picked (or rejected) as an antecedent on the baÂ­ sis of applying a number of different indicators and secondly, that most anaphors had a relatively high number of candidates for antecedent. In terms of frequency of use ("number of nonzero applications"/"number of anaphors"), the most freÂ­ quently used indicator proved to be referential disÂ­ tance used in 98.9% of the cases, followed by term preference (97.8%), givenness (83.3%), lexical reitÂ­ eration (64.4%), definiteness (40%), section heading (37.8%), immediate reference (31.1%) and collocaÂ­ tion (11.1%). As expected, the most frequent indicaÂ­ tors were not the most discriminative ones. 3.3 Comparison to similar approaches: comparaÂ­. tive evaluation of Breck Baldwin's CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogÂ­ NIAC (Baldwin 1997) approach which features "high precision coreference with limited knowledge and linguistics resources". The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate. Given that our approach is robust and returns anÂ­ tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC's "resolve all" version by simulating it manually on the same training data used in evaluation B above. CogNIAC successfully resolved the pronouns in 75% of the cases. This result is comparable with the results described in (Baldwin 1997). For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.  languages An attractive feature of any NLP approach would be its language "universality". While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adaptaÂ­ tion. We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish. As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997). For the time being, we are using the same scores for Polish. The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998). The sample texts conÂ­ tained 180 pronouns among which were 120 inÂ­ stances of exophoric reference (most being zero proÂ­ nouns). The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolvÂ­ ing anaphors (with critical success rate of 86.2%). Similarly to the evaluation for English, we comÂ­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor. Our preference-based approach showed clear suÂ­ periority over both baseline models. The first BaseÂ­ line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%. ThereÂ­ fore, the 93.3% success rate (see above) demonÂ­ strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences. We have recently adapted the approach for AraÂ­ bic as well (Mitkov & Belguith 1998). Our evaluaÂ­ tion, based on 63 examples (anaphors) from a techÂ­ nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %).  We have described a robust, knowledge-poor apÂ­ proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger. Evaluation shows a success rate of 89.7% for the genre of techÂ­ nical manuals and at least in this genre, the approach appears to be more successful than other similar methods. We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).
 Semi-supervised Verb Class Discovery Using Noisy Features  We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs. The feature set was previously shown to work well in a supervised learning setting, using known English verb classes. In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task. We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties. We find that the unsupervised method we tried cannot be consistently applied to our data. However, the semi- supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well.  Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies. Learning the argument structure properties of verbsâthe semantic roles they assign and their mapping to syntactic positionsâis both particularly important and difficult. A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003). Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002). We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs. Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003). As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000). However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise. Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers. It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features. We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003). In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf. Dorr and Jones, 1996; Schulte im Walde and Brew, 2002). On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002). By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages. However, a general feature space means that most features will be irrelevant to any given verb discrimination task. In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to âthe curse of dimensionalityâ? In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand. In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner. Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery. In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993). To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features. The unsupervised feature selection method, on the other hand, was not usable for our data. In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs. We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results. We conclude with a discussion of related work, our contributions, and future directions.  Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002). Levinâs classes form a hierarchy of verb groupings with shared meaning and syntax. Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions. It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind. Thus, the features serve as approximations to the underlying distinctions among classes. Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details. Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb. We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb. In addition to approximating the syntactic frames themselves, we also want to capture regularities in the mapping of arguments to particular slots. For example, the location argument, the truck, is direct object in I loaded the truck with hay, and object of a preposition in I loaded hay onto the truck. These allowable alternations in the expressions of arguments vary according to the class of a verb. We measure this behaviour using features that encode the degree to which two slots contain the same entitiesâthat is, we calculate the overlap in noun (lemma) usage between pairs of syntactic slots. Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001). In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors. The Animacy Features (76 features) Semantic properties of the arguments that fill certain roles, such as animacy or motion, are more challenging to detect automatically. Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001). We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as âpersonâ by our chunker (Abney, 1991).  We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods. Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values. 3.1 The Verb Classes. Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks. These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions. For example, some classes differ in both their semantic roles and frames, while others have the same roles in different frames, or different roles in the same frames.1 Here we summarize the argument structure distinctions between the classes; Table 1 below lists the classes with their Levin class numbers. Benefactive versus Recipient verbs. Mary baked... a cake for Joan/Joan a cake. Mary gave... a cake to Joan/Joan a cake. These dative alternation verbs differ in the preposition and the semantic role of its object. 1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs. Admire versus Amuse verbs. I admire Jane. Jane amuses me. These psychological state verbs differ in that the Experiencer argument is the subject of Admire verbs, and the object of Amuse verbs. Run versus Sound Emission verbs. The kids ran in the room./*The room ran with kids. The birds sang in the trees./The trees sang with birds.These activity verbs both have an Agent subject in the in transitive, but differ in the prepositional alternations they allow. Cheat versus Steal and Remove verbs. I cheated... Jane of her money/*the money from Jane. I stole... *Jane of her money/the money from Jane. These classes also assign the same semantic arguments, but differ in their prepositional alternants. Wipe versus Steal and Remove verbs. Wipe... the dust/the dust from the table/the table. Steal... the money/the money from the bank/*the bank. These classes generally allow the same syntactic frames, but differ in the possible semantic role assignment. (Location can be the direct object of Wipe verbs but not of Steal and Remove verbs, as shown.) Spray/Load versus Fill versus Other Verbs of Putting (several related Levin classes). I loaded... hay on the wagon/the wagon with hay. I filled... *hay on the wagon/the wagon with hay. I put... hay on the wagon/*the wagon with hay. These three classes also assign the same semantic roles but differ in prepositional alternants. Note, however, that the options for Spray/Load verbs overlap with those of the other two types of verbs. Optionally Intransitive: Run versus Change of State versus âObject Dropâ. The horse raced./The jockey raced the horse. The butter melted./The cook melted the butter. The boy played./The boy played soccer.These three classes are all optionally intransitive but as sign different semantic roles to their arguments (Merlo and Stevenson, 2001). (Note that the Object Drop verbs are a superset of the Benefactives above.) For many tasks, knowing exactly what PP arguments each verb takes may be sufficient to perform the classification (cf. Dorr and Jones, 1996). However, our features do not give us such perfect knowledge, since PP arguments and adjuncts cannot be distinguished with high accuracy. Using our simple extraction tools, for example, the PP argument in I admired Jane for her honesty is not distinguished from the PP adjunct in I amused Jane for the money. Furthermore, PP arguments differ in frequency, so that a highly distinguishing but rarely used alternant will likely not be useful. Indicators of PP usage are thus useful but not definitive. Ve rb Cl as s C la ss N u m b er # Ve rbs Be ne fa cti ve 26. 1, 26. 3 3 5 Re ci pi en t 13. 1, 13. 3 2 7 Ad mi re 31. 2 3 5 A m us e 31. 1 1 3 4 Ru n 51. 3.2 7 9 So un d E mi ssi on 43. 2 5 6 C he at 10. 6 2 9 St ea l an d Re m ov e 10. 5, 10. 1 4 5 Wi pe 10. 4.1 , 10. 4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi ll 9.8 6 3 Ot he r V. of Pu tti ng 9.1 â6 4 8 C ha ng e of St at e 45. 1â 4 1 6 9 O bj ec t Dr op 26. 1, 26. 3, 26. 7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2). 3.2 Verb Selection. Our experimental verbs were selected as follows. We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below). Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense. Table 1 above shows the number of verbs in each class at the end of this process. Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003). We began with this same set of 20 verbs per class for our current work. We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs). All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments). 3.3 Feature Extraction. All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains. Since it is a general corpus, we do not expect any strong overall domain bias in verb usage. We used the chunker (partial parser) of Abney (1991) to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it. Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame. From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.  4.1 Clustering Parameters. We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments. In performing hierarchical clustering, both a vector distance measure and a cluster distance (âlinkageâ) measure are specified. We used the simple Euclidean distance for the former, and Ward linkage for the latter. Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods. We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes. To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy. In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff. However, we did experiment with (as in Strehl et al., 2000), and found that performance was generally better (even on our measure, described below, that discounts oversplitting). This supports our intuition that the approach may enable us to find more consistent clusters at a finer grain, without too much fragmentation. 4.2 Evaluation Measures. We use three separate evaluation measures, that tap into very different properties of the clusterings. 4.2.1 Accuracy We can assign each cluster the class label of the majority of its members. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size. As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clusteringâthat is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be. The theoretical maximum is, of course, 1. To calculate a random baseline, we evaluated 10,000 random clusterings with the same number of verbs and classes as in each of our experimental tasks. Because the achieved depends on the precise size of clusters, we calculated mean over the best scenario (with equal-sized clusters), yielding a conservative estimate (i.e., an upper bound) of the baseline. These figures are reported with our results in Table 2 below. 4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good. Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification. The formula is as follows (Hubert and Arabie, 1985): where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between themâ there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not. It is scaled so that perfect agreement yields a value of 1, whereas random groupings (with the same number of groups in each) get a value around 0. It is therefore considered âcorrected for chance,â given a fixed number of clusters.3 In tests of the measure on some contrived cluster- ings, we found it quite conservative, and on our experimental clusterings it did not often attain values higher than .25. However, it is useful as a relative measure of good-. ness, in comparing clusterings arising from different feature sets. 4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes. Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs. However, since we fix our number of clusters to the number of classes, the measure remains informative. 3 In our experiments for estimating the baseline, we in-. deed found a mean value of 0.00 for all random clusterings. 1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89  W e re p or t he re th e re su lt s of a n u m be r of cl us te ri n g ex - pe ri m en ts, us in g fe at ur e se ts as fo ll o w s: (1 ) th e fu ll fe at ur e sp ac e; (2 ) a m an ua ll y se le ct ed su bs et of fe at ur es ; (3 ) u n- su pe rv is ed se le ct io n of fe at ur es ; an d (4 ) se mi su p er vi se d se le ct io n, us in g a su pe rv is ed le ar ne r ap pl ie d to se ed ve rb s to se le ct th e fe at ur es . F or ea ch ty pe of fe at ur e se t, w e pe rf or m ed th e sa m e te n cl us te ri n g ta sk s, sh o w n in th e fir st co lu m n of Ta bl e 2. T he se ar e th e sa m e ta sk s pe rf or m ed in th e su pe rv is ed se t- ti n g of Jo an is an d St ev en so n (2 0 0 3) . T he 2- an d 3 w ay ta sk s, an d th ei r m ot iv at io n, w er e de sc ri be d in S ec ti o n 3. 1. T hr ee m ul ti w ay ta sk s ex pl or e pe rf or m an ce o ve r a la rg er n u m be r of cl as se s: T he 6 w ay ta sk in v ol v es th e C he at , St ea lâ R e m ov e, W ip e, S pr ay /L o a d, Fi ll, an d â O th er V er bs of P ut ti n g â cl as se s, al l of w hi ch u n de rg o si m il ar lo ca ti v e Figure 1: The dendrograms and values for the 2-way Wipe/StealâRemove task, using the Ling and Seed sets. The higher (.89 vs. .33) reflects the better separation of the data. regard to the target classes. We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters. Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster). A value of 0 suggests that a point is not clearly in a particular cluster. We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated. Essentially, the measure numerically captures what we can intuitively grasp in the visual differences between the dendrograms of âbetterâ and âworseâ clusterings. (A dendrogram is a tree diagram whose leaves are the data points, and whose branch lengths indicate similarity of subclusters; roughly, shorter vertical lines indicate closer clusters.) For example, Figure 1 shows two dendrograms using different feature sets (Ling and Seed, described in Section 5) for the same set of verbs from two classes. The Seed set has slightly lower values for and , but a much higher value (.89) for , indicating a better separation of the data. This captures what is reflected in the dendrogram, in that very short lines connect verbs low in the tree, and longer lines connect the two main clusters. The measure is independent of the true classification, and could be high when the other dependent measures are low, or vice versa. However, it gives important information about the quality of a clustering: The other measures being equal, a clustering with a higher value indicates tighter and more separated clusters, suggesting stronger inherent patterns in the data. alternations. To these 6, the 8-way task adds the Run and Sound Emission verbs, which also undergo locative alternations. The 13-way task includes all of our classes. The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments. These are the results of a 10-fold cross- validation (with boosting) repeated 50 times.4 In our earlier work, we found that cross-validation performance averaged about .02, .04, and .11 higher than test performance on the 2-way, 3-way, and multiway tasks, respectively, and so should be taken as an upper bound on what can be achieved. The third column of Table 2 gives the baseline we calculated from random clusterings. Recall that this is an upper bound on random performance. We use this baseline in calculating reductions in error rate of . The remaining columns of the table give the , , and measures as described in Section 4.2, for each of the feature sets we explored in clustering, which we discuss in turn below. 5.1 Full Feature Set. The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection). Although generally higher than the baseline, is well below that of the supervised learner, and and are generally low. 5.2 Manual Feature Selection. One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task. Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2. Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/StealâRemove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/StealâRemove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans. .66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23. All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results. C5.0 is supervised accuracy; Base is on random clusters. Full is full feature set; Ling is manually selected subset; Seed is seed-verb-selected set. See text for further description. indicated by the class description given in Levin. For each task, then, the linguistically-relevant subset is defined as the union of these subsets for all the classes in the task. The results for these feature sets in clustering are given in the second subcolumn (Ling) under each of the , , and measures in Table 2. On the 2-way tasks, the performance on average is very close to that of the full feature set for the and measures. On the 3-way and multiway tasks, there is a larger performance gain using the subset of features, with an increase in the reduction of the error rate (over Base ) of 67% over the full feature set. Overall, there is a small performance gain using the Ling subset of features (with an increase in error rate reduction from 13% to 17%). Moreover, the value for the manually selected features is almost always very much higher than that of the full feature set, indicating that the subset of features is more focused on the properties that lead to a better separation of the data. This performance comparison tentatively suggests that good feature selection can be helpful in our task. However, it is important to find a method that does not depend on having an existing classification, since we are interested in applying the approach when such a classification does not exist. In the next two sections, we present unsupervised and minimally supervised approaches to this problem. 5.3 Unsupervised Feature Selection. In order to deal with excessive dimensionality, Dash et al. (1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise. Unfortunately, this promising method did not prove practical for our data. We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure). Many feature sets performed very well, and some far outperformed our best results using other feature selection methods. However, across our 10 experimental tasks, there was no consistent range of feature ranks or feature set sizes that was correlated with good performance. While we could have selected a threshold that might work reasonably well with our data, we would have little confidence that it would work well in general, considering the inconsistent pattern of results. 5.4 Semi-Supervised Feature Selection. Unsupervised methods such as Dash et al.âs (1997) are appealing because they require no knowledge external to the data. However, in many aspects of computational linguistics, it has been found that a small amount of labelled data contains sufficient information to allow us to go beyond the limits of completely unsupervised approaches. In our domain in particular, verb class discovery âin a vacuumâ is not necessary. A plausible scenario is that researchers would have examples of verbs which they believe fall into different classes of interest, and they want to separate other verbs along the same lines. To model this kind of approach, we selected a sample of five seed verbs from each class. Each set of verbs was judged (by the authorsâ intuition alone) to be ârepresentativeâ of the class. We purposely did not carry out any linguistic analysis, although we did check that each verb was reasonably frequent (with log frequencies ranging from 2.6 to 5.1). For each experimental task, we ran our supervised Table 3: Feature counts for Ling and Seed feature sets. learner (C5.0) on the seed verbs for those classes, in a 5-fold cross-validation (without boosting). We extracted from the resulting decision trees the union of all features used, which formed the reduced feature set for that task. Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2. This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder. Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks. More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set. The increased reduction in error rate is particularly striking for the 2-way tasks, of 37% for the Seed set compared to 20% for the Ling set. Another striking result is the difference in values, which are very much higher than those for Ling (which are in turn much higher than for Full). Thus, not only do we see a sizeable increase in performance, we also obtain tighter and better separated clusters with our proposed feature selection approach. 5.5 Further Discussion. In our clustering experiments, we find that smaller subsets of features generally perform better than the full set of features. (See Table 3 for the number of features in the Ling and Seed sets.) However, not just any small set of features is adequate. We ran 50 experiments using randomly selected sets of features of cardinality , where 5We also tried directly applying the mutual information (MI) measure used in decision-tree induction (Quinlan, 1986). We calculated the MI of each feature with respect to the classification of the seed verbs, and computed clusterings using the features above a certain MI threshold. This method did not work as well as running C5.0, which presumably captures important feature interactions that are ignored in the individual MI calculations. is the number of classes (a simple linear function roughly approximating the number of features in the Seed sets). Mean over these clusterings was much lower than for the Seed sets, and was extremely low (below .08 in all cases). Interestingly, was generally very high, indicating that there is structure in the data, but not what matches our classification. This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery. We also find that our semi-supervised method (Seed) is linguistically plausible, and performs as well as or better than features manually determined based on linguistic knowledge (Ling). We might also ask, would any subset of verbs do as well? To answer this, we ran experiments using 50 different randomly selected seed verb sets for each class. We found that the mean and values are the same as that of the Seed set reported above, but mean is a little lower. We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others. This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach.  Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification. However, their study used a small set of five features manually devised for a set of three particular classes. Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work. The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks. However, Schulte im Waldeâs features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4). Performance differences may be due to the types of features (ours are noisier, but capture information beyond subcat), or due to the number or size of classes. While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be âeasierâ than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together. It is a question for future research to explore the effect of these variables in clustering performance.  We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. An unsupervised method we tried (Dash et al., 1997) did not prove useful, because of the problem of having no consistent threshold for feature inclusion. We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering. We showed that this feature set outperformed both the full and the manually selected sets of features on all three of our clustering evaluation metrics. Furthermore, the method is relatively insensitive to the precise makeup of the selected seed set. As successful as our seed set of features is, it still does not achieve the accuracy of a supervised learner. More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering. Furthermore, we might question the clustering approach itself, in the context of verb class discovery. Rather than trying to separate a set of new verbs into coherent clusters, we suggest that it may be useful to perform a nearest-neighbour type of classification using a seed set, asking for each new verb âis it like these or not?â In some ways our current clustering task is too easy, because all of the verbs are from one of the target classes. In other ways, however, it is too difficult: the learner has to distinguish multiple classes, rather than focus on the important properties of a single class. Our next step is to explore these issues, and investigate other methods appropriate to the practical problem of grouping verbs in a new language.  We are indebted to Allan Jepson for helpful discussions and suggestions. We gratefully acknowledge the financial support of NSERC of Canada and Bell University Labs.
 The Potsdam Commentary Corpus  A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.  A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the âPotsdam Commentary Corpusâ (henceforth âPCCâ for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus. Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers. The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide. (Again, the goal of also in structural features. As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers SuÂ¨ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence. The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences. For illustration, an English translation of one of the commentaries is given in Figure 1. The paper is organized as follows: Section 2 explains the different layers of annotation that have been produced or are being produced. Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future. Section 4 draws some conclusions from the present state of the effort.  The corpus has been annotated with six different types of information, which are characterized in the following subsections. Not all the layers have been produced for all the texts yet. There is a âcore corpusâ of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below. All annotations are done with specific tools and in XML; each layer has its own DTD. This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation. We will briefly discuss this point in Section 3.1. 2.1 Part-of-speech tags. All commentaries have been tagged with part-of-speech information using Brantsâ TnT1 tagger and the Stuttgart/TuÂ¨bingen Tag Set automatic analysis was responsible for this decision.) This is manifest in the lexical choices but 1 www.coli.unisb.de/â¼thorsten/tnt/ Dagmar Ziegler is up to her neck in debt. Due to the dramatic fiscal situation in Brandenburg she now surprisingly withdrew legislation drafted more than a year ago, and suggested to decide on it not before 2003. Unexpectedly, because the ministries of treasury and education both had prepared the teacher plan together. This withdrawal by the treasury secretary is understandable, though. It is difficult to motivate these days why one ministry should be exempt from cutbacks â at the expense of the others. Reicheâs colleagues will make sure that the concept is waterproof. Indeed there are several open issues. For one thing, it is not clear who is to receive settlements or what should happen in case not enough teachers accept the offer of early retirement. Nonetheless there is no alternative to Reicheâs plan. The state in future has not enough work for its many teachers. And time is short. The significant drop in number of pupils will begin in the fall of 2003. The government has to make a decision, and do it quickly. Either save money at any cost - or give priority to education. Figure 1: Translation of PCC sample commentary (STTS)2. 2.2 Syntactic structure. Annotation of syntactic structure for the core corpus has just begun. We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures. 2.3 Rhetorical structure. All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988). Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised. Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators. Thus we opted not to take the step of creating more precise written annotation guidelines (as (Carlson, Marcu 2001) did for English), which would then allow for measuring inter-annotator agreement. The motivation for our more informal approach was the intuition that there are so many open problems in rhetorical analysis (and more so for German than for English; see below) that the main task is qualitative investigation, whereas rigorous quantitative analyses should be performed at a later stage. One conclusion drawn from this annotation effort was that for humans and machines alike, 2 www.sfs.nphil.unituebingen.de/Elwis/stts/ stts.html 3 www.coli.unisb.de/sfb378/negra-corpus/annotate. html 4 www.wagsoft.com/RSTTool assigning rhetorical relations is a process loaded with ambiguity and, possibly, subjectivity. We respond to this on the one hand with a format for its underspecification (see 2.4) and on the other hand with an additional level of annotation that attends only to connectives and their scopes (see 2.5), which is intended as an intermediate step on the long road towards a systematic and objective treatment of rhetorical structure. 2.4 Underspecified rhetorical structure. While RST (Mann, Thompson 1988) proposed that a single relation hold between adjacent text segments, SDRT (Asher, Lascarides 2003) maintains that multiple relations may hold simultaneously. Within the RST âuser communityâ there has also been discussion whether two levels of discourse structure should not be systematically distinguished (intentional versus informational). Some relations are signalled by subordinating conjunctions, which clearly demarcate the range of the text spans related (matrix clause, embedded clause). When the signal is a coordinating conjunction, the second span is usually the clause following the conjunction; the first span is often the clause preceding it, but sometimes stretches further back. When the connective is an adverbial, there is much less clarity as to the range of the spans. Assigning rhetorical relations thus poses questions that can often be answered only subjectively. Our annotators pointed out that very often they made almost random decisions as to what relation to choose, and where to locate the boundary of a span. (Carlson, Marcu 2001) responded to this situation with relatively precise (and therefore long!) annotation guidelines that tell annotators what to do in case of doubt. Quite often, though, these directives fulfill the goal of increasing annotator agreement without in fact settling the theoretical question; i.e., the directives are clear but not always very well motivated. In (Reitter, Stede 2003) we went a different way and suggested URML5, an XML format for underspecifying rhetorical structure: a number of relations can be assigned instead of a single one, competing analyses can be represented with shared forests. The rhetorical structure annotations of PCC have all been converted to URML. There are still some open issues to be resolved with the format, but it represents a first step. What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly. 2.5 Connectives with scopes. For the âcoreâ portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright â but see Sections 3.2 and 3.3 below. Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information. We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes. This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English. For effectively annotating connectives/scopes, we found that existing annotation tools were not well-suited, for two reasons: â¢ Some tools are dedicated to modes of annotation (e.g., tiers), which could only quite un-intuitively be used for connectives and scopes. â¢ Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them. 5 âUnderspecified Rhetorical Markup Languageâ 6 This confirms the figure given by (Schauer, Hahn. Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose. It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives. The annotator can then âclick awayâ those words that are here not used as connectives (such as the conjunction und (âandâ) used in lists, or many adverbials that are ambiguous between connective and discourse particle). Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like âfor sub- junctor, mark all words up to the next comma as the first segmentâ), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse. When finished, the whole material is written into an XML-structured annotation file. 2.6 Co-reference. We developed a first version of annotation guidelines for co-reference in PCC (Gross 2003), which served as basis for annotating the core corpus but have not been empirically evaluated for inter-annotator agreement yet. The tool we use is MMAX8, which has been specifically designed for marking co-reference. Upon identifying an anaphoric expression (currently restricted to: pronouns, prepositional adverbs, definite noun phrases), the an- notator first marks the antecedent expression (currently restricted to: various kinds of noun phrases, prepositional phrases, verb phrases, sentences) and then establishes the link between the two. Links can be of two different kinds: anaphoric or bridging (definite noun phrases picking up an antecedent via world-knowledge). â¢ Anaphoric links: the annotator is asked to specify whether the anaphor is a repetition, partial repetition, pronoun, epithet (e.g., Andy Warhol â the PopArt artist), or is-a (e.g., Andy Warhol was often hunted by photographers. This fact annoyed especially his dog...). â¢ Bridging links: the annotator is asked to specify the type as part-whole, cause-effect (e.g., She had an accident. The wounds are still healing.), entity-attribute (e.g., She 2001), who determined that in their corpus of German computer tests, 38% of relations were lexically signalled. 7 www.cis.upenn.edu/â¼pdtb/ 8 www.eml-research.de/english/Research/NLP/ Downloads had to buy a new car. The price shocked her.), or same-kind (e.g., Her health insurance paid for the hospital fees, but the automobile insurance did not cover the repair.). 3.1 Retrieval. For displaying and querying the annoated text, we make use of the Annis Linguistic Database developed in our group for a large research effort (âSonderforschungsbereichâ) revolving around 9 2.7 Information structure. information structure. The implementation is In a similar effort, (GÂ¨otze 2003) developed a proposal for the theory-neutral annotation of information structure (IS) â a notoriously difficult area with plenty of conflicting and overlapping terminological conceptions. And indeed, converging on annotation guidelines is even more difficult than it is with co-reference. Like in the co-reference annotation, GÂ¨otzeâs proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet. We use MMAX for this annotation as well. Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such. The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning. Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive). They are also labelled for their topicality (yes / no), and this annotation is accompanied by a confidence value assigned by the annotator (since it is a more subjective matter). Finally, the focus/background partition is annotated, together with the focus question that elicits the corresponding answer. Asking the annotator to also formulate the question is a way of arriving at more reproducible decisions. For all these annotation taks, GÂ¨otze developed a series of questions (essentially a decision tree) designed to lead the annotator to the ap propriate judgement.  Having explained the various layers of annotation in PCC, we now turn to the question what all this might be good for. This concerns on the one hand the basic question of retrieval, i.e. searching for information across the annotation layers (see 3.1). On the other hand, we are interested in the application of rhetorical analysis or âdiscourse parsingâ (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5). basically complete, yet some improvements and extensions are still under way. The web-based Annis imports data in a variety of XML formats and tagsets and displays it in a tier-orientedway (optionally, trees can be drawn more ele gantly in a separate window). Figure 2 shows a screenshot (which is of somewhat limited value, though, as color plays a major role in signalling the different statuses of the information). In the small window on the left, search queries can be entered, here one for an NP that has been annotated on the co-reference layer as bridging. The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) â¢ the full text, â¢ the annotation values for the activated annotation set (co-reference), â¢ the actual annotation tiers, and â¢ the portion of text currently âin focusâ (which also appears underlined in the full text). Different annotations of the same text are mapped into the same data structure, so that search queries can be formulated across annotation levels. Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase. 3.2 Stochastic rhetorical analysis. In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines. Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method. For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%. Future work along these lines will incorporate other layers of annotation, in particular the syntax information. 9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based. rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries. The idea is to have a pipeline of shallow-analysis modules (tagging, chunk- ing, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see Section 2.4) into a knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations. In the rhetorical tree, nuclearity information is then used to extract a âkernel treeâ that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary). Thus we are interested not in extraction, but actual generation from representations that may be developed to different degrees of granularity. In order to evaluate and advance this approach, it helps to feed into the knowledge base data that is already enriched with some of the desired information â as in PCC. That is, we can use the discourse parser on PCC texts, emulating for instance a âco-reference oracleâ that adds the information from our co-reference annotations. The knowledge base then can be tested for its relation-inference capabilities on the basis of full-blown co-reference information. Conversely, we can use the full rhetorical tree from the annotations and tune the co-reference module. The general idea for the knowledge- based part is to have the system use as much information as it can find at its disposal to produce a target representation as specific as possible and as underspecified as necessary. For developing these mechanisms, the possibility to feed in hand-annotated information is very useful. 3.4 Salience-based text generation. Text generation, or at least the two phases of text planning and sentence planning, is a process driven partly by well-motivated choices (e.g., use this lexeme X rather than that more colloquial near-synonym Y ) and partly by con tation like that of PCC can be exploited to look for correlations in particular between syntactic structure, choice of referring expressions, and sentence-internal information structure. A different but supplementary perspective on discourse-based information structure is taken 11ventionalized patterns (e.g., order of informa by one of our partner projects, which is inter tion in news reports). And then there are decisions that systems typically hard-wire, because the linguistic motivation for making them is not well understood yet. Preferences for constituent order (especially in languages with relatively free word order) often belong to this group. Trying to integrate constituent ordering and choice of referring expressions, (Chiarcos 2003) developed a numerical model of salience propagation that captures various factors of authorâs intentions and of information structure for ordering sentences as well as smaller constituents, and picking appropriate referring expressions.10 Chiarcos used the PCC annotations of co-reference and information structure to compute his numerical models for salience projection across the generated texts. 3.5 Improved models of discourse. structure Besides the applications just sketched, the over- arching goal of developing the PCC is to build up an empirical basis for investigating phenomena of discourse structure. One key issue here is to seek a discourse-based model of information structure. Since DaneËsâ proposals of âthematic development patternsâ, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts. (Hartmann 1984), for example, used the term Reliefgebung to characterize the distibution of main and minor information in texts (similar to the notion of nuclearity in RST). (Brandt 1996) extended these ideas toward a conception of kommunikative Gewichtung (âcommunicative-weight assignmentâ). A different notion of information structure, is used in work such as that of (?), who tried to characterize felicitous constituent ordering (theme choice, in particular) that leads to texts presenting information in a natural, âflowingâ way rather than with abrupt shifts of attention. âested in correlations between prosody and dis course structure. A number of PCC commentaries will be read by professional news speakers and prosodic features be annotated, so that the various annotation layers can be set into correspondence with intonation patterns. In focus is in particular the correlation with rhetorical structure, i.e., the question whether specific rhetorical relations â or groups of relations in particular configurations â are signalled by speakers with prosodic means. Besides information structure, the second main goal is to enhance current models of rhetorical structure. As already pointed out in Section 2.4, current theories diverge not only on the number and definition of relations but also on apects of structure, i.e., whether a tree is sufficient as a representational device or general graphs are required (and if so, whether any restrictions can be placed on these graphâs structures â cf. (Webber et al., 2003)). Again, the idea is that having a picture of syntax, co-reference, and sentence-internal information structure at oneâs disposal should aid in finding models of discourse structure that are more explanatory and can be empirically supported.  The PCC is not the result of a funded project. Instead, the designs of the various annotation layers and the actual annotation work are results of a series of diploma theses, of studentsâ work in course projects, and to some extent of paid assistentships. This means that the PCC cannot grow particularly quickly. After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth. Hence we decided to select ten commentaries to form a âcore corpusâ, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence. Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004). 11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here. rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step. The kind of annotation work presented here would clearly benefit from the emergence of standard formats and tag sets, which could lead to sharable resources of larger size. Clearly this poses a number of research challenges, though, such as the applicability of tag sets across different languages. Nonetheless, the prospect of a network of annotated discourse resources seems particularly promising if not only a single annotation layer is used but a whole variety of them, so that a systematic search for correlations between them becomes possible, which in turn can lead to more explanatory models of discourse structure.
 A Bootstrapping Algorithm for Automatically Harvesting Semantic Relations  In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems.  Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic relations, aiming at effectively supporting NLP applications, emphasizing two major points that have been partially neglected by previous systems: generality and weak supervision. From the one side, Espresso is intended as a general-purpose system able to extract a wide variety of binary semantic relations, from the classical is-a and part-of relations, to more specific and domain oriented ones like chemical reactants in a chemistry domain and position succession in political texts. The system architecture is designed with generality in mind, avoiding any relation-specific inference technique. Indeed, for each semantic relation, the system builds specific lexical patterns inferred from textual corpora. From the other side, Espresso requires only weak human supervision. In order to start the extraction process, a user provides only a small set of seed instances of a target relation (e.g. Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible.  To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NPâs part- NP]. This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances. In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)âs part-NP(movie#1)]. While making huge grounds on improving precision/recall, the system requires heavy supervision through manual semantic annotations. Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexicoPOS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its membersâ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora.  The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the relation at hand (e.g., Pablo Picasso is-a artist). Starting from these seeds, the algorithm begins a four-phase loop. In the first phase, the algorithm infers a set of patterns P that captures as many of the seed instances as possible in C. In the second phase, we define a reliability measure to select the best set of patterns P'âP. In phase three, the patterns in P' are used to extract a set of instances I. Finally, in phase four, Espresso scores each instance and then selects the best instances I' as input seeds for the next iteration. The algorithm terminates when a predefined stopping condition is met (for our preliminary experiments, the stopping condition is set according to the size of the corpus). For each induced pattern p and instance i, the information theoretic scores, rÏ(p) and rÎ¹(i) respectively, aim to express their reliability. Below, Sections 3.2â3.5 describe in detail these different phases of Espresso. 3.1. Term definition. Before one can extract relation instances from a corpus, it is necessary to define a tokenization procedure for extracting terms. Terms are commonly defined as surface representations of stable and key domain concepts [19]. Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms. We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor points. Hence, we can capture relations between complex terms, such as ârecord of a criminal convictionâ part-of âFBI reportâ. 3.2. Phase 1: Pattern discovery. The pattern discovery phase takes as input a set of instances I' and produces as output a set of lexical patterns P. For the first iteration I' = Is, the set of initial seeds. In order to induce P, we apply a slight modification to the approach presented in [20]. For each input instance i = {x, y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then generalized into a set of new sentences SGx,y by replacing all terminological expressions by a terminological label (TR). For example: âBecause/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT yâ is generalized as: âBecause/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT yâ All substrings linking terms x and y are then extracted from the set SGx,y, and overall frequencies are computed. The most frequent substrings then represent the set of new patterns P, where the frequency cutoff is experimentally set. Term generalization is particularly useful for small corpora, where generalization is vital to ease the data sparseness. However, the generalized patterns are naturally less precise. Hence, when dealing with bigger corpora, the system allows the use of Sx,yâªSGx,y in order to extract substrings. For our experiments, we used the set SGx,y . 3.3. Phase 2: Pattern filtering. In this phase, Espresso selects among the patterns P those that are most reliable. Intuitively, a reliable pattern is one that is both highly precise and one that extracts many instances. The recall of a pattern p can be approximated by the fraction of input instances in I' that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision). We thus prefer patterns that are highly associated with the input patterns I'. Pointwise mutual information [4] is a commonly used metric for measuring the strength of association between two events x and y: pmi(x, y ) = log P(x, y ) P(x)P(y ) We define the reliability of a pattern p, rÏ(p), as its average strength of association across each input instance i in I', weighted by the reliability of each instance i: â â ââ pmi(i, p) â r (i )â â r ( p ) = iâI â² â max Î¹ â pmi â  Ï I â² where rÎ¹(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum pointwise mutual information between all patterns and all instances. rÏ(p) ranges from [0,1]. The reliability of the manually supplied seed instances are rÎ¹(i) = 1. The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: pmi(i, p) = log x, p, y x,*, y *, p,* where |x, p, y| is the frequency of pattern p instantiated with terms x and y and where the asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual information is biased towards infrequent events. To address this, we multiply pmi(i, p) with the discounting factor suggested in [16]. The set of highest n scoring patterns P', according to rÏ(p), are then selected and retained for the next phase, where n is the number of patterns of the previous iteration incremented by 1. In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was previously discovered. Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than t instances are then discarded, no matter what their score is. In this paper, we experimentally set t to a value dependent on the size of the corpus. In future work, this parameter can be learned using a development corpus. Our reliability measure ensures that overly generic patterns, which may potentially have very low precision, are discarded. However, we are currently exploring a web-expansion algorithm that could both help detect generic patterns and also filter out their incorrect instances. We estimate the precision of the instance set generated by a new pattern p by looking at the number of these instances that are instantiated on the Web by previously accepted patterns. Generic patterns will generate instances with higher Web counts than incorrect patterns. Then, the Web counts can also be used to filter out incorrect instances from the generic patternsâ instantiations. More details are discussed in Section 4.3. 3.4. Phase 3: Instance discovery. In this phase, Espresso retrieves from the corpus the set of instances I that match any of the lexical patterns in P'. In small corpora, the number of extracted instances can be too low to guarantee sufficient statistical evidence for the pattern discovery phase of the next iteration. In such cases, the system enters a web expansion phase, in which new instances for the given patterns are retrieved from the Web, using the Google search engine. Specifically, for each instance iâ I, the system creates a set of queries, using each pattern in P' with its y term instantiated with iâs y term. For example, given the instance âItaly ; countryâ and the pattern [Y such as X] , the resulting Google query will be âcountry such as *â. New instances are then created from the retrieved Web results (e.g. âCanada ; countryâ) and added to I. We are currently exploring filtering mechanisms to avoid retrieving too much noise. Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out. A set of new instances is created for each instance iâ I by extracting sub-terminological expressions from x corresponding to the syntactic head of terms. For example, expanding the relation ânew record of a criminal convictionâ part-of âFBI reportâ, the following new instances are obtained: ânew recordâ part-of âFBI reportâ, and ârecordâ part-of âFBI reportâ. 3.5. Phase 4: Instance filtering. Estimating the reliability of an instance is similar to estimating the reliability of a pattern. Intuitively, a reliable instance is one that is highly associated with as many reliable patterns as possible (i.e., we have more confidence in an instance when multiple reliable patterns instantiate it.) Hence, analogous to our pattern reliability measure in Section 3.3, we define the reliability of an instance i, rÎ¹(i), as: â pmi(i, p) â r (p) r (i) = pâPâ² max pmi Î¹ Pâ² where rÏ(p) is the reliability of pattern p (defined in Section 3.3) and maxpmi is the maximum pointwise mutual information between all patterns and all instances, as in Section 3.3. Espresso finally selects the highest scoring m instances, I', and retains them as input for the subsequent iteration. In this paper, we experimentally set m = 200.  4.1. Experimental Setup. In this section, we present a preliminary comparison of Espresso with two state of the art systems on the task of extracting various semantic relations. 4.1.1. Datasets We perform our experiments using the following two datasets: Â TREC9: This dataset consists of a sample of articles from the Aquaint (TREC9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 â AP890131, AP890201 â AP890228, and AP890310 â AP890319. Â CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2]. We preprocess the corpora using the Alembic Workbench POStagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems: Â RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) Â PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one. To apply this algorithm, both datasets were first analyzed using the Minipar parser [14]. Â ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: Â succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC9 corpus. Â reaction: This relation occurs between chemical elements/molecules that can be combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM corpus. Â production: This relation occurs when a process or element/object produces a result. For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM corpus. For each semantic relation, we manually extracted a set of seed examples. The seeds were used for both Espresso as well as RH021. Table 1 lists a sample of the seeds as well as sample outputs from Espresso. 4.2. Precision and Recall. We implemented each of the three systems outlined in Section 4.1.2 and applied them to the TREC and CHEM datasets. For each output set, per relation, we evaluate the precision of the system by extracting a random sample of instances (50 for the TREC corpus and 20 for the 1 PR04 does not require any seeds.. Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The number in the parentheses for each relation denotes the total number of seeds. E CHEM corpus) and evaluating their quality manually using one human judge2. For each instance, the judge may assign a score of 1 for correct, 0 for incorrect, and Â½ for partially correct. Example instances that were judged partially correct include âanalyst is-a managerâ and âpilot is-a teacherâ. The precision for a given set of relation instances is the sum of the judgeâs scores divided by the number of instances. Although knowing the total number of instances of a particular relation in any nontrivial corpus is impossible, it is possible to compute the recall of a system relative to another systemâs recall. The recall of a system A, RA, is given by the following formula: C R A = C where CA is the number of correct instances of a particular relation extracted by A and C is the total number of correct instances in the corpus. Following [17], we define the relative recall of system A given system B, RA|B, as: RA|B = RA = C A P Ã A = A RB CB PB Ã B Using the precision estimates, PA, from our precision experiments, we can estimate CA â PA Ã |A|, where A is the total number of instances of a particular relation discovered by system A. 2 In future work, we will perform this evaluation using multiple judges in order to obtain confidence bounds and. agreement scores. Table 2. System performance on the is-a relation on the TREC9 dataset. Table 3. System performance on the is-a relation on the CHEM dataset. SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LLâ  SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LLâ  RH 02 5 7 , 5 2 5 2 8 . 0 % 5 . 3 1 RH 02 2 5 5 6 2 5 . 0 % 3 . 7 6 PR 04 1 , 5 0 4 4 7 . 0 % 0 . 2 3 PR 04 1 0 8 4 0 . 0 % 0 . 2 5 ES P 4 , 1 5 4 7 3 . 0 % 1 . 0 0 ES P 2 0 0 8 5 . 0 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances. â  Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances. â  Relative recall is given in relation to ESP. Table 4. System performance on the part-of relation on the TREC9 dataset. Table 5. System performance on the part-of relation on the CHEM dataset. SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LLâ  SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LLâ  RH 02 1 2 , 8 2 8 3 5 . 0 % 4 2 . 5 2 RH 02 1 1 , 5 8 2 3 3 . 8 % 5 8 . 7 8 ES P 1 3 2 8 0 . 0 % 1 . 0 0 ES P 1 1 1 6 0 . 0 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances. â  Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances. â  Relative recall is given in relation to ESP. Table 6. System performance on the succession relation on the TREC9 dataset. Table 7. System performance on the reaction relation on the CHEM dataset. SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LLâ  SYS TE M IN ST AN CE S PR EC ISI ON * RE L RE CA LLâ  RH 02 4 9 , 7 9 8 2 . 0 % 3 6 . 9 6 RH 02 6 , 0 8 3 3 0 % 5 3 . 6 7 ES P 5 5 4 9 . 0 % 1 . 0 0 ES P 4 0 8 5 % 1 . 0 0 * Precision estimated from 50 randomly sampled instances. â  Relative recall is given in relation to ESP. * Precision estimated from 20 randomly sampled instances. â  Relative recall is given in relation to ESP. Tables 2 â 8 reports the total number of instances, precision, and relative recall of each system on the TREC9 and CHEM corpora. The relative recall is always given in relation to the Espresso system. For example, in Table 2, RH02 has a relative recall of 5.31 with Espresso, which means that the RH02 system output 5.31 times more correct relations than Espresso (at a cost of much Table 8. System performance on the production relation on the CHEM dataset. SYSTEM INSTANCES PRECISION* REL RECALLâ  RH02 197 57.5% 0.80 ESP 196 72.5% 1.00 * Precision estimated from 20 randomly sampled instances. â  Relative recall is given in relation to ESP. lower precision). Similarly, PR04 has a relative recall of 0.23 with Espresso, which means that PR04 outputs 4.35 fewer correct relations than Espresso (also with a smaller precision). 4.3. Discussion. Experimental results, for all relations and the two different corpus sizes, show that Espresso greatly outperforms the other two methods on precision. However, Espresso fails to match the recall level of RH02 in all but the experiment on the production relation. Indeed, the filtering of unreliable patterns and instances during the bootstrapping algorithm not only discards the patterns that are unrelated to the actual relation, but also patterns that are too generic and ambiguous â hence resulting in a loss of recall. As underlined in Section 3.2, the ambiguity of generic patterns often introduces much noise in the system (e.g, the pattern [X of Y] can ambiguously refer to a part-of, is-a or possession relation). However, generic patterns, while having low precision, yield a high recall, as also reported by [11]. We ran an experiment on the reaction relation, retaining the generic patterns produced during Espressoâs selection process. As expected, we obtained 1923 instances instead of the 40 reported in Table 7, but precision dropped from 85% to 30%. The challenge, then, is to harness the expressive power of the generic patterns whilst maintaining the precision of Espresso. We propose the following solution that helps both in distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances produced by generic patterns. Unlike Girju et al. [11] that propose a highly supervised machine learning approach based on selectional restriction, ours is an unsupervised method based on statistical evidence obtained from the Web. At a given iteration in Espresso, the intuition behind our solution is that the Web is large enough that correct instances will be instantiated by many of the currently accepted patterns P. Hence, we can distinguish between generic patterns and incorrect patterns by inspecting the relative frequency distribution of their instances using the patterns in P. More formally, given an instance i produced by a generic or incorrect pattern, we count how many times i instantiates on the Web with every pattern in P, using Google. The instance i is then considered correct if its web count surpasses a given threshold. The pattern in question is accepted as a generic pattern if a sufficient number of its instances are considered correct, otherwise it is rejected as an incorrect pattern. Although our results in Section 4.2 do not include this algorithm, we performed a small experiment by adding an a-posteriori generic pattern recovery phase to Espresso. We tested the 7,634 instances extracted by the generic pattern [X of Y] on the CHEM corpus for the part-of relation. We randomly sample 200 of these instances and then queried Google for these instances using the pattern [X consists of Y]. Manual evaluation of the 25 instances that occurred at least once on Google showed 50% precision. Adding these instances to the results from Table 5 decreases the system precision from 60% to 51%, but dramatically increases Espressoâs recall by a factor of 8.16. Furthermore, it is important to note that there are several other generic patterns, like [Xâs Y], from which we expect a similar precision of 50% with a continual increase of recall. This is a very exciting avenue of further investigation.  We proposed a weakly supervised bootstrapping algorithm, called Espresso, for automatically extracting a wide variety of binary semantic relations from raw text. Given a small set of seed instances for a particular relation, the system learns reliable lexical patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances. There are many avenues of future work. Preliminary results show that Espresso generates highly precise relations, but at the expense of lower recall. As mentioned above in Section 4.3, we are working on improving system recall with a web-based method to identify generic patterns and filter their instances. Early results appear very promising. We also plan to investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here that negative instances will play a key role in determining the selectional restriction on generic patterns. Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso, and if these relations can benefit NLP applications such as QA.  The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for evaluating the outputs of the systems.
 Wide-Coverage Semantic Analysis with Boxer  Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts. The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles. The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic. Boxerâs performance on the shared task for comparing semantic represtations was promising. It was able to produce complete DRSs for all seven texts. Manually inspecting the output revealed that: (a) the computed predicate argument structure was generally of high quality, in particular dealing with hard constructions involving control or coordination; (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others arenât; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases. Boxer is distributed with the C&C tools and freely available for research purposes. 277  Boxer is an open-domain tool for computing and reasoning with semantic representations. Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called âboxesâ because of the way they are graphically displayed) for English sentences and texts. There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005).  2.1 Combinatory Categorial Grammar. As a preliminary to semantics, we need syntax. Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001). CCG lends itself extremely well for this task because it is lexically driven and has only few âgrammarâ rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression). Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it. Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001). We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena. 2.2 Discourse Representation Theory. DRT is a formal semantic theory originally designed by Kamp to cope with anaphoric pronouns and temporal relations (Kamp, 1981). DRT uses an explicit intermediate semantic representation, called DRS (Discourse Representation Structure), for dealing with anaphoric or other contextually sensitive linguistic phenomena such as ellipsis and presupposition. We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1. Discourse Representation Structures (DRSs). 2. Underspecified DRSs (DRSs + merge + alfa). 3. Î»-DRSs (UDRSs + lambda + application) DRSs are the representations corresponding to natural language sentences or texts. This is the core DRT language compatible with first-order logic. The DRS language employed by Boxer is a subset of the one found in Kamp and Reyle (1993). We define the syntax of DRSs below with the help of BackusNaur form, where non-terminal symbols are enclosed in angle brackets. The non-terminal <ref> denotes a discourse referent, and <symn> an n-place predicate symbol. <expe > ::= <ref> <expt > ::= <drs> <ref>â <drs> ::= <condition>â <condition> ::= <basic> | <complex> <basic> ::= <sym1 >(<expe >) | <sym2 >(<expe >,<expe >) | <named>(<expe >,<nam>,<sort>) <complex> ::= <expt > | <expt >â<expt > | <expt >â¨<expt > | <ref>:<expt > DRSs are structures comprising two parts: 1) a set of discourse referents; and 2) a set of conditions constraining the interpretation of the discourse referents. Conditions can be simple properties of discourse referents, express relations between them, or be complex, introducing (recursively) subordinated DRSs. The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics (Kamp and Reyle, 1993), where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations. The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet (Kipper et al., 2008), and has some attractive formal properties (Dowty, 1989). There is only one way to state that an individual is participating in an eventânamely by relating it to the event using a binary relation expressing some thematic role. Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear. Finally, it also allows us to characterize the meaning of thematic roles independently of the meaning of the verb that describes the event. We wonât show the standard translation from DRS to FOL here (Blackburn et al., 2001; Bos, 2004; Kamp and Reyle, 1993). Intuitively, translating DRSs into first-order formulas proceeds as follows: each discourse referent is translated as a first-order quantifier, and all DRS-conditions are translated into a conjunctive formula of FOL. Discourse referents usually are translated to existential quantifiers, with the exception of those declared in antecedents of implicational DRS-conditions, that are translated as universal quantifiers. Obviously, negated DRSs are translated as negated formulas, disjunctive DRSs as disjunctive formulas, and implicational DRSs as formulas with material implication. Boxer outputs either resolved semantic representations (in other words, completely disambiguated DRSs), or underspecified representations, where some ambiguities are left unresolved in the semantic representation. This level of representation is referred to as underspecified DRS, or UDRS for short. It is a small extension of the DRS language given in the previous section and is defined as follows: <expt > ::= <udrs> <udrs> ::= <drs> | (<expt >;<expt >) | (<expt >Î±<expt >) Note here that expressions of type t are redefined as UDRSs. UDRSs are either ordinarly DRSs, DRSs conjoined by the merge (for which we use the semicolon), or NP/N: A N/N: record N: date Î»q.Î»p.( x ;q@x;p@x) Î»p.Î»x.( y record(y) nn(y,x) ;p@x) Î»x. date(x) [fa] N: record date y Î»x.( record(y) nn(y,x) ; ) date(x) . . . [merge] y Î»x. record(y) nn(y,x) date(x) [fa] NP: A record date y Î»p.( x ; record(y) nn(y,x) date(x) ;p@x) . . . [merge] x y Î»p. record(y) nn(y,x) date(x) ;p@x Figure 1: Derivation with Î»-DRSs, including Î²-conversion, for âA record dateâ. Combinatory rules are indicated by solid lines, semantic rules by dotted lines. DRS composed by the Î±-operator. The merge conjoins two DRSs into a larger DRS â semantically the merge is interpretated as (dynamic) logical conjunction. Merge- reduction is the process of eliminating the merge operation by forming a new DRS resulting from the union of the domains and conditions of the argument DRSso of a merge, respectively (obeying certain constraints). Figure 1 illustrates the syntax- semantics interface (and merge-reduction) for a derivation of a simple noun phrase. Boxer adopts Van der Sandtâs view as presupposition as anaphora (Van der Sandt, 1992), in which presuppositional expressions are either resolved to previously established discourse entities or accommodated on a suitable level of discourse. Van der Sandtâs proposal is cast in DRT, and therefore relatively easy to integrate in Boxerâs semantic formalism. The Î±-operator indicates information that has to be resolved in the context, and is lexically introduced by anaphoric or presuppositional expressions. A DRS constructed with Î± resembles the protoDRS of Van der Sandtâs theory of presupposition (Van der Sandt, 1992) although they are syntactically defined in a slightly different way to overcome problems with free and bound variables, following Bos (2003). Note that the difference between anaphora and presupposition collapses in Van der Sandtâs theory. The types are the ingredients of a typed lambda calculus that is employed to construct DRSs in a bottom-up fashion, compositional way. The language of lambda DRSs is an extension of the language of (U)DRS defined before: <expe > ::= <ref> | <vare > <expt > ::= <udrs> | <vart > <expÎ± > ::= (<exp(Î²,Î±)> @ <varÎ² >) | <varÎ± > <exp(Î±,Î²)> ::= Î»<varÎ± >.<expÎ² > | <var(Î±,Î²)> Hence we define discourse referents as expressions of type e, and DRSs as expressions of type t . We use @ to indicate function application, and the Î»-operator to bind free variables over which we wish to abstract.  3.1 Preprocessing. The input text needs to be tokenised with one sentence per line. In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007). The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents. An example of a CCG derivation is shown in Figure 2. a virus --[lex] --[lex] by np:nb/n n ---------------------[lex] -----------[fa] Cervical cancer caused ((s:pss\np)\(s:pss\np))/np np:nb ---[lex] --[lex] ---[lex] --------------------------------------[fa] n/n n is s:pss\np (s:pss\np)\(s:pss\np) ------------[fa] ----------------[lex] -----------------------------------------------[ba] n (s:dcl\np)/(s:pss\np) s:pss\np ------------[tc] ---------------------------------------------------------------------[fa] np s:dcl\np --------------------------------------------------------------------------------------[ba] s:dcl Figure 2: CCG derivation as generated by the C&C tools 3.2 Lexicon. In CCG, the syntactic lexicon comprises the set of lexical categories. CCGbank hosts more than a thousand different categories. The semantic lexicon defines a suitable mapping from categories to semantic representations. In the context of Boxer, these semantic representations are defined in the shape of lambda-DRSs. Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency. Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations. So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category. For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category. Although there is a one-to-one mapping between the CCG categories and semantic types â and this must be the case to ensure the semantic composition process proceeds without type clashes â the actual instantiations of a semantic type can differ even within the scope of a single CCG category. For example, the category n/n can correspond to an adjective, a cardinal expression, or even common nouns and proper names (in the compound expressions). In the latter two cases the lexical entry introduces a new discourse referent, in the former two it does not. To account for this difference we also need to look at the part of speech that is assigned to a token. 3.3 Resolution. Boxer implements various presupposition triggers introduced by noun phrases, including personal pronouns, possessive pronouns, reflexive pronouns, emphasising pronouns, demonstrative pronouns, proper names, other-anaphora, definite descriptions. In addition, some aspects of tense are implemented as presupposition triggers, too. Anaphora and presupposition resolution takes place in a separate stage after building up the representation, following the resolution algorithm outlined in Bos (2003). The current implementation of Boxer aims at high precision in resolution: personal pronouns are only attempted to be resolved to named entities, definite descriptions and proper names are only linked to previous discourse referents if there is overlap in the DRS-conditions of the antencedent DRS and alpha-DRS. If no suitable antecedent can be found, global accommodation of the anaphoric discourse referent and conditions will take palce. Because Boxer has the option to output unresolved DRSs too, it is possible to include external anaphora or coreference resolution components. 3.4 Example Analysis. We illustrate the capabilities of Boxer with the following example text shown below (aka as Text 2 of the shared task).1 The text consists of three sentences, the second being a coordinated sentence. It contains a passive construction, three pronouns, relative clauses, control verbs, and a presupposition trigger other. Text 2 Cervical cancer is caused by a virus. That has been known for some time and it has led to a vaccine that seems to prevent it. Researchers have been looking for other cancers that may be caused by viruses. The output of Boxer for this text is shown in Figure 3. Only the box format is shown here â Boxer is also able to output the DRSs in Prolog or XML encodings. It was run without analysing tense and aspect and without discourse segmentation (both of these are possible in Boxer, but still undergo development, and are therefore disregarded here). As we can see from the example and Boxerâs analysis various things go right and various things go wrong. Boxer deals fine with the passive construction (assigned the 1 This text was taken from the Economist Volume 387 Number 8582, page 92. The third sentence has been simplified. appropriate semantic role), the relative clauses, and the control construction (vaccine is the agent of the prevent event). It also handles the presupposition trigger anaphorically linking the mention of other cancers in the third sentence with the phrase cervical cancer in the first sentence, and asserting an inequality condition in the DRS. Boxer failed to resolve three pronouns correctly. These are all accommodated at the global level of DRS, which is the DRS on the left-hand side in Figure 3. All of the pronouns have textual antecedents: the abstract pronoun that in the second sentence refers to the fact declared in the first sentence. The first occurrence of it in the second sentence also seems to refer to this fact â the second occurrence of it refers to cervical cancer mentioned in the first sentence. bin/boxer --input working/step/text2.ccg --semantics drs --box --resolve --roles verbnet --format no %%% %%% | x0 x1 x2 | | x3 x4 x5 | | x6 x7 | | x8 x9 x10 x11 | | x13 x14 x15 x16 x17 | %%% |------------| |--------------| |--------------| |------------------------| |---------------------| %%% (| thing(x0) |+(| cancer(x3) |+(| know(x6) |+(| lead(x8) |+| researcher(x13) |)))) %%% | neuter(x1) | | cervical(x3) | | time(x7) | | vaccine(x9) | | look(x14) | %%% | neuter(x2) | | cause(x4) | | event(x6) | | seem(x10) | | agent(x14,x13) | %%% | | | virus(x5) | | theme(x6,x0) | | proposition(x11) | | cancer(x15) | %%% | event(x4) | | for(x6,x7) | | event(x10) | | | %%% | theme(x4,x3) | | | | event(x8) | | | | | %%% | by(x4,x5) | | agent(x8,x1) | | |----------| | %%% | | | agent(x10,x9) | | | | x15 = x3 | | %%% | theme(x10,x11) | | | | | %%% | to(x8,x9) | | cause(x16) | %%% | | | virus(x17) | %%% | | x12 | | | event(x16) | %%% | x11:|---------------| | | theme(x16,x15) | %%% | | prevent(x12) | | | by(x16,x17) | %%% | | event(x12) | | | for(x14,x15) | %%% | | agent(x12,x9) | | | event(x14) | %%% | | theme(x12,x2) | | | | %%% | | | | %%% | | Attempted: 3. Completed: 3 (100.00%). Figure 3: Boxer output for Shared Task Text 2  Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008). Boxer was able to produce semantic representation for all text without any further modifications to the software. For each text we briefly say what was good and bad about Boxerâs analysis. (We wonât comment on the performance on the second text, as this is the text proposed by ourselves and already discussed in the previous section.) Text 1: An object is thrown with a horizontal speed ... Good: The resulting predicate argument structure was fine overall, including a difficult control construction (âhow long does it take the object to fall ...â). The definite description âthe objectâ was correctly resolved. The conditional got correctly anal- ysed. Bad: The measure phrase â125 m highâ got misinterpreted as noun-noun comn- pound. The definite description âthe fallâ was not linked to the falling event mentioned before. Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions. Text 3: John went into a restaurant ... Good: The pronouns were correctly resolved to the proper name âJohnâ rather than âthe waiterâ, even though this is based on the simple strategy in Boxer to link third- person pronouns to named entities of type human. The coordination construction âwarm and friendlyâ got correctly analysed (distributively), and the control construction âbegan to read his bookâ received a proper predicate argument structure. Bad: Boxer doesnât deal with bridging references introduced by relational nouns, so expressions like âthe cornerâ were not linked to other discourse entities. Text 4: The first school for the training of leader dogs ... Good: The named entities were correctly recognised and classified (locations and proper names). The VP coordination in the first and later sentences was correctly analysed. The expression âthis schoolâ got correctly linked to the schhol mentioned earlier in the text. The time expression â1999â got the right interpretation. Bad: The adjectives/determiners âfirstâ and âseveralâ didnât receive a deep analysis. The complex NP âJoao Pedro Fonseca and Marta Gomesâ was distributively interpreted, rather than collective. The pronoun âtheyâ wasnât resolved. The preposition âInâ starting the second sentence was incorrectly analysed by the parser. Text 5: As the 3 guns of Turret 2 were being loaded ... Good: The discourse structures invoked by the sentence initial adverbs âAsâ and âWhenâ was correctly computed. Predicate argument structure overall good, including treatment of the relative clauses. The expression âthe propellantâ was correctly resolved. Time expressions in the one but last sentence got a correct analysis. Bad: The name âTurret 2â was incorrectly analysed (not as a compound). The adverbs âyetâ and âthenâ got a shallow analysis. The first-person pronoun âIâ was not resolved to the crewman. Comments: The quotes were removed in the tokenisation phase, because the C&C parser, being trained on a corpus without quotes, performs badly on texts containing quotes. Text 6: Amid the tightly packed row houses of North Philadelphia ... Good: The named entities were correctly recognised and classified as locations. The various cases of VP coordination all got properly analysed. The numerical and date expressions got correct representations. Bad: The occurrences of the third-person neuter pronouns were not resolved. The preposition âAmidâ was not correctly analysed. Text 7: Modern development of wind-energy technology and applications ... Good: Correct interpretation of time expressions â1930sâ and â1970sâ. Correct pred icate argument structure overall. Bad: âModernâ was recognised as a proper name. The noun phrase âwind-energy technology and applicationsâ was distributively analysed with âwind-energyâ only applying to âtechnologyâ. The sentence-initial adverb âSinceâ did not introduce proper discourse structure. The units of measurement in the last two sentences were not recognised as such. The tricky time expression âmid-80âsâ only got a shallow interpretation.  Boxer is a wide-coverage system for semantic interpretation. It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic. The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis. Boxerâs performance on the shared task for comparing semantic represtations was promising. It was able to produce DRSs for all texts. We canât quantify the quality of Boxerâs output, as we donât have gold standard representations at our disposal. Manually inspecting the output gives us the following impression: â¢ computed predicate argument structure is generally of good quality, including hard constructions involving control or coordination; â¢ discourse structure triggered by conditionals, negation or discourse adverbs is overall correctly computed; â¢ some measure and time expressions are correctly analysed, others arenât; â¢ several shallow analyses are given for lexical phrases that require deep analysis; â¢ bridging references and pronouns are not resolved in most cases; but when they are, they are mostly correctly resolved (high precision at the cost of recall). Finally, a comment on availability of Boxer. All sources of Boxer are available for download and free of noncommercial use. It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer
  For developing a data-driven text rewriting algorithm for paraphrasing, it is essential to have a monolingual corpus of aligned paraphrased sentences. News article headlines are a rich source of paraphrases; they tend to describe the same event in various different ways, and can easily be obtained from the web. We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases, one based on clustering, and the other on pairwise similarity-based matching. We show that the latter performs best on the task of aligning paraphrastic headlines.  In recent years, text-to-text generation has received increasing attention in the field of Natural Language Generation (NLG). In contrast to traditional concept-to-text systems, text-to-text generation systems convert source text to target text, where typically the source and target text share the same meaning to some extent. Applications of text-to-text generation include sum- marization (Knight and Marcu, 2002), question- answering (Lin and Pantel, 2001), and machine translation. For text-to-text generation it is important to know which words and phrases are semantically close or exchangable in which contexts. While there are various resources available that capture such knowledge at the word level (e.g., synset knowledge in WordNet), this kind of information is much harder to get by at the phrase level. Therefore, paraphrase acquisition can be considered an important technology for producing resources for text-to-text generation. Paraphrase generation has already proven to be valuable for Question Answering (Lin and Pantel, 2001; Riezler et al., 2007), Machine Translation (CallisonBurch et al., 2006) and the evaluation thereof (RussoLassner et al., 2006; Kauchak and Barzilay, 2006; Zhou et al., 2006), but also for text simplification and explanation. In the study described in this paper, we make an effort to collect Dutch paraphrases from news article headlines in an unsupervised way to be used in future paraphrase generation. News article headlines are abundant on the web, and are already grouped by news aggregators such as Google News. These services collect multiple articles covering the same event. Crawling such news aggregators is an effective way of collecting related articles which can straightforwardly be used for the acquisition of paraphrases (Dolan et al., 2004; Nelken and Shieber, 2006). We use this method to collect a large amount of aligned paraphrases in an automatic fashion.  We aim to build a high-quality paraphrase corpus. Considering the fact that this corpus will be the basic resource of a paraphrase generation system, we need it to be as free of errors as possible, because errors will propagate throughout the system. This implies that we focus on obtaining a high precision in the paraphrases collection process. Where previous work has focused on aligning news-items at the paragraph and sentence level (Barzilay and Elhadad, 2003), we choose to focus on aligning the headlines of news articles. We think this approach will enable us to harvest reliable training material for paraphrase generation quickly and efficiently, without having to worry too much about the problems that arise when trying to align complete news articles. For the development of our system we use data which was obtained in the DAESO-project. This project is an ongoing effort to build a Parallel Monolingual Treebank for Dutch (Marsi Proceedings of the 12th European Workshop on Natural Language Generation, pages 122â125, Athens, Greece, 30 â 31 March 2009. Qc 2009 Association for Computational Linguistics document, and each original cluster as a collection of documents. For each stemmed word i in sentence j, T Fi,j is a binary variable indicating if the word occurs in the sentence or not. The T F âI DF score is then: TF.IDFi = T Fi,j Â· log | Table 1: Part of a sample headline cluster, with sub-clusters and Krahmer, 2007) and will be made available through the Dutch HLT Agency. Part of the data in the DAESO-corpus consists of headline clusters crawled from Google News Netherlands in the period AprilâAugust 2006. For each news article, the headline and the first 150 characters of the article were stored. Roughly 13,000 clusters were retrieved. Table 1 shows part of a (translated) cluster. It is clear that although clusters deal roughly with one subject, the headlines can represent quite a different perspective on the content of the article. To obtain only paraphrase pairs, the clusters need to be more coherent. To that end 865 clusters were manually subdivided into sub-clusters of headlines that show clear semantic overlap. Sub- clustering is no trivial task, however. Some sentences are very clearly paraphrases, but consider for instance the last two sentences in the example. They do paraphrase each other to some extent, but their relation can only be understood properly with |{dj : ti â dj }| |D| is the total number of sentences in the cluster and |{dj : ti â dj }| is the number of sen tences that contain the term ti. These scores are used in a vector space representation. The similarity between headlines can be calculated by using a similarity function on the headline vectors, such as cosine similarity. 2.1 Clustering. Our first approach is to use a clustering algorithm to cluster similar headlines. The original Google News headline clusters are reclustered into finer grained sub-clusters. We use the k-means implementation in the CLUTO1 software package. The k-means algorithm is an algorithm that assigns k centers to represent the clustering of n points (k < n) in a vector space. The total intra-cluster variances is minimized by the function k V = (xj â Âµi)2 i=1 xj âSi where Âµi is the centroid of all the points xj â Si.The PK1 cluster-stopping algorithm as pro posed by Pedersen and Kulkarni (2006) is used to find the optimal k for each sub-cluster: C r(k) â mean(C r[1...âK ]) world knowledge. Also, there are numerous headlines that can not be sub-clustered, such as the first P K 1(k) = std(C r[1...âK ]) three headlines shown in the example. We use these annotated clusters as development and test data in developing a method to automatically obtain paraphrase pairs from headline clusters. We divide the annotated headline clusters in a development set of 40 clusters, while the remainder is used as test data. The headlines are stemmed using the porter stemmer for Dutch (Kraaij and Pohlmann, 1994). Instead of a word overlap measure as used byHere, C r is a criterion function, which mea sures the ratio of withincluster similarity to betweencluster similarity. As soon as P K 1(k) ex ceeds a threshold, k â 1 is selected as the optimum number of clusters. To find the optimal threshold value for cluster- stopping, optimization is performed on the development data. Our optimization function is an F - score: (1 + Î²2) Â· (precision Â· recall) Barzilay and Elhadad (2003), we use a modified FÎ² = (Î²2 precision + recall) T F âI DF word score as was suggested by Nelken Â· and Shieber (2006). Each sentence is viewed as a 1 http://glaros.dtc.umn.edu/gkhome/views/cluto/ We evaluate the number of aligments between possible paraphrases. For instance, in a cluster of four sentences, 4) = 6 alignments can be made. In our case, precision is the number of alignments retrieved from the clusters which are relevant, divided by the total number of retrieved alignments. Recall is the number of relevant retrieved aligments divided by the total number of relevant alignments. We use an FÎ² -score with a Î² of 0.25 as we favour precision over recall. We do not want to optimize on precision alone, because we still want to retrieve a fair amount of paraphrases and not only the ones that are very similar. Through optimization on our development set, we find an optimal threshold for the PK1 algorithm thpk1 = 1. For each original cluster, k-means clustering is then performed using the k found by the cluster stopping function. In each newly obtained cluster all headlines can be aligned to each other. 2.2 Pairwise similarity. Our second approach is to calculate the similarity between pairs of headlines directly. If the similarity exceeds a certain threshold, the pair is accepted as a paraphrase pair. If it is below the threshold, it is rejected. However, as Barzilay and Elhadad (2003) have pointed out, sentence mapping in this way is only effective to a certain extent. Beyond that point, context is needed. With this in mind, we adopt two thresholds and the Cosine similarity function to calculate the similarity between two sentences: cos(Î¸) = V 1 Â· V 2 V 1 V 2 where V 1 and V 2 are the vectors of the two sentences being compared. If the similarity is higher than the upper threshold, it is accepted. If it is lower than the lower theshold, it is rejected. In the remaining case of a similarity between the two thresholds, similarity is calculated over the contexts of the two headlines, namely the text snippet that was retrieved with the headline. If this similarity exceeds the upper threshold, it is accepted. Threshold values as found by optimizing on the development data using again an F0.25-score, are T hlower = 0.2 and T hupper = 0.5. An optional final step is to add alignments that are implied by previous alignments. For instance, if headline A is paired with headline B, and headline B is aligned to headline C , headline A can be aligned to C as Ty pe Precision Recallk m ea ns cl us ter in g 0.91 0.43 clu ste rs on lyk m ea ns cl us ter in g 0.66 0.44 all he ad lin es pa irw ise si mi lar ity 0.93 0.39 clu ste rs on ly pa irw ise si mi lar ity 0.76 0.41 all he ad lin es Table 2: Precision and Recall for both methods Pl ay st ati on 3 m or e ex pe nsi ve th an co m pe tit or P l a y s t a t i o n 3 w i l l b e c o m e m o r e e x p e n s i v e t h a n X b o x 3 6 0 So ny po stp on es Blu Ra y m ov ie s So ny po stp on es co mi ng of blu ra y dv ds Pri ce s Pl ay st ati on 3 kn ow n: fro m 49 9 eu ro s E3 20 06 : Pl ay st ati on 3 fro m 49 9 eu ro s So ny PS 3 wi th Blu R ay for sal e fro m No ve m be r 11 th PS 3 av ail abl e in Eu ro pe fro m No ve m be r 17 th Table 3: Examples of correct (above) and incorrect (below) alignments well. We do not add these alignments, because in particular in large clusters when one wrong alignment is made, this process chains together a large amount of incorrect alignments.  The 825 clusters in the test set contain 1,751 sub- clusters in total. In these sub-clusters, there are 6,685 clustered headlines. Another 3,123 headlines remain unclustered. Table 2 displays the paraphrase detection precision and recall of our two approaches. It is clear that k-means clustering performs well when all unclustered headlines are artificially ignored. In the more realistic case when there are also items that cannot be clustered, the pairwise calculation of similarity with a back off strategy of using context performs better when we aim for higher precision. Some examples of correct and incorrect alignments are given in Table 3.  Using headlines of news articles clustered by Google News, and finding good paraphrases within these clusters is an effective route for obtaining pairs of paraphrased sentences with reasonable precision. We have shown that a cosine similarity function comparing headlines and using a back off strategy to compare context can be used to extract paraphrase pairs at a precision of 0.76. Although we could aim for a higher precision by assigning higher values to the thresholds, we still want some recall and variation in our paraphrases. Of course the coverage of our method is still somewhat limited: only paraphrases that have some words in common will be extracted. This is not a bad thing: we are particularly interested in extracting paraphrase patterns at the constituent level. These alignments can be made with existing alignment tools such as the GIZA++ toolkit. We measure the performance of our approaches by comparing to human annotation of sub- clusterings. The human task in itself is hard. For instance, is we look at the incorrect examples in Table 3, the difficulty of distinguishing between paraphrases and non-paraphrases is apparent. In future research we would like to investigate the task of judging paraphrases. The next step we would like to take towards automatic paraphrase generation, is to identify the differences between paraphrases at the constituent level. This task has in fact been performed by human annotators in the DAESO-project. A logical next step would be to learn to align the different constituents on our extracted paraphrases in an unsupervised way.  Thanks are due to the Netherlands Organization for Scientific Research (NWO) and to the Dutch HLT Stevin programme. Thanks also to Wauter Bosma for originally mining the headlines from Google News. For more information on DAESO, please visit daeso.uvt.nl.
 Identification and Treatment of Multiword Expressions applied to Information Retrieval  The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.  One of the motivations of this work is to investigate if the identification and appropriate treatment of Multiword Expressions (MWEs) in an application contributes to improve results and ultimately lead to more precise man-machine interaction. The term âmultiword expressionâ has been used to describe a large set of distinct constructions, for instance support verbs, noun compounds, institutionalized phrases and so on. Calzolari et al. (2002) defines MWEs as a sequence of words that acts as a single unit at some level of linguistic analysis. The nature of MWEs can be quite heterogeneous and each of the different classes has specific characteristics, posing a challenge to the implementation of mechanisms that provide unified treatment for them. For instance, even if a standard system capable of identifying boundaries between words, i.e. a tokenizer, may nevertheless be incapable of recognizing a sequence of words as an MWEs and treating them as a single unit if necessary (e.g. to kick the bucket meaning to die). For an NLP application to be effective, it requires mechanisms that are able to identify MWEs, handle them and make use of them in a meaningful way (Sag et al., 2002; Baldwin et al., 2003). It is estimated that the number of MWEs in the lexicon of a native speaker of a language has the same order of magnitude as the number of single words (Jackendoff, 1997). However, these ratios are probably underestimated when considering domain specific language, in which the specialized vocabulary and terminology are composed mostly by MWEs. In this paper, we perform an application-oriented evaluation of the inclusion of MWE treatment into an Information Retrieval (IR) system. IR systems aim to provide users with quick access to data they are interested (BaezaYates and RibeiroNeto, 1999). Although language processing is not vital to modern IR systems, it may be convenient (Sparck Jones, 1997) and in this scenario, NLP techniques may contribute in the selection of MWEs for indexing as single units in the IR system. The selection of appropriate indexing terms is a key factor for the quality of IR systems. In an ideal system, the index terms should correspond to the concepts found in the documents. If indexing is performed only with the atomic terms, there may be a loss of semantic content of the documents. For example, if the query was pop star meaning celebrity, and the terms were indexed individually, the relevant documents may not be retrieved and the system would 101 Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101â109, Portland, Oregon, USA, 23 June 2011. Qc 2011 Association for Computational Linguistics return instead irrelevant documents about celestial bodies or carbonated drinks. In order to investigate the effects of indexing of MWEs for IR, the results of queries are analyzed using IR quality metrics. This paper is structured as follows: in section 2 we discuss briefly MWEs and some of the challenges they represent. This is followed in section 3 by a discussion of the materials and methods employed in this paper, and in section 4 of the evaluation performed. We finish with some conclusions and future work.  The concept of Multiword Expression has been widely viewed as a sequence of words that acts as a single unit at some level of linguistic analysis (Calzolari et al., 2002), or as Idiosyncratic interpretations that cross word boundaries (or spaces) (Sag et al., 2002). One of the great challenges of NLP is the identification of such expressions, âhiddenâ in texts of various genres. The difficulties encountered for identifying Multiword Expressions arise for reasons like: â¢ the difficulty to find the boundaries of a multi- word, because the number of component words may vary, or they may not always occur in a canonical sequence (e.g. rock the boat, rock the seemingly intransigent boat and the bourgeois boat was rocked); â¢ even some of the core components of an MWE may present some variation (e.g. throw NP to the lions/wolves/dogs/?birds/?butterflies); â¢ in a multilingual perspective, MWEs of a source language are often not equivalent to their word-by-word translation in the target language (e.g. guardachuva in Portuguese as umbrella in English and not as ?store rain). The automatic discovery of specific types of MWEs has attracted the attention of many researchers in NLP over the past years. With the recent increase in efficiency and accuracy of techniques for preprocessing texts, such as tagging and parsing, these can become an aid in improving the performance of MWE detection techniques. In terms of practical MWE identification systems, a well known approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. This approach is implemented in a lexico- graphic tool called Xtract. More recently there has been the release of the mwetoolkit (Ramisch et al., 2010) for the automatic extraction of MWEs from monolingual corpora, that both generates and validates MWE candidates. As generation is based on surface forms, for the validation, a series of criteria for removing noise are provided, including some (language independent) association measures such as mutual information, dice coefficient and maximum likelihood. Several other researchers have proposed a number of computational techniques that deal with the discovery of MWEs: Baldwin and Villavicencio (2002) for verb-particle constructions, Pearce (2002) and Evert and Krenn (2005) for collocations, Nicholson and Baldwin (2006) for compound nouns and many others. For our experiments, we used some standard statistical measures such as mutual information, point- wise mutual information, chi-square, permutation entropy (Zhang et al., 2006), dice coefficient, and t-test to extract MWEs from a collection of documents (i.e. we consider the collection of documents indexed by the IR system as our corpus).  Based on the hypothesis that the MWEs can improve the results of IR systems, we carried out an evaluation experiment. The goal of our evaluation is to detect differences between the quality of the standard IR system, without any treatment for MWEs, and the same system improved with the identification of MWEs in the queries and in the documents. In this section we describe the different resources and methods used in the experiments. 3.1 Resources and Tools. For this evaluation we used two large newspaper corpora, containing a high diversity of terms: â¢ Los Angeles Times (Los Angeles, USA1994) â¢ The Herald (Glasgow, Scotland1995) Together, both corpora cover a large set of subjects present in the news published by these newspa pers in the years listed. The language used is American English, in the case of the Los Angeles Times and British English, in the case of The Herald. Hereafter, the corpus of the Los Angeles Times will be referred as LA94 and The Herald as GH95. Together, they contain over 160,000 news articles (Table 1) and each news article is considered as a document. C or pu s D oc u m en ts L A 9 4 1 1 0 . 2 4 5 G H 9 5 5 6 . 4 7 2 T o t a l 1 6 6 . 7 1 7 Table 1: Total documents The collection of documents, as well as the query topics and the list of relevance judgments (which will be discussed afterwards), were prepared in the context of the CLEF 2008 (Cross Language Evaluation Forum), for the task entitled Robust-WSD (Acosta et al., 2008). This task aimed to explore the contribution of the disambiguation of words to bilingual or monolingual IR. The task was to assess the validity of word-sense disambiguation for IR. Thus, the documents in the corpus have been annotated by a disambiguation system. The structure of a document contains information about the identifier of a term in a document (TERM ID), the lemma of a term (LEMA) and also its morphosyntactic tag (POS). In addition, it contains the form in which the term appeared in the text (WF) and information of the term in the WordNet (Miller, 1995; Fellbaum, 1998) as SYNSET SCORE and CODE, both not used for <TERM ID="GH950102000000-126" LEMA="underworld" POS="NN"> <WF>underworld</WF> <SYNSET SCORE="0.5" CODE="06120171-n"/> <SYNSET SCORE="0.5" CODE="06327598-n"/> </TERM> Figure 1: Structure of a term in the original documents In this paper, we extracted the terms located in the LEMA attribute, in other words, in their canonical form (e.g. letter bomb for letter bombs). The use of lemmas and not the words (e.g. write for wrote, written, etc.) to the formation of the corpus, avoids linguistic variations that can affect the results of the experiments. As a results, our documents were formed only by lemmas and the next step is the indexing of documents using an IR system. For this task we used a tool called Zettair (Zettair, 2008), which is a compact textual search engine that can be used both for the indexing and for querying text collections. Porterâs Stemmer (Porter, 1997) as implemented in Zettair was also used. Stemming can provide further conflation of related terms. For example, bomb and bombing were not merged in the lemmatized texts but after stemming they are conflated to a single representation. After indexing, the next step is the preparation of the query topics. Just as the corpus, only the lemmas of the query topics were extracted and used. The test collection has a total of 310 query topics. The judgment of whether a document is relevant to a query was assigned according to a list of relevant documents, manually prepared and supplied with the material provided by CLEF. We used Zettair to generate the ranked list of documents retrieved in response to each query. For each query topic, the 1,000 top scoring documents were selected. We used the cosine metric to calculate the scores and rank the documents. Finally, to calculate the retrieval evaluation metrics (detailed in Section 3.5) we used the tool trec eval. This tool compares the list of retrieved documents (obtained from Zettair) against the list of relevant documents (provided by CLEF). 3.2 Multiword Expression as Single Terms. In this work, we focused on MWEs composed of exactly two words (i.e. bigrams). In order to incorporate MWEs as units for the IR system to index, we adopted a very simple heuristics that concatenated together all terms composing an MWE using â â (e.g. letter bomb as letter bomb). Figure 2 exemplifies this concatenation. Each bigram present in a predefined dictionary and occurring in a document is treated as a single term, for indexing and retrieval purposes. The rationale was that documents containing specific MWEs can be indexed more adequately than those containing the words of the expression separately. As a result, retrieval quality should increase. <TERM ID="GH950102000000-126" LEMA="underworld" POS="NN"> <WF>underworld</WF> <SYNSET SCORE="0.5" CODE="06120171-n"/> <SYNSET SCORE="0.5" CODE="06327598-n"/> </TERM> Original Topic: - What was the role of the Hubble telescope in proving the existence of black holes? Modified Topic: - what be the role of the hubble telescope in prove the existence of black hole ? black_hole <num>141</num> Figure 2: Modified query. <title> letter bomb for kiesbauer find information on the explosion of a letter bomb in the studio of the tv channel pro7 presenter arabella kiesbauer . 3le.t3ter_bMombullettitewr_obormdbEtv_xcpharnensesl ions Dictionaries </title> In order to determine the impact of the quality of the dictionary used in the performance of the IR system, we examined several different sources of MWE of varying quality. The dictionaries containing the MWEs to be inserted into the corpus as a single term, are created by a number of techniques involving automatic and manual extraction. Below we describe how these MWE dictionaries were created. â¢ Compound Nouns (CN) - for the creation of this dictionary, we extracted all bigrams contained in the corpus. Since the number of available bigrams was very large (99,744,811 bi- grams) we filtered them using the information in the original documents, the morphosyntactic tags. Along with the LEMA field, extracted in the previous procedure, we also extracted the value of the field POS (part-of-speech). In order to make the experiment feasible, we used only bigrams formed by compound nouns, in other words, when the POS of both words was NN (Noun). Thus, with bigrams consisting of sequences of NN as a preprocessing step to eliminate noise that could affect the experiment, the number of bigrams with MWE candidates was reduced to 308,871. The next step was the selection of bigrams that had the highest frequency in the text, so we chose candidates occurring at least ten times in the whole corpus. As a result, the first list of MWEs was composed by 15,001 bigrams, called D1. â¢ Best Compound Nouns - after D1, we refined the list with the use of statistical methods. The methods used were the mutual information and chi-square. It was necessary to obtain frequency values from Web using the search tool Yahoo!, because despite the number of terms per genre of our corpus would bias the counts. For this work we used the number of pages in which a term occurs as a measure of frequency. With the association measures based on web frequencies, we generated a ranking in decreasing order of score for each entry. We merged the rankings by calculating the average rank between the positions of each MWE; the first 7,500 entries composed the second dictionary, called D2. â¢ Worst Compound Nouns - this dictionary was created from bigrams that have between five and nine occurrences and are more likely to co- occur by chance. It was created in order to evaluate whether the choice of the potentially more noisy MWEs entailed a negative effect in the results of IR, compared to the previous dictionaries. The third dictionary, with 17,328 bi- grams, is called D3. â¢ Gold Standard - this was created from a sub- list of the Cambridge International Dictionary of English (Procter, 1995), containing MWEs. Since this list contains all types of MWEs, it was necessary to further filter these to obtain compound nouns only, using morphosyn- tactic information obtained by the TreeTagger (Schmid, 1994), which for English is reported to have an accuracy of 96.36%â (Schmid, 1994). Formed by 568 MWEs, the fourth dictionary will be called D4. â¢ Decision Tree - created from the use of the J48 algorithm (Witten and Frank, 2000) from Weka (Hall et al., 2009), a data mining tool. With this algorithm it is possible to make a MWE classifier in terms of a decision tree. This requires providing training data with true and false examples of MWE. The training set contained 1,136 instances, half true (D4) and half false MWEs (taken from D3). After combining several statistical methods, the best result for classification was obtained with the use of mutual information, chi-square, pointwise mutual information, and Dice. The model obtained from Weka was applied to test data containing15,001 MWE candidates (D1). The 12,782 bi tionary, called D5. â¢ Manual - for comparative purposes, we also #Relevant P recision(P ) = n #Retrieved created two dictionaries by manually evaluating the text of the 310 query topics. The first dictionary contained all bigrams which would achieve a different meaning if the words were concatenated (e.g. space shuttle). This dictio #Retrieved #Relevant n #Retrieved Recall(R) = #Relevant (1) (2) nary, was called D6 and contains 254 expressions. The other one was created by a specialist (linguist) who classified as true or false a list of MWE candidates from the query topics. The linguist selection of MWEs formed D7 with 178 bigrams. 3.4 Creating Indices. For the experiments, we needed to manipulate the corpus in different ways, using previously built dictionaries. The MWEs from dictionaries have been inserted in the corpus as single terms, as described Precision and Recall are set-based measures, therefore, they do not take into consideration the ordering in which the relevant items were retrieved. In order to evaluate ranked retrieval results the most widely used measurement is the average precision (AvP ). AvP emphasizes returning more relevant documents earlier in the ranking. For a set of queries, we calculate the Mean Average Precision (MAP) according to Equation 3 (Manning et al., 2008). before. For each dictionary, an index was created in 1 M AP (Q) = |Q| mj 1 P (R ) (3) the IR system. These indices are described below: 1. Baseline (BL) - corpus without MWE.. 2. Compound Nouns (CN) - with 15 MWEs of. D1. 3. Best CN (BCN) - with 7,500 MWEs of D2..  6. Decision Tree (DT) - with 12,782 MWEs of. D5. 7. Manual 1 (M1) - with 254 MWEs of D6.. 8. Manual 2 (M2) - with 178 MWEs of D7.. 3.5 Evaluation Metrics. To evaluate the results of the IR system, we need to use metrics that estimate how well a userâs query was satisfied by the system. IR evaluation is based on recall and precision. Precision (Eq. 1) is the portion of the retrieved documents which is actually relevant to the query. Recall (Eq. 2) is the fraction of the relevant documents which is retrieved by the |Q| j=1 mj k=1 where |Q| is the number of queries, Rjk is the set of ranked retrieval results from the top result until document dk , and mj is the number of relevant documents for query j. 4 Experiment and Evaluations. The experiments performed evaluate the insertion of MWEs in results obtained in the IR system. The analysis is divided into two evaluations: (A) total set of query topics, where an overview is given of the MWE insertion effects and (B) topics modified by MWEs, where we evaluate only the query topics that contain MWEs. 4.1 Evaluation A. This evaluation investigates the effects of inserting MWEs in documents and queries. After each type of index was generated, MWEs were also included in the query topics, in accordance to the dictionaries used for each index (for Baseline BL, the query topics had no modifications). With eight corpus variations, we obtained individual results for each one of them. The results presented in Table 2 were summarized by the ab the MAP for the entire set of query topics. In total, 6,379 relevant documents are returned for the 310 query topics. is almost 20% of cases and the WCN, the difference between gain and loss is less than 2%. Table 3: BCN x Baseline Table 2: Results â Evaluation A. It is possible to see a small improvement in the results for the indices M1 and M2 in relation to the baseline (BL). This happens because the choice of candidate MWEs was made from the contents of the document topics and not, as with other indices, from the whole corpus. Considering the indices built with MWEs extracted from the corpus, the best result is index GS.In second place, comes the CN index, with a subtle improvement over the Baseline. BL surprisingly got a better result than the Best and Worst CN. The loss in retrieval quality as a result from MWE identification for BCN was not expected. When comparing the gain or loss in MAP of individual query topics, we can see how the index BCN compares to the Baseline: BCN had better MAP in 149 and worse MAP in 108 cases. However, the average loss is higher than the average gain, this explains why BL obtains a better result overall. In order do decide if one run is indeed superior to another, instead of using the absolute MAP value, we chose to calculate a margin of 5%. The intuition behind this is that in IR, a difference of less than 5% between the results being compared is not considered significant (Buckley and Voorhees, 2000). To be considered as gain the difference between the values resulting from two different indices for the same query topic should be greater than 5%. Differences of less than 5% are considered ties. This way, MAP values of 0.1111 and 0.1122 are considered ties. Given this margin, we can see in Tables 3 and 4 that the indices BCN and WCN are better compared to the baseline. In the case of BCN, the gain Table 4: WCN x Baseline Finally, this first experiment guided us toward a deeper evaluation of the query topics that have MWEs, because there is a possibility that the MWE insertions in documents can decrease the accuracy of the system on topics that have no MWE. 4.2 Evaluation B. This evaluation studies in detail the effects on the document retrieval in response to topics in which there were MWEs. For this purpose, we used the same indices used before and we performed an individual evaluation of the topics, to obtain a better understanding on where the identification of MWEs improves or degrades the results. As each dictionary was created using a different methodology, the number of expressions contained in each dictionary is also different. Thus, for each method, the number of query topics considered as having MWEs varies according to the dictionary used. Table 5 shows the number of query topics containing MWEs for each dictionary used, and as a consequence, the percentage of modified query topics over the complete set of 310 topics. First, it is interesting to observe the values of MAP for all topics that have been altered by the identification of MWEs. These values are shown in Table 6. As shown in Table 6 we verified that the GS index obtained the best result compared to others. This M Table 5: Topics with MWEs In de x M A P C N 0. 10 11 B C N 0. 09 39 W C N 0. 12 24 G S 0. 23 93 D T 0. 11 93 M 1 0. 12 62 M 2 0. 12 36 Table 6: Results - Evaluation B was somewhat expected since the MWEs in that dictionary are considered ârealâ MWEs. After GS, best results were obtained from the manual indices M1 and M2. The index that we consider as containing the lowest confident MWEs (WCN), obtained better results than Decision Trees, Nominal Compounds and Best Nominal Compounds, in this order. One possible reason for this to happen is that the number of MWEs inserted is higher than in the other indices. Compared with the BL, all indices with MWE insertion have improved more than degraded the results, in quantitative terms. Our largest gain was with the index GS, where 55.56% of the topics have improved, but the same index showed the highest percentage of loss, 22.22%. Analyzing the WCN, we can identify that this index has the lowest gain compared to all other indices: 32.14%, although having also the lowest loss. But, 60.71 % of the topics modified had no significant differences compared to the Baseline. Thus, we can conclude that the WCN index is the one that modifies the least the result of a query. The indices CN and BCN had a similar result, and knowing that a dictionary used to create BCN is a subset of the dictionary CN, we can conclude that the gain values, choosing the best MWE candidates, <TERM ID="GH950102000000-126" LEMA="underworld" POS="NN"> <WF>underworld</WF> <SYNSET SCORE="0.5" CODE="06120171-n"/> <SYNSET SCORE="0.5" CODE="06327598-n"/> </TERM> Original Topic: - What was the role of the Hubble telescope in proving the existence of black holes? Modified Topic: - what be the role of the hubble telescope in prove the existence of black hole ? black_hole <num>141</num> <title> letter bomb for kiesbauer find information on the explosion of a letter bomb in the studio of the tv channel pro7 presenter arabella kiesbauer . letter_bomb letter_bomb tv_channel </title> Figure 3: Topic #141 Table 7 shows the top ten scoring documents retrieved for query topic 141 in the baseline. The relevant document (in bold) is the fourth position in the Baseline. After inserting the expression letter bomb twice (because it occurs twice in the original topic), and tv channel that were in dictionary D1 used by the CN index, the relevant document is scored higher and as a consequence is returned in the first position of the ranking(Table 8) . The MAP of this topic has increased 75 percentage points, from 0.2500 in Baseline to 1.000 in the CN index. We see also that the document that was in first position in the Baseline ranking, has its score decreased and was ranked in fourth position in the ranking given by the CN. This document contained information on a âsmall bomb located outside the of the Russian embassyâ and has is not relevant to topic 141, being properly relegated to a lower position. An interesting fact about this topic is that only the MWE letter bomb influences the result. This was verified as in the index BCN, whose dictionary does not have this MWE, the topic was changed only because of the MWE tv channel and there was no gain or loss for the result. The second highest gain was of M1 index, in topic 173. The gain was of 28 percentage points. On. the other hand, we found a downside in M1 and M2 indices, although they improved results on average, they have reached very high values of loss in some topics. Po sit io n D o c u m e n t S c o r e P 1 L A 04 30 94 02 30 0. 47 09 00 P 2 G H9 50 823 00 01 05 0. 45 99 94 P 3 G H9 51 120 00 01 82 0. 43 95 36 P 4 G H9 50 610 00 01 64 0. 43 07 84 P 5 G H9 50 614 00 01 22 0. 42 87 66 P 6 L A 09 18 94 04 25 0. 42 84 29 P 7 G H9 50 829 00 00 82 0. 42 29 41 P 8 G H9 50 220 00 01 62 0. 41 19 68 P 9 G H9 50 318 00 01 31 0. 40 60 06 P 1 0 G H9 50 829 00 00 37 0. 40 28 06 Table 7: Ranking for Topic #141 - Baseline Po sit io n D o c u m e n t S c o r e P 1 G H9 50 610 00 01 64 0. 45 79 50 P 2 G H9 50 614 00 01 22 0. 43 67 53 P 3 G H9 50 823 00 01 05 0. 42 39 38 P 4 L A 04 30 94 02 30 0. 42 17 57 P 5 G H9 51 120 00 01 82 0. 40 01 23 P 6 G H9 50 829 00 00 82 0. 39 31 95 P 7 L A 09 18 94 04 25 0. 38 66 13 P 8 G H9 50 705 00 01 00 0. 38 41 16 P 9 G H9 50 220 00 01 62 0. 38 21 57 P 1 0 G H9 50 318 00 01 31 0. 38 04 71 Table 8: Ranking for Topic #141CN In sum, the MWEs insertion seems to improve retrieval bringing more relevant documents, due to a more precise indexing of specific terms. However, the use of these expressions also brought a negative impact for some cases, because some topics require a semantic analysis to return relevant documents (as for example topic 130, which requires relevant documents to mention the causes of the death of Kurt Cobain â documents which mention his death without mentioning the causes were not considered relevant).  This work consists in investigating the impact of Multiword Expressions on applications, focusing on compound nouns in Information Retrieval systems, and whether a more adequate treatment for these expressions can bring possible improvements in the indexing these expressions. MWEs are found in all genres of texts and their appropriate use is being targeted for study, both in linguistics and computing, due to the different characteristic variations of this type of expression, which ends up causing problems for the success of computational methods that aim their processing. In this work we aimed at achieving a better understanding of several important points associated with the use of Multiword Expressions in IR systems. In general, the MWEs insertion improves the results of retrieval for relevant documents, because the indexing of specific terms makes it easier to retrieve specific documents related to these terms. Nevertheless, the use of these expressions made the results worse in some c]ases, because some topics require a semantic analysis to return relevant documents. Some of these documents are related to the query, but do not satisfy all criteria in the query topic. We conclude also that the quality of MWEs used directly influenced the results. For future work, we would like to use other MWE types and not just compound nouns as used in this work. Other methods of extraction and a further study in Named Entities are good themes to complement this subject. A variation of corpora, different from newspaper articles, because each domain has a specific terminology, can also be an interesting subject for further evaluation.
 A Bayesian hybrid method for context-sensitive spelling correction  Two classes of methods have been shown to be useful for resolving lexical ambiguity. The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word. These methods have complementary coverage: the former captures the lexical "atmosphere" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction. Decision lists are found, by and large, to outperform either component method. However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence. A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.  Two classes of methods have been shown useful for resolving lexical ambiguity. The first tests for the presence of particular context words within a certain distance of the ambiguous target word. The second tests for collocations - patterns of words and part-of-speech tags around the target word. The context-word and collocation methods have complementary coverage: the former captures the lexical "atmosphere" (discourse topic, tense, etc.), while the latter captures local syntax. Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists. The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be. Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax. This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence. A method is presented for doing this, based on Bayesian classifiers. The work reported here was applied not to accent restoration, but to a related lexical disamÂ­ biguation task: context-sensitive spelling correction. The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert. where dessert was misspelled as desert. This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words. We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods. \Ve then apply each of the two component methods mentioned aboveÂ­ context words and collocations. \Ve try two ways of combining these components: decision lists, and Bayesian classifiers. \Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams. The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation. The final section draws some conclusions.  Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon. Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among). These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words. \Ve treat context-sensitive spelling correction as a task of word disambiguation. The ambiguity among words is modelled by confusion sets. A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set. Thus if C = {deserÂ·t, desserÂ·t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be. This treatment requires a collection of confusion sets to start with. There are several ways to obtain such a collection. One is based on finding words in the dictionary that are one typo away from each other [Mays et al., 1991).1 Another finds words that have the same or similar pronunciations. Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of "\Vords Commonly Confused" in the back of the Random House unabridged dictionary [Fiexner, 1983]. A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error). We will make the simplifying assumption that both kinds of errors are equally bad. In practice, however, false negatives are much worse, as users get irritated by programs that badger them with bogus complaints. However, given the probabilistic nature of the methods that will be presented below, it would not be hard to modify them to take this into account. We would merely set a confidence threshold, and report a suggested correction only if the probability of the suggested word exceeds the probability of the user's original spelling by at least the threshold amount. The reason this was not done in the work reported here is that setting this confidence threshold involves a certain subjective factor (which depends on the user's "irritability threshold"). Our simplifying assumption allows us to measure performance objectively, by the single parameter of prediction accuracy. 1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set. For instance, cat might have the confusion set {lwf,carÂ·, ... }, hat might have {cat,had, ... }, and so on. We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the "confusable" relation is no longer transitive.  This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of "minimal competency" for comparison with the other methods Context words Tests for particular words within Â±k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via. decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers. Each method will be described in terms of its operation on a single confusion set C = {Wt, ... , wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context. The methods handle multiple confusion sets by applying the same technique to each confusion set independently. Each method involves a training phase and a test phase. The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera. and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993]. 3.1 Baseline method. The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus. For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (oÂ·r left as) desert. Table 1 shows the performance of the baseline method for 18 confusion sets. This collection of confusion sets will be used for evaluating the methods throughout the paper. Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus. Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases. For example, the members of the confusion set {I, me} occurred 840 times in the test corpus, the breakdown being 744 I and 96 me. The baseline method predicted I every time, and thus was right 744 times, for a score of 744/840 = 0.886. Essentially the baseline method measures how accurately one can predict words using just their prior probabilities. This provides a lower bound on the performance we would expect from the other methods, which use more than just the priors. 3.2 Component method 1: Context words. One clue about the identity of an ambiguous target word comes from the words around it. For instance, if the target word is ambiguous between desert and dessert, and we see words like arid, sand, and sun nearby, this suggests that the target word should be desert. On the other hand, words such as chocolate and delicious in the context imply desserÂ·t. This observation is the basis for the method of context words. The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; 's distribution they most closely follow. C on fu si on se t No. of No. of t r a i n i n g t e s t c a s e s c a s e s M os t Baseline f r e q u e n t w o r d w h et h e r, w e at h er 3 3 1 2 4 5 I, m e 61 2. 5 84 0 its , it' s 19 .5 1 3. 57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u'r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y' re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets. The "Most frequent word" column gives the word in the confusion set that occurred most frequently in the training corpus. (In subsequent tables, confusion sets will be referred to by their most frequent word.) The "Baseline" column gives the prediction accuracy of the baseline system on the test corpus. Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework. The task is to pick the word Wi that is most probable, given the context words Cj observed within a Â±k-word window of the target word. The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k. , c_ 1, c1, ... , cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem. Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word. This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation. We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus. The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within Â±k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way. Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj). The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity. Gale et al. interpolate between the two so as to minimize the overall inaccuracy. We have pursued an alternative approach to the problem of estimating the likelihood terms. We start with the observation that there is no need to use every word in the Â±k-word window to discriminate among the words in the confusion set. If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence. We implement this by introducing a "minimum occurrences" threshold, Tmin. It is currently set to 10. We then ignore a context word c if: L m; < Tmin or L (Af;- m;) < Tmin l i n l5i n where m; and A{ are defined as above. In other words, c is ignored if it practically never occurs within the context of any w;, or if it practically always occurs within the context of every w;. In the former case, we have insufficient data to measure its presence; in the latter, its absence. Besides the reason of insufficient data, a second reason to ignore a context word is if it does not help discriminate among the words in the confusion set. For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help. By ignoring such words, we eliminate a source of noise in our discrimination procedure, as well as reducing storage requirements and run time. To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set. If the observed association is not judged to be significant,3 then c is discarded. The significance level is currently set to 0.05. Figure 1 pulls together the points of the preceding discussion into an outline of the method of context words. In the training phase, it identifies a list of context words that are useful for discriminating among the words in the confusion set. At run time, it estimates the probability of each word in the confusion set. It starts with the prior probabilities, and multiplies them by the likelihood of each context word from its list that appears in the Â±k-word window of the target word. Finally, it selects the word in the confusion set with the greatest probability. The main parameter to tune for the method of context words is k, the half-width of the context window. Previous work [Yarowsky, 1994] shows that sma.ller values of k (3 or 4) work well for resolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semantic ambiguities. \Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax. In the rest of this paper, this value of k will be used. 2 We are interpreting the condition "c. occurs within a Â±k-word window of w;" as a binary feature - either it happens, or it does not. This allows us to handle context words in the same Bayesian framework as will be used later for other binary features (see Section 3.3). A more conventional interpretation is to take into account the number of occurrences of each Cj within the Â±k-word window, and to estimate p(cilw;) accordingly. However, either interpretation is valid, as long as it is applied consistently - that is, both when estimating the likelihoods from training data, and when classifying test cases. 3 An association is significant if the probability that it occurred by chance is low. This is not a statement about. the strength of the association. Even a wea.k association may be judged significant if there are enough da.ta to support it. Measures of the strength of association will be discussed in Section 3.4. Training phase (1) Propose all words a.s candidate context words. (2) Count occurrences of each candidate context word in the training corpus. (3) Prune context words that have insufficient data or are uninformative discriminators. (4) Store the remaining context words (and their associated statistics) for use at run time. Run time (1) Initialize the probability for each word in the confusion set to its prior probability. (2) Go through the list of context words that was saved during training. For each context word that appears in the context of the ambiguous target word, update the probabilities. (3) Choose the word in the confusion set with the highest probability. Figure 1: Outline of the method of context words. Table 2 shows the effect of varying k for our usual collection of confusion sets. It can be seen that performance generally degrades as k increases. The reason is that the method starts picking up spurious correlations in the training corpus. Table 4 gives some examples of the context words learned for the confusion set {peace, piece}, with k = 24. The context words co1Â·ps, united, nations, etc., all imply peace, and appear to be plausible (although united and nations are a counterexample to our earlier assumption of independence). On the other hand, consider the context word how, which allegedly also implies peace. If we look back at the training corpus for the supporting data for this word, we find excerpts such a.s: But oh, how I do sometimes need just a moment of rest, and peace .. No ma.tter how earnest is our quest for guaranteed peace .. How best to destroy your peace ? There does not seem to be a necessary connection here between how and peace; the correlation is probably spurious. Although we are using a chi-square test expressly to filter out such spurious correlations, we can only expect the test to catch 95% of them (given that the significance level was set to 0.05). As mentioned above, most of the legitimate context words show up for small k; thus as k gets large, the limited number of legitimate context words gets overwhelmed by the 5% of the spurious correlations that make it through our filter. 3.3 Component method 2: Collocations. The method of context words is good at capturing generalities that depend on the presence of nearby words, but not their order. \Vhen order matters, other more syntax-based methods, such as collocations and trigrams, are appropriate. In the work reported here, the method of collocations was used to capture order dependencies. A collocation expresses a pattern of syntactic elements around the target word. We allow two types of syntactic elements: words, and part-of-speech tags. Going back to the {desert, dessert} example, a collocation that would imply desert might be: PREP the C on fu si on se t B a s e l i n e C w or ds Cwords Cwords Cwords Â± 3 Â± 6 Â± 1 2 Â± 2 4 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e pr in ci pl e si gh t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . . 5 7 5 0 . 5 3 8 0 . 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.922 0.927 0.922 0 . 9 1 0 . 8 6 2 0.795 0.743 0.702 0 . 8 6 1 0.849 0.801 0.743 0 . 9 3 1 0.901 0.896 0.855 0 . 7 9 1 0.795 0.793 0.755 0 . 7 4 7 0.741 0.759 0.716 0 . 8 1 6 0.783 0.774 0.736 0 . 6 4 6 0.622 0.636 0.639 0 . 6 3 9 0.614 0.602 0.614 0 .. 5 7 5 0.575 0.585 0.498 0 . 7 5 9 0.697 0.671 0.586 0 . 5 3 0 0.530 0.521 0.557 0 . 6 9 5 0.526 0.516 0.558 0 . 7 5 4 0.705 0..574 0.574 0 . 7 2 6 0.623 0.557 0.466 0 . 2 9 0 0.290 0.290 0.435 0 . 4 5 5 0.2.50 0.364 0.318 A vg no. of contex t words 2 7 . 9 36.9 55.9 92.9 Table 2: Performance of the method of context words as a function of k, the half-width of the context window. The bottom line of the table shows the number of context words learned, averaged over all confusion sets, also as a function of k. This collocation would match the sentences: Travelers entering from the desert were confounded ... along with some guerrilla fighting in the desert. two ladies who lay pinkly nude beside him in the desert Matching part-of-speech tags (here, PREP) against the sentence is done by first tagging each word in the sentence with its set of possible part-of-speech tags, obtained from a dictionary. For instance, walk has the ta.g set {Ns, v}, corresponding to its use as a singular noun and as a verb.4 For a tag to match a word, the ta.g must be a member of the word's tag set. The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ). The method of collocations was implemented in much the same way as the method of context words. The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;. An ambiguous target word is then classified by finding all collocations that match its context. Each collocation provides some degree of evidence 4 0ur tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc., a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's). 45 for each word in the confusion set. This evidence is combined using Bayes' rule. In the end, the Wi with the highest probability, given the evidence, is selected. A new complication arises for collocations, however, in that collocations, unlike context words, cannot be assumed independent. Consider, for example, the following collocations for desert: PREP the in the the These collocations are highly interdependent- we will say they conflict. To deal with this problem, we invoke our earlier observation that there is no need to use all the evidence. If two pieces of evidence conflict, we simply eliminate one of them, and base our decision on the rest of the evidence. We identify conflicts by the heuristic that two collocations conflict iff they overlap. The overlapping portion is the factor they have in common, and thus represents their lack of independence. This is only a heuristic because we could imagine collocations that do not overlap, but still conflict. Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right. Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one. Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength. This preserves the strongest non-conflicting evidence as the basis for our answer. The strength of a collocation reflects its reliability for decision-making; a further discussion of strength is deferred to Section 3.4. Figure 2 ties together the preceding discussion into an outline of the method of collocations. The method is described in terms of "features" rather than "collocations" to reflect its full generality; the features could be context words a.s well as collocations. In fact, the method subsumes the method of context words -it does everything that method does, and resolves conflicts among its features as well. To facilitate the conflict resolution, it sorts the features by decreasing strength. Like the method of context words, the method of collocations has one main parameter to tune: f, the maximum number of syntactic elements in a collocation. Since the number of collocations grows exponentially with e, it was only practical to vary f from 1 to 3. We tried this on some practice confusion sets, and found that a.ll values ofÂ£ gave roughly comparable performance. We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3). Table 3 shows the results of varyingÂ£ for the usual confusion sets. There is no clear winner; each value ofÂ£ did best for certain confusion sets. Table 5 gives examples of the collocations learned for {peace, piece} withÂ£= 2. A good deal of redundancy can be seen among the collocations. There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps). Many of the collocations a.t the end of the list appear to be overgeneral and irrelevant. 3.4 Hybrid method 1: Decision lists. Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while colloÂ­ cations capture drder-dependent generalities. Ya.rowsky proposed decision lists as a way to get the best of both methods. The idea is to make one big list of all features - in this case, context words and collocations. The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making. An ambiguous target word is then classified by running down the list and matching each feature against the target context. The first feature that 46 Training phase (1) (2) (3) (3.5) (4) Propose all possible features as candidat e features. Count occurren ces of each candidat e feature in the training corpus. P r u n e f e a t u r e s t h a t h a v e i n s u f f i c i e n t d a t a . o r a r e u n i n f o r m a t i v e d i s c r i m i n a t o r s . S o r t t h e r e m a i n i n g f e a t u r e s i n o r d e r o f d e c r e a s i n g s t r e n g t h . Store the list of features (and their associat ed statistics ) for use at run time. Run time (1) Initialize the probability for each word in the confusion set to its prior probability. (2) Go through the sorted list of features that was saved during training. For each feature that matches the context of the ambiguous target word, and does not conflict with a feature accepted previously, update the probabilities. (3) Choose the word in the confusion set with the highest probability. Figure 2: Outline of the method of collocations. Differences from the method of context words are highlighted in boldface. The method is described in terms of "features" rather than "collocations" to reflect its full generality. matches is used to classify the target word. Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance. The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take "features" in that figure to include both context words and collocations. The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature. This obviates the need for resolving conflicts between features. Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined. Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!) = abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2. It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!) = max: p( w;if) ' As an example of using the metric, suppose f is the context word arid, and suppose that arid coÂ­ occurs 10 times with desert and 1 time with dessert in the training corpus. Then reliability'(!) == max(10/11, 1/11) = 10/11 = 0.909. This value measures the extent to which the presence of the feature is unambiguously correlated with one particular w;. It can be thought of as the feature's reliability a.t picking out that w; from the others in the confusion set. 5Jn fact, we guarantee that this inequalit.y holds by performing smoothing before calculating strength. We smooth the data by adding 1 to the count. of how many times each feature was observed for each w;. 47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no. of colloc ations 3 3 . 9 263.1 98.5.4 Table 3: Performance of the method of collocations as a function of f, the maximum length of a collocation. The bottom line of the table shows the number of collocations learned, averaged over all confusion sets, also as a function of e. One peculiar property of the reliability metric is that it ignores the prior probabilities of the words in the confusion set. For instance, in the arid example, it would award the same high score even if the total number of occurrences of desert and dessert in the training corpus were 50 and 5, respectively - in which case arid's performance of 10/11 would be exactly what one would expect by chance, and therefore hardly impressive. Besides the reliability metric, therefore, we also considered an alternative metric: the uncertainty coefficient of x, denoted U(xiy) [Press et al., 1988, p..501]. U(xiy) measures how much additional information we get about the presence of the feature by knowing the choice of word in the confusion set.6 U(xiy) is calculated as follows: U(xiy) H(x) H(xiy) H ( x ) H ( x i y ) H ( x )-p(f)lnp(f)- p( .J)lnp(-.J) - Lp(w;) (p(flw;)lnp(flw;) + p( .Jiw;)lnp(â¢flw;)) The probabilities are ca.lculated for the population consisting of all occurrences in the training corpus of any w;. For instance, p(f) is the probability of feature f being present within this 6 This definition may seem backwards, but. is appropriate for use on the right-hand side of Bayes' rule, where the choice of word in the confusion set is t.he "given". 48 C on te xt w or d pe ac e pzece co rp s p e a c e u n i t e d n a t i o n s o u r h e a r t j u s t i c e s t a t e a m e r i c a n a i d i n t e r n a t i o n a l w o m e n w a r w o r l d p i e c e o v e r m u s t g r e a t u n d e r h o w 4 9 1 4 1 1 2 0 0 1 5 0 2 7 1 1 2 0 1 2 0 1 2 0 1 1 0 1 1 0 1 1 0 1 0 0 2 0 1 4 0 3 1 1 . 5 1 1 4 1 1 1 1 1 1 1 0 1 1 0 1 t w o f o r a b o u t e v e r y l i t t l e l o n g o n e t h e so 5 1 2 8 3 3 8 4 9 4 9 5 1 0 6 1 1 1 4 2 3 1 7 9 113 9 1 4 1 6 2 2 To tal oc cu rr en ce s 1 8 4 126 Table 4: Excerpts from the list of 43 context words learned for {peace, piece} with k = 24. Each line gives a context word, and the numÂ­ ber of peace and piece occurrences for which that context word occurred within Â±k words. The last line of the table gives the total numÂ­ ber of occurrences of peace and piece in the training corpus. Table 5: Excerpts from the sorted list of 98 collocations learned for {peace, piece} with Â£ = 2. Each line gives a collocation, and the number of peace and piece occurrences it matched. The last line of the table gives the total number of occurrences of peace and piece in the training corpus. 49 population. Applying the U(xjy) metric to the arid example, the value returned now depends on the number of occurrences of deserÂ·t and dessert in the training corpus. If these numbers are 50 and 5, then U(xly) = 0.0, reflecting the uninformativeness of the arid feature in this situation. If instead the numbers are 50 and 500, then U(:rjy) = 0.402, indicating arid's better-than-chance ability to pick out desrrt (10 out of 50 occurrences) over dessert (1 out of 500 occurrences). To compare the two strength metrics, we tried both on some practice confusion sets. Sometimes one metric did substantially better, sometimes the other. In the balance, the reliability metric seemed to give higher performance. This metric is therefore the one that will be used from here on. It was also used for all experiments involving the method of collocations. Table 6 shows the performance of decision lists with each metric for the usual confusion sets. As with the practice confusion sets, we see sometimes dramatic performance differences between the two metrics, and no clear winner. For instance, for {I, me}, the reliability metric did better than U(xjy) (0.980 versus 0.808); whereas for {between, among}, it did worse (0.659 versus 0.800). Further research is needed to understand the circumstances under which each metric performs best. Focusing for now on the reliability metric, Table 6 shows that the method of decision lists does, by and large, accomplish what it set out to do - namely, outperform either component method alone. There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence. But as the reliability and U(xjy) metrics indicate, it is not completely clear how the metric should be defined. This problem is addressed in the next section. 3.5 Hybrid method 2: Bayesian classifiers. The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations. In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem. \Ve hypothesize that even better performance can be obtained by taking into account all available evidence. This section presents a method of doing this based on Bayesian classifiers. Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength. It classifies an ambiguous target word by matching each feature in the list in turn against the target context. Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary. This method is essentially the same as the one for collocations (see Figure 2), except that it uses context words as well as collocations for the features. The only new wrinkle is in checking for conflicts between features (in step (2) at run time), as there are now two kinds of features to consider. If both features are context words, we say the features never conflict (as in the method of context words). If both features are collocations, we say they conflict iff they overlap (as in the method of collocations). The new case is if one feature is a context word, and the other is a collocation. Consider, for example, the context word walk, and the following collocations: (1) (2) (3) CONJ walk v PREP 7 1Â£ we use the U (xiy) metric instead, then d cision lists fall down on different examples; e.g., {its, it's}. 50 C on fu si on se t Ba sel in e Cwords Collocs Â± 3 2 Dl ist Dlist R el y U(xiy) w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0.902 0.931 0 . 8 8 6 0.914. 0.981 0 . 8 6 3 0.862 0.945 0 . 8 6 1 0.861 0.909 0 . 8 0 7 0.931 0.965 0 . 7 8 0 0.791 0.853 0 . 7 4 1 0.747 0.821 0 . 7 2 6 0.816 0.887 0 . 6 2 7 0.646 0.646 0 . 6 1 4 0.639 0.639 0 . 5 7 . 5 0..575 0.807 0 . 5 3 8 0.759 0.730 0 . 5 3 0 0.530 0.840 0 . 4 4 2 0.695 0.789 0 . 3 9 3 0.754 0.869 0 . 3 0 6 0.726 0.932 0 . 2 9 0 0.290 0.812 0 . 1 1 4 0.455 0.318 0. 93 5 0.829 0. 98 0 0.808 0. 93 1 0.805 0. 93 2 0.892 0. 96 7 0.961 0. 84 2 0.933 0. 82 1 0.654 0. 86 8 0.896 0. 62 9 0.667 0. 62 7 0.651 0. 80  0. 65 9 0.800 0. 84 0 0.840 0. 78 9 0.726 0. 85 2 0.836 0. 91 4 0.906 0. 81 2 0.841 0. 43 2 0.568 Table 6: Performance of decision lists with the reliability and U(xiy) strength metrics. To some extent, all of these collocations conflict with walk. Collocation (1) is the most blatant case; if it matches the target context, this logically implies that the context word walk will match. If collocation (2) matches, this guarantees that one of the possible tags of walk will be present nearby the target word, thereby elevating the probability that walk will match within Â±k words. If collocation (3) matches, this guarantees that there are two positions nearby the target word that are incompatible with walk, thereby reducing the probability that walk will match. If we were to treat all of these cases as conflicts, we would end up losing a great deal of (potentially useful) evidence. Instead, we adopt the more relaxed policy of only flagging the most egregious conflicts - here, the one between collocation (1) and walk. In general, we will say that a collocation and a context word conflict iff the collocation contains an explicit test for the context word. Table 7 compares all methods covered so far - baseline, two component methods, and two hybrid methods. (A sixth method, trigrams, is included as well-it will be discussed in Section 4.) The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set. Occasionally it scores slightly less than collocations; this appears to be due to some averaging effect where noisy context words are dragging it down. Occasionally too it scores less than decision lists, but never by much; on the whole, it yields a modest but consistent improvement, and in the case of {between, among}, a sizable improvement. We believe the improvement is due to considering all of the evidence, rather than just the single strongest piece, which makes the method more robust to inaccurate judgements about which piece of evidence is "strongest". 51 C on fu si on se t B as eli 11 e C w or ds Collocs Dlist Bayes Â± 3 : : ; 2 R e l y R e l y Trigra ms w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 5 0 .. 5 3 8 0 .. 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . : 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.931 0.93.5 0.935 0 . 9 1 4 0.981 0.980 0.985 0 . 8 6 2 0.945 0.931 0.942 0 . 8 6 1 0.909 0.932 0.924 0 . 9 3 1 0.96.5 0.967 0.973 0 . 7 9 1 0.853 0.842 0.869 0 . 7 4 7 0.821 0.821 0.827 0 . 8 1 6 0.887 0.868 0.901 0 . 6 4 6 0.646 0.629 0.662 0 . 6 : 3 9 0.639 0.627 0.639 0 .. 5 7 . 0 . 7 . 5 9 0.730 0.6.59 0.786 0 .. 5 3 0 0.840 0.840 0.840 O . G 9. 0 . 7 . 5 4 0.869 0.8.52 0.8.52 0 . 7 2 6 0.932 0.914 0.916 0 . 2 9 0 0.812 0.812 0.812 0 . 4 . 5 . 0.8 73 0.9 85 0.9 65 0.9 55 0.7 80 0.9 78 0.9 75 0.9 58 0.6 36 0.6 51 0.5 74 0.. 538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction. 4 Evaluation. While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods. We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995]. Schabes's method can be viewed as performing an abductive inference: given a sentence conÂ­ taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence. It answers this question by substituting each w; in turn into the sentence. The w; that produces the highest-probability sentence is selected. Sentence probabilities are calculated using a part-of-speech trigram model. We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7. It can be seen that trigrams and the Bayesian hybrid method each have their better moments. Trigrams are at their worst when the words in the confusion set have the same part of speech. In this case, trigrams can distinguish between the words only by their prior probabilitiesÂ­ this follows from the way the method calculates sentence probabilities. Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method. In such cases, the Bayesian hybrid method is clearly better. On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}Â­ trigrams are often better than the Bayesian method. We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence. This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method. This is a research direction we plan to pursue.  The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations. Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be. This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence. A method for doing this, based on Bayesian classifiers, was presented. It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists. A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise. This is a direction we plan to pursue in future research.  We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.
 OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION  Test abstract  The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November. Participants were invited to enter their systems in as many as four different task-oriented evaluations. The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time. The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years. The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume. All except the Scenario Template task are defined independently of any particular domain. This paper surveys the results of the evaluation on each task and, to a more limited extent, across tasks. Discussion of the results for each task is organized generally under the following topics:  Results on task as whole;  Results on some aspects of task;  Performance on "walkthrough article." The walkthrough article is an article selected from the test set. Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers. EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows:  Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.  Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.  Template Element (TE) --Extract basic information related to organization and person entities, drawing evidence from anywhere in the text.  Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event. The two SGML-based tasks required innovations to tie system-internal data structures to the original text so that the annotations could be inserted by the system without altering the original text in any other way. This capability has other useful applications as well, e.g., it enables text highlighting in a browser. It also facilitates information extraction, since some of the information in the extraction templates is in the form of literal text strings, which some systems have in the past had difficulty reproducing in their output. The inclusion of four different tasks in the evaluation implicitly encouraged sites to design general-purpose architectures that allow the production of a variety of types of output from a single internal representation in order to allow use of the full range of analysis techniques for all tasks. Even the simplest of the tasks, Named Entity, occasionally requires in-depth processing, e.g., to determine whether "60 pounds" is an expression of weight or of monetary value. Nearly half the sites chose to participate in all four tasks, and all but one site participated in at least one SGML task and one extraction task. The variety of tasks designed for MUC6 reflects the interests of both participants and sponsors in assessing and furthering research that can satisfy some urgent text processing needs in the very near term and can lead to solutions to more challenging text understanding problems in the longer term. Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem. Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns. The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting. Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1]. CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium. The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994. This period comprised the "evaluation epoch." As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants. The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query. It can also be used to do unranked, Boolean retrievals. The Boolean retrieval method was used in the initial probing of the corpus to identify candidates for the Scenario Template task, because the Boolean retrieval is relatively fast, and the unranked results are easy to scan to get a feel for the variety of nonrelevant as well as relevant documents that match all or some of the query terms. Once the scenario had been identified, the ranked retrieval method was used, and the ranked list was sampled at different points to collect approximately 200 relevant and 200 nonrelevant articles, representing a variety of article types (feature articles, brief notices, editorials, etc.). From those candidate articles, the training and test sets were selected blindly, with later checks and corrections for imbalances in the relevant/nonrelevant categories and in article types. From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks. The selection was again done blindly, with later checks to ensure that the set was fairly representative in terms of article length and type. Note that although Named Entity, Coreference and Template Element are defined as domain-independent tasks, the articles that were used for MUC6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task. The manually filled templates were created with the aid of Tabula Rasa, a software tool developed for the Tipster Text Program by New Mexico State University Computing Research Laboratory. NAMED ENTITY The Named Entity (NE) task requires insertion of SGML tags into the text stream. The tag elements are ENAMEX (for entity names, comprising organizations, persons, and locations), TIMEX (for temporal expressions, namely direct mentions of dates and times), and NUMEX (for number expressions, consisting only of direct mentions of currency values and percentages). A TYPE attribute accompanies each tag element and identifies the subtype of each tagged string: for ENAMEX, the TYPE value can be ORGANIZATION, PERSON, or LOCATION; for TIMEX, the TYPE value can be DATE or TIME; and for NUMEX, the TYPE value can be MONEY or PERCENT. Text strings that are to be annotated are termed markables. As indicated above, markables include names of organizations, persons, and locations, and direct mentions of dates, times, currency values and percentages. Non-markables include names of products and other miscellaneous names ("Macintosh," "Wall Street Journal" (in reference to the periodical as a physical object), "Dow Jones Industrial Average"); names of groups of people and miscellaneous usages of person names ("Republicans," "GrammRudman," "Alzheimer['s]"); addresses and adjectival forms of location names ("53140 Gatchell Rd.," "American"); indirect and vague mentions of dates and times ("a few minutes after the hour," "thirty days before the end of the year"); and miscellaneous uses of numbers, including some that are similar to currency or percentage expressions ("[Fees] 1 3/4," "12 points," "1.5 times"). The evaluation metrics used for NE are essentially the same as those used for the two template-filling tasks, Template Element and Scenario Template. The following breakdowns of overall scores on NE are computed:  by slot, i.e., for performance across tag elements, across TYPE attributes, and across tag strings;  by subcategorization, i.e., for performance on each TYPE attribute separately;  by document section, i.e., for performance on distinct subparts of the article, as identified by the SGML tags contained in the original text: <HL> ("headline"), <DD> ("document date"), <DATELINE>, and <TXT> (the body of the article). NE Results Overall Fifteen sites participated in the NE evaluation, including two that submitted two system configurations for testing and one that submitted four, for a total of 20 systems. As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites. On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well. It was also unexpected that one of the systems would match human performance on the task. Human performance was measured by comparing the 30 draft answer keys produced by the annotator at NRaD with those produced by the annotator at SAIC. This test measures the amount of variability between the annotators. When the outputs are scored in "key-to-response" mode, as though one annotator's output represented the "key" and the other the "response," the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%. The top-scoring system, the baseline configuration of the SRA system, achieved an F-measure of 96.42 and a corresponding error score of 5%. In considering the significance of these results from a general standpoint, the following facts about the test set need to be remembered: 96.42 95.66 94.92 94.00 93.65 93.33 92.88 92.74 92.61 91.20 90.84 89.06 88.19 85.82 85.73 84.95 5 7 8 10 10 11 10 12 12 13 14 18 19 20 23 22 96 95 93 92 94 92 94 92 89 91 91 84 86 85 80 82 97 96 96 96 93 95 92 93 96 91 91 94 90 87 92 89 Table 1. Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA "fast" configuration 95.66, SRA "fastest" configuration 92.61, SRA "nonames" configuration 94.92, SRI 94.0, Sterling Software 92.74..  It represents just one style of writing "the Chrysler division" (currently, only "Chrysler" (journalistic) and has a basic basic toward financial news and a specific bias toward the topic of the Scenario Template task.  It was very small (only 30 articles). There were no markable time expressions in the test set, and there were only a few markable percentage expressions. The results should also be qualified by saying that they reflect performance on data that makes accurate usage of upper and lower case distinctions. What would performance be on data where case provided no (reliable) clues and for languages where case doesn't distinguish names? SRA ran an experiment on an uppercase version of the test set that showed 85% recall and 89% precision overall, with identification of organization names presenting the greatest problem. That result represents nearly a 10-point decrease on the F-measure from their official baseline. The case-insensitive results would be slightly better if the task guidelines themselves didn't depend on case distinctions in certain situations, as when identifying the right boundary for the organization name span in a string such as would be tagged). NE Results on Some Aspects of Task Figures 1 and 2 show the sample size for the various tag elements and TYPE values. Note that nearly 80% of the tags were ENAMEX and that almost half of those were subcategofized as organization names. As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names. Organization names are varied in their form, consisting of proper nouns, general vocabulary, or a mixture of the two. They can also be quite long and complex and can even have internal punctuation such as a commas or an ampersand. Sometimes it is difficult to distinguish them from names of other types, especially from person names. Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc.. The difference that recourse to lists can make in performance is seen by comparing two runs made by SRA. The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration. The changes occurred only in performance on identifying organizations. BBN conducted a comparative test in which the experimental configuration used a larger lexicon than the baseline configuration, but the exact nature of the difference is not known and the performance differences are very small. As with the SRA experiment, the only differences in performance between the two BBN configurations are with the organization type. The University of Durham reported that they had intended to use gazetteer and company name lists, but didn't, because they found that the lists did not have much effect on their system's performance. The error scores for persons, dates, and monetary expressions was less than or equal to 10% for the large majority of systems. Several systems posted scores under 10% error for locations, but none was able to do so for oganizations. For percentages, about half the systems had 0% error, which reflects the simplicity of that particular subtask. Note that the number of instances of percentages in the test set is so small that a single mistake could result in an error of 6%. Slot-level performance on ENAMEX follows a different pattern for most systems from slot-level performance on NUMEX and TIMEX. The general pattern is for systems to have done better on the TEXT slot than on the TYPE slot for ENAMEX tags and for systems to have done better on the TYPE slot than on the TEXT slot for NUMEX and TIMEX tags. Errors on the TEXT slot are errors in finding the right span for the tagged string, and this can be a problem for all three subcategories of tag. The TYPE slot, however, is a more difficult slot for ENAMEX than for the other subcategories. It involves a three-way distinction for ENAMEX and only a two-way distinction for NUMEX and TIMEX, and it offers the possibility of confusing names of one type with names of another, especially the possibility of confusing organization names with person names. Looking at the document section scores in table 3, we see that the error score on the body of the text was much lower than on the headline for all but a few systems. There was just one system that posted a higher error score on the body than on the headline, the baseline NMSU CRL configuration, and the difference in scores is largely due to the fact that the system overgenerated to a greater extent on the body than on the headline. Its basic strategy for 96.42 0 95.66 0 0 7 7 94.92 0 0 8 8 94.00 0 0 20 9 93.65 0 2 16 10 93.33 0 4 38 9 92.88 0 0 18 10 92.74 0 0 22 11 92.61 100 0 18 9 91.20 0 0 30 13 90.84 3 11 19 14 89.06 3 4 28 18 88.19 0 0 22 20 85.82 0 6 18 21 85.73 0 44 53 21 84.95 0 0 50 21 Table 3. NE document subsection scores (ERR metric), in order of decreasing overall F-measure (P&R) headlines was a conservative one: tag a string in the headline as a name only if the system had found it in the body of the text or if the system had predicted the name based on truncation of names found in the body of the text. Most, if not all, the systems that were evaluated on the NE task adopted the basic strategy of processing the headline after processing the body of the text. The interannotator variability test provides reference points indicating human performance on the different aspects of the NE task. The document section results show 0% error on Document Date and Dateline, 7% error on Headline, and 6% error on Text. The subcategory error scores were 6% on Organization, 1% on Person, and 4% on Location, 8% on Date, and 0% on Money and Percent. These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates. Analysis of the results shows that some Date errors were a result of simple oversight (e.g., "fiscal 1994") and others were a consequence of forgetting or misinterpreting the task guidelines with respect to determining the maximal span of the date expression (e.g., tagging "fiscal 1993's second quarter" and "Aug. 1" separately, rather than tagging "fiscal 1993's second quarter, ended Aug. 1" as a single expression in accordance with the task guidelines). NE Results on "Walkthrough Article" In the answer key for the walkthrough article there are 69 ENAMEX tags (including a few optional ones), six TIMEX tags and six NUMEX tags. Interannotator scoring showed that one annotator missed tagging one instance of "Coke" as an (optional) organization, and the other annotator missed one date expression ("September"). Common mistakes made by the systems included missing the date expression, "the 21st century," and spuriously identifying "60 pounds" (which appeared in the context, "Mr. Dooner, who recently lost 60 pounds over three-and-a-half months .... ") as a monetary value rather than ignoring it as a weight. In addition, a number of errors identifying entity names were made; some of those errors also showed up as errors on the Template Element task and are described in a later section of this paper. COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe. The variety of high-frequency phenomena covered by the task is partially represented in the following hypothetical example, where all bracketed text segments are considered coreferential: 428 [Motor Vehicles International Corp.] announced a major management shakeup .... [MVI] said the chief executive officer has resigned .... [The Big 10 auto maker] is attempting to regain market share .... [It] will announce significant losses for the fourth quarter .... A [company] spokesman said [they] are moving [their] operations to Mexico in a cost-saving effort .... [MVI, [the first company to announce such a move since the passage of the new international trade agreement],] is facing increasing demands from unionized workers .... [Motor Vehicles International] is [the biggest American auto exporter to Latin America]. The example passage covers a broad spectrum of the phenomena included in the task. At one end of the spectrum are the proper names and aliases, which are inherently definite and whose referent may appear anywhere in the text. In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus. On the periphery of the central phenomena are markables whose status as coreferring expressions is determined by syntax, such as predicate nominals ("Motor Vehicles International is the biggest American auto exporter to Latin America") and 100 90 80 70 60 50 40 30 20 10 0 0 10 20 30 appositives ("MVI, the first company to announce such a move since the passage of the new international trade agreement"). At the far end of the spectrum are bare common nouns, such as the prenominal "company" in the example, whose status as a referring expression may be questionable. An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task. The algorithm compares the equivalence classes defined by the coreference links in the manually-generated answer key and the system-generated response. The equivalence classes are the models of the identity equivalence coreference relation. Using a simple counting scheme, the algorithm obtains recall and precision scores by determining the minimal perturbations required to align the equivalence classes in the key and response. No metrics other than recall and precision were defined for this task, and no statistical significance testing was performed on the scores. CO Results Overall In all, seven sites participated in the MUC6 coreference evaluation. Most systems achieved approximately the same levels of performance: five of the seven systems were in the 51%-63% recall O0  40 50 60 70 80 90 100 Recall Figure 3. Overall recall and precision on the CO task 2 2 Key to recall and precision scores: UDurham 36R/44P, UManitoba 63R/63P, UMass 44R/51P, NYU 53R/62P, UPenn 55R/63P, USheffield 51R/71P, SRI 59R/72P.. range and 62%-72% precision range. About half the systems focused only on individual coreference, which has direct relevance to the other MUC6 evaluation tasks. A few of the evaluation sites reported that good name/alias recognition alone would buy a system a lot of recall and precision points on this task, perhaps about 30% recall (since proper names constituted a large minority of the annotations) and 90% precision. The precision figure is supported by evidence from the NE evaluation. In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities. In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted. The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal "key to response" scoring mode. The amount of agreement between the two annotators was found to be 80% recall and 82% precision. There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc.. Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns. CO Results on Some Aspects of Task and on "Walkthrough Article" To keep the annotation of the evaluation data fairly simple, the MUC6 planning committee decided not to design the notation to subcategorize linkages and markables in any way. Two useful attributes for the equivalence class as a whole would be one to distinguish individual coreference from type coreference and one to identify the general semantic type of the class (organization, person, location, time, currency, etc.). For each NP in the equivalence class, it would be useful to identify its grammatical type (proper noun phrase, definite common noun phrase, bare singular common noun phrase, personal pronoun, etc.). The decision to minimize the annotation effort makes it difficult to do detailed quantitative analysis of the results. An analysis by the participating sites of their system's performance on the walkthrough article provides some insight into performance on aspects of the coreference task that were dominant in that article. The article contains about 1000 words and approximately 130 coreference links, of which all but about a dozen are references to individual persons or individual organizations. Approximately 50 of the anaphors are personal pronouns, including reflexives and possessives, and 58 of the markables (anaphors and antecedents) are proper names, including aliases. The percentage of personal pronouns is relatively high (38%), compared to the test set overall (24%), as is the percentage of proper names (40% on this text versus an estimate of 30% overall). Performance on this particular article for some systems was higher than performance on the test set overall, reaching as high as 77% recall and 79% precision. These scores indicate that pronoun resolution techniques as well as proper noun matching techniques are good, compared to the techniques required to determine references involving common noun phrases. For common noun phrases, the systems were not required to include the entire NP in the response; the response could minimally contain only the head noun. Despite this flexibility in the expected contents of the response, the systems nonetheless had to implicitly recognize the full NP, since to be considered coreferential, the head and its modifiers all had to be consistent with another markable. TEMPLATE ELEMENT The Template Element (TE) task requires extraction of certain general types of information about entities and merging of the information about any given entity before presentation in the form of a template (or "object"). For MUC6 the entities that were to be extracted were limited to organizations and persons) The ORGANIZATION object contains attributes ("slots") for the string representing the organization name (ORG NAME), for strings representing any abbreviated versions of the name (ORG_ALIAS), for a string that describes the particular organization (ORG_DESCRIPTOR), for a subcategory of the type of organization (ORG_TYPE, whose permissible values are GOVERNMENT, COMPANY, and OTHER), and for canonical forms of the specific and general location of the organization (ORG LOCALE and ORG_COUNTRY). The PERSON object contains 3The task documentation includes definition of an "artifact" entity, but that entity type was not used in MUC6 for either the dry run or the formal run. The entity types that were involved in the evaluation are the same as those required for the Scenario Template task. slots only for the string representing the person name (PER_NAME), for strings representing any abbreviated versions of the name (PERALIAS), and for strings representing a very limited range of titles (PER_TITLE). The task places heavy emphasis on recognizing proper noun phrases, as in the NE task, since all slots except ORG_DESCRIPTOR and PERTITLE expect proper names as slot fillers (in string or canonical form, depending on the slot. However, the organization portion of the TE task is not limited to recognizing the referential identity between full and shortened names; it requires the use of text analysis techniques at all levels of text structure to associate the descriptive and locative information with the appropriate entity. Analysis of complex NP structures, such as appositional structures and postposed modifier adjuncts, is needed in order to relate the locale and descriptor to the name in "Creative Artists Agency, the big Hollywood talent agency" and in "Creative Artists Agency, a big talent agency based in Hollywood." Analysis of sentence structures to identify grammatical relations such as predicate nominals is needed in order to relate those same pieces of information in "Creative Artists Agency is a big talent agency based in Hollywood." Analysis of discourse structure is needed in order to identify long-distance relationships. The answer key for the TE task contains one object for each specific organization and person mentioned in the text. For generation of a PERSON object, the text must provide the name of the person (full name or part of a name). For generation of an ORGANIZATION object, the text must provide either the name (full or part) or a descriptor of the organization. Since the generation of these objects is independent of the relevance criteria imposed by the Scenario Template (ST) task, there are many more ORGANIZATION and PERSON objects in the TE key than in the ST key. For the formal evaluation, there were 606 ORGANIZATION and 496 PERSON objects in the TE key, versus 120 ORGANIZATION and 137 PERSON objects in the ST key. The same set of articles was used for TE as for ST; therefore, the content of the articles is oriented toward the terms and subject matter covered by the ST task, which concerns changes in corporate management. 4 One effect of this bias is simply the number of entities mentioned in the articles: for the 4 The method used for selecting the articles for the test set is described at the beginning of this article.. test set used for the MUC6 dry run, which was based on a scenario concerning labor union contract negotiations, there were only about half as many organizations and persons mentioned as there were in the test set used for the formal run. TE Results Overall Twelve systems -- from eleven sites, including one that submitted two system configurations for testing--were tested on the TE task. All but two of the systems posted F-measure scores in the 7080% range, and four of the systems were able to achieve recall in the 7080% range while maintaining precision in the 8090% range, as shown in the figure 4. Human performance was measured in terms of variability between the outputs produced by the two NRaD and SAIC evaluators for 30 of the articles in the test set (the same 30 articles that were used for NE and CO testing). Using the scoring method in which one annotator's draft key serves as the "key" and the other annotator's draft key serves as the "response," the overall consistency score was 93.14 on the F-measure, with 93% recall and 93% precision. TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5. Figure 6 indicates the relative amount of error contributed by each of the slots in the ORGANIZATION object. It is evident that the more linguistic processing necessary to fill a slot, the harder the slot is to fill correctly. The ORG_COUNTRY slot is a special case in a way, since it is required to be filled when the ORG_LOCALE slot is filled. (The reverse is not the case, i.e., ORG_COUNTRY may be filled even if ORG_LOCALE is not, but this situation is relatively rare.) Since a missing or spurious ORG_LOCALE is likely to incur the same error in ORG_COUNTRY, the error scores for the two slots are understandably similar. 5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision.. ((XI 90  ,,v 80   04 ~) 5O 4O 20 10 0 .. 0 10 220 30 41) 50 60 7(1 80 91) Recall I(X) Figure 4. Overall recall and precision on the TE task 6 10090 80 ° "7" o qb l  70 60 50 40 30 20 10 0 0 10 20 30 40 50 60 70 80 90 100 Recall Figure 5. Organization and Person object recall and precision on the TE task 6Key to recall and precision scores: BBN 66R/79P, UDurham 49R/60P, Lockheed-Martin 76R/77P, UManitoba 71R/78P, UMass 53R/72P, MITRE 71R/85P, NYU 62R/83P, USheffield 66R/74P, SRA baseline configuration 75R/86P, SRA "noref" configuration 74R/87P, SRI 74R/76P, Sterling Software 72R/83P. 432 8o I ,o l II ii 40 3o 2o ,~ 10- type name alias country locale descriptor ORGANIZATION Slot= Figure 6. Best and average error per response fill Organization object slot scores for TE task With respect to performance on ORG_DESCRIPTOR, note that there may be multiple descriptors (or none) in the text. However, the task does not require the system to extract all descriptors of an entity that are contained in the text; it requires only that the system extract one (or none). Frequently, at least one can be found in close proximity to an organization's name, e.g., as an appositive ("Creative Artists Agency, the big Hollywood talent agency"). Nonetheless, performance is much lower on this slot than on others. Leaving aside the fact that descriptors are common noun phrases, which makes them less obvious candidates for extraction than proper noun phrases would be, what reasons can we find to account for the relatively low performance on the ORG_DESCRIPTOR slot? One reason for low performance is that an organization may be identified in a text solely by a descriptor, i.e., without a fill for the ORG_NAME slot and therefore without the usual local clues that the NP is in fact a relevant descriptor. It is, of course, also possible that a text may identify an organization solely by name. Both possibilities present increased opportunities for systems to undergenerate or overgenerate. Also, the descriptor is not always close to the name, and some discourse processing may be requ~ed in order to identify it --this is likely to increase the opportunity for systems to miss the information. A third significant reason is that the response fill had to match the key fill exactly in order to be counted correct; there was no allowance made in the scoring software for assigning full or partial credit if the response fill only partially matched the key fill. It should be noted that human performance on this task was also relatively low, but it is unclear whether the degree of disagreement can be accounted for primarily by the reasons given above or whether the disagreement is attributable to the fact that the guidelines for that slot had not been finalized at the time when the annotators created their version of the keys. TE Results on "Walkthrough Article" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems. Viewed from the perspective of the TE task, the walkthrough article presents a number of interesting examples of entity type confusions that can result from insufficient processing. There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations. Errors of these kinds result in a penalty at the object level, since the extracted information is contained in the wrong type of object. Examples of each of these types of error appear below, along with the number of systems that committed the error. (An experimental configuration of the SRA system produced the same output as the baseline configuration and has been disregarded in the tallies; thus, the total number of systems tallied is eleven.) 1. Miscategorizations of entities as person (PER_NAME or PER_ALIAS) instead of organization (ORG_NAME or ORG_ALIAS). 433  Six systems: McCannErickson (also extracted with the name of "McCann," "One McCann," "While McCann"; organization category is indicated clearly by context in which full name appears, "John Dooner Will Succeed James At Helm of McCannErickson" in headline and "Robert L. James, chairman and chief executive officer of McCannErickson, and John J. Dooner Jr., the agency's president and chief operating officer" in the body of the article) eSix systems: J. Walter Thompson (also extracted with the name of "Walter Thompson"; organization category is indicated by context, "Peter Kim was hired from WPP Group's J. Walter Thompson last September...") eFour systems: Fallon McElligott (organization category is indicated by context, "...other ad agencies, such as Fallon McElligott") eOne system: Ammirati & Puris (the presence of the ampersand is a clue, as is the context, "...president and chief executive officer of Ammirati & Puris"; but note that the article also mentions the name of one of the company's founders, Martin Puris)  organization (ORG NAME) instead of location (ORG_LOCALE) eTwo systems: Hollywood (location category is indicated by context, "Creative Artists Agency, the big Hollywood talent agency") . Miscategorization of nonrelevant entities as organization name, alias or descriptor (ORG NAME, ORG_ALIAS, ORG_DESCRIPTOR). oSix systems: New York Times (publication name in phrase, "a framed page from the New York Times"; without sufficient context, the name can be ambiguous in its reference to a physical object versus an organization) eThree systems: Coca-Cola Classic (product name deriving from "Coca-Cola," which appears separately in several places in the article and is occasionally ambiguous even in context between product name and organization name) eOne system: Not Butter (part of product name, "I Can't Believe It's Not Butter") eOne system: Taster (part of product name, "Taster's Choice")  One system: Choice (part of product name, "Taster's Choice") eFive systems: a hot agency (nonspecific use of indefinite in phrase "...is interested in acquiring a hot agency") Given the variety of contextual clues that must be taken into account in order to analyze the above entities correctly, it is understandable that just about any given system would commit at least one of them. But the problems are certainly tractable; none of the fifteen TE entities in the key (ten ORGANIZATION entities and five PERSON entities) was miscategofized by all of the systems. In addition to miscategorization errors, the walkthrough text provides other interesting examples of system errors at the object level and the slot level, plus a number of examples of system successes. One success for the systems as a group is that each of the six smaller ORGANIZATION objects and four smaller PERSON objects (those with just one or two filled slots in the key) was matched perfectly by at least one system; in addition, one larger ORGANIZATION object and two larger PERSON objects were perfectly matched by at least one system. Thus, each of the five PERSON objects in the key and seven of the ten ORGANIZATION objects in the key were matched perfectly by at least one system. The three larger ORGANIZATION objects that none of the systems got perfectly correct are for the McCannErickson, Creative Artists Agency, and Coca-Cola companies. Common errors in these three ORGANIZATION objects included missing the locale/country or failing to organization's alias with its name. descriptor identify or the SCENARIO TEMPLATE A Scenario Template (ST) task captures domain-and task-specific information. Three scenarios were defined in the course of MUC6: (1) a scenario concerning the event of organizations placing orders to buy aircraft with aircraft manufacturers (the "aircraft order" scenario); (2) a scenario concerning the event of contract negotiations between labor unions and companies (the "labor negotiations" scenario); (3) a scenario concerning changes in corporate managers occupying executive posts (the "management succession" scenario). The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation. One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure. In this article, the management succession scenario will be used as the basis for discussion. The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure. At the top level is the TEMPLATE object, of which there is one instantiated for every document. This object points down to one or more SUCCESSION_EVENT objects if the document meets the event relevance criteria given in the task documentation. Each event object captures the changes occurring within a company with respect to one management post. The SUCCESSION_EVENT object points down to the Ib~AND_OUT object, which in turn points down to PERSON Template Element objects that represent the persons involved in the succession event. The IN_AND_OUT object contains ST-specific information that relates the event with the persons. The ORGANIZATION Template Element objects are present at the lowest level along with the PERSON objects, and they are pointed to not only by the IN_AND_OUT object but also by the SUCCESSION_EVENT object. The organization pointed to by the event object is the organization where the relevant management post exists; the organization pointed to by the relational object is the organization that the person who is moving in or out of the post is coming from or going to. The scenario is designed around the management post rather than around the succession act itself. Although the management post and information associated with it are represented in the SUCCESSION_EVENT object, that object does not actually represent an event, but rather a state, i.e., the vacancy of some management post. The relational-level Iih~AND_OUT objects represent the personnel changes pertaining to that state. ST Results Overall Nine sites submitted a total of eleven systems for evaluation on the ST task. All the participating sites also submitted systems for evaluation on the TE and NE tasks. All but one of the development teams (UDurham) had members who were veterans of MUC5. Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant. Marginally relevant event objects are marked in the answer key as being optional, which means that a system is not penalized if it does not produce such an event object. The approximate 5050 split between relevant and nonrelevant texts was Template Level (Doc_Nr) JCCESSION_EVE/~ (Post, Vacancy_Reason) In_and_Out r IN_AND_OUT " Succession Org (New_Status, On_the_Job, Rel Other_Org) j IO Template Element Level PERSON ORGANIZATION 1ame, Per_Alias, (Org_Name, Org_Alias, Org_Descriptor, Per_Title) ~Q0rg_Type, Org_Locale, Org_Country) Figure 7. Management Succession Template Structure intentional and is comparable to the richness of the MUC3 "TST2" test set and the MUC4 "TST4" test set. (The test sets used for MUC5 had a much higher proportion of relevant texts.) Systems are measured for their performance on distinguishing relevant from nonrelevant texts via the text filtering metric, which uses the classic information retrieval definitions of recall and precision. For MUC6, text filtering scores were as high as 98% recall (with precision in the 80th percentile) or 96% precision (with recall in the 80th percentile). Similar tradeoffs and upper bounds on performance can be seen in the TST2 and TST4 results (see score reports in sections 2 and 4 of appendix G in [2]). However, performance of the systems as a group is better on the MUC6 test set. The text filtering results for MUC6, MUC4 (TST4) and MUC3 (TST2) are shown in figure 8. Whereas the Text Filter row in the score report shows the system's ability to do text filtering (document detection), the All Objects row and the individual Slot rows show the system's ability to do information extraction. The measures used for information extraction include two overall ones, the F-measure and error per response fill, and several other, more diagnostic ones (recall, precision, undergeneration, overgeneration, and substitution). The text filtering definition of precision is different from the information extraction definition of precision; the latter definition includes an element in the formula that accounts for the number of spurious template fills generated. The All Objects recall and precision scores are shown in figure 9. The highest ST F-measure score was 56.40 (47% recall, 70% precision). Statistically, large differences of up to 15 points may not be reflected as a difference in the ranking of the systems. Most of the systems fall into the same rank at the high end, and the evaluation does not clearly distinguish more than two ranks (see the paper on statistical significance testing by Chinchor in [1]). Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator's templates were treated as the "key" and the other annotator's templates were treated as the "response". No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks. The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed. Nonetheless, as one rough measure of progress in the area of information extraction as a whole, we can consider the F-measures of the top-scoring systems from the MUC5 and MUC6 evaluations. MUC6 56.40 MUC5 EJV 52.75 MUC5 JJV 60.07 MUC5 EME 49.18 MUC5 JME 56.31 Table 4. Highest P&R F-Measure scores posted for MUC6 and MUC5 ST tasks Note that table 4 shows four top scores for MUC5, one for each language-domain pair: English Joint Ventures (EJV), Japanese Joint Ventures (JJV), English Microelectronics (EME), and Japanese Microelectronics (JME). From this table, it may be reasonable to conclude that progress has been made, since the MUC6 performance level is at least as high as for three of the four MUC5 tasks and since that performance level was reached after a much shorter time. ST Results on Some Aspects of Task and on "Walkthrough Article" Three succession events are reported in the walkthrough article. Successful interpretation of three sentences from the walkthrough article is necessary for high performance on these events. The tipoff on the first two events comes at the end of the second paragraph: Yesterday, McCann made official what had been widely anticipated: Mr. James, 57 years old, is stepping down as chief executive officer on July 1 and will retire as chairman at the end of the year. He will be succeeded by Mr. Dooner, 45. The basis of the third event comes halfway through the two-page article: In addition, Peter Kim was hired from WPP Group's J. Walter Thompson last September as vice chairman, chief strategy officer, worldwide. 7Key to recall and precision scores: BBN 50R/59P, UDurham 33R/34P, Lockheed-Martin 43R/64P, UManitoba 39R/62P, UMass 36R/46P, NYU 47R/70P, USheffield 37R/73P, SRA baseline configuration 47R/62P, SRA "precision" configuration 32R/66P, SRA "recall" configuration 58R/46P, SRI 44R/61P. 437 Answer Key System Output Event James out, Dooner in as CEO of McCannJames out, Dooner in as CEO of McCann- #1 Erickson as a result of James departing the Erickson as a result of a reassignment of James; workforce; James is still on the job as CEO; James is no__! on the job as CEO any more, Dooner is not on the job as CEO yet, and his old his new job is at the same as his old job; Dooner job was with the same org as his new job. may or may not be on the job as CEO yet, and his old job was with the same org as his new job. (SRA satie_base system) Event James out, Dooner in as chairman of James out, Dooner in as chairman of #2 McCannErickson as a result of James departing McCannErickson as a result of James departing the workforce; James is still on the job as the workforce; James is no_4 on the job as chairman; Dooner is not on the job as chairman chairman any more; Dooner is already on the job yet, and his old job was with the same org as his as chairman, and his old job was with Ammirati new job. & Puris. (NYU system) Event Kim in as "vice chairman, chief strategy Kim in as vice chairman of WPP Group, #3 officer, worldwide" of McCannErickson, where where the vacancy existed for other/unknown the vacancy existed for other/unknown reasons; reasons; he may or may not be on the job in that he is already on the job in the post, and his old post yet, and the article doesn't say where his old job was with J. Walter Thompson job was. (BBN system) Table 5. Paraphrased summary of ST outputs for walkthrough article The article was relatively straightforward for the annotators who prepared the answer key, and there were no substantive differences in the output produced by each of the two annotators. Table 5 contains a paraphrased summary of the output that was to be generated for each of these events, along with a summary of the output that was actually generated by systems evaluated for MUC6. The system-generated outputs are from three different systems, since no one system did better than all other systems on all three events. The substantive differences between the system-generated output and the answer key are indicated by underlining in the system output. Recurring problems in the system outputs include the information about whether the person is currently on the job or not and the information on where the outgoing person's next job would be and where the incoming person's previous job was. Note also that even the best system on the third event was unable to determine that the succession event was occurring at McCannEfickson; in addition, it only partially captured the full title of the post. To its credit, however, it did recognize that the event was relevant; only two systems produced output that is recognizable as pertaining to this event. One common problem was the simple failure to recognize "hire" as an indicator of a succession. Two systems never filled the OTHER_ORG slot or its dependent slot, REL OTHER_ORG, despite the fact that data to fill those slots was often present; over half the IN_AND_OUT objects in the answer key contain data for those two slots. Almost without exception, systems did more poorly on those two slots than on any others in the SUCCESSION_EVENT and IN_AND_OUT objects; the best scores posted were 70% error on OTHER_ORG (median score of 79%) and 72% error on REL_OTHER ORG (median of 86%). Performance on the VACANCY_REASON and ON_THE JOB slots was better for nearly all systems. The lowest error scores were 56% on VACANCY_REASON (median of 70%) and 62% on ONZI'HE_JOB (median of 71%). The slot that most systems performed best on is NEWSTATUS; the lowest error score posted on that slot is 47% (median of 55%). This slot has a limited number of fill options, and the right answer is almost always either IN or OUT, depending on whether the person involved is assuming a post (IN) or vacating a post (OUT). Performance on the POST slot was not quite as good; the lowest error was 52% (median of 65%). The POST slot requires a text string as fill, and there is no finite list of possible fills for the slot. As seen in the third event of the walkthrough article, the fill can be an extended title such as "vice chairman, chief strategy officer, worldwide." For most events, however, the fill is one of a large handful of possibilities, including "chairman," "president," "chief executive [officer]," "CEO," "chief operating officer," "chief financial officer," etc. 438 DISCUSSION: CRITIQUE OF TASKS Named Entity The primary subject for review in the NE evaluation is its limited scope. A variety of proper name types were excluded, e.g. product names. The range of numerical and temporal expressions covered by the task was also limited; one notable example is the restriction of temporal expressions to exclude "relative" time expressions such as "last week". Restriction of the corpus to Wall Street Journal articles resulted in a limited variety of markables and in reliance on capitalization to identify candidates for annotation. Some work on expanding the scope of the NE task has been carried out in the context of a foreign- language NE evaluation conducted in the spring of 1996. This evaluation is called the MET (Multilingual Named Entity) and, like MUC6, was carried out under the auspices of the Tipster Text program. The experience gained from that evaluation will serve as critical input to revising the Engish version of the task. Coreference Many aspects of the CO task are in definite need of review for reasons of either theory or practice. One set of issues concerns the range of syntactically governed correference phenomena that are considered markable. For example, apposition as a markable phenomenon was restrictively defined to exclude constructs that could rather be analyzed as left modification, such as "chief executive Scott McNealy," which lacks the comma punctuation that would clearly identify "executive" as the head of an appositive construction. Another set of issues is semantic in nature and includes fimdamental questions such as the validity of including type coreferrence in the task and the legitimacy of the implied definition of coteference versus reference. If an antecedent expression is nonreferential, can it nonetheless be considered coreferential with subsequent anaphoric expressions? Or can only referring expressions corefer? Finally, the current notation presents a set of issues, such as its inability to represent multiple antecedents, as in conjoined NPs, or alternate antecedents, as in the case of referential ambiguity. In short, the preliminary nature of the task design is reflected in the somewhat unmotivated boundaries between markables and nonmarkables and in weaknesses in the notation. One indication of immaturity of the task definition (as well as an indication of the amount of genuine textual ambiguity) is the fact that over ten percent of the linkages in the answer key were marked as "optional." (Systems were not penalized if they failed to include such linkages in their output.) The task definition is now under review by a discourse working group formed in 1996 with representatives from both inside and outside the MUC commuity, including representatives from the spoken-language community. Template Element There are miscellaneous outstanding problems with the TE task. With respect to the ORGANIZATION and PERSON objects, there are issues such as rather fuzzy distinctions among the three organization subtypes and between the organization name and alias, the extremely limited scope of the person title slot, and the lack of a person descriptor slot. The ARTIFACT object, which was not used for either the dry run or the formal evaluation, needs to be reviewed with respect to its general utility, since its definition reflects primarily the requirements of the MUC5 microelectronics task domain. There is a task-neutral DATE slot that is defined as a template element; it was used in the MUC6 dry run as part of the labor negotiation scenario, but as currently defined, it fails to capture meaningfully some of the recurring kinds of date information. In particular, problems remain with normalizing various types of date expressions, including ones that are vague and/or require extensive use of calendar information. Scenario Template The issues with respect to the ST task relate primarily to the ambitiousness of the scenario templates defined for MUC6. Although the management scenario contained only five domain- specific slots (disregarding slots containing pointers to other objects), it nonetheless reflected an interest in capturing as complete a representation of the basic event as possible. As a result, a few "peripheral" facts about the event were included that were difficult to define in the task documentation and/or were not reported clearly in many of the articles. Two of the slots, VACANCY_REASON and ON_THE_JOB, had to be filled on the basis of inference from subtle linguistic cues in many cases. An entire appendix to the scenario definition is devoted to heuristics for filling the ON_THE JOB slot. These two slots caused problems for the annotators as well as for the systems. The annotators' problems with VACANCY_REASON may have had more to do with understanding what the scenario definition was saying than with understanding what the news articles were saying. The annotators' problems with ONZI'HE_JOB were probably more substantive, since the heuristics documented in the appendix were complex and sometimes hard to map onto the expressions found in the news articles. A third slot, REL_OTHER_ORG, required special inferencing on the basis of both linguistics and world knowledge in order to determine the corporate relationship between the organization a manager is leaving and the one the manager is going to. There may, in fact, be just one organization involved --the person could be leaving a post at a company in order to take a different (or an additional) post at the same company. Defining a generalized template structure and using Template Element objects as one layer in the structure reduced the amount of effort required for participants to move their system from one scenario to another. Further simplification may be advisable in order to focus on core information elements and exclude somewhat idiosyncratic ones such as the three slots described above. In the case of the management succession scenario, a proposal was made to eliminate the three slots discussed above and more, including the relational object itself, and to put the personnel information in the event object. Much less information about the event would be captured, but there would be a much stronger focus on the most essential information elements. This would possibly lead to significant improvements in performance on the basic event-related elements and to development of good end-user tools for incorporating some of the domain-specific patterns into a generic extraction system. CONCLUSIONS The results of the evaluation give clear evidence of the challenges that have been overcome and the ones that remain along dimensions of both breadth and depth in automated text analysis. The NE evaluation results serve mainly to document in the MUC context what was already strongly suspected: 1. Automated identification is extremely accurate when identification of lexical pattern types depends only on "shallow" information, such as the form of the string that satisfies the pattern and/or immediate context; 2. Automated identification is significantly less accurate when identification is clouded by uncertainty or ambiguity (as when case distinctions are not made, when organizations are named after persons, etc.) and must depend on one or more "deep" pieces of information (such as world knowledge, pragmatics, or inferences drawn from structural analysis at the sentential and suprasentential levels). The vast majority of cases are simple ones; thus, some systems score extremely well --well enough, in fact, to compete overall with human performance. Commercial systems are available already that include identification of those defined for this MUC6 task, and since a number of systems performed very well for MUC6, it is evident that high performance is probably within reach of any development site that devotes enough effort to the task. Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one. The TE evaluation task makes explicit one aspect of extraction that is fundamental to a very broad range of higher-level extraction tasks. The identification of a name as that of an organization (hence, instantiation of an ORGANIZATION object) or as a person (PERSON object) is a named entity identification task. The association of shortened forms of the name with the full name depends on techniques that could be used for NE and CO as well as for TE. The real challenge of TE comes from associating other bits of information with the entity. For PERSON objects, this challenge is small, since the only additional bit of information required is the person's title ("Mr.," "Ms.," "Dr.," etc.), which appears immediately before the name/alias in the text. For ORGANIZATION objects, the challenge is greater, requiring extraction of location, description, and identification of the type of organization. Performance on TE overall is as high as 80% on the F-measure, with performance on ORGANIZATION objects significantly lower (70th percentile) than on PERSON objects (90th percentile). Top performance on PERSON objects came close to human performance, while performance on ORGANIZATION objects fell significantly short of human performance, with the caveat that human performance was measured on only a portion of the test set. Some of the shortfall in performance on the ORGANIZATION object is due to inadequate discourse processing, which is needed in order to get some of the non-local instances of the ORG_DESCRIPTOR, ORG LOCALE and ORG_COUNTRY slot fills. In the case of ORG_DESCRIPTOR, the results of the CO evaluation seem to provide further evidence for the relative inadequacy of current techniques for relating entity descriptions with entity names. Systems scored approximately 1525 points lower (F-measure) on ST than on TE. As defined for MUC6, the ST task presents a significant challenge in terms of system portability, in that the test procedure requ~ed that all domain-specific development be done in a period of one month. For past MUC evaluations, the formal run had been conducted using the same scenario as the dry run, and the task definition was released well before the dry run. Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels. However, there were at least three factors that might lead one to expect higher levels of performance than seen in previous MUC evaluations: 1. The standardized template structure minimizes the amount of idiosyncratic programming required to produce the expected types of objects, links, and slot fills. 2. The fact that the domain-neutral Template Element evaluation was being conducted led to increased focus on getting the low- level information correct, which would carry over to the ST task, since approximately 25% of the expected information in the ST test set was contained in the low-level objects. 3. Many of the veteran participating sites had gotten to the point in their ongoing development where they had fast and efficient methods for updating their systems and monitoring their progress. It appears that there is a wide variety of sources of error that impose limits on system effectiveness, whatever the techniques employed by the system. In addition, the short time frame allocated for domain-specific development naturally makes it very difficult for developers to do sufficient development to fill complex slots that either are not always expected to be filled or are not crucial elements in the template structure. Sites have developed architectures that are at least as general-purpose techniques as ever, perhaps as a result of having to produce outputs for as many as four different tasks. Many of the sites have emphasized their pattern-matching techniques in discussing the strengths of their MUC6 systems. However, we still have full-sentence parsing (e.g. USheffield, UDurham, UManitoba); we sometimes have expectations of "deep understanding" (cf. UDurham's use of a world model) and sometimes not (cf. UManitoba's production of ST output directly from dependency trees, with no semantic representation per se). Some systems completed all stages of analysis before producing outputs for any of the tasks, including NE. Six of the seven sites that participated in the coreference evaluation also participated in the MUC6 information extraction evaluation, and five of the six made use of the results of the processing that produced their coreference output in the processing that produced their information extraction output. The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations. Other sources of excitement are the spinoff efforts that the NE and CO tasks have inspired that bring these tasks and their potential applications to the attention of new research groups and new customer groups. In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure. Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas. The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well.  The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a "community" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors. Individual thanks go to Ralph Grishman of NYU for serving as program co- chair, to Nancy Chinchor for her critical efforts on virtually all aspects of MUC6, and to the other members of the program committee, which included Chinatsu Aone of SRA Corp., Lois Childs of Lockheed Martin Corp., Jerry Hobbs of SRI International, Boyan Onyshkevych of the U.S. Dept. of Defense, Marc Vilain of The MITRE Corp., Takahiro Wakao of the Univ. of Sheffield, and Ralph Weischedel of BBN Systems and Technologies. The author would also like to acknowledge the critical behind-the-scenes computer support rendered at NRaD by Tim Wadsworth, who passed away suddenly in August 1995, leaving a lasting empty spot in my work and my heart.
