The sentences in the DSO collection were tagged with parts of speech using TnT (Brants, 2000) trained on the Brown Corpus itself.  The English POS-tagging has been carried out using freely available TNT tagger (Brants, 2000). This proposition is quite viable as statistical POS taggers like TnT (Brants, 2000) are available. We use TnT (Brants, 2000), a second order Markov Model tagger. For PoS tagging and lemmatization, we combine GENIA (with its built-in, occasionally deviant to kenizer) and TnT (Brants, 2000), which operates on pre-tokenized inputs but in its default models trained on financial news from the Penn Tree bank. Tag the tokens with PTB-style POS tags using a tagger (Brants, 2000). For example, Petrov et al (2012) build supervised POS taggers for 22 languages using the TNT tagger (Brants, 2000), with an average accuracy of 95.2%. Forun aligned words, we simply assign a random POS and very low probability, which does not substantially affect transition probability estimates. In Step 6 we build a tagger by feeding the es ti mated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger. based on tree-structures of various complexity in the tree-adjoining grammar model. Using such tags, Brants (2000) has achieved the automated tagging of a syntactic-structure-based set of grammatical function tags including phrase-chunk and syntactic-role modifiers trained in supervised mode from a tree bank of German. We also incorporated part-of speech tagging, using the TnT tagger (Brants, 2000) retrained on the GENIA corpus gold standard part of-speech tagging. POS Majority lexical type noun count-noun-le c-n-f verb trans-nerg-str-verb-le haben-auxf adj adj-non-prd-le adv intersect-adv-le Table 5: POS tags to lexical types mapping Again for comparison, we have built another simple baseline model using the TnT POS tagger (Brants, 2000). The texts were POS-tagged using TnT (Brants,2000). The freely-available POS lexicon from Sharoff et al (2008), specifically the file for the POS tagger TnT (Brants, 2000), contains full words (239,889 unique forms), with frequency information. We use a corpus of 5 million words automatically tagged by TnT (Brants, 2000) and freely available online (Sharoff et al, 2008). Because we want to make linguistically-informed corruptions, we corrupt only the words we have information for, identifying the words in the corpus which are found in the lexicon with the appropriate POS tag. We also select only words which have inflectional morphology: nouns, verbs, adjectives, pronouns, and numerals.7 4.2.1 Determining word properties (step 1) We use the POS tag to restrict the properties of a word, regardless of how exactly we corrupt it. To POS tag, we use the HMM tagger TnT (Brants, 2000) with the model from http: //corpus.leeds.ac.uk/mocky/. After finishing the corrections, we experimented with training and testing the TnT tagger (Brants,2000) on the& quot; old& quot; and on the& quot; corrected& quot; version of NEGRA?. To make them useful, the necessary preprocessing steps must have been done. The texts were first automatically segmented and tokenized and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective WILS training data. POS tags, on the other, represent more of a challenge with only 91.6% NORM LEMMA POS Agreed tokens (out of 57,845) 56,052 55,217 52,959 Accuracy (%) 96.9% 95.5% 91.6% Table 3: Inter-annotator agreement agreement between two annotators, which is cons id erably lower than the agreement level reported for annotating a corpus of modern German using STTS, at 98.6% (Brants, 2000a). We further plan to retrain state-of the-art POS taggers such as the TreeTagger and TnT Tagger (Brants, 2000b) on our data. Finally, we plan to investigate how linguistic annotations can be automatically integrated in the TEI annotated version of the corpus to produce TEI con formant output.
 In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). For example, Jing (2000) trained her system on a set of 500 sentences from the Benton Foundation (http: //www.benton.org) and their reduced forms written by humans. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Jing (2000) was perhaps the first to tackle the sentence compression problem. Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). Jing and McKeown (H. Jing, 2000) studied a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (Jing, 2000). The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts. To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). Jing (2000) was perhaps the first to tackle the sentence compression problem. Table 5 shows a 5 sentence summary created using algorithm 1 for the paper A00-1043 (Jing, 2000). To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). In these approaches, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries. In addition, an automatic evaluation method based on context-free deletion decisions has been proposed by Jing (2000). Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005).
Choi (2000) used the rank of the cosine, rather than the cosine itself, to measure the similarity of sentences. The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3. (Choi, 2000) Table 1 gives the corpus statistics. (Choi, 2000) Segmentation accuracy was measured by the probabilistic error metric proposed by Beeferman, et al (1999) Low indicates high accuedges in the minimum-cost path, then the resulting segmentation often contains very small segments consisting of only one or two sentences. in Table 3 are slightly different from those listed in Table 6 of Choi's paper (Choi, 2000). which are implemented in Java (Choi, 2000), due to the difference in programming languages. As dataset the Choi dataset (Choi, 2000) is used. In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). The C99 algorithm (Choi, 2000) makes a linear segmentation based on a divisive clustering strategy and the cosine similarity measure between any two minimal units. Choi (2000) designed an artificial dataset, built by concatenating short pieces of texts that have been extracted from the Brown corpus. C99 KA to denote the C99 algorithm (Choi, 2000) when the aver age number of boundaries in the reference data was provided to the algorithm. For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000). Therefore, we only use manual transcriptions of these lectures. Synthetic Corpus Also as part of our analysis, we used the synthetic corpus created byChoi (2000) which is commonly used in the evaluation of segmentation algorithms. We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately. We also plot the Receiver Operating Character is tic (ROC) curve to gauge performance at a finer level of discrimination (Swets, 1988). We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000). Comparison with local dependency models We compare our system with the state-of-the-art similarity-based segmentation system developed by Choi (2000). We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a held out development set of three lectures. Existing methods for topic segmentation typically assume that fragments of text (e.g. sentences or sequences of words of a fixed length) with similar lexical distribution are about the same topic; the goal of these methods is to find the boundaries where the lexical distribution changes (e.g. Choi (2000), Malioutov and Barzilay (2006)). Nevertheless, we will compare our topic identification approach to a state-of-the-art topic segmentation algorithm (Choi, 2000) in the evaluation. Choi 2000 Choi's (2000) state-of-the-art approach to finding segment boundaries. We use the freely available C99 software described in Choi (2000), varying a parameter that allows us to control the average number of sentences per segment and reporting the best result on the test data.
We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest).
 As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000). Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank. We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997). After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article. The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003). In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus. The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows. Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically. Thus, it is informative to compare our results with those reported in (Blaheta and Charniak, 2000) for this same task. First, because of the different definition of a correctly identified constituent in the parser's output, we apply our method to a greater portion of all labels produced by the parser (95% vs. 89% reported in (Blaheta and Charniak, 2000)). TiMBL performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (Blaheta and Charniak, 2000). The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents. Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions. Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size.
For instance, Chodorow and Leacock (2000) point out that the word concentrate is usually used as a noun in a general corpus whereas it is a verb 91% of the time in essays written by non-native learners of English. Chodorow and Leacock (2000) try to identify errors on the basis of context, as we do here, and more specifically a 2 word window around the word of interest, from which they consider function words and POS tags. N-gram-based approaches to the problem of error detection have been proposed and implemented in various forms by Atwell (1987), Bigert and Knutsson (2002), and Chodorow and Leacock (2000) amongst others. Chodorow and Leacock (2000) use a mutual information measure in addition to raw frequency of n grams. Among unsupervised checkers, Chodorow and Leacock (2000) exploits negative evidence from edited textual corpora achieving high precision but low recall, while Tsao and Wible (2009) uses general corpus only. For example, Chodorow and Leacock (2000) exploit bigrams and trigrams of function words and part-of-speech (PoS) tags, while Sun et al (2007) use labeled sequential patterns of function, time expression, and part-of-speech tags. For example, unsupervised systems of (Chodorow and Leacock, 2000) and (Tsao and Wible, 2009) leverage word distributions in general and/or word-specific corpus for detecting erroneous usages while (Hermet et al, 2008) and (Gamon and Leacock, 2010) use Web as a corpus. Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). An example is the error detection method (Chodorow and Leacock, 2000), which identifies unnatural sequences of POSs as grammatical errors in the writing of learners. Our method outperforms Microsoft Word03 and ALEK (Chodorow and Leacock, 2000) from Educational Testing Service (ETS) in some cases. An unsupervised method (Chodorow and Leacock, 2000) is employed to detect grammatical errors by inferring negative evidence from TOEFL administrated by ETS. In addition, we compared our technique with two other methods of checking errors, Microsoft Word03 and ALEK method (Chodorow and Leacock, 2000). In this paper, we compare our technique with the grammar checker of Microsoft Word03 and the ALEK (Chodorow and Leacock, 2000) method used by ETS. Chodorow and Leacock (2000) utilized mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus. The filter-based system combines unsupervised detection of a set of possible errors (Chodorow and Leacock, 2000) with hand-crafted filters designed to reduce this set to the largest subset of correctly flagged errors and the smallest possible number of false positives. Chodorow and Leacock (2000) found that low-frequency bigrams (sequences of two lexical categories with a negative log-likelihood) are quite reliable predictors of grammatical errors.
The recent approach for editing extracted text spans (Jing and McKeown,2000) may also produce improvement for our algorithm. Jing and McKeown (2000) first extract sentences, then remove redundant phrases, and use (manual) recombination rules to produce coherent output. Mean ratings for automatic compressionsnally, we added a simple baseline compression algorithm proposed by Jing and McKeown (2000) which removed all prepositional phrases, clauses, to infinitives, and gerunds. Like the work of Jing and McKeown (2000) and Mani et al (1999), our work was inspired by the summarization method used by human abstractors. Our two-step model essentially belongs to the same category as the works of (Mani et al, 1999) and (Jing and McKeown, 2000). We found that the deletion of lead parts did not occur very often in our summary, unlike the case of Jing and McKeown (2000). The task of sentence compression (or sentence reduction) can be defined as summarizing a single sentence by removing information from it (Jing and McKeown, 2000). First, splitting and merging of sentences (Jing and McKeown, 2000), which seems related to content planning and aggregation. Rewrite operations other than deletion tend to be hand-crafted and domain specific (Jing and McKeown, 2000). Jing and McKeown (2000) studied what edits people use to create summaries from sentences in the source text. Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998) (Finley and Harabagiu, 2002) (DeJong, 1982) and (2) approaches using Natural Language Generation (NLG) systems (Saggion and Lapalme, 2002) (Jing and McKeown, 2000). Close to the problem studied here is Jing and McKeown's (Jing and McKeown,2000) cut-and-paste method founded on Endres Niggemeyer's observations. Jing and McKeown (2000) manually analyzed 30 human-written summaries, and find that 19% of sentences can not be explained by cut-and-paste operations from the source text.
More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. [Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few.
Section 5 compares our approach too thiers in the literature, in particular that of (Miller et al., 2000). The basic approach we described is very similar to the one presented in (Miller et al, 2000) however there are a few major differences: in our approach the augmentation of the syn tactic tags with semantic tags is straightforward due to the fact that the semantic constituents are matched exactly 5. The semantic annotation required by our task is much simpler than that employed by (Miller et al, 2000). One possibly beneficial extension of our work suggested by (Miller et al, 2000) would be to add semantic tags describing relations between entities (slots), in which case the semantic constraints would not be structured strictly on the two levels used in the current approach, respectively frame and slot level. Similar to the approach in (Miller et al, 2000) we initialized the SLM statistics from the UPenn Tree bank parse trees. Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns. One interesting system that does not belong to the above class is that of Miller et al (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model. (Miller et al, 2000) have combined entity recognition, parsing, and relation extraction into a jointly-trained single statistical parsing model that achieves improved performance on all the subtasks. Part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable. Miller et al (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types. Whereas Miller et al (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance. The syntactic model in (Miller et al, 2000) is similar to Collins', but does not use features like subcat frames and distance measures. Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2. Miller et al (2000) adapt a probabilistic context-free parser for information extraction by augmenting syntactic labels with entity and relation labels. Most of the approaches for relation extraction rely on the mapping of syntactic dependencies, such as SVO, onto semantic relations, using either pattern matching or other strategies, such as probabilistic parsing for trees augmented with annotations for entities and relations (Miller et al 2000), or clustering of semantically similar syntactic dependencies, according to their selectional restrictions (Gamallo et al, 2002). This includes parsing and relation extraction (Miller et al, 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al, 2004). For example, Miller et al (2000) showed that performing parsing and information extraction in a joint model improves performance on both tasks. Miller et al (2000) address the task of relation extraction from the statistical parsing viewpoint. Rule-based methods (Miller et al, 2000) employ a number of linguistic rules to capture relation patterns.
More recently, a domain independent system has been trained on general function tags such as Manner and Temporal by Blaheta and Charniak (2000). These features are generated using the Stanford parser (Klein and Manning, 2003) and a function tagger (Blaheta and Charniak, 2000). In function labeling level, EXT that signifies degree, amount of the predicates should be grouped into adverbials like in the work of (Blaheta and Charniak, 2000) and (Merlo and Musillo, 2005). As in the work of (Blaheta and Charniak, 2000) and (Merlo and Musillo, 2005), to avoid calculating excessively optimistic values, constituents bearing the O label are not counted in for computing overall precision, recall and F-score. The features we used are borrowed from feature trees described in (Blaheta and Charniak, 2000). A trivial difference is that in our system the head for prepositional phrases is defined as the prepositions themselves (not the head of object of prepositional phrases (Blaheta and Charniak, 2000)), because we think that the preposition itself is a more distinctive attribute for different semantic meanings. Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically. Thus, it is informative to compare our results with those reported in (Blaheta and Charniak, 2000) for this same task. The difference in the accuracy is due to two reasons. First, because of the different definition of a correctly identified constituent in the parser? s output, we apply our method to a greater portion of all labels produced by the parser (95% vs. 89% reported in (Blaheta and Charniak, 2000)). TiMBL performed well on tasks where structured, more complicated and task-specific statistical models have been used previously (Blaheta and Charniak, 2000). Many researchers ((Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicate argument recognition tasks, such as semantic roles.  While the function of a constituent and its structural position are often correlated, they some (Blaheta and Charniak, 2000) talk of function tags. Following (Blaheta and Charniak, 2000), we refer to the first class as syntactic function labels, and to the second class as semantic function labels. Like previous work (Blaheta and Charniak, 2000), we complete the sets of syntactic and semantic labels by labelling constituents that do not bear any function label with a NULL label. Following (Blaheta and Charniak, 2000), incorrectly parsed constituents will be ignored (roughly 11% of the total) in the evaluation of the precision and recall of the function labels, but not in the evaluation of the parser. In work that predates the availability of Framenet and Propbank, (Blaheta and Charniak, 2000) define the task of function labelling for the first time and highlight its relevance for NLP. Blaheta and Charniak (2000) assume a richer input representation consisting of labelled trees produced by a tree bank grammar parser, and use the tree bank again to train a further procedure that assigns grammatical function tags to syntactic constituents in the trees. Blaheta and Charniak (2000) report an F-score of 87% for assigning grammatical function tags to constituents, but the task, and therefore the scoring method, is rather different.
In a sense this is encouraging, as it motivates our most exciting future work: augmenting this simple model to explicitly capture complementary information such as distributional semantics (Blei et al, 2003), diathesis alternations (McCarthy, 2000) and selectional preferences (OSeaghdha, 2010). In contrast to comparing head nouns directly, McCarthy (2000) instead compares the selectional preferences for each of the two slots (captured by a probability distribution over WordNet). The approach of McCarthy (2000), on the other hand, addresses the generalization problem by comparing probability distributions over WordNet. As mentioned above, McCarthy (2000) suggested the use of selectional profiles to capture generalizations over argument slots, so that two argument slots could be effectively compared for detecting alternations.  In the first method (that of McCarthy, 2000), the two profiles become identical. We evaluate our method on the causative alternation in order for comparison to the earlier methods of McCarthy (2000) and Merlo and Stevenson (2001). This means that the kind of straightforward propagation method used by McCarthy (2000) is not applicable to selectional profiles of this type. The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. By comparison, McCarthy (2000) attained 73% accuracy on her set of hand-selected test verbs in a similar task; however, when applied to our various sets of randomly selected verbs, our replication of her method performed very poorly, rarely reaching above chance performance. The mapping for the dependents in the alternation can be taken from existing lexical resources (Dorr, 1997), learned from corpora (McCarthy, 2000) or learned from existing lexicons (Bond et al, 2002). As in McCarthy (2000), we cast argument alternation detection as a comparison of sense profiles across two different argument positions of a verb. Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. Because we demonstrate our new SPD measure on the same problem as McCarthy (2000), we provide more detail of her method here, for comparison. In McCarthy (2000), an error analysis reveals that the best method has more false positives than false negatives - some slots are considered overly similar because the sense profiles are compared at a coarse-grained level, losing fine semantic distinctions. In McCarthy (2000), the distributions are propagated to the lowest common subsumers (i.e., the nodes labelled B, C, and D). In the first method (that of McCarthy, 2000), the two profiles become identical. We evaluate our method on the causative alternation in order for comparison to the earlier method of McCarthy (2000). This means that the kind of straightforward propagation method used by McCarthy (2000) is not applicable to sense profiles of this type. The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them.
We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to~in and verbs associated with a following infinitive marker to~to. This work is to be distinguished from supervised part-of-speech disambiguation systems, which use labeled training data (Church, 1988), unsupervised disambiguation systems, which use a dictionary of possible tags for each word (Merialdo, 1994), or prototype driven systems which use a small set of prototypes for each class (Haghighi and Klein, 2006). Much of this work offers the prospect that a disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., Church (1988)) can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency. The first work on this topic was done back in the eighties (Church, 1988). Church's Parts of speech [Church, 1988] performs not only part-of-speech analysis, but it also identities the most simple kinds of noun phrases mostly sequences of determiners, premodifiers and nominal heads by inserting brackets around them. The appendix in [Church, 1988] lists the analysis of a small text. This is quite feasible using statistical taggers like those of Garside (1987), Church (1988) or Foster (1991) which achieve performance upwards of 97% on unrestricted text. The shallow parser constructs Verb Groups (VGs) and basic Noun Phrases (NPs), also called BaseNPs [Church 1988]. Several approaches provide similar output based on statistics (Church 1988, Zhai 1997, for example), a finite-state machine (AitMokhtar and Chanod 1997), or a hybrid approach combining statistics and linguistic rules (Voutilainen and Padro 1997). As we said at the out 211 set, we don't necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work. Instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (Merialdo, 1992), (Church, 1988), (Hajji, Hladk~, 1997)) used in the past (notwithstanding some exceptions, such as Maximum Entropy and rule-based taggers), we are using here a direct approach to modeling, for which we have chosen an exponential probabilistic model. Regardless of whether one is using HMMs, maximum entropy conditional sequence models, or other techniques like decision trees, most systems work in one direction through the sequence (normally left to right, but occasionally right to left ,e.g., Church (1988)). Figure 2: Change in the polarity of the sentences citing Church (1988) paper how the way a published work is perceived by the research community over time. Figure 2 shows the result of this analysis when applied to the work of Kenneth Church (1988) on part-of-speech tagging. For example, the analysis illustrated in Figure 2 shows that the work of Ken Church (1988) on part-of-speech tagging received significant positive feedback during the 1990s and until early 2000s before it started to receive more negative feedback. The high accuracy achieved by a corpus-based approach to part-of-speech tagging and noun phrase parsing, as demonstrated by (Church, 1988), has inspired similar approaches to other problems in natural language processing, including syntactic parsing and word sense disambiguation (WSD). Termight uses a part of speech tagger (Church, 1988) to identify a list of candidate terms which is then filtered by a manual pass. The multi-word terms match a small set of syntactic patterns defined by regular expressions and are found by searching a version of the document tagged with parts of speech (Church, 1988). More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993). The algorithm runs in lockstep with a part-of-speech tagger (Church, 1988), which is used for deciding possible word replacements.
Following [Rambow and Korelsky, 1992], we call this additional planning task sentence planning (even though some operations may cross sentence boundaries). Tasks in the generation process have been divided into three stages (Rambow and Korelsky,1992): the text planner has access only to information about communicative goals, the dis course context, and semantics, and generates a non-linguistic representation of text structure and content. This paper addresses the area of text generation known as micro planning [Levelt 1989, Panaget 1994, Huang and Fiedler 1996], or sentence planning [Rambow and Korelsky 1992]. The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al, 1992), and JOYCE (Rambow and Korelsky, 1992). The second approach is natural language generation (NLG), which customarily divides the generation process into three modules (Rambow and Korelsky, 1992): (1) Text Planning, (2) Sentence Planning, and (3) Surface Realization.
The two systems we use are ENGCG (Karlsson et al, 1994) and the Xerox Tagger (Cutting et al, 1992). The Xerox Tagger 1, XT, (Cutting et al, 1992) is a statistical tagger made by Doug Cutting, Julian Kupiec, Jan Pedersen and Penelope Sibun in Xerox PARC. There are many POS taggers developed using different techniques for many major languages such as transformation-based error-driven learning (Brill, 1995), decision trees (Black et al, 1992), Markov model (Cutting et al, 1992), maximum entropy methods (Ratnaparkhi, 1996) etc for English. The initial phase relies on a parser that draws on the SPECIALIST Lexicon (McCray et al 1994) and the Xerox Part-of-Speech Tagger (Cutting et al 1992) to produce an underspecified categorial analysis. The phrases in our bibliographic and clinical samples were then submitted to an underspecified syntactic analysis described by Rindflesch et al (2000) that draws on a stochastic tagger (see (Cutting et al, 1992) for details) as well as the SPECIALIST Lexicon, a large syntactic lexicon of both general and medical English that is distributed with the UMLS. As a common strategy, POS guessers examine the endings of unknown words (Cutting et al 1992) along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (Weischedel et aL, 1993). The tagger used is thus one that does not need tagged and disambiguated material to be trained on, namely the XPOST originally constructed at Xerox Parc (Cutting et al 1992, Cutting and Pedersen 1993). In such cases, additional information may be coded into the HMM model to achieve higher accuracy (Cutting et al, 1992). The semi-supervised model described in Cutting et al (1992), makes use of both labeled training text and some amount of unlabeled text.  (Chanod and Tapanainen, 1995) compare two tagging frameworks for tagging French, one that is statistical, built upon the Xerox tagger (Cutting et al., 1992), and another based on linguistic constraints only. The XEROX tagger comes with a list of built-in ending guessing rules (Cutting et al,1992). This analysis depends on the SPECIALIST Lexicon and the Xerox part-of-speech tagger (Cutting et al, 1992) and provides simple noun phrases that are mapped to concepts in the UMLS Metathesaurus using MetaMap (Aronson, 2001). The prime public domain examples of such implementations include the Trigrams? n? Tags tagger (Brandts 2000), Xerox tagger (Cutting et al 1992) and LT POS tagger (Mikheev 1997). It is also possible to train statistical models using unlabeled data with the expectation maximization algorithm (Cutting et al, 1992).  It has been known for some years that good performance can be realized with partial tagging and a hidden Markov model (Cutting et al, 1992).  (Cutting et al, 1992) reported very high results (96% on the Brown corpus) for unsupervised POS tagging using Hidden Markov Models (HMMs) by exploiting hand-built tag dictionaries and equivalence classes. In the tagging literature (e.g., Cutting et al (1992)) an ambiguity class is often composed of the set of every possible tag for a word.
Candidates considered in the semantic tagging process are noun phrases NP, proposition phrases PP, verb phrases VP, adjectives ADJ and adverbs ADV. To gather these candidates we used the Brill transformational tagger (Brill, 1992) for the part-of speech step and the CASS partial parser for the parsing step (Abney, 1994). The noun phrase extraction module uses Brill's POS tagger [Brill (1992)] and a base NPchunker [Ramshaw and Marcus (1995)].  In order to include features describing verb tense, we use Brill's part-of-speech tagger (Brill, 1992). Each part of speech (POS) is taken to be a feature, whose value is a count of the number of occurrences in the given utterance. In order to extract the linguistic features necessary for the ME model in WSD tasks, all sentences containing the target word are automatically part-of speech (POS) tagged using the Brill POS tagger (Brill, 1992). In the part-of-speech literature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al, 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al, 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones. Part of speech tags are assigned by Brill's tagger (Brill, 1992). We looked at three different lemmatizers: the lemmatizing backend of the XTAG project (XTAG Re search Group, 2001) 4, Celex (Baayen et al, 1995), and the lemmatizing component of an enhancedTBL tagger (Brill, 1992). TBL tagger (Brill, 1992) 7, and a TnT-style trigram tagger (Halacsy et al, 2007). All corpora were stemmed (Karp et al, 1992) and part-of-speech tagged (Brill, 1992). In order to extract the linguistic features necessary for the ME model, all sentences containing the target word were automatically part of-speech (POS) tagged using the Brill POS tagger (Brill, 1992). All 15,863 documents were tagged by a part-of-speech tagger (Brill, 1992) and stemmed using WordNet information (Fellbaum, 1998).  Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e.g. Cardie 1994).  Rule based POS tagging methods extract rules from training corpus and use these rules to tag new sentences (Brill, 1992) (Brill, 1994). Automated part of speech tagging (Brill, 1992) is a useful technique in term extraction (Frantziet al, 2000), a domain closely related to named entity recognition. In future work, we plan to identify such adjectives in Google excerpts using a Part of Speech tagger (Brill, 1992). The CONLL 2000 tags against which we measure our own results are in fact assigned by the Brill tagger (Brill 1992), and while these may not correlate perfectly with those that would have been assigned by a human linguist, we believe that the correlation is likely to be good enough to allow for an informative evaluation of our method. For the part-of-speech tagging problem, it is known that assigning the most common part of speech for each lexical item gives a baseline of 90% accuracy [Brill, 1992].
On the one hand, the linguistically-based or rule-based approaches use linguistic information such as PoS tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (Ananiadou, 1994), (Dagan and Church, 1994). Such a tool has proved extremely useful not only for translators, but also for bilingual lexicographers (Langlois, 1996) and terminologists (Dagan and Church, 1994). Our system is most similar to Termight (Dagan and Church, 1994) and TransSearch (Macklovitch et al, 2000).
Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94). Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful.  However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. On this point, I agree with Merialdo (1994) and Elworthy (1994).  Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting.  Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. We considered three taggers: the El worthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al, 2006), an enhanced. In [Elworthy, 1994], similar experiments were run. The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy.
Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994). that is hopefully better than any of its ingredients, an idea pionieered in (Frederking and Nirenburg, 1994). Frederking and Nirenburg (1994) produced the first MEMT system by combining outputs from three different MT engines based on their knowledge of the inner workings of the engines. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003). Now back in machine translation, we do find some work addressing such concern: Frederking and Nirenburg (1994) develop a multi-engine MT or MEMT architecture which operates by combining outputs from three different engines based on the knowledge it has about inner workings of each of the component engines. Brown and Frederking (1995) is a continuation of Frederking and Nirenburg (1994) with an addition of a n-gram based mechanism for a candidate selection. The first example of this approach was the multi-engine MT system (Frederking and Nirenburg, 1994), which builds a chart using the translation units inside each input system and then uses a chart walk algorithm to find the best cover of the source sentence. Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations. System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In Machine Translation (MT), there is a long tradition of combining multiple machine translations, as through a Multi-Engine MT (MEMT) architecture; the origins of this are generally credited to Frederking and Nirenburg (1994). System combination procedures, on the other hand, generate translations from the output of multiple component systems by combining the best fragments of these outputs (Frederking and Nirenburg, 1994). It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). It proves that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). Combinations of MT systems into multi-engine architectures have a long tradition, starting perhaps with (Frederking and Nirenburg, 1994).
finding sentence boundaries (Reynar and Ratnaparkhi, 1997). This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the Stanford Parser (Klein and Manning, 2003). We trained a publicly available sentence splitter (Reynar and Ratnaparkhi, 1997) on a small manually annotated sample (1,000 sentences per domain per language) and applied it to our corpora. We also used MXTerminator (Reynar and Ratnaparkhi, 1997) for sentence segmentation, MINIPAR (Lin, 1993) for lemmatization and dependency parsing, and MATLAB3 for SVD computation. Sentence segmentation Off-the-shelf sentence segmentators tend to be trained on newswire texts (Reynar and Ratnaparkhi, 1997), which significantly differ from the noisy text in our corpus. Another statistical system, mxTerminator (Reynar and Ratnaparkhi, 1997) employs simpler lexical features of the words to the left and right of the candidate period. One common objection to supervised SBD systems is an observation in (Reynar and Ratnaparkhi, 1997), that training data and test data must be a good match, limiting the applicability of a model trained from a specific genre. Many alternatives suggest themselves to expand the options, including maximum entropy models, which have been previously successfully applied to, inter alia, sentence boundary detection (Reynar and Ratnaparkhi, 1997), and transformation-based learning, as used in part-of-speech tagging and statistical parsing applications (Brill, 1995). The corpus was prepared using MXTerminator (Reynar and Ratnaparkhi,1997) for sentence segmentation, BBN Identifinder (Bikel et al, 1999) for named entity recognition, as well as the aforementioned ASSERT for identification of verb predicate-argument structures and PropBank-style semantic role labeling of the arguments. As the text part may consist of more than one sentence, we first perform sentence splitting using Mxterminator (Reynar and Ratnaparkhi, 1997), a maximum 83 entropy-based end of sentence classifier trained onthe Penn Treebank data. We have tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997). Obtained by segmenting (Reynar and Ratnaparkhi, 1997) the interviewee turns, and discarding sentences with only one word. finding sentence boundaries (Reynar and Ratnaparkhi, 1997). Table 1 presents information about article length (measured in sentences, as determined by the sentence separator of Reynar and Ratnaparkhi (1997)), vocabulary size, and token/type ratio for each domain. Each abstract set was prepared for annotation as follows: the order of the abstracts was randomized and the abstracts were broken into sentences using Mxterminator (Reynar and Ratnaparkhi, 1997). It can be resolved fairly easily with rules in the form of regular expressions or in a machine-learning framework (Reynar and Ratnaparkhi, 1997). The contents of these URLs were collected and only distinct web pages were retained. We use an HTMLparser3 to extract the textual con tents, and perform sentence segmentation (Reynar and Ratnaparkhi, 1997) on the parsed web pages. To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997). Sentence splitting, using mxterminator (Reynar and Ratnaparkhi, 1997). To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006).
To identify associative constructions, we first process our texts using Conexor's FDG parser (Tapanainen and Jarvinen, 1997). For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. It is developped at the Xerox Research Centre Europe (XRCE) and shares the same computationnal paradigm as the PNLPL approach (Jensen, 1992) and the FDGP approach (Tapanainen and Jarvinen, 1997).
This type of model is used to facilitate the syntactic annotation of the NEGRA corpus of German newspaper texts (Skut et al, 1997). For our experiments, we use the NEGRA corpus (Skut et al, 1997). As data we use version 2 of the Negra (Skut et al1997) tree bank, with the common training.  According to Skut et al (1997) tree banks have to meet the following requirements: 1. that could best deal with the free word order displayed by Basque syntax (Skut et al, 1997). In contrast, some other tree banks, such as the German NeGra and TIGER tree banks allow annotation with crossing branches (Skut et al, 1997). Non-local dependencies can then be expressed directly by grouping all dependent elements under a single node. Our data source is the German NeGra tree bank (Skut et al, 1997). The parsing models we present are trained and tested on the NEGRA corpus (Skut et al, 1997), a hand parsed corpus of German newspaper text containing approximately 20,000 sentences. The present paper addresses this question by proposing a probabilistic parsing model trained on Negra (Skut et al, 1997), a syntactically annotated corpus for German. The annotation scheme (Skut et al, 1997) is modeled to a certain extent on that of the Penn Treebank (Marcuset al, 1993), with crucial differences.  German is considerably more in ectional which means that discarding functional information is more harmful, and which explains why the NEGRA an notation has been conceived to be quite at (Skut et al, 1997). The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and co reference in formation (Negra) (Skut et al, 1997).  CKK uses the Dubey and Keller (2003) parser, which is trained on the Negra corpus (Skut et al, 1997). Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA 10 (Skut et al 1997) and the Chinese CTB10 (Xue et al 2002) both containing 2200+ sentences 10 words after removing punctuation. A comparison of unlexicalised PCFG parsing (Kubler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the Tu? Ba D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991). 
Bikel et al (Bikel et al, 1997) report on Nymble, an HMM-based name tagging system operating in English and Spanish. Most commonly, feature-based classifiers use a set of capitalisation features and a sentence-initial feature (Bikel et al, 1997). Nymble (Bikel et al, 1997) uses statistical learning to acquire a Hidden Markov Model (HMM) that recognises NEs in text. (Bikelet al, 1997) are other examples of the use of HMMs. Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al (1997), McCallum et al (2000) and Laerty et al (2001). We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al 1997). The HMM tagger generally follows the Nymble model (Bikel et al 1997), and uses best-first search to generate N-Best hypotheses for each input sentence. The base system is an HMM based tagger, similar to (Bikel et al, 1997). The alternative to true casing text is to destroy case information in the training material SNORIFY procedure in (Bikel et al, 1997). The typical machine learning approaches for English NE are transformation-based learning [Aberdeen et al 1995], hidden Markov model [Bikel et al. 1997], maximum entropy model [Borthwick, 1999], support vector machine learning [Eunji Yi et al 2004], unsupervised model [Collins et al 1999] and etc. Many of the previous studies of Bio-NER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al, 1997), the dictionary HMM model (Kouetal., 2005) and Maximum Entropy Markov Mod els (MEMMs) (Finkel et al, 2004). These include rule-based systems [Krupka 1998], Hidden Markov Models (HMM) [Bikel et al 1997] and Maximum Entropy Models (MaxEnt) [Borthwick 1998]. constrained HMM Our original HMM is similar to the Nymble [Bikel et al 1997] system that is based on bigram statistics. In addition, an automatic named entity tagger (Bikel et al, 1997) was run on the sentences to map proper nouns to a small set of semantic classes. A. wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. Another related work is (Bikel et al, 1997) which used HMMs as part of its modelling for the name finding problem in information extraction. A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al, 1997, Bikel et al, 1997). Our baseline name tagger is based on an HMM that generally follows the Nymble model (Bikel et al 1997). As hidden Markov models have been used both for name finding (Bikel et al (1997)) and tokenization (Cutting et al. In addition, an automatic named entity tagger (Bikel et al, 1997) was run on the sentences to map proper nouns to a small set of semantic classes.
The only exception is in (Wacholder et al 1997) where the reported performance for the sole semantic disambiguation task of PNs is 79%. Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). Ravin and Kazi (1999) further refined the method of solving co-reference through measuring context similarity and integrated it into Nominator (Wacholder et al, 1997), which was one of the first successful systems for named entity recognition and co-reference resolution. The entity identification and in-document co reference components resemble the Nominator system (Wacholder et al, 1997). As Wacholder et al (1997) noted, it is fairly common for one of the mentions of an entity in a document to be a long, typical surface form of that entity (e.g., George W. Bush), while the other mentions are shorter surface forms (e.g., Bush).  We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997). Wacholder et al (1997) use hand-written rules and knowledge bases to classify proper names into broad categories. Approaches to this problem include Wacholder et al (1997), focusing on the variation of surface name for a given referent, and Smith and Crane (2002), resolving geographic name ambiguity. A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al, 1997, Bikel et al, 1997).
 These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from speech, event, and reference time orderings, discussed below). We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can be realized by the fast portable realizer RealPro (Lavoie and Rambow, 1997).  In this sense, they are functionally similar to the REALPRO system (Lavoie and Rambow, 1997). We automatically converted the phrase structure output of the Collins parser into the syntactic dependency representation used by our syntactic realizer, RealPro (Lavoie and Rambow, 1997). Discourse relations are essentially rhetorical structure theory (RST) relations [Mann and Thompson, 1987], and messages are represented using a deep-syntactic representation, which is loosely based on RealPro [Lavoie and Rambow, 1997]. Currently, FLO supports the LinGOrealiser (Carrollet al, 1999), but we are also looking at FLO modules for RealPro (Lavoie and Rambow, 1997) and FUF/SURGE (Elhadad et al, 1997). In the second phase, the sentence plan ranker (SPR) ranks sentence plans generated by the SPG, and then selects the top-ranked out put as input to the surface realizer, RealPro (Lavoie and Rambow, 1997). It passes propositions and a goal to the sentence-level realization module which uses templates to build the deep syntactic structures required by the RealPro realizer (Lavoie and Rambow, 1997) for generating a string that communicates the goal. As rightly pointed out by Belz (2008), traditional wide coverage realizers such as KPML (Bateman et al, 2005), FUF/SURGE (Elhadad and Robin, 1996) and RealPro (Lavoie and Rambow, 1997), which were also intended as off-the-shelf plug-in realizers still tend to require a considerable amount of work for integration and fine-tuning of the grammatical and lexical resources.  Realization is achieved with the RealPro surface realizer (Lavoie and Rambow, 1997). The top ranked sentence plan output by the SPR is input to the RealPro surface realizer which produces a surface linguistic utterance (Lavoie and Rambow, 1997). Finally, surface realization is performed by interfacing RealPro (Lavoie and Rambow, 1997) with a language model. The realizer takes each sentence in the story and reformulates it into input compatible with the RealPro (Lavoie and Rambow,1997) text generation engine. The top-ranked candidateis selected for presentation and verbalized using a language model interfaced with RealPro (Lavoie and Rambow, 1997), a text generation engine. The surface realization process is performed by RealPro (Lavoie and Rambow (1997)). Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as REALPRO (Lavoie and Rambow, 1997), ALETH GEN (Coch, 1996), KPML (Bateman, 1997), FUF/SURGE (Elhadad and Robin, 1996), HALO GEN (Langkilde, 2000), YAG (McRoy et al, 2000), and OPENCCG (White, 2006). Realisation involves two logically distinguishable tasks. Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998), represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework (Lavoie and Rambow, 1997).
The SCFs applicable to each verb are extracted automatically from corpus data using the system of Briscoe and Carroll (1997). On the other hand, we find automatic approaches to the induction of verb subcategorization information at the syntax-semantics interface for a large number of languages ,e.g. Briscoe and Carroll (1997) for English; Sarkar and Zeman (2000) for Czech; Schulteim Walde (2002a) for German; Messiant (2008) for French. Briscoe and Carroll (1997) and Korhonen et al (2000) use a grammar and a sophisticated parsing tool for argument-adjunct distinction. 67 search, but a starting point would be the approach by (Briscoe and Carroll, 1997).  (Brent 1993) estimated pe according to the acquisition system? s performance, while (Briscoe and Carroll 1997) calculated pe from the distribution of SCF types in ANLT and SCF tokens in Susanne as shown in the fol lowing equation. Further more, (Briscoe and Carroll 1997) applied their acquired English SCF lexicon to an intermediate parser, and reported a 7% improvement of both phrase-based precision and recall. We briefly outline our SCF extraction system for automatically extracting SCFs from corpora, which was based on the design proposed in Briscoe and Carroll (1997). We used the 14 verbs 4 selected by Briscoe and Carroll (1997) and evaluated our results of these verbs against the SCF entries in two gold standards: COMLEX (Grishman et al, 1994) and a manually constructed SCF set from the training data. Our experiments show that 14 verbs used in Briscoe and Carroll (1997) are ask, begin, believe, cause, expect, find, give, help, like, move, produce, provide, seem and sway. (Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct subcategorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative subcategorization frequency for individual predicates. In contrast, the system of (Briscoe and Carroll, 1997) distinguishes 163 verbal subcategorisation classes by means of a statistical shallow parser, a classifier of subcategorisation classes, and a priori estimates of the probability that any verb will be a member of those classes. It is a well-documented fact (Briscoe and Carroll,1997) that subcategorisation frames (and their frequencies) vary across domains. Several substantial resources exist: e.g., hand-crafted large-scale grammars like the English Resource Grammar (ERG Flickinger (2000)) and the Dutch Alpino Grammar (Bouma et al, 2001). Unfortunately, the construction of these resources is the manual result of human efforts and therefore likely to contain errors of omission and commission (Briscoe and Carroll, 1997). The preliminary experiment on biomedical verb classification (Korhonen et al, 2006) employed basic syntactic features only: SCFs extracted from corpus data using the system of Briscoe and Carroll (1997) which operates on the output of a domain-independent robust statistical parser (RASP) (Briscoe and Carroll, 2002). (Briscoe and Carroll, 1997)) is to extract SCFs from parse trees, introducing an unnecessary dependence on the details of a particular parser. Table 2: New Verb Classes 1000 citations, on average, for each verb. Our method for SCF acquisition (Korhonen, 2002) involves first using the system of Briscoe and Carroll (1997) to acquire a putative SCF distribution for each test verb from corpus data. The text is parsed using the RASP parser (Briscoe and Carroll, 2002), and sub categorizations are extracted using the system of Briscoe and Carroll (1997). In their study, they first acquire fine-grained SCFs using the unsupervised method proposed by Briscoe and Carroll (1997) and Korhonen (2002). We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of Briscoe and Carroll (1997) and (ii) applying a clustering mechanism to this information.
FERGUS (Bangalore and Rambow,2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Fergus (Bangalore and Rambow, 2000) used the Penn TreeBank as a corpus, requiring a more substantial transformation algorithm since it requires a lexical predicate-argument structure instead of the TreeBank's representation. Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways. WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al, 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000). These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000). Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000).  In recent years, there has been a steady stream of research in statistical text generation (see Langkilde and Knight (1998), and Bangalore and Rambow (2000)).  In other words, in generating a form f to express an input, one wants to maximize the probability of the form, P (f), with respect to some gold-standard corpus, and thus express the in put in a way that resembles the realizations in the corpus most closely (Bangalore and Rambow, 2000). FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. Both the model of Amalgam and that presented here differ considerably from the n-gram models of Langkilde and Knight (1998), the TAG models of Bangalore and Rambow (2000), and the stochastic generation from semantic representation approach of Soricutand Marcu (2006). Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. 
Unlike nouns, many adjectives are inherently subjective, and the number of adjectives in texts correlates with human judgements of their subjectivity (Hatzivassiloglou and Wiebe, 2000). Words that encode a desirable state (e.g., beautiful) have a positive SO, while words that represent undesirable states (e.g. absurd) have a negative SO (Hatzivassiloglou and Wiebe, 2000). Based on (Hatzivassiloglou and Wiebe, 2000) and (Turney, 2002), we consider four types of structures (as shown in Table 5) during sentiment phrase extraction. Separately, Hatzivassiloglou and Wiebe (2000) report a statistical correlation between the number of adjectives in a text and human judgments of subjectivity. Subsequently, Hatzivassiloglou and Wiebe (2000) showed that automatically detected gradable adjectives are a useful feature for subjectivity classification, while Wiebe (2000) introduced lexical features in addition to the presence/absence of syntactic categories. In addition, the presence of semantically oriented (positive and negative) words in a sentence is an indicator that the sentence is subjective (Hatzivassiloglou and Wiebe, 2000).
This is similar to the idea of topic signature introduced in (Lin and Hovy 2000). As a byproduct of our approach, we also propose an extractive summarization model based on phrasal queries to select the summary-worthy sentences in the conversation based on query terms and signature terms (Lin and Hovy, 2000). We use a method described in (Lin and Hovy, 2000) in order to identify such term sand their associated weight. Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization. In this regard, we use a method similar to Lin and Hovy (2000) to identify signature terms and subsequently use them to discard sentences that contain none or few such terms. There are signature terms for different topic texts (Lin and Hovy, 2000). Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff's sketch engine (Kilgarriff et al, 2004). We plan also to add other lexical functions to enrich our database with a ws. We plan to experiment. These are essentially scripts that provide information that is very useful in general reasoning as well as reasoning for NLP (e.g., Schank and Abelson 1977, Lin and Hovy 2000, Clark and Porter 2000). Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). the input and all words of the summary Topic signatures are words highly descriptive of the input, as determined by the application of log likelihood test (Lin and Hovy, 2000). Under this approach, topic representations like those introduced in (Lin and Hovy, 2000) and (Harabagiu, 2004) are used to identify aset of text passages that are relevant to a user's domain of interest. A similar approach is explored in Biryukov et al (2005), which uses Topic Signatures (Lin and Hovy, 2000) constructed around the target individual's name to identify sentences to be included in the biography. In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000). Topic Signatures (TS) are word vectors related to a particular topic (Lin and Hovy, 2000). Two methods are used: topic signature (Lin and Hovy, 2000): a topic signature is a family of related terms (topic, signature), where topic is the target concept and signature is a vector related ms. The topic in e formula is assigned with the domain ame. Our domain topic set contains 288 words extracted from the collection of student papers using topic-lexicon ex traction software; our feature (domain Word). The software extracts topic words based on topic signatures (Lin and Hovy, 2000), and was kindly provided by Annie Louis. Among them, query relevance, centroid (Radev et al, 2004) and signature term (Lin and Hovy, 2000) are most remarkable.
On the one hand machine learning is used to automate as much as possible the tasks an IE expert would perform in application development (Cardie 1997) (Yangarber et al 2000). learned, otherwise go to step 4 Previous algorithms which use this approach include those described by Yangarber et al (2000) and Stevenson and Greenwood (2005). The extraction patterns used by both Yangarber et al (2000) and Stevenson and Greenwood (2005) were based on SVO tuples extracted from dependency trees. Yangarber et al (2000) suggested a method where patterns were compared based on their distribution across documents in a corpus. Yangarber et al (2000) proposed an algorithm for learning extraction patterns for a small number of examples which greatly reduced the burden on the application developer and reduced the knowledge acquisition bottleneck. Yangarber et al (2000) chose an approach motivated by the assumption that documents containing a large number of patterns already identified as relevant to a particular IE scenario are likely to contain further relevant patterns. This approach has been shown to successfully acquire useful extraction patterns which, when added to an IE system, improved its performance (Yangarber et al, 2000). Architecture This architecture has been inspired by several existing seed-oriented minimally supervised ma chine learning systems, in particular by Snowball (Agichtein and Gravano, 2000) and ExDisco (Yangarber et al, 2000). ExDisco (Yangarber et al,2000) uses a bootstrapping mechanism to find new extraction patterns using unannotated texts and some seed patterns as the initial input. For example, the AutoSlog system (Riloff, 1993) uses pat terns which match certain grammatical categories, mainly nouns and verbs, in phrase chunked text while Yangarber et al (2000) use subject-verb object tuples derived from a dependency parse. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized ,e.g. Riloff (1996), Yangarber et al (2000), and Sekine (2006). Bootstrapping approaches are employed in (Riloff, 1996), (Yangarber et al, 2000), (Yangarber, 2003), and (Stevenson and Greenwood, 2005) in order to find IE patterns for domain-specific event extraction. For example, EXDISCO (Yangarber et al., 2000) used Wall Street Journal articles for training. meets the density criterion (as defined in (Yangarber et al, 2000)). AutoSlog TS, does not require a pre-annotated corpus, but does require one that has been split into subsets that are relevant vs. non-relevant subsets to the scenario. (Yangarber et al, 2000) attempts to find extraction patterns, without a pre-classified corpus, starting from a set of seed patterns. We first present the basic algorithm for pattern acquisition, similar to that presented in (Yangarber et al., 2000).  For an indirect evaluation of the quality of the learned patterns, we employ the text-filtering evaluation strategy, as in (Yangarber et al, 2000). Information Extraction (IE) systems typically use extraction patterns (e.g., Soderland et al (1995), Riloff (1996), Yangarber et al (2000), Califf and Mooney (2003)) or classifiers (e.g., Freitag (1998), Freitag and McCallum (2000), Chieu et al (2003), Bunescu and Mooney (2004)) to extract role fillers for events. We have chosen this evaluation strategy because this indirect approach was shown to correlate well with a direct evaluation, where the learned patterns were used to customize an IE system (Yangarberet al, 2000).
A statistical treatment of Question 1 is presented by Yeh (2000).  The others are always statistically significant at p? 0.005, calculated with approximate randomization (Yeh, 2000). advantage of test data with a label distribution similar to the training set. The best F score and WER are obtained using the combination of all three dictionaries, HB-dict+GHM-dict+S-dict. Furthermore, the difference between the results using HB-dict+GHM-dict+S-dict and HB-dict+GHMdict is statistically significant (p 0.01), based on the computationally-intensive Monte Carlo method of Yeh (2000), demonstrating the contribution of Sdict. We think that our tag-word pairs are effective because they are selected by a linguistically meaning Significance was measured with the randomized significance test described in (Yeh, 2000). The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi 3 We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). All statistical significance testing is done via the stratified shuffling test (Yeh, 2000). Table 5 shows a 0.4 percent F-score improvement over the baseline for that section, which is statistically significant at p &lt; 0.001, using the stratified shuffling test (Yeh, 2000). To determine the statistical significance of the difference in the performance of the systems we analyzed, we use the model described in (Yeh, 2000) as implemented in (Pado, 2006). It is also interesting to note that the best result on the validation set for estimation We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). All statistical significance tests in these experiments use the computationally-intensive randomisation test described in Yeh (2000), with p < 0.05. A stratified shuffling-based randomization test (Yeh, 2000) shows that the differences are statistically significant (p<0.05). We use the approximate randomization test (Yeh, 2000) for statistical significance of the difference between the basic sequential CRF and our second round CRF, which has additional features derived from the output of the first CRF. Statistical significance is tested using randomised estimation (Yeh, 2000) with p &lt; 0.05. We test the statistical significance of all above-baseline results using randomised estimation (p &lt; 0.05; Yeh (2000)), and present all such results in bold in our results tables. A wide range of exact and asymptotic tests as well as computationally intensive randomisation tests (Yeh, 2000) are available and add to the confusion about an appropriate choice. The aim of this paper is to formulate a statistical model that interprets the evaluation of ranking methods as a random experiment. Standard deviations for F scores were estimated with bootstrap re sampling (Yeh, 2000). We calculate statistical significance of performance differences using stratified shuffling (Yeh, 2000). However, when combined with word similarity features, context support improves over the basic method at a level of statistical significance (based on randomised estimation, p &lt; 0.05: Yeh (2000)), indicating the complementarity of the two methods, especially on Twitter data. Differences between models are tested for significance using stratified shuffling (Yeh, 2000), using a standard number of 10000 iterations.
First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b).  Alternative measures for the evaluation of one-to-one word links have been proposed in (Och and Ney, 2000a; Och and Ney, 2003). Furthermore, we do not split MWU links as proposed by (Och and Ney, 2000a). As in other uses of parallel corpora, good alignment is essential in order for the results to be meaningful (Och and Ney, 2000). Clearly we would benefit from better matching and alignment techniques, and we wonder if perhaps some of the alignment techniques used for parallel multi-lingual corpora (Och and Ney, 2000) could be adapted to help align our text-data corpora. We know that better alignment models have been proposed and extensively compared (Och and Ney, 2000). The data is from the Canadian Hansard, and reference alignments were originally produced by Franz Och and Hermann Ney (Och and Ney, 2000). Recent statistical machine translation (SMT) algorithms generate such a translation by incorporating an inventory of bilingual phrases (Och and Ney, 2000). Word alignment using GIZA++ toolkit (Och and Ney, 2000), the default configuration as available in training scripts for Moses. The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied. In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a). In order to improve transition models in the HMM based alignment, Och and Ney (2000a) extended the transition models to be word-class dependent. In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al, 1996, Och and Ney, 2000a). We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a). In (Och and Ney, 2000a), the word null is introduced to generate the French words that don &apos; t align to any English words. Tests sentence pairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a).  only presents AER results that are calculated after combination of word alignments of both E F and F E directions based on a set of heuristics proposed by Och and Ney (2000b). Alignments of both directions are generated and then are combined by heuristic rules described in (Och and Ney 2000b).
We integrate both empirical and symbolic knowledge sources as features into our system which outperforms the best known methods in statistical machine translation. Previous work on defining subtasks within statistical machine translation has been performed on ,e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). Cao and Li (2002) and Ismail and Manandhar (2010) the context of text units is used to identify term mappings. So far as we are aware, only Cao and Li (2002), who treat only base noun phrase (NP) mapping, consider the problem this way. Our work differs from that of Cao and Li (2002) in several ways. First they consider only terms consisting of noun noun pairs. Cao and Li (2002) have achieved 91% accuracy for the top three candidates using the Web as a comparable corpus. Cao and Li (2002) restricted candidate bilingual compound term pairs by consulting a seed bilingual lexicon and requiring their constituent words to be translation of each other across languages. On the other hand, in the framework of bilingual term correspondences estimation of this paper, the computational complexity of enumerating translation candidates can be easily avoided with the help of cross-language retrieval of relevant news texts. Furthermore, unlike Cao and Li (2002), bilingual term correspondences for compound terms are not restricted to compositional translation. In the first approach, translation candidates are validated through the search engine (Cao and Li, 2002). The scoring function? I is intended to evaluate the approach proposed in (Cao and Li, 2002). (Cao and Li, 2002) also proposed a method of compositional translation estimation for compounds. In the method of (Cao and Li, 2002), the translation candidates of a term are composition ally generated by concatenating the translation of the constituents of the term and are validated directly through the search engine. In this paper, we evaluate the approach proposed in (Cao and Li, 2002) by introducing a total scoring function 17 that is based on validating translation candidates directly through the search engine. Another important difference is that in (Fujii and Ishikawa, 2001), they evaluate only the performance of cross-language information retrieval but not that of translation estimation. (Cao and Li, 2002) proposed a method of compositional translation estimation for compounds. In the proposed method of (Cao and Li, 2002), translation candidates of a term are compositionally generated by concatenating the translation of the constituents of the term and are re-ranked by measuring contextual similarity against the source language term. One of the major differences of the technique of (Cao and Li, 2002) and the one proposed in this paper is that in (Cao and Li, 2002), they do not use the domain/topic specific corpus. Cao and Li (2002) propose a new method to translate base noun phrases. (Cao and Li, 2002) acquire noun phrase translations by making use of web data. Cao and Li (2002) described a new method for base noun phrase translation by using Web data. We translate NN compounds by way of a two-phase procedure, incorporating generation and selection (similarly to Cao and Li (2002) and Langkilde and Knight (1998)). One piece of research relatively closely related to our method is that of Cao and Li (2002), who use bilingual bootstrapping over Chinese and English web data in various forms to translate Chinese NN compounds into English.
For this task, we use a named entity recognizer (Isozaki and Kazawa, 2002). Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers. Several approaches used classifiers such as decision trees or SVMs (Isozaki and Kazawa, 2002). Isozaki and Kazawa (2002) compared three commonly used methods for named entity recognition the SVM with quadratic kernel, maximal entropy method, and a rule based learning system, and showed that the SVM-based system performed better than the other two. (Since we used Isozaki's methods (Isozaki and Kazawa, 2002), the run-time complexity is not a problem.) Kudo and Matsumoto (2002) proposed an SVM-based Dependency Analyzer for Japanese sentences. In natural language applications, the size |SV| tends to be very large (Isozaki and Kazawa, 2002), often above 10,000. Kernel Expansion (Isozaki and Kazawa, 2002) is used to transform the d-degree polynomial kernel based classifier into a linear one, with a modified decision function y (x) =sgn (w xd+ b). However, even the sparse-representation version of w tends to be very large: (Isozaki and Kazawa, 2002) report that some of their second degree expanded NER models were more than 80 times slower to load than the original models (and 224 times faster to classify). This approach obviously does not scale well, both to tasks with more features and to larger degree kernels. PKE Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003). Support vector machines (e.g., Vapnik (1995), Joachims (1998)) are a different kind of kernel method that, unlike KPCA methods, have al ready gained high popularity for NLP applications (e.g., Takamura and Matsumoto (2001), Isozaki and Kazawa (2002), Mayfield et al (2003)) including the word sense disambiguation task (e.g., Cabezas et al (2001)). For example, an SVM-based NE-chunker run sat a rate of only 85byte/sec, while previous rule based system can process several kilobytes per second (Isozaki and Kazawa, 2002). Isozaki et al propose an XQK (eXpand the Quadratic Kernel) which can make their Named-Entity recognizer drastically fast (Isozaki and Kazawa, 2002). For the experiments reported here, we adopt a Support Vector Machine (SVM) learning paradigm not only because it has recently been used with success in different tasks in natural language processing (Isozaki and Kazawa, 2002), but it has been shown particularly suitable for text categorization (Kumar and Gopal, 2009) where the feature space is huge, as it is in our case. (Ikehara et al, 1997), which is similar to WordNet in English, as the attributes of the node. The chunks and their relations in the texts were analyzed by cabocha (Kudo and Matsumoto, 2002), and named entities were analyzed by the method of (Isozaki and Kazawa, 2002). Isozaki and Kazawa (2002) studied the use of SVM instead. Isozaki (Isozaki and Kazawa,2002) controls the parameters of a statistical morphological analyzer so as to produce more fine-grained output. Isozaki (Isozaki and Kazawa, 2002) introduces the thesaurus NTT Goi Taikei (Ikehara et al, 1999) to augment the Table 5: The depth of redundant analysis and the extraction accuracy Pair Wise Method Depth of morph. In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). Kernel expansion (KE) was proposed by Isozaki and Kazawa (2002) to convert Eq.  Words, chunks and their relations in the texts were analyzed by CaboCha (Kudoand Matsumoto, 2002), and named entities were analyzed by the SVM-based NE tagger (Isozaki and Kazawa, 2002).
This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently byWiddows and Dorow (2002). The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (Veronis, 2004) or Pagerank (Agirre et al, 2006). In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002).  The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. This is the same general observation exploited by (Widdows and Dorow, 2002), who try to find graph regions that are more connected internally than externally. The major guideline in this part of the evaluation was to compare our results with previous work having a similar goal (Widdows and Dorow, 2002).  The only difference from the (Widdows and Dorow, 2002) experiment is the usage of pairs rather than single words. This was not needed in (Widdows and Dorow, 2002) because they had directly accessed the word graph, which may be an advantage in some applications. The Russian evaluation posed a bit of a problem because the Russian WordNet is not readily available and its coverage is rather small. Fortunately, the subject list is such that WordNet words (Widdows and Dorow, 2002) also reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones. This metric was reported to be 82% in (Widdows and Dorow, 2002). In order to identify such useful patterns, for each pattern we build a graph following (Widdows and Dorow, 2002). The graph is constructed from a node for each content word, and a directed arc from the node x to y if the corresponding content words appear in the pattern such that x precedes y. In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. In order to identify symmetric patterns, for each pattern we define a pattern graph G (P), as proposed by (Widdows and Dorow, 2002). The concept discovery algorithmis essentially the same as used by (Davidov and Rappoport, 2006) and has some similarity with the one used by (Widdows and Dorow, 2002). We have also performed an indirect comparison to (Widdows and Dorow, 2002). While there is a significant number of other related studies on concept acquisition (see Section 2), Most are supervised and/or use language-specific tools. All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). This also allows indirect comparison to several other studies, thus (Widdows and Dorow, 2002) reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones.
V. Ng and C. Cardieanalysed in (Ng and Cardie, 2002) the impact of such a prefiltering on their co reference resolution engine. Various natural language processing (NLP) tasks benefit from the identification of elliptical subjects, primarily anaphora resolution (Mitkov, 2002) and co-reference resolution (Ng and Cardie, 2002). As an improvement, Ng and Cardie (2002a) and Ng (2004) train a separate model to classify an anaphor as either anaphoric or non-anaphoric. The output of this classifier can be used either as a pre-filter (Ng and Cardie,2002a) so that non-anaphoric anaphors will not be precessed in the co reference system, or as a set of features in the co reference model (Ng, 2004). This is different from (Ng and Cardie, 2002a; Ng, 2004) where their anaphoricty models are trained independently of the co reference model, and it is either used as a pre-filter, or its output is used as features in the co reference model.  Ng and Cardie (2002a) trains a separate anaphoricity classifier in addition to a coreference model. Ng and Cardie (2002) and Poesio et al (2005) have tested the impact of such a detector on the overall co reference resolution performance with encouraging results. Ng and Cardie (2002) and Uryupina (2003) do not limit to definite NPs but deal with all types of NPs. Notice the confusing use of the term anaphoric in (Ng and Cardie, 2002) for describing their chain-starting filtering module. Other partially capture the differential preferences between different anaphorsvia different sample selection strategies during training (Ng and Cardie, 2002b; Uryupina, 2004). For classifiers, we replicate the procedures of Ng and Cardie (2002b). This is very similar to the approach of Ng and Cardie (2002a).   In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no improvements, or even worse performance. In fact, Ng and Cardie (2002a) challenged the motivation for the inclusion of such detectors, reporting no improvements or even worse performance. Ng and Cardie (2002a) directly investigate the question of whether employing a discourse-new prediction component improves the performance of a Method R P Baseline 100 72.2 Syntactic Heuristics 43 93.1 Synt. Heuristics+ S1+ EHP+ DO+ V 79.1 84.5 Table 2: Discourse-new prediction results by Bean and Riloffcoreference resolution system (specifically, the system discussed in (Ng and Cardie, 2002b)). Traditional learning-based co reference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)).
Different measures have been proposed, which are not easy to evaluate (see (Lin and Pantel, 2002) for proposals). NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). The labeled classes are acquired in three stages: 1) extraction of a noisy pool of pairs of a class label and a potential class instance, by applying a few Is-A extraction patterns, selected from (Hearst, 1992), to Web documents: (fruits, apple), (fruits, corn), (fruits, mango), (fruits, orange), (foods, broccoli), (crops, lettuce), (flowers, rose); 2) extraction of unlabeled clusters of distributionally similar phrases, by clustering vectors of contextual features collected around the occurrences of the phrases within Web documents (Lin and Pantel, 2002).  For CBC we simply used the same parameter values as reported in (Lin and Pantel, 2002). (Schutze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. Options for identifying interesting classes include manually created methods (WordNet (Miller et al, 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al, 2006). Mutual information (MI) is an information theoric measure and has been used in many NLP tasks, including clustering words (e.g. Lin and Pantel, 2002).   To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. Clustering by committee has also been used to discover concepts from a text by grouping terms into conceptually related clusters (Lin and Pantel, 2002).
  It is also binary feature and all the punctuations in the punctuation character list come from Penn Chinese Tree bank 5.1 (N.Xue et al,2002). At last, all the punctuations in Penn Chinese Treebank 5.1 (N.Xue et al,2002). reported accuracies of 93% and 93.74% on CTB-I (Xue et al, 2002) (100K words) and CTB 5.0 (500K words), respectively, each using a Maximum Entropy approach. Furthermore, to evaluate our reranking method's impact on the POS tagging task, we also performed 10-fold cross-validation tests on the 250k Penn 205Chinese Treebank (CTB) (Xue et al, 2002).  We use the Penn Chinese Treebank (Xue et al, 2002) as our corpus and the ontology/lexicon HowNet (Dong and Dong, 2000) to get ontological features for nouns. We follow the experimental setup in (Reichartand Rappoport, 2008), running the algorithm on English, German and Chinese corpora: the WSJ Penn Treebank (English), the Negra corpus (Brants, 1997) (German), and version 5.0 of the Chinese Penn Tree bank (Xue et al, 2002). In the Penn Chinese Treebank (CTB) (Xue et al, 2002) non-local dependencies are represented in terms of empty categories (ECs) and (for some of them) co indexation with antecedents, as exemplified in Figure 1. After analyzing the dependency structure of sentences in Penn Chinese Treebank 5.1 (Xue et al, 2002), we found an interesting phenomenon: if we define a main-root as the head of a sentence, and define a subsentence as a sequence of words separated by punctuations, and the head1 of these words is the child of main root or main-root itself, then the punctuations that depend on main-root can be a separator of sub-sentences. The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al, 2002), corresponding to articles 1-325 of PTB, which have English translations with gold standard parse trees (Bies et al, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al, 2002) developed on the similar annotation scheme of PTB, these parsing techniques were also transferred to the Chinese language. While we did not have human-annotated gold standard parses for our training data, we did have human annotated parses for the Chinese side of our test data, which was taken from the Penn Chinese Treebank (Xue et al, 2002). We next tested U DOP on two additional domains from Chinese and German which were also used in Klein and Manning (2002, 2004): the Chinese tree bank (Xue et al 2002) and the NEGRA corpus (Skut et al 1997). We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al 1997) and the Chinese CTB10 (Xue et al 2002) both containing 2200+ sentences 10 words after removing punctuation. The closest result in the literature is Xue et al (2002), who retrain the Ratnaparkhi (1996) tagger and reach accuracies of 93% using CTB-I. However, our performance on tagging when trained on Training I and tested on just the XH part of the test set is 94.44%, which might be a more relevant comparison to Xue et al (2002). For English and Chinese we use sections 2-21 of the Penn Treebank (PTB) (Marcus et al,1993) and sections 1-270 of the Chinese Treebank (CTB) (Xue et al, 2002) respectively. We train on sections 1-270 of the Penn Chinese Treebank (Xue et al, 2002), similarly reduced (CTB10).
Our approach to QC follows that of (Li and Roth, 2002). We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. This scheme is more suitable here than other common answer-typing schemata such as the one in Li and Roth (2002), which tend to focus on questions asking for factual knowledge. This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-question specific unit lists consistently achieve higher answer identification performance than fixed-type, general purpose answer typing (which propose all numerical entities as answer candidates). For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. (Li and Roth, 2002) propose a system based on SNoW. The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1. (Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy. It could be also interested to test the combination between a better QC system, the current one by Li and Roth's for instance (Li and Roth, 2002), and our machine translation method. Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. With the increasing popularity of statistical NLP, Li and Roth (2002), Hacioglu and Ward (2003) and Zhang and Lee (2003) used supervised learning for question classification on a data set from UIUC that is now standard1. Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al, 1999). Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task. in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs.  Answer types are determined using classification rules similar to Li and Roth (2002). The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). 1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier.
These tools could use the type hierarchy to predict where conflicts are likely to arise and bring these to the engineer's attention, possibly inspired by the approach under development at CSLI for the dynamic maintenance of the LinGO Redwoods tree bank (Oepen et al, 2002). Fuchss et al (2004) supported the claim by investigating MRS structures in the Redwoods corpus (Oepen et al, 2002). Furthermore, to test the grammar precision and accuracy, we use two tree banks: Redwoods (Oepen et al, 2002) for English and Hinoki (Bond et al, 2004) for Japanese. We now briefly describe the Redwoods treebanking environment (Oepen et al, 2002), our parse selection models and their performance. Penn Treebank (Marcus et al, 1993) the HPSG LinGo Redwoods Treebank (Oepen et al, 2002), and a smaller dependency tree bank (Buchholz and Marsi, 2006). We do so in the context of Redwoods (Oepen et al, 2002), a tree bank that contains HPSG analyses for sentences from the Verbmobil appointment scheduling and travel planning domains. We show that sample selection metrics based on tree entropy (Hwa, 2000) and disagreement between two different parse selection models significantly reduce the number of annotated sentences necessary to match a given level of performance according to random selection. To address this limitation, the Redwoods treebank has been created to provide annotated training material to permit statistical models for ambiguity resolution to be combined with the precise interpretations produced by the ERG (Oepen et al, 2002). The structure of our treebank is inspired by the Redwoods tree bank of English (Oepen et al,2002) in which utterances are parsed and the annotator selects the best parse from the full analyses derived by the grammar. We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus (Oepen et al, 2002). Although annotated corpora exist for HPSG, such corpora do not exist in significant volumes and are limited to a few small domains (Oepen et al, 2002). To overcome this limitation for the HPSG English ResourceGrammar (ERG, Flickinger (2000)), the Redwoods treebank has been created to provide annotated training material (Oepen et al, 2002). With the evolution of the grammar, the treebank as the output from the grammar changes over time (Oepen et al, 2002). This kind of dynamic, discriminant-based treebanking was pioneered in the Redwoods treebank of English (Oepen et al, 2002), so we refer to it as Redwoods-style treebanking.  However, despite research on HPSG processing efficiency (Oepen et al, 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al, 2002b; Toutanova and Manning, 2002). However, despite the development of methods to improve HPSG parsing efficiency (Oepen et al, 2002a), the exhaustive parsing of all sentences in a tree bank is still expensive. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al, 2002), following Zhang et al (2006). In this paper, we evaluate the truth of these assumptions on the MRS expressions which the ERGcomputes for the sentences in the Redwoods Treebank (Oepen et al, 2002). As a test corpus, we use the Redwoods Treebank (Oepen et al, 2002) which contains 6612 sentences. The structure of our treebank is inspired by the Redwoods treebank of English in which utterances are parsed and the annotator selects the best parse from the full analyses derived by the grammar (Oepen et al., 2002).
Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser.  The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007).
For parsing, we mapped all unknown words to unknown word symbols, and applied the Viterbi algorithm as implemented in Schmid (2004), exploiting its ability to deal with highly-ambiguous grammars. The starred results are statistically significant improvements over the Baseline (at confidence p > 0.05). English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson (2005), and the German side was parsed using BitPar (Schmid, 2004) without the function and morphological annotations. The key linguistic knowledge sources that we use are morphological analysis and generation of German based on SMOR, a morphological analyzer/generator of German (Schmid et al 2004) and the BitPar parser, which is a state-of-the-art parser of German (Schmid, 2004). We use a frequency-based notation because we use out of-the-box software Bitpar (Schmid, 2004) which implements inside-outside estimation Bitpar reads in frequency models and converts them to relative frequency models. The reestimation was carried out using Bitpar (Schmid,2004) for inside-outside estimation. For our parsing results we use BitPar, a fast and freely available general PCFG parser (Schmid, 2004). The German V-O pairs were extracted from a syntactic analysis of the HGC carried out using the BitPar parser (Schmid, 2004). We used only V-O pairs because they cons ti tute far more sense-discriminative contexts than, for example, verb-subject pairs, but we plan to examine these and other grammatical relationships in future work. An existing SCFG parser (Schmid, 2004) was then used, with a simple unknown word heuristic, to generate the Viterbi n-best parses with n= 100, and, after removing the address labels, all equal parses and their probabilities were summed, and the one with highest probability chosen. Compact binarization (Schmid, 2004) tries to minimize the size of the binarized grammar. Compact binarization was introduced in Schmid (2004), based on the intuition that a more compact grammar will help achieve a highly efficient CKY parser, though from our experiment it is not always true. We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation. This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various tree bank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis. We experimented with increasingly rich grammars read off of the tree bank.  BitPar (Schmid, 2006) is a probabilistic context free parser using bit-vector operations (Schmid, 2004). For Experiment II we trained BitPar (Schmid, 2004), a parser for highly ambiguous PCFG grammars, on the two tree banks.  The remaining sentences are part-of speech tagged and lemmatized using TreeTagger (Schmid, 2004). We tokenize and sentence split the data with the default DKProsegmenter, and then use TreeTagger (Schmid, 2004) to POS-tag and chunk the sentences. In our experiments, we used the BitParparser (Schmid, 2004) and a PCFG which was extracted from a version of the PENN tree bank that was automatically annotated with features in the style of (Klein and Manning, 2003). We parse all German and English articles with BitPar (Schmid, 2004) to extract verb-argument relations.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. The parser used in this paper is described in Clark and Curran (2004b). A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. Clark and Curran (2004b) describes two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (wit hand without a lexical head); lexical category features, consisting of word category pairs at the leaf nodes; and root category features, consisting of head word category pairs at the root nodes. The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis.
In contrast, Blatz et al (2004) introduced a sentence level QE system where an arbitrary threshold is used to classify the MT output as good or bad. The idea is explored more comprehensively in (Blatz et al, 2004). In this respect, the goal is more related to the area of confidence estimation for MT (Blatz et al, 2004). We compute sentence CMs by combining the scores given by a word CM based on the IBM model 1 (Brown et al, 1993), similar to the one described in (Blatz et al, 2004). The work of (Blatz et al, 2004) is among the best known study of sentence and word level features for translation error prediction. This resembles approaches that merge different classifiers (Riedel et al 2011) or attempt to estimate confidence of models (Blatz et al 2004). We used two different methods to combine subsequence features: Average value of subsequence-level scores, as done in (Blatz et al, 2004). To this end, we estimate a confidence score for each SMT hypothesis, using a discriminative classification framework reminiscent of Blatz et al (2004). A considerable amount of work has been done in the related area of confidence estimation for MT, for which Blatz et al (2004) provide a good overview. In contrast to most of the work on confidence estimation (Blatz et al, 2004), the features we use are not internal features of the MT system. 33 from automatic MT evaluation metrics (Blatz et al., 2004) such as BLEU (Papineni et al, 2002) at training time, it soon became clear that human labels result in significantly better models (Quirk, 2004). (Blatz et al, 2004) conducted extensive study incorporating various sentence-level and word-level features thru multi layer perceptron and naive Bayes algorithms for sentence and word confidence estimation. Blatz et al (2004) attempted sentence-level assessment using a set of 91 features (from the SMT system input and translation texts) and automatic annotations such as NIST and WER. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few.  The organizers have made available a baseline QE system that consists of a number of well established features (Blatz et al, 2004) and serves as a starting point for development. Blatz et al (2004) only investigated source n gram frequency statistics and source language model features, while other work mainly focused on target side features.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others.    Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04].  In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples.  We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details.
Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. individual target hypotheses (Zhao et al, 2004). Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. (Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. Refinements of this approach are described in (Zhao et al., 2004). These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). This adaptation technique was first proposed by Zhao et al (2004). Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation.
In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). 156 at the sentence levels moothed-bleu (Lin and Och, 2004) was used in this case. ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included. Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. Stemming is enabled (Lin and Och, 2004a). Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. We then used these coefficients to estimate the confidence interval, after excluding the top 25 and bottom 25 coefficients, following (Lin and Och, 2004). Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). The optimization objective is sentence level BLEU (Lin and Och, 2004). As we would like to avoid this problem, we use the smoothed sentence-level Bleuscoreas suggested in (Lin and Och, 2004). They obtained similar results in both cases (Lin and Och, 2004a). While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them: maximum recall can be achieved without resembling the gold standard text decomposition; and maximum precision can be achieved with only a few overlapped elements.BLEU (Papineni et al, 2001a) computes the n gram precision while the metric ROUGE (LinandOch, 2004a) computes the n-gram recall. The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. Using the notation by (Lin and Och, 2004), we denote the skip bigram overlap between two sentences X and Y as Skip2 (X, Y).
(Xia and McCord, 2004) describe an approach for translation from French to English, where reordering rules are acquired automatically. Our method differs from that of (Xia and McCord, 2004) in a couple of important respects. Xia and McCord (2004) extracted reordering rules automatically from bilingual corpora for English-to-French translation; Collins et al (2005) used linguistically-motivated clause restructuring rules for German-to-English translation; Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation; Katz-Brown and Collins (2008) applied a very simple reverse ordering to Japanese-to-English translation, which reversed the word order in Japanese segments separated by a few simple cues; Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for re ordering English to subject-object-verb order like Korean and Japanese. While our framework can be applied to any translation system in which it is possible to derive a token-level alignment from the input source tokens to the output target tokens, it is of particular practical interest when applied to a system that performs reordering as a preprocessing step (Xia and McCord, 2004). (Xia and McCord, 2004), and perform these two operations in separate steps, the latter conditioned on the former, Birch et al (2010) a remaking a much stronger assumption when they perform these simulations: they are assuming that lexical choice and word order are entirely independent. Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al, 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al, 2007) or parse trees (Xia and McCord, 2004). Preprocessing approaches can use either hand-written rules targeting known language differences (e.g. Collins et al (2005), Li et al (2009)), or automatically learnt rules (e.g. Xia and McCord (2004), Zhang et al (2007b)), which are basically language independent. In future work our novel approach might allow to make use of lexicalized reorder rules as in (Xia and McCord, 2004) or syntactic rules as in (Wang et al, 2007). To the best of our knowledge, the authors of (Xia and McCord, 2004) were the first to address this problem in the statistical MT paradigm. (Xia and McCord, 2004) present a similar approach, with a notable difference: the re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences. Many such systems exist, with results being mixed; we review several here. Xia and McCord (2004) (English-to-French translation, using automatically-extracted reordering rules) train on the Canadian Hansard. The most notable models are given by Xia and McCord (2004), Collins et al (2005), Li et al (2007) and Wang et al (2007). Xia and McCord (2004) parsed the source and target sides of the training data and then automatically extracted the rewriting patterns. The technique proposed in this study is most similar to the one proposed for French-to-English translation task in Xia and McCord (2004), where the authors present a hybrid system for French English translation based on the principle of automatic rewrite patterns extraction using a parse tree and phrase alignments. As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or pre ordering (Xia and McCord, 2004). In particular, systems that use source language syntax allow for the handling of long distance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology. Figure 1: An example with a source sentence F re ordered into target order F, and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering.the pre-ordering approach to machine translation (Xia and McCord, 2004), which performs translation as a two step process of reordering and translation (Figure 1). Preprocessing approaches involving the use of a syntactic parse of the source sentence to change the word order to more closely match the word order of the target language have been studied by Niessen and Ney (2004), Xia and McCord (2004), Dra ?bek and Yarowsky (2004), Collins et al. Syntax-based reordering rules can be used as a preprocessing step for PB-SMT (and other approaches), to decrease the word-order and syntactic distortion between the source and target languages (Xia and McCord, 2004). The underlying formalisms used has been quite broad and include simple formalisms such as ITGs (Wu, 1997), hierarchical synchronous rules (Chiang, 2005), string to tree models by (Galley et al, 2004) and (Galley et al, 2006), synchronous CFG models such (Xia and McCord, 2004) (Yamada and Knight, 2001), synchronous Lexical Functional Grammar inspired approaches (Probst et al, 2002) and others. Some of the previous approaches include (Collins et al, 2005), (Xia and McCord, 2004).
While replicating earlier experiments, Banko and Moore (2004) discovered that performance was highly dependent on cleaning tag dictionaries using statistics gleaned from the tokens. Second, the expectation maximization algorithm for bi tag HMMs is efficient and has been shown to be quite effective for acquiring accurate POS taggers given only a lexicon (tag dictionary) and certain favorable conditions (Banko and Moore, 2004). As Banko and Moore (2004) discovered when 5 Note that the POS tag information is not used in these experiments, except for by the C& amp; C tagger. To consider the effect of the CCG-based initialization for lexicons with differing ambiguity, I use tag cutoffs that remove any lexical entry containing a category that appears with a particular word less than X% of the time (Banko and Moore, 2004), as well as using no cutoffs at all. However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methods depends strongly on the precise nature of the supervised training data (in their case, the ambiguity of the tag lexicon available to the system), which makes it more difficult to understand the behaviour of such systems. Banko and Moore (2004) compared unsupervised HMM and transformation-based taggers trained on the same portions of the Penn Treebank, and showed that the quality of the lexicon used for training had a high impact on the tagging results. Duh and Kirchhoff (2005) presented a minimally supervised approach to tagging for dialectal Arabic (Colloquial Egyptian), based on a morphological analyzer for Modern Standard Arabic and unlabeled texts in a number of dialects. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). Author Language Average accuracy Toutanova et al (2003) English 97.24% Banko and Moore (2004) English 96.55% Dandapat and Sarkar (2006) Bengali 84.37% Rao et al (2007) Hindi 76.34% Bengali 72.17% Telegu 53.17% Rao and Yarowsky (2007) Hindi 70.67% Bengali 65.47% Telegu 65.85% Sastry et al (2007) Hindi 69.98% Bengali 67.52% Telegu 68.32% Ekbal et al (2007) Hindi 71.65% Bengali 80.63% Telegu 53.15% Ours Assamese 85.64% and searched in the affix-probability table. As for contextualized lexical probabilities, our extension is very similar to Banko and Moore (2004) who use P (wi|ti? 1, ti ,ti+1) lexical probabilities and found, on the Penn Treebank, that incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%. One difficulty with their approach ,noted by Banko and Moore (2004), is the treatment of unseen words: their method requires a full dictionary that lists what tags are possible for each word. If we follow Banko and Moore (2004) and construct a full (no OOV) morphological lexicon from the tagged version of the test corpus, we obtain 96.95% precision where theirs was 96.59%. The reason why Banko and Moore (2004) get less than HunPos is not because their system is inherently worse, but rather because it lacks the engineering hacks built into TnT and HunPos. Smith and Eisner (2005) employ a contrastive estimation tech 1As (Banko and Moore, 2004) point out, unsupervised tagging accuracy varies wildly depending on the dictionary employed. We define the unsupervised part-of-speech (POS) tagging problem as predicting the correct part-of speech tag of a word in a given context using an unlabeled corpus and a dictionary with possible word? tag pairs0 The performance of an unsupervised POS tagging system depends highly on the quality of the word7ujh tag dictionary (Bankoand Moore, 2004). by Banko and Moore (2004), these works made use of filtered dictionaries: dictionaries in which only relatively probable analyses of a given word are preserved. Later, Banko and Moore (2004) observed that earlier unsupervised HMM-EM results were artificially high due to use of Optimized Lexicons, in which only frequent-enough analyses of each word were kept. In this paper, we show how these strategies may becombined straightforwardly to produce improvements on the task of learning super taggers from lexicons that have not been filtered in any way.1 We demonstrate their cross-lingual effectiveness on CCGbank (English) and the Italian CCG-TUT 1See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. We use the standard splits of the data used in semi-supervised tagging experiments (e.g.Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test.CCG-TUT. Banko and Moore (2004) showed that unsupervised tagger ac curacies on English degrade from 96% to 77% if the lexicon is not constrained such that only high frequency tags exist in the POS-set for each word. Some of these are annotation errors in the tree bank (Banko and Moore, 2004, Figure 2): such (mis)taggings can severely degrade the accuracy of part-of-speech disambiguators, without additional supervision (Banko and Moore, 2004,? 5, Table 1).
A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). Peng et al (2004) uses the CRFs to address this issue. In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. We follow the format from Peng et al (2004). represents the CRF model from Peng et al (2004), and the last row represents our model. RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. 3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004).  Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. We also used lexical features consulting a dictionary: one is to check if any of the above defined character n-grams appear in a dictionary (Peng et al, 2004), and the other is to check if there are any words in the dictionary that start or end at the current character boundary. text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi 196 that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word.
and (2) the relation between predicates, question stem and the words that determine the answer type (Narayanan and Harabagiu, 2004). Ever since Gildea and Jurafsky (2002), SRL has become an important technology used in applications requiring semantic interpretation, ranging from information extraction (Frank et al, 2007) and question answering (Narayanan and Harabagiu, 2004), to practical problems including textual entailment (Burchardt et al, 2007) and pictorial communication systems (Goldberg et al, 2008). In particular, the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al, 2003), and co-reference resolution (Kong et al, 2009). Systems for addressing complex information needs are interesting because they provide an opportunity to explore the role of semantic structures in question answering ,e.g., (Narayanan and Harabagiu, 2004). For instance, information extraction (Surdeanu et al, 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al, 2003), and co-reference resolution (Ponzetto and Strube, 2006). Lexical semantic features are known to be helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu,2004). Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation ,information extraction (Hirschman et al, 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). Semantic types and role labelling are helpful in both deep (Tetreault, 2005) and shallow interpretation tasks (Narayanan and Harabagiu, 2004). One such question answering system (Narayanan and Harabagiu, 2004) takes PropBank/FrameNet annotations as input, uses the PropBank targets to indicate which actions are being described with which arguments and produces an answer using probabilistic models of actions as the tools of inference. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation (Boas 2002). Harabagiu, 2004) computed automatically from collections of documents relevant to a scenario in order to approximate the semantic content of a scenario, (Narayanan and Harabagiu, 2004) employed formal models of the interrelated events, actions, states, and relations implicit to a scenario in order to produce fine-grained, context sensitive inferences that could be used to answer questions. Examples include information extraction (Surdeanu et al, 2003), question answering (Narayanan and Harabagiu, 2004), machine translation (Boas, 2005), and summarization (Melli et al, 2005). Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al, 2003), which document the surface realization of semantic roles in real world corpora. It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al, 2003) and Question Answering (Narayanan and Harabagiu, 2004). Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al, 2005) together with the semantic frames from FrameNet (Baker et al, 1998) to create an inference mechanism to improve QA. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al, 2005) together with the semantic frames from FrameNet (Baker et al, 1998) to create an inference mechanism to improve QA. The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al, 2003), and question answering (Narayanan and Harabagiu, 2004). Robust and accurate automatic semantic role assignment, a prerequisite for the wide-range use of semantic roles in NLP, has been investigated in a number of studies and shared tasks. Narayanan and Harabagiu (2004) were the first to stress the importance of semantic roles in answering complex questions.
Pantel et al (2004) proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performance and efficiency.  Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). Lately there has been a lot of interest in acquiring such text patterns using a set of hypernymy examples ,e.g. Pantel et al (2004) and Snow (2006). However, following Pantel et al (2004), we assume that the recall of the baseline is 1 and estimate the relative recall RRS|B of the system S with respect to the baseline B using their respective precision scores PS and PB and number of instances extracted by them |S| and |B| as: RRS|B= PS? |S|PB? |B| 6.3 Gold Standard. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al,2004). KAON Text-To-Onto (Maedche and Staab, 2004) applies text mining algorithms for English and German texts to semi-automatically create an ontology, which includes algorithms for term extraction, for concept association extraction and for ontology pruning. Pattern-based approaches to extract hy ponym/hypernym relationships range from hand-crafted lexico-syntactic patterns (Hearst, 1992) to the automatic discovery of such patterns by e.g. a minimal edit distance algorithm (Pantel et al, 2004). Pantel et al (2004) extended is-a relation acquisition towards terascale, and automatically identified hypernym patterns by minimal edit distance. Many relationship classification methods utilize some language-dependent preprocessing, like deep or shallow parsing, part of speech tagging and 228 named entity annotation (Pantel et al, 2004). Useful semantic relations can be extracted from large corpora using relatively simple patterns (e.g., (Pantel et al, 2004)). Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004). The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al, 2004). For example, the pattern& quot; NP1 ,a|an NP2& quot;, ranked among the top IS-A pat terns by (Pantel et al, 2004), can represent both apposition (entailing) and a list of co-hyponyms (non-entailing). Due to the need for POS tagging and/or parsing, these types of methods have been evaluated only on fixed corpora1, although (Pantel et al, 2004) demonstrated how to scale up their algorithms for the Web. Pantel et al, (2004) proposed, in the scenario of extracting is-a relations, one pattern-based approach and compared it with a baseline syntactic distributional similarity method (called syntactic co-occurrence in their paper). Patterns were shown to be very useful in all sorts of lexical acquisition tasks, giving high precision results at relatively low computational costs (Pantel et al, 2004).  Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al 2004). (Pantel et al 2004) reduce the depth of the linguistic data used but still requires POS tagging. Many papers directly target specific applications, and build lexical resources as a side effect. Named Entity Recognition can be viewed as an in stance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co clustering. Note how small they are, when compared to (Pantel et al 2004), which took 4 days for a smaller corpus using the same CPU.
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004).  Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010). As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces.
For evaluation the parser returns dependency structures, but we have also developed a module which builds first order semantic representations from the derivations, which can be used for inference (Bos et al, 2004). CCG-based syntactic parsing (Bos et al, 2004). Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004). The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. Bos et al (2004) derive semantic interpretations from a wide-coverage categorial grammar. There are several differences between this and RASP-RMRS, but the most important arise from the differences between CCG and RASP. On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steed man, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al, 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics. Additionally, Bos et al (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004). As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004). At the other end of the spectrum, Bos et al (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory. Bos et al (2004) present an algorithm for building semantic representations from CCG parses but requires fully specified CCG derivations in the training data. For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004). We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). Although this number is slightly less than 92.3%, as reported by Bos et al, (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences.
Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al, 2004). Punyakanok et al, (2004) further showed that constituent-by-constituent (C by-C) tagging is better than P-by-P. In this paper, we focus on phrase structure parsing with function labelling as a post-processing step. Integer linear programs have already been successfully used in related fields including semantic role labelling (Punyakanok et al, 2004), relation and entity classification (Roth and Yih, 2004), sentence compression (Clarke and Lapata, 2008) and dependency parsing (Martins et al, 2009). This basic architecture was introduced by Punyakanok et al (2004) for the task of semantic role labelling and since then has been applied to different NLP tasks without significant changes. We use the inference process introduced by (Punyakanok et al, 2004). Experiments performed combining the best and second output of the joint parser and enforcing domain constraints via ILP (Punyakanok et al, 2004) showed no significant improvements. Following (Punyakanok et al, 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. As we did in the last year's system (Cheetal., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al, 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label? NULL?). However, Punyakanok et al (2004) showed that constituent-by-constituent (C-by-C) tagging is better than P-by-P. ILP models have been successfully applied in several natural language processing tasks, including relation extraction (Roth and Yih, 2004), semantic role labeling (Punyakanok et al, 2004) and the generation of route directions (Marciniak and Strube, 2005). Punyakanok et al (2004) formulated an Integer Linear Programming (ILP) model for SRL. ILP method was first applied to SRL in (Punyakanok et al, 2004). constraints (Punyakanok et al,2004)? without having to call out to ILP optimizers. ILP has been applied to various NLP problems including semantic role labeling (Punyakanok et al, 2004), which is similar to dependency labeling: both can benefit from verb specific information. Actually, (Punyakanok et al, 2004) take into account to some extent verb specific information. Some other work paid much attention to the robust SRL (Pradhan et al, 2005b) and post inference (Punyakanok et al, 2004). must have 3 arguments of a particular grammatical role. Among the approaches to overcome this restriction, i.e. that allow for global, theory based constraints, Integer Linear Programming (ILP) has been applied to NLP (Punyakanok et al, 2004). ILP has been applied to various NLP problems, including semantic role labeling (Punyakanok et al., 2004), extraction of predicates from parse trees (Klenner, 2005) and discourse ordering in generation (Althaus et al, 2004). Recently, the integer programming framework has been widely adopted by researchers to solve other NLP tasks besides POS tagging such as semantic role labeling (Punyakanok et al, 2004), sentence compression (Clarke and Lapata, 2008) ,decipherment (Ravi and Knight, 2008) and dependency parsing (Martins et al, 2009). In (Punyakanok et al, 2004), several more constraints are considered.
This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by the MINIPAR parser. (signed integers representing positive and negative feelings) (Kim and Hovy, 2004). In particular, they have been an essential ingredient for fine grained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al (2005)). Kim and Hovy (2004) try to determine the final sentiment orientation of a given sentence by combining sentiment words within it.   In separate qualitative experiments done by Pang et al (2002), 97 Wilson et al (2005) and Kim and Hovy (2004), the agreement between human judges when given a list of sentiment-bearing words is as low as 58% and no higher than 76%. Kim and Hovy (2004) start with two lists of positive and negative seed words. Kim and Hovy (2004) found the polarity of subjective expressions.  The system of Kim and Hovy (2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others. This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels. Kim and Hovy (2004) select candidate sentiment sentences and use word-based sentiment classifiers to classify unseen words into a negative or positive class. The lexicons are generated from manually selected seeds for a broad domain such as Health or Business, following an approach similar to (Kim and Hovy, 2004). Kim and Hovy (2004), among others, have combined the two tasks, identifying subjective text and detecting its sentiment polarity. Kim and Hovy (Kim and Hovy, 2004) used WordNet synonyms and antonyms to expand two lists of positive and negative seed words. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder task than polarity classification with lower human agreement and automatic performance. There are only two prior approaches addressing word sense subjectivity or polarity classification. Kim and Hovy proposed two probabilistic models to estimate the strength of polarity (Kim and Hovy, 2004). However, Kim and Hovy (2004) and Andreevskaiaand Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. 
Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.  For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008).  Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). The first abstractive compression method was proposed by Cohn and Lapata (2008). The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it com pares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). 
Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors. As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. The best results of 92.15% are reported by De Felice and Pulman (2008). In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings.  T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al. Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. 
The decision tree uses different context features for the prediction of different attributes (Schmid and Laws, 2008). Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger). RFTagger (Schmid and Laws, 2008 ,pos, morph),.  The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. Additional German tags are obtained using the RFTagger toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.). For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs. However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case. For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008). The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs. These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tag set containing approximately 800 tags. For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RF-Tagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German. The results are not directly comparable to the RF-Tagger as it was evaluated on a different part of the Tiger Treebank and trained on a larger part of the Treebank. The lexicon supplies entries for additional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). ArgumentMappedWordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLex plus (Meyers et al, 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs.  We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008).
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. (Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). (Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008).  For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b).
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. This type of similarity is reminiscent of relational analogies investigated in Turney (2008). Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy.
In this paper, we use the following baseline parsers: MaltParser (Nivre et al, 2007) for transition-based parsing; MSTParser (McDonald et al, 2005) (with sibling 2-edge factors) and Bohnet Parser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al, 2006) for constituency-based parsing. Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010).  This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). The three dependency parsers are: MaltParser (Nivre et al, 2006), Mate (Bohnet, 2010) 2 and MSTParser (McDonald and Pereira, 2006). For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. Please refer to Table 4 of Bohnet (2010) for the complete feature list. We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs.  Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al, 2013) Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency. As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010).
When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. We evaluated our model on the same dataset used in Zhu et al (2010), an aligned corpus of MainEW and SimpleEW sentences. However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions. Our results also show that a more general model not restricted to specific rewrite operations like Zhu et al (2010) obtains superior results and has better coverage. Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task. Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning.   While Zhu et al (2010) have demonstrated that their approach outperforms a PBMT approach in terms of Flesch Reading Ease test scores, we are not aware of any studies that evaluate PBMT for sentence simplification with human judgements.  Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity.
Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010).  The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010).
(Davidov et al, 2010) used 50 hash tags and 15 emoticons as noisy labels to create a dataset for twitter sentiment classification.  Evaluation is performed on several datasets of tweets that have been annotated for polarity: the Stanford Twitter Sentiment set (Go et al, 2009), Davidov et al (2010) use 15 emoticons and 50 Twitter hash tags as proxies for sentiment in a similar manner, but their evaluation is indirect. Davidov et al (2010) propose utilizing twitter hash tag and smileys to learn enhanced sentiment types.
Instead, it takes care of fillers and gaps through a threading& quot; technique (Karttunen 1986:77). Consider the following rules taken from D-PATR (Karttunen, 1986). The formst is that of D-PATR/Karttunen 1986. Particularly well known examples are reported in Kaplan (1983) (see also Kiparsky, 1985), Shieber (1984), Evans (1985), Phillips and Thompson (1985), Jensen et al (1986) and Karttunen (1986). Karttunen (1986) and Shieber (1986) describe systems in which FSs may be modified by default statements in such a way that this property does not automatically hold.
In this paper we present an algorithm for generating sentences using unification categorial grammars (UCGs, Zeevat et al 1987) but which extends to any categorial grammar with unification (e.g. categorial unification grammars, Uszkoreit 1986, Karttunen 1987). The lexical types are organised into an inheritance hierarchy, constrained by expressions of a simple feature based category description language, inspired by previous attempts to integrate categorial grammars and unification-based grammars, e.g. Uszkoreit (1986) and Zeevat et al (1987). Unificationbased versions of Categorial Grammar, known as CUG or UCG, have attracted considerable attention recently (see, for instance, Uszkoreit, 1986, Karttunen, 1986, Bouma, 1988, Bouma et al, 1988, and Calder et al, 1988). Higher order versions of categorial grammars like the ones being produced in the CUG/UCG-frameworks. The family of grammar models that are based on such formalisms include Generalized Phrase Structure Grammar (GPSG) [Gazdar et al 1985], Lexical Functional Grammar (LFG) [Bresnan 1982], Functional Unification Grammar (bUG) [Kay 1984], Head-Driven Phrase Structure Grammar (I-IPSG) [Pollard and Sag 1988], and Categorial Unification Grammar (CUG) [Karttunen 1986, Uszkoreit 1986, Zeevat et al 1987]. Use of unification (a core operation in HPSG) in CG dates at least as far back as Karttunen (1986, 1989), Uszkoreit (1986), and Zeevat (1988).
Consider, for example, the following sentence, taken from the Hansard corpus of the proceedings of the Canadian parliament [Brown et al 1988]: (1) They know full well that the companies held tax money aside for collection later on the b~sis that the government said it was going to collect it. Parallel corpora have received a lot of attention since the advent of statistical machine translation (Brown et al, 1988) where they serve as training material for the underlying alignment models. The most successful translation models that are found in the literature exploit finite-state machinery. The approach started with the so-called IBM models (Brown et al, 1988), implementing a set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence. Developing a better TM is a fundamental issue for those applica tions. Researchers at IBM first described such a statistical TM in (Brown et al, 1988). Most algorithms for bilingual word alignment to date have been based on the probabilistic translation models first proposed by Brown et al (1988, 1990), especially Model I and Model 2. Brown et al (1988) suggested that MT can be statistically approximated to the transmission of information through a noisy channel. As mentioned above, the MDI2B model is closely related to the IBM2 model (Brown et al, 1988). In the field of eomputationa.1 linguistics, mutual information [Brown et al, 1988],  2 [Church and Hanks, 1990], or a likelihood ratio test [Dunning, 199a] are suggested. In general a statistical machine translation system is composed of three components: a language model, a translation model, and a decoder (Brown et al, 1988). The language model tells how probable a given sentence is in the source language, the translation model indicates how likely it is that a particular target sentence is a translation of a given source sentence, and the decoder is what actually takes a source sentence as input and produces its translation as output. This is the task of finding for a word in one language words of a similar meaning in a second language. The results of this can be used to aid manual construction of resources or directly aid translation. This task was first approached as a distributional similarity-like problem by Brown et al (1988).
Consequently, lexicalized grammars offer significant parsing benefits (Schabes et al, 1988) as the number of applications of productions (i.e., derivation steps) is clearly bounded by the length of the input string. Restrictions on possible relations are attached to the words ,e.g., expressed as valencies in the case of dependency relations, yielding a strictly lexicalized grammar in the sense of Schabes et al (1988). The lexical distribution of grammatical knowledge one finds in many lexiealized grammar formalisms (e.g., LTAGS (Schabes et al, 1988) or HPSG (Pollard& amp; Sag, 1994)) is still constrained to declarative notions. At the same time, there is growing interest in parsing with more sophisticated lexicalized grammar formalisms, such as Lexical Functional Grammar (LFG) (Bresnan, 1982), Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al, 1988), Head driven Phrase Structure Grammar (HPSG) (Pollardand Sag, 1994) and Combinatory Categorial Grammar (CCG) (Steedman, 2000), which represent deep syntactic structures that can not be expressed in a shallower formalism designed to represent only aspects of surface syntax, such as the dependency formalism used in current mainstream dependency parsing.  However, virtually all Tree Adjoining Grammars (TAG ,seee.g., (Schabes et al, 1988)) used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (Schabes and Waters,1995). They employed a CCG (Steedman, 2000) or LTAG (Schabes et al, 1988) parser to acquire syntactic/semantic structures, which would be passed to statistical classifier as features. Various parsing techniques have been developed for lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al,1988), and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994). Head-Driven Phrase Structure Grammar (HPSG) is classified into lexicalized grammars (Schabes et al, 1988). The Fergus system (Bangalore and Rambow, 2000) uses LTAG (Lexicalized Tree Adjoining Grammar (Schabes et al, 1988)) for generating a word lattice containing realizations and selects the best one using a trigram model. White and Baldridge (2003) developed a chart generator for CCG (Combinatory Categorial Grammar (Steedman, 2000)) and proposed several techniques for efficient generation such as best-first search, beam thresholding and chunking the input logical forms (White, 2004). Conventional approaches to subcategorization, such as Definite Clause Grammar (Pereira and Warren, 1980), Categorial Grammar (Ades and Steedman, 1982), PATR-II (Shieber, 1986), and lexicalized TAG (Schabes et al 1988) all deal with complementation by including in one form or another a notion of& quot; subcategorization frame& quot; that specifies a sequence of complement phrases and constraints on them. Derivation trees, the structural description in LTAG (Schabes et al, 1988), represent the association of lexical items i.e., elementary trees. In lexicalized grammatical formalisms such as Lexicalized Tree Adjoining Grammar (Schabes et al, 1988, LTAG), Combinatory Categorial Grammar (Steedman, 2000, CCG) and Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994, HPSG), it is possible to separate lexical category assignment? the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates? from the combinatory processes that make use of such categories? such as parsing and surface realization.
We relate the algorithm to proposals by Shieber (1988). Shieber (1988) claims that the problem of logical equivalence reduces to the knowledge representation problem. Two further problems are the treatment of unary rules and functors with what Shieber (1988) calls vestigial semantics, which we prefer to call identity semantics. The obvious solution is to use a lemma table or chart (as discussed by Pereira and Warren 1984 and Shieber 1988). Shieber (1988) states that to guarantee completeness in using a precomputed entry in the chart, the entry must subsume the formula being generated top-down. And Bi-directional Grammar Shieber proposed auniform architecture for sentence parsing and generation based on the Early type deduction mechanism (Shieber,1988). Shieber (1988) gave the first use of Earley's algorithm for generation, but this algorithm does not. As Shieber (1988) noted, the main shortcoming of Earley generation is a lack of goal-directedness that results in a proliferation of edges. In generation, as Shieber (1988) and Appelt (1989) observe, a situation may arise in which the representation supplied as input to the process (perhaps by another program) is not itself directly suitable, but is logically equivalent to one that is. The use of distinct grammars for parsing and generation could provide a solution to this problem, but it raises others connected with management of the resulting system. As (Shieber, 1988) showed, the problem with such words is that they can not be selected on the basis of the input semantics. In (Shieber, 1988), a chart-based bottom-up generator is presented which is devoid of an indexing scheme: all word edges leave and enter the same vertex and as a result, interactions must be considered explicitly between new edges and all edges currently in the chart. Shieber (1988) showed that parsing charts can be also used in generation and raised the question, which we take up again here, of whether they constitute a natural uniform architecture for parsing and generation. In this respect, it differs from the proposal of Shieber (1988) which starts with all word edges leaving and entering a single vertex. Shieber (1988) has generalized this method so as to adapt to sentence generation as well. In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al, 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988). The use of such a semantic filter in bottom-up evaluation requires the grammar to obey the semantic monotonicity constraint in order to ensure completeness (Shieber, 1988) (see below). In order to obtain a generator similar to the bottom-up generator as described in Shieber (1988) the compilation process can be modified such that only lexical entries are extended with magic literals. The first demonstration of using charts for generation appeared in Shieber (1988). In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically non monotonic grammars, yet unlike top down methods, it also permits left-recursion. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes.
A Feature-based TAG (Vijay-Shanker and Joshi,1988) consists of a set of (auxiliary or initial) elementary trees and of two tree-composition operations: substitution and adjunction. A Feature-based TAG (FTAG, (Vijay-Shanker and Joshi, 1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction. They arise from top-bottom feature identifications parallel to the unifications performed in FTAG (Vijay-Shanker and Joshi, 1988) and from identifications of global features. A Feature based TAG (FTAG, (Vijay-Shanker and Joshi,1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction.  The equations of top and bottom features linked to specific node positions in the elementary trees are parallel to the syntactic unifications in FTAG (Vijay-Shanker and Joshi, 1988). A Feature based TAG (FTAG, (Vijay-Shanker and Joshi,1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction.
 Another related line of work is the word sense disambiguation algorithm proposed in (Veronis and Ide, 1990), where a large neural network is built by relating words through their dictionary definitions. Current set of semantic relation types m MindNet These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al (1996). This deficiency limits the characterization of word pairs such as river bank (Wilks et al 1996) and write pen (Veronis and Ide 1990) to simple relatedness, whereas the labeled relations of MindNet specify precisely the relations river --Part -& gt; bank and write --Means --& gt; pen. Inverted semrel structure from a definition of motorist Researchers who produced spreading activation networks from MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Since the model estimation has been reduced to simple update calculations, the proposed model is similar to conventional spreading activation approaches, which have been applied, for example, to word sense disambiguation (Veronis and Ide, 1990).
Previous work on this approach, largely based on Karlsson's original proposal [Karlsson, 1990], is documented in [Karlsson et ai., forthcoming]. For a description of the Constraint Grammar approach we refer thereader to (Karlsson, 1990). Our work is partly based on the work done with the Constraint Grammar framework that was originally proposed by Fred Karlsson (1990).
The construction of bilingual knowledge base, in the development of example-based machine translation systems (Sato and Nagao, 1990), is vitally critical. For example, (Sato and Nagao, 1990} combine a measure of structural similarity with a measure of word distance in order to obtain the overall distance measure that is used for matching. This is the same idea as example-based machine translation (Sato and Nagao, 1990 and Furuse et .al., 1994). In the wake of the pioneering work of Nagao (1984), Brown et al (1990) and Sato and Nagao (1990), Machine Translation (MT) research has increasingly focused on the issue of how to acquire translation knowledge from aligned parallel texts.
Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. This channel model is formalised using a Synchronous Tree Adjoining Grammar (STAG) (Shieber and Schabes, 1990), which matches words from the reparandum to the repair. between syntax and semantics (Shieber and Schabes, 1990). The states could be more refined than those shown above: the state for the subject, for example, should probably be not NP but a pair (Npl, NP3s) .STSG is simply a version of synchronous tree adjoining grammar or STAG (Shieber and Schabes, 1990) that lacks the adjunction operation. For further background, refer to the work of Shieber and Schabes (1990) and Shieber (1994). In theory, stochastic tree transducers and some versions of synchronous grammars provide solutions for the non-isomorphic tree based transduction problem and hence possible solutions for MT. Synchronous Tree Adjoining Grammars, proposed by (Shieber and Schabes, 1990), were introduced primarily for semantics but were later also proposed for translation. A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g. Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990)). Shieber and Schabes (1990) describe a synchronous tree adjoining grammar. At the same time, grammar theoreticians have proposed various generative synchronous grammar formalisms for MT, such as Synchronous Context Free Grammars (S-CFG) (Wu, 1997) or Synchronous Tree Adjoining Grammars (S-TAG) (Shieber and Schabes, 1990). For example, Synchronous TAGs, proposed by (Shieber and Schabes, 1990), which were introduced primarily for semantics but were later also proposed for translation. The TAG channel model defines a stochastic mapping of source sentences X into observed sentences Y. There are several ways to define transducers using TAGs such as Shieber and Schabes (1990), but the following simple method, inspired by finite-state transducers, suffices for the application here. The tree fragments that are obtained by decomposing the synchronous trees in this fashion are similar to the Synchronous Tree Insertion Grammar of (Shieber and Schabes, 1990). We developed a tree traversal algorithm that decomposes parallel trees into all minimal tree fragments. One natural solution is to restrict our candidate triples to those given by a synchronous context free grammar (SCFG) (Shieber and Schabes, 1990). Our mapping between semantic and syntactic constituents bears resemblance to the pairings in Synchronous TAG (Shieber and Schabes, 1990). Recently there is a swing back to incorporating more linguistic information again, but this time linguistic insight carefully guides the setup of empirically learned models. Shieber (2007) recently argued that probabilistic Synchronous Tree Adjoining Grammars (Shieber and Schabes, 1990) have the right combination of properties that satisfy both linguists and empirical MT practitioners. Shieber and Schabes (1990) offer a synchronous version of TAG (STAG), allowing the construction of a pair of trees in lockstep fashion using the TAG operations of substitution and adjunction on tree pairs.  STAG (Shieber and Schabes, 1990) (Synchronized TAG) for each member of the TAG (Tree Adjoining Grammar) family. The meaning of the word& quot; synchronized& quot; here is exactly the same as in STAG (Shieber and Schabes, 1990). As first described by Shieber and Schabes (1990), STAG can be used to provide a semantics for a TAG syntactic analysis by taking the tree pairs to represent a syntactic analysis synchronized with a semantic analysis. For example, Figure 2 (a) contains a sample English syntax/semantics grammar fragment that can be used to analyze the sentence? John apparently likes Mary?.
As mentioned in (Emele and Zajac, 1990), the proposed approach inevitably leads to the consequence that the data structure becomes slightly complicated.  Implementations of sorted feature formalisms such as TDL (Krieger and Schifer, 1994), ALE (Carpenter, 1993), CUF (DSrre and Dorna, 1993), TFS (Emele and Zajac, 1990) and others have been used successfully for the development and testing of large grammars and lexicons, but they may be too slow for actual use in applications 180 because they are generally built on top of Prolog or LISP, and can therefore not be as efficient as the built-in unification of Prolog. A more radical approach however rooted in the traditional model is to fully map the typed unification grammars [Emele and Zajac, 1990 on the SNAP.  This gives the possibility to define more general relations, and in particular functions can be defined in a way similar to, for example, (Johnson and Rosner, 1989) and (Emele and Zajac, 1990). In this way the user can create an inheritance hierarchy which is similar but not identical to how inheritance is used in other formalisms uch as TFS (Emele and Zajac, 1990) or ALE (Carpenter, 1992).
Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical case frame expectations. Researchers since Dagan and Itai (1990) have variously argued for and against the utility of collocation statistics between nouns and parents for improving the performance of pronoun resolution. Dagan an dItai (1990) use the distribution of a pronoun's context to determine which candidate antecedents can fit the context. Dagan and Itai (1990), for example, developed a statistical approach for pronominal anaphora, but the information they used was simply the patterns obtained from the previous analysis of the text. Consider the example given in the work of Dagan and Itai (1990): (1) They know full well that companies held tax money aside for collection later on the basis that the government said it was going to collect it2. Dagan and Itai (1990 )proposed a heuristics-based approach to pronoun resolution. One of the earliest methods for using predicate-argument frequencies in pronoun resolution is that of Dagan and Itai (1990).
In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al, 1994). Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate asegmentation result. In our system, the well-known forward maximum matching algorithm (Chen and Liu, 1992) is implemented. (Chen and Liu,1992) and many subsequent works), or uses the position of characters in a word as the basis for segmentation (Xue, 2003). In terms of processing model, this is a contradiction since segmentation should be the pre-requisite of dictionary lookup and should not presuppose lexical information. The classical model, described in (Chen and Liu, 1992) and still adopted in many recent works, considers text segmentation as a tokenization. This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). Take for example maximum matching, which was a popular algorithm at the early stage of research (Chen and Liu, 1992).
Currently, the most attractive architecture is as in Figure 1 (Karttunen et al (1992)): 1284 String alternation rules Formative lexicon] I~ Compilation &apos; 1 Compilation (Lexicon FSA) (Rule FSTs) Lexicalransducel2, j Figure l: Architecture of Two-Level Morphology. Earlier examples of such approaches include lexc (Karttunen et al, 1992), FASTR (Jacquemin,2001), HABIL (Alegria et al, 2004), and Mul ti flex discussed below. See also (Savary, 2008) for a detailed contrastive analysis of Multiflex with respect to 10 other systems for a lexical description of MWUs in different languages such as (Karttunen et al, 1992), (Jacquemin, 2001), and (Alegria et al, 2004). A second important advance in computational morphology was the recognition by Karttunen et al (1992) that a cascade of composed FSTs could implement the two-level model. related forms of the word pair using a lexical transducer for English (Karttunen et al, 1992). It was observed somewhere around 1990 at Xerox that the rule sets may be composed with the lexicon transducers in an efficient way and that the resulting transducer was roughly similar in size as the lexicon transducer itself (Karttunen et al, 1992).
First, as in (Reiter and Dale, 1992), we stipulate that some attributes of entities are more important than others, and that some words more naturally describe those attributes. Unlike in traditional generation, the starting point in1The predicate dog1 is selected because it has a dis tin guished status, referred to as type in Reiter and Dale (1992). The incremental algorithm (Reiter and Dale, 1992) is the most widely discussed attribute selection algorithm. In terms of the referring expression generation algorithm described by (Reiter and Dale, 1992), in which the description which eliminates the most dis tractors is selected, our 1http: //www.cs.waikato.ac.nz/ml/weka/ 2not all positive examples were visible 159results suggest that the human subjects chose to reduce the size of the dis tractor set before producing a description, presumably in order to reduce the computational load required to calculate the optimal description. Reiter and Dale (1992) describe a fast algorithm for generating referring expressions in the context of a natural language generation system. As shown in [Reiter and Dale, 1992], a referring expression must communicate enough information to be able to uniquely identify the intended referent in the current discourse context, but avoiding the presence of redundant or otherwise unnecessary modifiers. It would be also interesting to compare our solution with different approaches found in the literature, as for example [Reiter and Dale, 1992] or [Krahmer and Theune, 2000] for the referring expression generation, and the one of Dalianis and Hovy [Dalianis and Hovy, 1996] for the aggregation. In natural language generation, the process of generating referring expressions occurs in stages (Reiter and Dale, 1992). We plan to incorporate the results of this study in an extension of (Reiter and Dale, 1992) algorithm that would take into account other types of properties of the objects like visual salience, temporal attributes (for example time elapsed between mentions), if it participated in an action (like the case of a door opening, or a button being pushed) or its importance to the overall task completion.  For example, (Reiter and Dale, 1992) apply generalizations about the salience of properties of objects and conventions about what words make base level attributions to incrementally select words for inclusion in a description. The construction and maintenance of context according to models of text planning [Reiter and Dale, 1992], allow the author to break a complex CG into a manageable collection of small utterances. For micro planning, we have implemented the algorithm for reference planning described in [Reiter and Dale, 1992] and the aggregation algorithm described in [Shaw, 1995]. The authors present in the paper a computational framework for the generation of spatial locative expressions in such contexts, relying on the Reiter and Dale (Reiter and Dale, 1992) algorithm. Another interesting work related to referring expression generation in spatial environments can be found in (Varges, 2005). This choice should be considered in terms of particular implementation details. Reference agents rely on the Reiter and Dale algorithm (Reiter and Dale, 1992).
In stochastic tree-adjoining grammar (Schabes, 1992), this lack of context-sensitivity is overcome by assigning probabilities to larger structural units. In (Schabes, 1992) it is proposed to infer a stochastic TAG from a large training corpus using an inside-outside-like iterative algorithm. In (Pereira and Schabes, 1992), 90.36% bracketing accuracy was reported using a stochastic CFG trained on bracketings from the ATIS corpus. Our general estimation method also has practical applications in cases one uses a probabilistic context-free grammar to approximate strictly more powerful rewriting systems, as for instance probabilistic tree adjoining grammars (Schabes, 1992). In recent years, the statistical parsing community has begun to reach out; for syntactic formalisms that recognize the individuality of words, link grammars (Sleator and &apos; Pemperley, 1991) and lexicalized tree-adjoining grammars (Schabes, 1992) have now received stochastic treatments. Although it is beyond the scope of our research, we conjecture that there exists a Monte Carlo disambiguation algorithm for at least Stochastic Tree-Adjoining Grammar (Schabes, 1992). Examples of the first approach can be seen most clearly with the usage of CNF grammars by the Inside-Outside algorithm (Pereira and Schabes, 1992, Lari and Young, 1990).
 Recently, Yarowsky (1992) has found a way to extend our use of the Bayesian techniques by training on the Roget's Thesaurus (Chapman, 1977) 2 and G-rolier's Encyclopedia (1991) instead of the Canadian Hansards, thus circumventing many of the objections to our use of the Hansards. Yarowsky (1992) inputs a 100-word context surrounding a polysemous word and scores each of the 1042 Roget Categories by: 1[ P r (w lRoget Categoryi) w in context The program can also be run in a mode where it takes unrestricted text as input and tags each word with its most likely Roget Category. Table 1 shows the performance of Yarowsky (1992) on twelve words which have been previously discussed in the literature. In fact, both Black (1988) and Yarowsky (1992) report 72% performance on this very same word. In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn't be too concerned about this one deviation.  Semantic tags are assigned from on-line thesauras like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992) 1. Yarowsky (1992) used Roget's Thesaurus categories as classes for word senses.  The best examples of this approach has been the resent work of Yarowsky (Yarowsky, 1992), (Yarowsky, 1993), (Yarowsky, 1995). Much of the research in this area has been compromised by the fact that researchers have focused on lexical ambiguities that are not true word sense distinctions, such as words translated differently across two languages (Gale, Church, and Yarowsky, 1992) or homophones (Yarowsky, 1993). Pragmatic domain codes can be used to disambiguate (usually nominal) senses, as was shown by (Bruce and Guthrie, 1992) and (Yarowsky, 1992).  Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). Yarowsky (Yarowsky, 1992) used instead thesauri for their experiments. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al 1992, Yarowsky 1992), as well as in similar predicate argument structure contexts (e.g., Grishman and Sterling 1994). As in prior work including B&L, we rely on the intuition that the senses of words are hinted at by their contextual information (Yarowsky, 1992). From the perspective of a generative process, neighboring words of a target are generated by the target's underlying sense. 
Hearst (1992) found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques. The hypernyms used to label the internal nodes of that hierarchy are chosen in a simple fashion; pattern-matching as in Hearst (1992) is used to identify candidate hypernyms of the words dominated by a particular node, and a simple voting scheme selects the hypernyms to be used. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al, 2004). Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Automatically extracted lexico-syntactic patterns have been successfully employed in various term extraction tasks (Hearst, 1992). Phrases such as also known as, is a, part of, is an example of all indicate various of semantic relations. Such indicative phrases have been successfully applied in various tasks such as synonym extraction, hyponym extraction (Hearst, 1992) and fact extraction (Pasca et al, 2006). The idea of searching a large corpus for specific lexico-syntactic phrases to indicate a semantic relation of interest was first described by Hearst (1992). Hearst (1992) pioneered the use of lexical syntactic patterns for automatic extraction of lexical semantic relationships. Table 1: The patterns we used for entailment acquisition based on (Hearst, 1992) and (Pantel et al, 2004). The pioneering work of Hearst (1992) applied fixed patterns like NP1, especially NP2 to derive that NP1 is a hypernym of NP2. The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2The number of ranked concepts that pass CPT changes in each iteration. They build a classifier using the different hypernym patterns and find among the highest precision patterns those of (Hearst, 1992). Recently, (Ritter et al, 2009) reported hypernym learning using (Hearst, 1992) patterns and manually tagged common and proper nouns. (Hearst, 1992) If any pattern matches the structure of the dependency parse tree, the hypernym can be extracted. The procedure uses a collection of Web documents and applies some IsA extraction patterns selected from (Hearst, 1992). Hearst (1992) was the first to propose a pattern-based approach to this task using lexico-syntactic patterns to automatically extract hyponyms and this technique has frequently been used for ontology learning. Initial research on taxonomy learning focused on identifying in a given text lexico-syntactic patterns that suggest hyponymy relations (Hearst, 1992). These patterns, as those proposed by Hearst (1992) and used in many projects since, may not be 100% reliable. Hearst (1992) pioneered using patterns to extract hyponym (is-a) relations. Berland and Charniak (1999) proposed a system for part-of relation extraction, based on the (Hearst 1992) approach.
For a treatment of DOP in more formal terms we refer to (Bod, 1992a). In (Bod, 1992b) super strong equivalence relations between other stochastic grammars are studied. It is easy to show that an input string can be parsed with conventional parsing techniques, by applying subtrees instead of rules to the input string (Bod, 1992a). In their place we added a new feature, the probability of a rule's source side tree given its root label, which is essentially the same model used in Data-Oriented Parsing (Bod, 1992). Due to this extension, the one to one mapping between a derivation and a parse tree, which holds in CFGs, does not hold any more; many derivations might generate the same parse-tree ,rl &apos; his seemingly spurious ambiguity turns out crucial for statistical disambiguation as defined in (Bod, 1992) and in (Schabes and Waters, 1993), where the derivations are considered different stochastic processes and their probabilities all contribute to the probability of the generated parse. context-free rulesCharniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4. It occurs because many systems, such as the ones proposed by (Bod, 1992), (Galley, et .al., 2004), and (Langkilde and Knight, 1998) represent their result space in terms of weighted partial results of various sizes that may be assembled in multiple ways. The Data-Oriented Parsing (DOP) method suggested in Scha (1990) and developed in Bod (19921995) is a probabilistic parsing strategy which does not single out a narrowly predefined set of structures as the statistically significant ones. In Bod (1992, 1993a), a first instantiation of this model is given, called DOP1, which uses (1) labelled trees for the utterance analyses, (2) subtrees for the fragments, (3) node substitution for combining subtrees, and (4), the sum of the probabilities of all distinct ways of generating an analysis as a def &apos ;mition of the probability of that analysis. Bod (1992, 1993a) shows that conventional context-free parsing techniques can be used in creating a parse forest for a sentence in DOP1. Data-Oriented Parsing Bothprobabil is tic and non-probabilistic DOP are based on the DOP model in Bod (1992) which extracts a Stochastic Tree-Substitution Grammar. Bod (1992) demonstrated that DOP can be implemented using conventional context-free parsing techniques. Next, parsing proceeds with the subtrees that are triggered by the dialogue context C (provided that all subtrees are converted into equivalent rewrite rules -see Bod 1992, Sima &apos; an 1995).
Bourigault &apos; s LECTER [Bourigault, 1992] is a surface-syntactic analyser that extracts &apos; maximal length noun phrases &apos; -mainly sequences of determiners, premodifiers, nominal heads, and certain kinds of post modifying prepositional phrases and adjectives from French texts for terminology applications. Some methods (Bourigault (1992), Ananiadou (1994)) rely purely on linguistic information, namely morpho-syntactic features of term candidates. On the grammar-based side, Bourigault (1992) describes a system for extracting& quot; terminological noun phrases& quot; from French text. Bourigault (1992) reports a tool, LEXTER, for extracting terminologies from texts.
For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic.   The correct rate of tagging of these models has reached 95%, in part by using a very large amount of training data (e.g., 1,000,000 words in Schmid, 1994).   A statistical-based suffix learner is presented in (Schmid, 1994).  The TreeTagger showed an accuracy of 96.06% (Schmid, 1994a).  There have been attempts to apply neural networks to POS tagging (e.g., (Schmid, 1994)). We use the Tree Tagger (Schmid, 1994) for all POS tagging except for Arabic, where we use the tagger described in Diab et al. We use TreeTagger (Schmid, 1994) to produce POS tags and then open class words are restricted if the POS tagger assigned a tag with a probability over a certain threshold. A statistical-based suffix learner is presented in (Schmid, 1994). 
N-gram models arc usually used for scoring (Gu et al, 1991) (Nagata, 1994), but their training requires the sentences of the corpus to be manualy segmented, and even class-tagged if class-based N-gram is used, as in (Nagata, 1994).  Around 95% word segmentation accuracy is reported by using a word-based language model and the Viterbi-like dynamic programming procedure [Nagata, 1994, Takeuchi and Matsumoto, 1995, Yamamoto, 1996]. We used the Viterbi-like dynamic programming procedure described in [Nagata, 1994] to get the most likely word segmentation. Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al, 1996].  Nagata (1994) proposed a stochastic word segmenter based on a word-gram model to solve the word segmentation problem. Nagata (1994) reported an accuracy of about 97% on a test corpus in the same domain using a learning corpus of 10,945 sentences in Japanese. Fully stochastic language models (e.g. Nagata 1994), on the other hand, do not allow such manual cost manipulation and precisely for that reason, improvements in segmentation accuracy are harder to achieve. The best accuracy reported for statistical methods to date is around 95% (e.g. Nagata 1994). We use the Viterbi algorithm to find the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the N best sets. An algorithm that can provide a solution for Step 2 will be a simpler version of the algorithm used to find the maximum probability solution in Japanese morphological analysis (Nagata, 1994). To find the sequence, Nagata proposed a probabilistic language model for non-segmented languages (Nagata, 1994). In this paper, we apply a dynamic programming search (Nagata, 1994) to k best MIRA.
The ANLT lexicon was derived semi-automatically from a machine readable dictionary (LDOCE), and although the COMLEX syntax dictionary (Grishman et al, 1994), which was derived with much greater amounts of human effort, has a slightly better performance, the difference is not great. We extend our previous work (Chodorowetal., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). We also experimented with other features such as augmenting the model with verb-preposition preferences derived from CoMlex (Grishman et al, 1994), and querying the Google Terabyte N-gram corpus with the same patterns used in the combination features. Although a number of freely available, large scale and accurate SCF lexicons exist ,e.g. COMLEX (Grishman et al 1994), VerbNet (Kipperet al 2008) for English, availability and limitations in size and coverage remain an inherent is sue. The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT (Boguraev and Briscoe,1987) and COMLEX (Grishman et al, 1994) dictionaries. We obtain our SCF data using the sub categorization acquisition system of Briscoe and Carroll (1997). We expect the use of this system to be beneficial: it employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses, and a comprehensive SCF classifier, which incorporates 163 SCF distinctions, a super set of those found in the ANLT (Boguraev et al,1987) and COMLEX (Grishman et al, 1994) dictionaries. The lexicon is based on COMLEX (Grishman et al., 1994). We based our POS table lookup on NYU's COMLEX (Grishman et al 1994). SCF FramesThe SCFs recognized by the classifier were obtained by manually merging the frames exemplified in the COMLEX Syntax (Grishman et al, 1994), ANLT (Boguraev et al, 1987 )and/or NOMLEX (Macleod et al, 1997) dictionaries and including additional frames found by manual inspection of unclassifiable examples during development of the classifier. Carmel comes with a wide-coverage English grammar that is compatible with the wide-coverage COMLEX lexicon (Grishman et al, 1994). This fairly comprehensive classification incorporates 163 different sub categorization frames (SCFs), a superset of those listed in the ANLT (Boguraev et al, 1987) and COMLEX Syntax dictionaries (Grishman et al, 1994). was available and/or from the COMLEX Syntax dictionary (Grishman et al, 1994) all the SCFs taken by its member verbs. Important examples are LDOCE (Procter 1987), ComLex (Grishman et al 1994), PAROLE (Ruimy et al 1998). Itincorpo rates 168 SCF distinctions, a superset of those found in the COMLEX Syntax (Grishman et al, 1994) and ANLT (Boguraev et al, 1987) dictionaries. For example, we included the examination of a body part belonging to a person in the domain, and this was expressed through a Saxon genitive in English but a prepositional phrase (with the subsidiary NPs in the reverse order) in the other languages. To test our assumptions about efficiency and scalability we inferred a larger Tbox, subcategorisation frames and mappings using a pre-existing data set of verb frames for English encoded using the COMLEX subcategorisation frame inventory (Grishmanet al, 1994).  Other resources such as COMLEX syntax dictionary (Grishman et al., 1994) and English Verb Classes and Alternations (EVCA) (Levin, 1993) can provide verb subategorization information and syntactic paraphrases, but they are indexed by words thus not suitable to use in generation directly. It also exploits the classification given bythe COMLEX lexicon (Grishman et al, 1994) in order to calculate the deep-subject of infinitive verbs. the CARMEL grammar and semantic interpretation framework (Rose, 2000), and the COMLEX lexicon (Grishman et al, 1994). From the different diagnostics proposed in the literature some are quite consistent among various authors (R. Grishman et al 1994, C. Pollard and I. Sag 1987, C. Verspoor 1997).
Principle-based parsers are also designed with universal grammar in mind (Lin 1994), but have yet to demonstrate large scale coverage in several languages. In this paper, we adopt Minipar (Lin, 1994) to parse sentences and to construct syntactic trees. English dependency trees are provided by Minipar (Lin, 1994). It is based on Principar, which is described in Lin (1994). These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. We used Minipar (Lin 1994), a broad coverage parser, to parse 3GB of newspaper text from the Aquaint (TREC-9) collection. We parse questions and candidate sentences with MiniPar (Lin, 1994), a fast and robust parser for grammatical dependency relations. We used Minipar (Lin 1994), a broad coverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500words/second on a PIII-750 with 512MB memory. We parse a 6 GB newspaper (TREC9 and TREC 2002 collection) corpus using the dependency parser Minipar (Lin, 1994). For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set. For the algorithm discussed in Section 4.1, we derived our descriptive properties using the output of the dependency analysis generated by the Minipar (Lin, 1994) dependency parser.  We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). We used Minipar (Lin 1994), a broad coverage parser, to analyze text. We used Minipar (Lin 1994), a broad coverage parser, to parse two 3GB corpora (TREC-9 and TREC-2002). English dependency trees are provided by Minipar (Lin, 1994). The Simplified Clause: In order to extract clauses from the text, we use Lin's parser MINI PAR (Lin, 1994).  To get the context values and implement the syntactic filters, we parsed our corpora with Minipar (Lin,1994). Widely used dependency parsers which generate deep dependency representations include Minipar (Lin, 1994), which uses a declarative grammar, and the Stanford parser (Levy and Manning, 2004), which performs a conversion from a standard phrase-structure parse.
Karlgren and Cutting (1994) use a combination of structural markers (e.g., noun count), lexical markers (e.g., it count), and token-level markers (e.g., words per sentence average ,type/token ratio, etc.). Work on automated genre classification was first carried out by Karlgren and Cutting (1994). Following Karlgren and Cutting (1994), I tested my approach on all three levels of granularity.  In one experiment in (Karlgren and Cutting, 1994) the sub genres under fiction are grouped together, leading to 10 gen res to classify. Results on 10-genre Brown Corpus. The result is also significantly better than prior work on the Browncor pus in (Karlgren and Cutting, 1994) (who use the whole corpus as test as well as training data).  In addition, we improve on the training accuracy of 52% reported in (Karlgren and Cutting, 1994). We are also interested in structural accuracy (S Acc) to see whether the structural SVMs make fewer& quot; big& quot; mistakes. Both works dispensed with the more complex features proposed by Karlgren and Cutting (1994) which showed promising results. The simplest kind of formality measure is based on word length, which is often used directly as an indicator of formality for applications such as genre classification (Karlgren and Cutting, 1994).
However, Fung and Church (1994) point out that such a constraint does not exist between languages across language groups such as Chinese and English. Constraints Image IP techniques Alignment Pattern Resolution Structure Edge Convolution Phrase preserving One-to-one Texture Feature Sentence extraction Non-crossing Line Hough Discourse transform information (Ker and Chang 1996), cognates (Simard 1992), K-vec (Fung and Church 1994), DTW (Fung and McKeown 1994), etc. As Fung and Church (1994) we wish to estimate the bilingual lexicon directly. 
This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a). Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage. Brill and Resnik (1994) used the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples. For example, in (Brill and Resnik, 1994) clustering PP heads according to WordNetsynsets produced only a 1% improvement in a PP disambiguation task, with respect to the non-clustered method. This huge number of tokens can be explained by the fact that the lexicon used for tokenization and tagging integrates many multi-word expressions which are not part of these mantic lexicon for (Brill and Resnik, 1994) and 0.77 for (LauerandDras, 1994)), but a direct comparison is difficult inasmuch as only three-word sequences (V N P, for (Brill and Resnik, 1994) and N N N for (Lauer and Dras, 1994)) were used for evaluation in those works, and the language studied is English. In later stages of processing, a corpus-based approach (Brill and Resnik, 1994) is used to deal with ambiguities that cannot be solved with syntactic information only, in particular attachments of prepositional phrases, gerunds and infinitive constructions. For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995). A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information.    Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven TransformationBased Learning (Brill and Resnik, 1994), which were clearly outperformed by both IB1 and IBI-IG, even though e.g. Brill~ Resnik used more elaborate feature sets (words and WordNet classes). Brill and Resnik (1994) applied Error-Driven Transformation-Based Learning to this task, using the verb, noun1, preposition, and noun2 features. Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994). We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al, 1994) to objectively compare the performances of the systems. (10) The lower bound for the B&R data is 63% (Brill and Resnik, 1994) and for the IBM data is 52% (Ratnaparkhi et al, 1994).
    In the second column, recall and precision are relative to polysemous verbs only, and in spite of an obvious decrease compare well with related work carried out with different methods (see, for instance, [Agirre and Rigau, 1996]), and are in fact very promising if one considers the comparatively small size of EB, and that only part of its attested words are semantically disambiguated. On the contrary, the Conceptual Density (CD) (Agirre and Rigau, 1996) is a flexible semantic similarity which depends on the generalizations of word senses not referring to any fixed level of the hierarchy. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al (1995) and Agirre and Rigau (1996). Conceptual Density (CD) was originally introduced by (Agirre and Rigau, 1996). Agirre and Rigau (Agirre and Rigau 1996) use a conceptual distance formula that was created to by sensitive to the length of the shortest path that connects the concepts involved, the depth of the hierarchy and the density of concepts in the hierarchy. Conceptual Density (Agirre and Rigau 1996) is the paradigmatic component chosen to discriminate semantically among potential noun corrections. The semantic classes of x and y are then inferred using conceptual density (Agirre and Rigau 1996), a Word Net-based measure applied to all instantiation of x and y in the corpus.  To overcome the problem of varying link distances, Agirre and Rigau (1996) propose a semantic similarity measure (referred to as& quot; conceptual density& quot;) which is sensitive to i) the length of the path, ii) the depth of the nodes in the hierarchy (deeper nodes are ranked closer) and iii) the density of nodes in the sub hierarchies (concepts involved in a denser sub hierarchy are ranked closer than those in a more sparse region). To compute similarity for nouns we adopt conceptual density (cd) (Agirre and Rigau, 1996), a semantic similarity model previously applied to word sense disambiguation tasks. In (Agirre and Rigau, 1996) a method called Conceptual Distance is proposed to reduce this problem, but the reported performance in disambiguation still do not reach 50%. A method not evaluated by (Patwardhan et al., 2003) and using another semantic relatedness measure (conceptual density) is (Agirre and Rigau, 1996).  This proposal has a partial similarity with the Conceptual Density (Agirre and Rigau, 1996) and DRelevant (Va ?zquez et al, 2004) to get the concepts from a hierarchy that they associate with the sentence. Another method which could be used for class labelling is given by the conceptual density algorithm ofAgirre and Rigau (1996), which those authors applied to word sense disambiguation. Note that the notion of density here is not to be confused with the conceptual density used by Agirre and Rigau (1996), which is essentially a semantic similarity measure by itself.
But (Kennedy and Boguraev, 1996a) show that the Lappin and Leass algorithm still provides good results (75%) even without complete parse. They suggest also (Kennedy and Boguraev, 1996b) that anaphora resolution is part of the discourse referents resolution. This module is based partly on the system described by Kennedy and Boguraev 1996, with the various weighting factors based on theirs, but designed so that the weights can be trained given appropriate data. In addition, Dagan and Itai (1991) undertook additional pre-editing such as the removal of sentences for which the parser failed to produce a reasonable parse, cases where the antecedent was not an NP etc.; Kennedy and Boguraev (1996) manually removed 30 occurrences of pleonastic pronouns (which could not be recognised by their pleonastic recogniser) as well as 6 occurrences of it which referred to a VP or prepositional constituent. Kennedy and Boguraev (1996), for example, need additional information about whether a certain discourse referent is embedded or not, plus a pointer to the COREF class associated to the referent, while Mitkov's approach needs a score associated to each noun phrase. RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b). As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. In Kennedy and Boguraev (1996) it is proposed an algorithm for anaphor resolution which is a modified and extended version of that developed by Lappin and Leass (1994). Our framework will allow us a similar approach to that of Kennedy and Boguraev (1996), but we will automatically get syntactic information from partial parsing. The grammar in Figure 4 will only parse coordinated prepositional phrases (pp), coordinated noun phrases (np), pronouns (p), conjunctions (conj) and verbs (verb) in whatever order that they appear in the text and it will allow us to work in a similar way that the algorithm mentioned in Kennedy and Boguraev (1996). improved the accuracy (83%) in pronominal references to the work of Kennedy and Boguraev (1996) (75%), but we have also improved that approach since we automatically. It presupposes fine grained methods for the identification of cohesive ties 76 between (sentence) units in a text; describing the computational basis for developing such methods is outside of the scope of this paper (howeveb see (Kennedy and Boguraev, 1996), (Fellbaum, 1999), (Kelleb 1994)), as is the complete framework for lexical cohesion analysis we have developed. More recently, Kennedy and Boguraev (1996) propose an algorithm for anaphora resolution that is actually a modified and extended version of the one developed by Lappin and Leass (1994). L&L demonstrated with a system called RAP that a (manually-tuned) weight-based scheme for integrating pronoun interpretation preferences can achieve high performance on real data, in their case, 86% accuracy on a corpus of computer training manuals. Dagan et al (1995) then developed a post processor based on predicate-argument statistics that was used to override RAP's decision when it failed to express a clear preference between two or more antecedents, which resulted in a modest rise in per Kennedy and Boguraev (1996, henceforth, K&B) adapted L&L's algorithm to rely on far less syntactic analysis (noun phrase identification and rudimentary grammatical role marking), with performance in the 75% range on mixed genres. Kennedy and Boguraev (1996) then report a 75% accuracy for an algorithm that approximates Lappin and Leass's with more robust and coarse-grained syntactic input. Part of the pronoun resolution performance here enables a preliminary comparison with the results reported in (1) Lappin and Leass (1994) and (2) Kennedy and Boguraev (1996). preferred candidate antecedents Kennedy and Boguraev (1996) approximated the above components with a poorer syntactic input, which is an output of a part-of-speech tagger with grammatical function information, plus NPs recognized by finite-state patterns and NPs &apos; adjunct and subordination contexts recognized by heuristics. For example, we anticipate that sentences with quotation marks will be problematic, as other researchers have observed that quoted text requires special handling for pronoun resolution (Kennedy and Boguraev, 1996).  
Hence, if we were given the perfect knowledge of the possible syntactic frames, verbs can be classified into the correct classes almost perfectly (Dorr and Jones, 1996). Dorr and Jones (1996) showed that perfect knowledge of the allowable syntactic frames for a verb allows 98% accuracy in type assignment to Levin classes.  Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and sub categorization acquisition (Korhonen, 2002).
Structures and rules for parsing with the (Eisner, 1996) algorithm. In that work, as here, inference is simply the Eisner first-order parsing model (Eisner, 1996) shown in Figure 2. In order to score higher-order features, each chart item maintains a list of signatures, which represent subtrees consistent with the chart item. context-free rules Charniak (1996) Collins (1996), Eisner (1996) context-free rules, headwords Charniak (1997) context-free rules, headwords, grandparent nodes Collins (2000) context-free rules, headwords, grandparent nodes/rules, bi grams, two-level rules, two-level bi grams, non headwords Bod (1992) all fragments within parse trees Scope of Statistical Dependencies Model Figure 4. Viterbi decoding is done using Eisner's algorithm (Eisner, 1996). 1 of these edges, using an O (n3) dynamic programming algorithm (Eisner, 1996) for projective trees. But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996). In a slightly more general formulation, it was first published by Eisner (1996). For MST Parser, we use 1st order features and a projective decoder (Eisner, 1996). It combines online Peceptron learning (Collins, 2002) with a parsing model based on the Eisner algorithm (Eisner, 1996), extended so as to jointly assign syntactic and semantic labels. The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996). Eisner (1996) algorithm with non-projective rewriting and second order features. Examples of this include McDonald and Pereira's (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson's (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels. The best projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the Chu Liu-Edmonds (CLE) algorithm (McDonald et al, 2005b). Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O (n3) time. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O (n2) exact parsing methods for non projective languages like Czech. It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996). The (Eisner, 1996) algorithm is typically used for projective parsing. In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The Eisner (1996) algorithm and its variants are commonly used in data-driven dependency parsing.
In the case of English, partially motivated by Message Understanding Conferences (MUCs) (Grishman and Sundheim, 1996), a number of coreference resolution methods have been proposed. A condition of attending the MUC workshops was participation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC 6 (1995) (Grishman and Sundheim, 1996). Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al, 1999), and ontology population (Suchanek et al, 2007). The topic of the sixth MUC (MUC-6) was management succession events (Grishman and Sundheim, 1996). Figure 2 is a simplified event from the the MUC-6 evaluation similar to one described by Grishman and Sundheim (1996). As demonstrated by prior work (Grishman and Sundheim, 1996), grammar-based IE systems can be effective in many scenarios. When it was introduced, in the 6th Message Understanding Conference (Grishman and Sundheim, 1996), the named entity recognition task comprised three entity identification and labeling subtasks: ENAMEX (proper names and acronyms designating persons, locations, and organizations), TIMEX (absolute temporal terms) and NUMEX (numeric expressions, monetary expressions, and percentages). The NER task was introduced with the 6th Message UnderstandingConference (MUC) in 1995 (Grishman and Sundheim, 1996). The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC 6 Evaluation (Grishman and Sundheim, 1996).  In a previous attempt to define predicate-argument structure, Semeval, the effort was abandoned because so many constructs would require detailed attention and resolution, and because most information-extraction systems did not generate full predicate-argument structures (most likely because the task did not require it) (Grishmanand Sundheim, 1996). Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003).  We used 6 categories of entity types, which are the major categories defined in the MUC (Grishman and Sundheim 1996) or ACE project (ACE homepage). For illustration purposes, we extended it here by MUC (Grishman and Sundheim, 1996) entity types such as Person, Organization, etc. Diversity IE traditionally targets a selected event type (Grishman and Sundheim, 1996).
This would be more difficult in the HMM alignment model (Vogel et al., 1996). The most classical approaches are the probabilistic IBM models (Brown et al, 1993) and the HMM model (Vogel et al, 1996).  Word alignments were induced from the HMM based alignment model (Vogel et al, 1996), initialized with the bi lexical parameters of IBM Model 1 (Brown et al, 1993). Much of the additional work on generative modeling of 1 to N word alignments is based on the HMM model (Vogel et al, 1996). Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. Any aligner such as (Al-Onaizan et al, 1999) or (Vogel et al, 1996) can be used to obtain word alignments. For each sentence in the training, three types of word alignments are created: maximum entropy alignment (Ittycheriah and Roukos, 2005), GIZA++ alignment (Och and Ney, 2000), and HMM alignment (Vogel et al, 1996). They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al, 1996). We use the IBM Model 1 (Brown et al, 1993) and the Hidden Markov Model (HMM, (Vogel et al, 1996)) to estimate the alignment model. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al (1996), which adds a first-order dependency. The starting point is the final alignment generated using GIZA++'s implementation of IBM Model 1 and the Aachen HMM model (Vogel et al, 1996). Generative word alignment models including IBM models (Brown et al, 1993) and HMM word alignment models (Vogel et al, 1996) have been widely used in various types of Statistical Machine Translation (SMT) systems. Because such approaches directly learn a generative model over phrase pairs, they are theoretically preferable to the standard heuristics for extracting the phrase pairs from the many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Hidden Markov Model (HMM) (Vogel et al,1996). Inspired by HMM word alignment (Vogel et al, 1996), our second distance measure is based on jump width. Starting with the IBM models (Brown et al,1993), researchers have developed various statistical word alignment systems based on different models, such as hidden Markov models (HMM) (Vogel et al, 1996), log-linear models (OchandNey, 2003), and similarity-based heuristic methods (Melamed, 2000). One class of particularly useful features assesses the goodness of the alignment path through the source sentence (Vogel et al., 1996). While classical approaches for word alignment are based on generative models (e.g., IBM models (Brown et al, 1993) and HMM (Vogel et al, 1996)), word alignment can also be viewed as a matching problem, where each word pair is associated with a score reflecting the desirability of aligning that pair, and the alignment is then the highest scored matching under some constraints. This data is used to train a word alignment model, such as IBM Model 1 (Brown et al, 1993) or HMM-based word alignment (Vogel et al, 1996).
It stands to reason that long sentences will be harder to process automatically and this reasoning has motivated the first approaches to text simplification (Chandrasekar et al, 1996). Chandrasekar et al (1996) introduced a two stage process, first transforming from sentence to syntactic tree, then from syntactic tree to new sentence;. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Early attempts to text simplification were based on rule-based methods where rules were designed following linguistic intuitions (Chandrasekar et al, 1996). Simplification decisions about whether to simplify a text or sentence have been studied following rule-based paradigms (Chandrasekar et al, 1996) or trainable systems (Petersen and Ostendorf,2007) where a corpus of texts and their simplifications becomes necessary. Sentence simplification can also serve to preprocess the input of other tasks, such as summarization (Knight and Marcu, 2000), parsing, machine translation (Chandrasekar et al, 1996), semantic role labeling (Vickrey and Koller, 2008) or sentence fusion (Filippova and Strube, 2008). Together with syntactic analysis and transformations similar to those of Chandrasekar et al (1996), they employed lexical simplification based on looking up synonyms in WordNet and extracting Kucera-Francis frequency from the Oxford Psycholinguistic Database (Quinlan, 1992). or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996).  Chandrasekar et al (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. Text simplification has been associated with techniques that deal not only with helping readers with reading disabilities, but also to help NLP systems (Chandrasekar et al, 1996). 
(Shen and Lapata, 2007) has shown that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question. For example, Shen and Lapata (2007) show the potential improvement that FrameNet can bring on the performance of a Question Answering (QA) system. The alignment of answers to question types as a semantic role labelling task using similar methods was explored by Shen and Lapata (2007). The state-of-the-art in semantic role labelling has now advanced so much that a number of studies have shown that automatically inferred semantic argument structures can lead to tangible performance gains in NLP applications such as information extraction (Surdeanu et al, 2003), question answering (Shen and Lapata, 2007) or recognising textual entailment (Burchardt and Frank, 2006). FrameNet has a shorter history in NLP applications than WordNet, but lately more and more researchers have been demonstrating its potential to improve the quality of question answering (Shen and Lapata, 2007) and recognizing textual entailment (Burchardt et al, 2009). Recently, predicate argument structure analysis has attracted the attention of researchers because this information can increase the precision of text processing tasks, such as machine translation, information extraction (Hirschman et al, 1999), question answering (Narayanan and Harabagiu, 2004) (Shen and Lapata, 2007), and summarization (Melli et al., 2005). Shen and Lapata (2007) developed an answer extraction module that incorporates FrameNet style semantic role information. Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). Recent studies (e.g. Shen and Lapata (2007)) show that the use of FrameNet can potentially improve the performance of Question Answering systems. Yet, Shen and Lapata (2007) also point out that the low coverage of the current version of FrameNet significantly limits the expected boost in performance. Other work incorporating syntactic and linguistic information into IR includes early research by (Smeaton, O Donnell and Kelledy, 1995), who employed tree structured analytics (TSAs) resembling dependency trees, the use of syntax to detect paraphrases for question answering (QA) (Lin and Pantel, 2001), and semantic role labelling in QA (Shen and Lapata, 2007). Semantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). In particular, resources annotated with the surface realization of semantic roles, like FrameNet (Baker et al, 1998) or PropBank (Palmeret al, 2005) have shown to convey an improvement in several NLP tasks, from question answering (Shen and Lapata, 2007) to textual entailment (Burchardt et al, 2007) and shallow semantic parsing (Giuglea and Moschitti, 2006).
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010).  We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p > 5 in sign test).  Experiments were conducted to evaluate tree edit models for three tasks: recognizing textual entailment (Giampiccolo et al, 2007), paraphrase identification (Dolan et al, 2004), and an answer selection task (Wang et al, 2007) for question answering (Voorhees, 2004).  For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). The experimental setup is the same as in Wang et al (2007). We compare our tree edit model to three other systems as they are reported by Wang et al (2007). The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p > 0.05). This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks.  In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009).
 Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems are not required to translate the whole context but just the target word. (Carpuat and Wu, 2007) report an improvement in translation quality by incorporating a WSD system directly in a phrase-based translation system. The output of this model is incorporated into the machine translation system by providing the WSD probabilities for a phrase translation as extra features in a log-linear model (Carpuat and Wu, 2007). (Chan and Ng, 2007) introduce a system very similar to that of (Carpuat and Wu, 2007), but as applied to hierarchical phrase-based translation. Another WSD approach incorporating context-dependent phrasal translation lexicons is given in (Carpuatand Wu, 2007) and has been evaluated on several translation tasks. Second, instead of disambiguating phrase senses as in (Carpuat and Wu, 2007), we model word selection independently of the phrases used in the MT models. Carpuat and Wu (2007b) and Chan et al. (2007) showed improvents by integrating word-sense-disambiguation (WSD) system into a phrase-based (Koehn, 2004) and a hierarchical phrase-based (Chiang, 2005) SMT system, respectively.  Carpuat and Wu (2007) approached the issue as a Word Sense Disambiguation problem. In Chan and Ng (2007) a classifier exploits information such as local collocations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. Unlike a full blown machine translation task (Carpuat and Wu, 2007), annotators and systems will not be required to translate the whole context but just the target word. As described in (Carpuat and Wu, 2007), the disambiguation model plays an important role in the machine translation task. In the study more closely related to our work, Carpuat and Wu (2007) propose a novel method to train a phrasal lexical disambiguation model to benefit translation candidates selection in machine translation. Current work in machine translation has shown that word sense disambiguation can play an important role by using the surrounding words as context to disambiguate terms (Carpuat and Wu, 2007) (Apidianaki, 2009). Another WSD approach incorporating context-dependent phrasal translation lexicons is presented by Carpuat and Wu (2007) and has been evaluated on several translation tasks. In Carpuat and Wu (2007), another state-of-the-art WSD engine (a combination of naive Bayes, maximum entropy, boosting and Kernel PCA models) is used to dynamically determine the score of a phrase pair under consideration and, thus, let the phrase selection adapt to the context of the sentence. The key insight here is that Word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007) and Chan et al (2007), who successfully used sense information to boost state-of-the-art statistical MT. Carpuat and Wu (2007a, 2007b) and Chan et al (2007) have integrated word sense disambiguation (WSD) and phrase sense disambiguation (PSD) into SMT systems. Key papers by Carpuat and Wu (2007) and Chan et al (2007) showed that word-sense disambiguation (WSD) techniques relying on source-language context can be effective in selecting translations in phrase-based and hierarchical SMT.
[vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). These figures are the MCMC settings that provided the best results in Johnson (2007). There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work. As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007).    For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). (Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state.
There are many possible methods for evaluating clustering quality (Rosenberg and Hirschberg, 2007). We evaluate both the inferred phonetic categories and words using the clustering evaluation measure V-Measure (VM; Rosenberg and Hirschberg, 2007). F is not suitable for comparing results with different cluster numbers (Rosenberg and Hirschberg, 2007). In the first one, unsupervised evaluation, systems' answers were evaluated according to: (1) V Measure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al, 2009). This section presents the measures of unsupervised evaluation, i.e V-Measure (Rosenberg and Hirschberg, 2007) and (2) paired F-Score (Artiles et al, 2009). V-Measure (Rosenberg and Hirschberg, 2007) assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness. Homogeneity refers to the degree that each cluster consists of data points primarily belonging to a single GS class, while completeness refers to the degree that each GS class consists of data points primarily assigned to a single cluster (Rosenberg and Hirschberg, 2007). The V-measure (VM) (Rosenberg and Hirschberg, 2007) is an information theoretic metric that reports the harmonic mean of homogeneity (each cluster should contain only instances of a single class) and completeness (all instances of a class should be members of the same cluster). A significant limitation of F-Score is that it does not evaluate the make up of clusters beyond the majority class (Rosenberg and Hirschberg, 2007). These two limitations define the matching problem of F-Score (Rosenberg and Hirschberg, 2007) which can lead to: (1) identical scores between different clustering solutions, and (2) inaccurate assessment of the clustering quality. ubsequently, we present the use of V measure (Rosenberg and Hirschberg, 2007) as an evaluation measure that can overcome the current limitations of F-Score. As it can be observed, F-Score assesses the quality of a clustering solution by considering two different angles, i.e. homogeneity and completeness (Rosenberg and Hirschberg, 2007). The former situation is present, due to the fact that F-Score does not consider the make-up of the clusters beyond the majority class (Rosenberg and Hirschberg, 2007). V-measure assesses the quality of a clustering solution by explicitly measuring its homogeneity and its completeness (Rosenberg and Hirschberg, 2007). This happens when each Gs class is included in all clusters with a distribution equal to the distribution of sizes (Rosenberg and Hirschberg, 2007). This is due to the fact that V-measure considers as the worst solution in terms of completeness the one, in which each class is represented by every cluster, and specifically with a distribution equal to the distribution of cluster sizes (Rosenberg and Hirschberg, 2007). Two evaluation metrics are used during the unsupervised evaluation in order to estimate the quality of the clustering solutions, the V-Measure (Rosenberg and Hirschberg, 2007) and the paired F Score (Artiles et al, 2009). Recently, two measures have been proposed that avoid many of the weaknesses of previous measures and exhibit several attractive properties (see Sections 2 and 3): the VI measure (Meila, 2007) and the V measure (Rosenberg and Hirschberg, 2007). As noted in (Rosenberg and Hirschberg, 2007), these measures evaluate not only the quality of the proposed clustering but also of the mapping scheme. As noted by (Rosenberg and Hirschberg, 2007), the Q measure does not explicitly address the completeness of the suggested clustering.
We further note that our results are different from that of (Hughes and Ramage, 2007) as they use extensive feature engineering and weight tuning during the graph generation process that we have not been able to reproduce. Hughes and Ramage (2007) find that PPR+cos has high correlation with human similarity judgments on WordNet-based graphs.     Hughes and Ramage (2007) use random walks over WordNet, incorporating information such as meronymy and dictionary glosses.  Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets.  For example, (Hughes and Ramage, 2007) used random walks on Wordnet graph to measure lexical semantic relatedness between words. As for Personalized PageRank, it was used for word sense disambiguation (Agirre and Soroa, 2009), and for measuring lexical relatedness of words in a graph built from WordNet (Hughes and Ramage, 2007). As can be seen from the Table, our approach with the Weighted Overlap signature comparison improves over the similar approach of Hughes and Ramage (2007) which, however, does not involve the disambiguation step and considers a word as a whole unit as represented by the set of its senses.  Unlike some approach like (Hughes and Ramage, 2007), which performs well on some datasets but poorly on others, combing the VSMs from heterogeneous sources is more robust.  Graph-based methods have been successfully applied to evaluate word similarity using available ontologies, where the underlying graph included word senses and semantic relationships between them (Hughes and Ramage, 2007). For instance, Hughes and Ramage (2007) constructed a graph which represented various types of word relations from WordNet, and compared random-walk similarity to similarity assessments from human subject trials.
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided.  For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details.  The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below.  For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples.
We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al, 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large. For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al, 2007), and use probabilistic grammar models other than PCFGs.  While Liang et al (2007) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data. (General grammars with infinite numbers of nonterminals were studied by (Liang et al, 2007b)). In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split merge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al, 2007a). However, because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting (Liang et al, 2007).  We'd like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive state splitting found in the iHMM and iPCFG (Liang et al., 2007). It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al (2007). First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007). Future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of alpha; a look at other non-parametric extensions of PCFGs, such as infinite PCFGs (Liang et al2007) for finding a set of non-terminals permitting more fine-grained topics.
We perform term disambiguation on each document using an entity extractor (Cucerzan, 2007). More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy and by Milne and Witten (2008) who built on Cucerzan's work. Cucerzan (2007) employed context vectors consisting of phrases and categories extracted from Wikipedia. This entity disambiguation data set was introduced by Cucerzan (2007). we implemented the approach brought by Cucerzan (2007) based on our best understanding. e. To model these characteristics, Bunescu and Pasca (2006) and Cucerzan (2007) incorporate information from Wikipedia articles, Artiles et al (2007) use Webpage content, Mann and Yarowsky (2003) extract biographic facts.  Cucerzan (2007) disambiguated the names by combining the BOW model with the Wikipedia category information. Cucerzan (2007) and Bunescu (2006) used Wikipedia's category information in the disambiguation process.  Cucerzan (2007), by contrast to the above, used Wikipedia primarily for Named Entity Disambiguation, following the path of Bunescu and Pasca (2006). As in our paper, and unlike the above mentioned works, Cucerzan (2007) made use of the explicit Category information found within Wikipedia. Cucerzan (2007) did not make use of the Category information in identifying the class of a given entity.  Bunescu and Pasca (2006) and Cucerzan (2007) presented important pioneering work in this area, but suffer from several limitations including Wikipedia specific dependencies, scale, and the assumption of a KB entry for each entity. Previous work by Bunescu and Pasca (2006) and Cucerzan (2007) aims to link entity mentions to their corresponding topic pages in Wikipedia but the authors differ in their approaches. We evaluated our system on two datasets: the Text Analysis Conference (TAC) track on Knowl edge Base Population (TAC-KBP) (McNamee and Dang, 2009) and the newswire data used by Cucerzan (2007) (Microsoft News Data). We downloaded the evaluation data used in Cucerzan (2007): 20 news stories from MSNBC with 642 entity mentions manually linked to Wikipedia and another 113 mentions not having any corresponding link to Wikipedia. 
In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007).  To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration.  Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007).  Zhou et al (2007) tested their system on the ACE 2004 data. Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. (2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set.
Moreover, this rule set substantially decreased the total times of rule application about 60%, compared with a constituent-based approach (Wang et al, 2007). The most similar work to this paper is that of Wang et al (2007). We argue that even though the rules by Wang et al (2007) exist, it is almost impossible to automatically convert their rules into rules that are applicable to dependency parsers. The training data, which included those data used in Wang et al (2007), contained 1 million pairs of sentences extracted from the Linguistic Data Consortium's parallel news corpora. We implemented the constituent-based preordering rule set in Wang et al (2007) for comparison, which is called WR07 below. The overall accuracy of this rule set is 60.0%, which is almost at the same level as the WR07 rule set (62.1%), according to the similar evaluation (200 sentences and one annotator) conducted in Wang et al (2007).   There are two reasons why the syntactic reordering approach improves over the baseline phrase-based SMT system (Wang et al, 2007). Collins et al (2005) propose German clause restructuring to improve German-English SMT, while Wang et al (2007) present similar work for Chinese English SMT. Both achieve BLEU score improvements for SMT: 25.2% to 26.8% for (Collins et al, 2005) and 28.52 to 30.86 for (Wang et al, 2007). (Wang et al, 2007) uses rules very similar to our own as they use the same language pair, although they reorder the Chinese, whereas we reorder the English.  Long-distance phrase movement is a common problem in Chinese-English MT, and many MT systems try to handle it (e.g., Wang et al. 2007). There is similar work for Chinese-English (Wang et al, 2007) and quite a few other languages.   Much like Wang et al (2007), we parse the English side of our corpora and reorder using predefined rules. Chinese ordering differs from English mainly in clause ordering (Wang et al, 2007) and within the noun phrase. Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al, 2005) and (Wang et al, 2007), respectively.
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. Sparse features used in reranking are extracted according to (Watanabe et al, 2007). (Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007).
5-gram word language models in English are trained on a variety of monolingual corpora (Brants et al, 2007). In the case of language models, we often have to remove low-frequency words because of a lack of computational resources, since the feature space of k grams tends to be so large that we sometimes need cutoffs even in a distributed environment (Brantset al, 2007). To scale LMs to larger corpora with higher-order dependencies, researchers have considered alternative parameterizations such as class-based models (Brown et al, 1992), model reduction techniques such as entropy-based pruning (Stolcke, 1998), novel represent ion schemes such as suffix arrays (Emami et al, 2007), Golomb Coding (Church et al, 2007) and distributed language models that scale more readily (Brants et al, 2007).  Here we choose to work with stupid back off smoothing (Brants et al, 2007) since this is significantly more efficient to train and deploy in a distributed framework than a context dependent smoothing scheme such as Kneser-Ney. Previous work (Brants et al, 2007) has shown it to be appropriate to large-scale language modeling. Brants et al (2007) have shown that each doubling of the training data from the news domain (used to build the language model), leads to improvements of approximately 0.5 BLEU points. For example, Brants et al (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. We build sentence specific zero-cutoff stupid-back off (Brants et al., 2007) 5-gram language models, estimated using 4.7B words of English newswire text, and apply them to rescore each 10000-best list. However, adding more data in an unsupervised sense is unlikely to significantly improve results (Brants et al, 2007). We aim to allow the estimation of large scale distributed models, similar in size to the ones in Brantset al (2007). In relation to language models, Brants et al (2007) recently proposed a distributed MapReduce infrastructure to build Ngram language models having up to 300 billion n-grams. Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model. This was expected, as it has been observed before that very simple smoothing techniques can perform well on large data sets, such as web data (Brants et al, 2007). Since standard spelling correction dictionaries (e.g. ASpell) are not able to capture the rich language used in web queries, large-scale knowledge sources such as Wikipedia (Li et al 2011), query logs (Chen et al 2007), and large n-gram corpora (Brants et al., 2007) are employed. Since that time, however, increasingly large amounts of language model training data have become available ranging from approximately one billion words (the Gigaword corpora from the Linguistic Data Consortium) to trillions of words (Brants et al, 2007). We implemented an N -gram indexer/estimator using MPI inspired by the MapReduce implementation of N-gram language model indexing/estimation pipeline (Brants et al, 2007). In machine translation, improved language models have resulted in significant improvements in translation performance (Brants et al., 2007). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid back off (Brants et al 2007). Our distributed 4-gram language model was trained on 600 million words of Arabic text, also collected from many sources including the Web (Brants et al, 2007).
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding.  Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder.
   For example, of ten treebanks for CoNLL-2007 shared task, none includes more than 500K tokens, while the sum of tokens from all tree banks is about two million (Nivre et al, 2007). The CoNLL-2007 shared tasks include two tracks: the Multilingual Track and Domain AdaptationTrack (Nivre et al, 2007). Our second-order parser still does not reproduce the state-of-the art results presented by similar systems (Nivre et al, 2007). Some dependency parsing systems prefer two-stage architecture: unlabeled parsing and dependency classification (Nivre et al, 2007). Data-driven dependency parsing has recently received extensive attention in the parsing community and impressive results have been obtained for a range of languages (Nivre et al, 2007). The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. The organizers of the CoNLL 2007 shared task noted that languages with free word order and high morphological complexity are the most difficult for dependency parsing (Nivre et al, 2007). Morphologically rich languages present new challenges, as the use of state of the art parsers for more configurational and non-inflected languages like English does not reach similar performance levels in languages like Basque, Greek or Turkish (Nivre et al, 2007a).   For the experiments, we used the best configuration for English at the CoNLL 2007 Shared Task on Dependency Parsing (Nivre et al, 2007) as our baseline. We'll use a simple example sentence to illustrate how our feature sets are extracted from CONLL formatted data (Nivre et al, 2007). First, we investigate the impact of using different flavours of Covington's algorithm (Covington, 2001) for non projective dependency parsing on the ten different languages provided for CoNLL-X Shared Task (Nivre et al, 2007). In this paper we showed the performance of three flavours of Covington's algorithm for non-projective dependency parsing on the ten languages provided for the CoNLL-X Shared Task (Nivre et al, 2007). We evaluated our system using the standard evaluation script provided by the organizers (Nivre et al, 2007). The DS representation is taken from the conversion procedure used in the CoNLL 2007 Shared Task on dependency parsing (Nivre et al, 2007). Dependency trees are representations of the syntactic structure of a sentence (Nivre et al 2007).
  One system (Hall et al, 2007b) extends this two-stage approach to a three-stage architecture where the parser and labeler generate an n-best list of parses which in turn is reranked.  This model is used by Marinov (2007) and in component parsers of the Nilsson ensemble system (Hall et al, 2007a). The most extreme case is the top performing Nilsson system (Hall et al, 2007a), which reached rank 1 for five languages and rank 2 for two more languages. However, Hall et al (2007a) point out that the official results for Chinese contained a bug, and the true performance of their system was actually much higher.  The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al, 2007), combining six transition-based parsers. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Halletal., 2007). Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. We implement a left-to-right arc-eager parsing model in a way that the parser scan through an input sequence from left to right and the right dependents are attached to their heads as soon as possible (Hall et al, 2007). Only one model was used for syntactic parsing in our system, in contrast to the existing work using an ensemble technique for further performance enhancement, e.g., (Hall et al, 2007).  This model is simple and works very well in the shared-tasks of CoNLL2006 (Nivre et al, 2006) and CoNLL2007 (Hall et al, 2007). In this work, we adopt a left-to-right arc-eager parsing model, that means that the parser scans the input sequence from left to right and right dependents are attached to their heads as soon as possible (Hall et al, 2007).  For the final system, feature models and training parameters were adapted from Hall et al (2007). The single parses were blended following the procedure of Hall et al (2007). system for English described in Hall et al (2007) was used as a baseline, and then optimized for this new task, focusing on feature selection.
Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. Carreras (2007) employs his own extension of Eisner's algorithm for the case of projective trees and second-order models that include head grandparent relations.    To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. For more details of the second-order parsing algorithm, see (Carreras, 2007). We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. The second order algorithm of Carreras (2007) uses in addition to McDonaldand Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. The dependency parser is based on Carreras's algorithm (Carreras, 2007) and second order spanning trees.  Carreras (2007) introduced the left-most and right-most grandchild as factors. We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). We start with the model introduced by Carreras (2007). Second order factors of Carreras (2007). 
We pruned the generated phrase tables following the method introduced in (Johnson et al., 2007). See (Johnson et al, 2007) for details. Significance-based filtering (Johnson et al 2007) was applied to the resulting phrase table. Johnson et al (2007) reduced the phrase table based on the significance testing of phrase pair co-occurrence in bilingual corpus. Various filtering techniques, such as (Johnson et al., 2007) and (Chen et al, 2008), have been applied to eliminate a large portion of the translation rules that were judged unlikely to be of value for the current translation. We filtered the obtained phrase table using the method described in (Johnson et al, 2007). Significance filtering of the phrase tables (Johnson et al,2007) implemented for Moses by Chris Dyer.   Johnson et al, (2007) presented a technique for pruning the phrase table in a PBMT system using Fisher's exact test. Yang and Zheng (2008) extended the work in Johnson et al, (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG). We consider all possible phrase-pairs in the training data, then use Fisher's Exact Test to filter out pairs with low correlation (Johnson et al, 2007). To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al (2007a). (Johnson et al, 2007) has presented a technique for pruning the phrase table in a phrase based SMT system using Fisher's exact test. In this paper, we extend the work in (Johnson et al, 2007) to a hierarchical phrase-based translation model, which is built on synchronous context free grammars (SCFG). Therefore, several approaches were proposed to filter these phrase-tables, reducing considerably their size without any loss of the quality, or even achieving improved performance (Johnson et al, 2007). First, statistically unreliable translation pairs (Johnson et al2007) are filtered out. The resulting translation pairs were then filtered with the significance pruning technique of (Johnson et al2007), using a+e as threshold. Johnson et al (2007) has shown that large portions of the phrase table can be removed without loss in translation quality. The work of Johnson et al (2007) is promising as it shows that large parts of the phrase table can be removed without affecting translation quality.
Due to space constraints, details and proof of correctness are available in Lopez (2007a). However, in machine translation most features can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b). We built grammars using its implementation of the suffix array extraction method described in Lopez (2007). We use the GIZA toolkit (Och and Ney, 2000), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och et al, 2003) to obtain word alignments, a translation model, language models, and the optimal weights for combining these mod els, respectively. Lopez (2007) extracts rules on-the-fly from the training bi text during decoding, searching efficiently for rule patterns using suffix arrays. Joshua (Li et al, 2009) is an implementation of Hiero (Chiang, 2007) using a suffix-array-based grammar extraction approach (Lopez, 2007). The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003). In this system, we use the GIZA++toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively. We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). Besides storing the whole grammar locally in memory, other approaches have been developed, such as suffix arrays, which lookup and extract rules on the fly from the phrase table (Lopez, 2007). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och,2003). This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007). (Lopez, 2007) proposed an extension of this method for retrieving discontinuous substrings, making it suitable for systems such as (Chiang, 2007).  The basis of the method in (Lopez, 2007) is to look for the occurrences of continuous substrings using a Suffix Array, and then intersect them to find the occurrences of discontinuous substrings. There is also an exponential number of discontinuous substrings, but (Lopez, 2007) only consider substrings of bounded size, limiting this problem. This hypergraph will not only fit the same role as the Prefix Tree of (Lopez, 2007), but also will allow us to easily implement different search strategies for flexible search (section 6). This allows in turn to compute by intersection the occurrences of discontinuous treelets, much like what is done in (Lopez, 2007) for discontinuous strings. In practice, the intersection operation will be implemented using merge and binary merge algorithms (Baeza-Yates and Salinger, 2005), following (Lopez, 2007).
Work such as (Cai et al, 2007) or (Boyd-Graber et al., 2007) use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambiguation. Abney and Light (1999) used tree structured multinomials to model selectional restrictions, which was later put into a Bayesian context for topic modeling (Boyd-Graber et al, 2007). Boyd-Graber et al (2007) integrate a model of random walks on the WordNet graph into an LDA topic model to build an unsupervised word sense disambiguation system. Boyd-Graber et al (2007) describe a related topic model, LDAWN, for word sense disambiguation that adds an intermediate layer of latent variables Z on which the Markov model parameters are conditioned. Gibbs sampling updates for LDAWN are given in Boyd-Graber et al (2007). Moreover, SemCor is also the dataset used in (Boyd-Graber et al, 2007), where a WordNet based topic model for WSD is introduced. STM10 achieves similar results as in LDAWN (Boyd-Graber et al, 2007) which was specifically designed for WSD. Boyd-Graber et al (2007) are the first to integrate semantics into the topic model framework. Previous approaches using topic models for sense disambiguation either embed topic features in a supervised model (Cai et al, 2007) or rely heavily on the structure of hierarchical lexicons such as WordNet (Boyd-Graber et al, 2007). In another unsupervised system, Boyd-Graber et al (2007) enhance the basic LDA algorithm by incorporating WordNet senses as an additional latent variable. This dataset, which consists of 320,000 articles, is significantly larger than SemCor, which is the dataset used by Boyd-Graber et al (2007). Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graberetal., 2007) and to encode clustering constraints (Andrzejewski et al, 2009). We took these frequencies and propagated them through the multilingual hierarchy, following LDAWN's (Boyd-Graber et al., 2007) formulation of information content (Resnik, 1995) as a Bayesian prior. LDAWN (Boyd-Graber et al 2007) models sets of words for the word sense disambiguation task.
This technique, first proposed by Sagae and Lavie (2006), was used in the highest scoring system in both the multilingual track (Hall et al, 2007a) and the domain adaptation track (Sagae and Tsujii, 2007). The best performing (closed class) system in the domain adaptation track used a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis (Sagae and Tsujii, 2007). Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Lastly we used a native dependency parser, the GENIA Dependency parser (GDep) by Sagae and Tsujii (2007). As far as pre-processing is concerned, we imported the sentence splitting, tokenization and GDep parsing results (Sagae and Tsujii, 2007) as prepared by the shared task organizers for all data sets (training, development and test). This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al, 2007). The parse trees were produced by the GDep parser (Sagae and Tsujii, 2007) and supplied by the challenge organisers. While the TURKU system exploits the Stanford dependencies from the McClosky-Charniak parser (Charniak and Johnson, 2005), and the JULIELab system uses the CoNLL-like dependencies from the GDep parser (Sagae and Tsujii, 2007), the TOKYO system overlays the Shared Task data with two parsing representations, viz. Enju PAS structure (Miyao and Tsujii, 2002) and GDep parser dependencies. GDep (Sagae and Tsujii, 2007), a native dependency parser.  Sagae and Tsujii (2007) used the co-training technique to improve performance. Sagae and Tsujii (2007) presented an co training approach for dependency parsing adaptation. Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shift reduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique. Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy. For sentences in the dataset, their dependency structures are extracted using GENIA Dependency parser (Sagae and Tsujii, 2007), and phrase structure using Brown self-trained biomedical parser (McClosky, 2009).  The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble-based methods. For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDepbeta2 (Sagae and Tsujii, 2007) parsers. Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. See Sagae and Tsujii (2007) for more information on the parser.
We also analyze the labeled corpus for opinion expressions and observe that many opinion expressions are used in multiple domains, which is identical with the conclusion presented by Kobayashiet al (2007).  Kobayashi et al (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation.   In (Kobayashi et al 2007), a pattern mining method was used. Kobayashi et al (2007) adopted a supervised learning technique to search for useful syntactic patterns as contextual clues. Kobayashi et al (2007) presented their work on extracting opinion units including: opinion holder, subject, aspect and evaluation.
Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). The various parameters in the IHMM model are set as the optimal values found in He et al (2008). The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008).  To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al 2008) for every pair of input hypotheses. The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. We compute the association score from a linear combination of two clues: surface similarity computed as Equation (2) and position difference based distortion score by following (He et al, 2008). IHMM-based: He et al (2008) propose an indirect hidden Markov model (IHMM) for hypothesis alignment. We compute the distortion model by following (He et al, 2008) for IHMM and CLA-based methods. We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first.  On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008).  Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al, 2008). He et al (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs.
Banea et al (2008) demonstrate that machine translation can perform quite well when extending the subjectivity analysis to multilingual environment, which makes it inspiring to replicate their work on lexicon-based sentiment analysis. Banea et al (2008) use machine translation for multilingual sentiment analysis. Another approach in obtaining subjectivity lexicons for other languages than English was explored in Banea et al (2008b).  Similar to the approach proposed in (Banea et al, 2008), Wan's method also uses machine translation to produced a labeled Chinese review corpus from the available labeled English review data. A primary question is whether such lexicons improve performance over a translate-to-English strategy (Banea et al, 2008). Banea et al (2008) use machine translation for multilingual sentiment analysis. Mihalcea et al, (2007) and Banea et al, (2008) used machine translation technique to leverage English resources for analysis in Romanian and Spanish languages. Another approach in obtaining subjectivity lexicons for other languages than English was explored by Banea et al (Banea et al,2008b).  Similarly, (Banea et al, 2008) propose a method based on machine translation to generate parallel texts, followed by a cross-lingual projection of subjectivity labels, which are used to train subjectivity annotation tools for Romanian and Spanish.  Figure 1: Examples of sentiments in multilingual text Banea et al (2008) have attributed the variations in the difficulty level of subjectivity learning to the differences in language construction. translating a source language training corpus into target language and creating a corpus based system in target language (Banea et al, 2008). Banea et al (2008) proposed several approaches for cross lingual subjectivity analysis by directly applying the translations of opinion corpus in source language to train the opinion classifier on target language. Various experiments of the first strategy are performed in (Banea et al,2008) for the subjective analysis task and an average 65 F1 score was reported. As stressed in some work (Banea et al., 2008), researchers have shown that in sentiment analysis, an approach in two steps is often beneficial, in which we first distinguish objective from subjective texts, and then classify subjective texts depending on their polarity (Kim and Hovy, 2006).  The above two methods have been used in (Banea et al, 2008) for Romanian subjectivity analysis, but the experimental results are not very promising. Although subjectivity tends to be preserved across languages - see the manual study in (Mihalcea et al, 2007), (Banea et al, 2008) hypothesize that subjectivity is expressed differently in various languages due to lexicalization, formal versus informal markers, etc.
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009). The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity.   However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008).
In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003).  When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008).  Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519).
For example, (Bannard and Callison-Burch, 2005) and (Callison-Burch, 2008) described a method to extract paraphrases from largely available bilingual corpora. The work of Callison-Burch (2008) has shown how the monolingual context of a sentence to paraphrase can be used to improve the quality of the acquired paraphrases. Human evaluators were then asked to score each pair of an original sentence and a paraphrased sentence with the following two 5-point scale grades proposed by Callison-Burch (2008): Grammaticality: whether the paraphrased sentence is grammatical (1: horrible, 5: perfect) Meaning: whether the meaning of the original sentence is properly retained by the paraphrased sentence (1: totally different, 5: equivalent). Table 1 shows the average of the original 5-point scale scores and the percentage of examples that are judged correct based on a binary judgment (Callison-Burch, 2008): an example is considered to be correct iff the grammaticality score is 4 or above and/or the meaning score is 3 or above. Note that Callison-Burch (2008) might possibly underestimate the chance agreement and overestimate the ? values, because the distribution of human scores would not be uniform. In a more recent publication Callison-Burch (2008) improved this method by using syntactic constraints and multiple languages in parallel.  A well-known problem of phrase-based methods to paraphrase or term variation acquisition is the fact that a large proportion of the term variations or paraphrases proposed by the system are superor sub-strings of the original term (Callison-Burch, 2008). In some sense this is a sort of syntactic constraint introduced in Callison-Burch (2008).  Callison-Burch (2008) attempts to improve the ranking by limiting paraphrases to be the same syntactic type. This refinement to BiP, proposed in Callison-Burch (2008), constrains paraphrases to be the same syntactic type as the original phrase in the pivoting step of the paraphrase table construction. More recently, Callison-Burch (2008) has improved performance of this pivoting technique by imposing syntactic constraints on the paraphrases. Subsequently, Prasad et al (2010b) used Callison-Burch's technique for identifying syntax-constrained paraphrases (Callison-Burch, 2008) to identify additional discourse connectives, some of which don't appear in the PDTB corpus and some of which appear in the corpus but were not identified and annotated as discourse connectives. While syntactical constraints have been proven to helpful in identifying good paraphrases (Callison-Burch, 2008), it is insufficient in our task because it cannot properly filter the candidates for the replacement. Following Callison-Burch (2008), we refine selection by requiring both the original phrase and paraphrase to be of the same syntactic type, which leads to more grammatical paraphrases. Furthermore, we compared our fragment pair collection with Callison-Burch (2008)'s approach on the same MSR corpus, only about 21% of the extracted paraphrases appear on both sides, which shows the potential to combine different resources. As for comparison, we choose two other paraphrase collections, one is acquired from parallel bilingual corpora (Callison-Burch, 2008) and the other is using the same fragment extraction algorithm on the MSR corpus. Our proposed method is based on the automatically acquired paraphrase dictionary described in Callison-Burch (2008), in which the application of paraphrases from the dictionary encodes secret bits. The paraphrase dictionary that we use was generated for us by Chris Callison-Burch, using the technique described in Callison-Burch (2008), which exploits a parallel corpus and methods developed for statistical machine translation.
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3.  This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008).  Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). Mi and Huang (2008) extend the tree-based rule extraction. Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest.
To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008). The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008). We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008).  The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008). The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008). Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). 
Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort. Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008).  Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion.  This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%.
 To achieve roughly state-of-theart performance, RECONCILEACL09 employs a fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient - it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident.   Inaddition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art.  Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. Illinois-Coref follows the architecture used in Bengtson and Roth (2008). For the ACE 2004 coreference task, a good performance in mention detection is typically achieved by training a classifier e.g., (Bengtson and Roth, 2008). We use the same features as Bengtson and Roth (2008), with the knowledge extracted from the OntoNotes-4.0 annotation. Although its strategy is simple, Bengtson and Roth (2008) show that with a careful design, it can achieve highly competitive performance. For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance (Bengtson and Roth, 2008). For an empirical evaluation of the contribution of a subset of these features to the mention-pair model, see Bengtson and Roth (2008).  We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. Note that we solve the above Best-Link inference using an efficient algorithm (Bengtson and Roth, 2008) which runs in time quadratic in the number of mentions. The baseline system applies the strategy in (Bengtson and Roth, 2008, Section 2.2) to learn the pairwise scoring functions using the Averaged Perceptron algorithm. Among them the mention pair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the state of-the-art performance (Bengtson and Roth, 2008). Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008).  Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011).
Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a > 0.99, so we skip this step. [cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score.  We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type.  We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). We experimented with the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al, 2009) (three models). For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy.   Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16).  We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5.  To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011).   Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser.  The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010).
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity.  Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. up to 1081 as per Tromble et al (2008). Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al (2008) replace the clipped n-gram count with the product of an n gram count and an n-gram indicator function. Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic.  We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond).  On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). 
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). Macherey et al (2008) use methods from computational geometry to compute the upper envelope. Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). Weights on feature functions are found by lattice MERT (Macherey et al, 2008). In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008).
 Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. The hybrid tree model (Lu et al, 2008) takes a transformative perspective that is in some ways more similar to our model. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. A novel grammar induction algorithm: To automatically induce such synchronous grammar rules, we propose a novel generative model that establishes phrasal correspondences between logical sub-expressions and natural language word sequences, by extending a previous model proposed for parsing natural language into meaning representations (Lu et al, 2008). Of particular interest is our prior work Lu et al (2008), in which we presented a joint generative process that produces a hybrid tree structure containing words, syntactic structures, and meaning representations, where the meaning representations are in a variable-free tree-structured form. Hybrid Tree in Lu et al (2008), a generative model was presented to model the process that jointly generates both natural language sentences and their underlying meaning representations of a variable-free tree structured form. In this section, we present a novel hybrid tree model that provides the following extensions over the model of Lu et al (2008). Since we allow a packed meaning forest representation rather than a fixed tree structure, the MR model parameters in this work should be estimated with the inside-outside algorithm as well, rather than being estimated directly from the training data by simple counting, as was done in Lu et al (2008). Motivated by the limitations of these previous methods, we propose a new generative alignment model that includes a full semantic parsing model proposed by Lu et al (2008). Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics. Our model is built on top of the generative semantic parsing model developed by Lu et al (2008). Lu et al (2008) introduced a generative semantic parsing model using a hybrid-tree framework. We use Lu et al (2008)'s generative model for this step, in which: P (w|e)=?? T over (w, m) P (T ,w|m) (2) where m is the MR logical form defined by event e and T is a hybrid tree defined over the NL? MR pair (w, m). Lu et al (2008) propose 3 models for generative semantic parsing :unigram, bigram, and mix gram (interpolation between the two). Our model is built on top of Lu et al (2008)'s generative semantic parsing model, which is also trained in several steps in its best-performing version. The bigram model of Lu et al (2008), which is the one used in this paper, must be trained using parameters previously learned for the IBM Model 1 and unigram model in order to exhibit the best performance. In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov + bag-of-words model for language generation.  The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008).
 Content-word negators are words that are not function words, but act semantically as negators (Choi and Cardie, 2008).  For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). According to Choi and Cardie (2008), voting algorithms that recognize content-word negators achieve a competitive performance, so we will use a variant of it for simplicity. Because none of the algorithms proposed by Choi and Cardie (2008) is designed to handle the neutral polarity, we invent our own version as shown in Figure 2. Choi and Cardie (2008) also focus on the expression-level polarity classification, but their evaluation setting is not as practical as ours in that they assume the inputs are guaranteed to be either strongly positive or negative. Choi and Cardie (2008) proposed a learning-based framework. Choi and Cardie (2008) present a more lightweight approach using compositional semantics towards classifying the polarity of expressions. The rules presented by Choi and Cardie (2008) are, however, much more specific, as they define syntactic contexts of the polar expressions. Unlike Choi and Cardie (2008), these rules require a proper parse and reflect grammatical relationships between different constituents. In this work we focus on explicit negation mentions, also called functional negation by Choi and Cardie (2008). Choi and Cardie (2008) combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysis. Here, the verbs prevent and ease act as content-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. Choi and Cardie (2008) hand-code compositional rules in order to model compositional effects of combining different words in the phrase. We extract all sentences containing strong (i.e. intensity is medium or higher), sentiment-bearing (i.e. polarity is positive or negative) expressions following Choi and Cardie (2008). An English polarity reversing word dictionary was constructed from the General Inquirer dictionary in the same way as Choi and Cardie (2008), by collecting words which belong to either NOTLW or DECREAS categories (The dictionary contains 121 polarity reversing words). Choi and Cardie (2008) categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate. Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics.
All differences between DE-Annotated and BASELINE are significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005) conduct additional experiments with a hierarchical phrase reordering model introduced by Galley and Manning (2008). We use a phrase-based system similar to Moses (Koehn et al, 2007) based on a set of common features including maximum likelihood estimates p ML (e|f) and p ML (f |e), lexically weighted estimates p LW (e|f) and p LW (f |e), word and phrase-penalties, a hierarchical reordering model (Galley and Manning, 2008), a linear distortion feature, and a modified KneserNey language model trained on the target-side of the parallel data. In (Galley and Manning, 2008) the authors present an extension of the famous MSD model (Tillman, 2004) able to handle long distance word-block permutations. From the word-to-word alignments, the system extracts a phrase table (Koehn et al, 2003) and hierarchical reordering model (Galley and Manning, 2008). Instead of just looking at the reordering relationship between individual phrases, the new feature examines the reordering of blocks of adjacent phrases (Galley and Manning, 2008) and improves translation quality when the material being reordered cannot be captured by single phrase. Galley and Manning (2008) present a hierarchical phrase reordering model aimed at improving non-local reorderings. Our MOS concept is also closely related to hierarchical reordering model (Galley and Manning, 2008) in phrase-based decoding, which computes o of b with respect to a multi-block unit that may go beyond b?. Given no constraint on maximum phrase length, the hierarchical phrase reordering model (Galley and Manning, 2008) also analyzes the adjacent bilingual phrases for bp and identifies its orientation as S. We plan to apply our method to the complex lexicalized reordering models, for example, the hierarchical reordering model (Galley and Manning, 2008) and the MEBTG reordering model (Xiong et al, 2006). In (Galley and Manning, 2008) a hierarchical orientation model is introduced that captures some non-local phrase reordering by a shift reduce algorithm. This method reduces the complexity to O (nbdmax) but fails to capture long distance reorderings (Galley and Manning, 2008). For example Galley and Manning (2008) propose a shift-reduce style method to allow hieararchical non-local reorderings in a phrase-based decoder. In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Fengetal., 2010). Thus far, they have been used to enable a hierarchical re-ordering model, or HRM (Galley and Manning, 2008), as well as an ITG constraint (Feng et al, 2010). The HRM (Galley and Manning, 2008) maintains similar re-ordering statistics, but determines orientation differently. Galley and Manning (2008) introduce a deterministic shift-reduce parser into decoding, so that the decoder always has access to the largest possible previous block, given the current translation history. Galley and Manning (2008) propose an algorithm that begins by running standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found. In all experiments we use phrase-orientation lexicalized reordering (Galley and Manning, 2008) which models monotone, swap, discontinuous orientations from both reordering with previous phrase pair and with the next phrase pair. Galley and Manning (2008) introduce three orientation models for lexicalized reordering: word-based, phrase-based and hierarchical orientation model.
In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split: 1-270 for training, 301-325 for development, and 271-300 for testing.  Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks.  However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation.  For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. Procedurally, our work is most closely related to that of Burkett and Klein (2008). We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008).
Take the following example from Poon and Domingos (2009), in which the same semantic relation can be expressed by a transitive verb or an attributive prepositional phrase: (1) Utah borders Idaho. To do so, we semi-automatically restrict the question-answer pairs by using the output of an unsupervised clustering semantic parser (Poon and Domingos, 2009). We used the question-answer pairs extracted by the Poon and Domingos (2009) semantic parser from the GENIA biomedical corpus that have been manually checked to be correct (295 pairs). More recent examples of similar techniques include the Resolver system (Yates and Etzioni, 2009) and Poon and Domingos's USP system (Poon and Domingos, 2009). On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. Our non-parametric model automatically discovers granularity of clustering appropriate for the dataset, unlike the parametric method of (Poon and Domingos, 2009) which have to perform model selection and use heuristics to penalize more complex models of semantics. In our case, the state space size equals the total number of distinct semantic clusters, and, thus, is expected to be exceedingly large even for moderate datasets: for example, the MLN model induces 18,543 distinct clusters from 18,471 sentences of the GENIA corpus (Poon and Domingos, 2009). In both cases, we follow (Poon and Domingos, 2009) in using the corpus of biomedical abstracts. Second, lambda calculus is a considerably more powerful formalism than the predicate-argument structure used in frame semantics, normally supporting quantification and logical connectors (for example, negation and disjunction), neither of which is modeled by our model or in (Poon and Domingos, 2009). Semantic classes correspond to lambda-form clusters in (Poon and Domingos, 2009) terminology. The work of (Poon and Domingos, 2009) models joint probability of the dependency tree and its latent semantic representation using Markov Logic Networks (MLNs) (Richardson and Domingos, 2006), selecting parameters (weights of first-order clauses) to maximize the probability of the observed dependency structures. In order to overcome this problem, (Poon and Domingos, 2009) group parameters and impose local normalization constraints within each group. We now turn to the QA task and compare our model (USP-BAYES) with the results of baselines considered in (Poon and Domingos, 2009). Other approaches are completely unsupervised, but do not tie the language to an existing meaning representation (Poon and Domingos, 2009). USP (Poon and Domingos, 2009) is based on Markov Logic Networks and attempts to create a full semantic parse in an unsupervised fashion. Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). Dependency information is useful for a wealth of natural language processing tasks, including question answering (Wang et al, 2007), semantic parsing (Poon and Domingos, 2009), and machine translation (Galley and Manning, 2009). Such annotated resources are scarce and expensive to create, motivating the need for unsupervised or semi-supervised techniques (Poon and Domingos, 2009).
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). A generic first-order expectation semiring is also provided (Li and Eisner, 2009). Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). For inside-outside algorithm, see (Li and Eisner, 2009). The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). For the detailed description, see Li and Eisner (2009) and its references. We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way.
However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009).   Labeled LDA (LLDA) (Ramage et al 2009a) can be used to solve this problem. Modeling Tweets in a Latent Space: Ramage et al (2010) also use hash tags to improve the latent representation of tweets in a LDA framework, Labeled-LDA (Ramage et al, 2009), treating each hashtag as a label. Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA (LLDA) (Ramage et al, 2009) and supervised LDA (sLDA) (Blei and McAuliffe, 2008) are powerful generative models that capture the underlying semantics of texts.  To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al, 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity's context across its mentions. Distant Supervision with Topic Models: To model unlabeled entities and their possible types, we apply LabeledLDA (Ramage et al, 2009), constraining each entity's distribution over topics based on its set of possible types according to Freebase. In other models, this input is sometimes used to "fix," i.e. deterministically hold constant topic assignments (Ramage et al, 2009). To enable this, we first take class labeled data (doesn't need to be multi-class labeled data unlike (Ramage et al 2009)) and identify the discriminating features for each class. Of these models, the most related one to SeededLDA is the Labeled LDA model (Ramage et al 2009). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al, 2009a) or FixedhLDA (Reisinger and Pas? ca, 2009) where the set of topics associated with a document is known a priori.  There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al, 2009), and sequential (Du et al, 2010) topic models. There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al, 2009), partially-labeled (Ramage et al, 2011), constrained (Andrzejewski et al, 2009) models, etc. We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics: Controled LDA (C-LDA) and Labeled LDA (L-LDA; Ramage et al (2009)). Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien et al, 2008).  L-LDA (Ramage et al, 2009) extends standard LDA to include supervision for specific target categories, yet in a different way: (i) The generative process includes a second observed variable, i.e. each document is explicitly labeled with a target category.
It is possible that the length of stay of an annotator in the pool is not independent of her diligence; for example, Callison-Burch (2009) found in his AMT experiments with tasks related to machine translation that lazy annotators tended to stay longer and do more annotations. We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon's Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. For example, Callison-Burch (2009) used MTurk to evaluate machine translations. (Callison-Burch, 2009) uses MTurk workers for manual evaluation of automatic translation quality and experiments with weighed voting to combine multiple annotations. The use of crowd sourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and Callison-Burch (2009). Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators. It has also been used in MT evaluation (Callison-Burch, 2009), though that evaluation used reference translations. As an example, among the collected material several translations in languages other than English revealed a massive and defective use of on-line translation tools by untrusted workers, as also observed by (Callison-Burch, 2009). We do not select German, French and other language pairs as they have already been explored by Callison-Burch (2009). Furthermore, previous research shows the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of natural language automation tasks (Snow, Jurafsy, & O'Connor, 2008). The value of this upper bound is quite consistent with the bound computed similarly by Callison-Burch (2009).   Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests.  Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk. Over the last several of years, Mechanical Turk, introduced by Amazon as "artificial artificial intelligence", has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). There have been several research papers on using MTurk to help natural language processing tasks, Callison-Burch (2009) used MTurk to evaluate machine translation results. On another way, an application can combine active learning (Arora et al, 2009) and crowd sourcing, asking non-expertise such as workers of Amazon Mechanical Turk to label crucial alignment links that can improve the system with low cost, which is now a promising methodology in NLP areas (Callison-Burch, 2009).
Finally, Suzuki et al (2009) present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.  Both Suzuki et al (2009) and Chen et al (2013) adopt the higher order parsing model of Carreras (2007), and Suzuki et al (2009) also incorporate word cluster features proposed by Koo et al (2008) in their system. Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al (2008) and Suzuki et al (2009), who utilized unsupervised word clusters created using the Brown et al (1992) hierarchical clustering algorithm. Although large amounts of unlabeled data are known to improve semi-supervised parsing (Suzuki et al, 2009), the best unsupervised systems use less data than is available for supervised training, relying on complex models instead: Headden et al's (2009) Extended Valence Grammar (EVG) combats data sparsity with smoothing alone, training on the same small subset of the tree-bank as the classic implementation of the DMV; Cohen and Smith (2009) use more complicated algorithms (variational EM and MBRdecoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). We prepared a total of 3.72 billion token text data as unsupervised data following the instructions given in (Suzuki et al, 2009).     Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al (2009) achieve state-of-the-art accuracy.  Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text).  Suzuki 2009 (Suzuki et al, 2009) reported the best reported result by combining a Semi supervised Structured Conditional Model (Suzuki and Isozaki, 2008) with the method of (Koo et al, 2008). Suzuki et al (2009) presented a semi supervised learning approach.  Suzuki et al (2009) also experiment with the same method combined with semi-supervised learning. 
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results.  QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010).  Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language.  Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. 
This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs. Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). (Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations. Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction. For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009). Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details. Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly. Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages. Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009). We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009). The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009). Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference. For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009). Documents are defined as speeches by a single speaker, as in (Mimno et al, 2009). In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours. Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009). Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles). Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora. A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic. To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics.
Full algorithmic details are presented in (Pantel et al, 2009).  KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). This combination was also used in (Pantel et al, 2009).
  For comparison purposes, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman and Ng (2009). The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. We follow the procedure described in (Rahman and Ng, 2009). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010).  Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3All nor B3None). The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). To extract NPs from the ACE-annotated documents, we train a mention extractor on the training texts (see Section 5.1 of Rahman and Ng (2009) for details), which recalls 83.6% of the NPs in the test set. Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. Details of the CR model can be found in Rahman and Ng (2009). Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009). The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster.
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting: each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions).  All systems except Haghighi and Klein (2009) and current work are fully supervised. We also compared to the strong deterministic system of Haghighi and Klein (2009). Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. We compared our output to the deterministic system of Haghighi and Klein (2009). The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously.  This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated.  The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009).  We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009).
Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009).     Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1.  Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser.  Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010).  For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). The semantics of the system is described in (Huang et al, 2009). Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features.  Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009).
A previous work that used structured kernels in Sentiment Analysis is the approach of Wu et al (2009). The results showed by Wu et al (2009) suggest that tree kernels on dependency trees are a good approach but we also plan to employ string kernels on this task. We compared our aspect identification approach against two baselines: a) the method proposed by Hu and Liu (2004), which was based on the association rule mining, and b) the method proposed by Wu et al (2009), which was based on a dependency parser. Afterwards, Wu et al (2009) utilized the dependency parser to extract the noun phrases and verb phrases from the reviews as the aspect candidates. Phrase dependency grammars have recently been used by Wu et al (2009) for feature extraction for opinion mining. For a monolingual task, Wu et al (2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies. We compared our approach against two state-of-the art methods: a) the method proposed by Hu and Liu (2004), which is based on the association rule mining, and b) the method proposed by Wu et al (2009), which is based on the dependency parser. For example, Wu et al (2009) identified aspects based on the features explored by dependency parser. A wide spectrum of tasks have been studied under review mining, ranging from coarse-grained document-level polarity classification (Pang et al,2002) to fine-grained extraction of opinion expressions and their targets (Wu et al, 2009).  For MaxEnt training, we tried three labeled data sets: one that was taken from the restaurant data set and manually annotated by us, and two from the annotated data set used in (Wu et al., 2009). To test this hypothesis, we tried two quite different training data sets, one from the cell phone domain and the other from the DVD player domain, both used in (Wu et al, 2009). Wu et al (2009) proposed a phrase level dependency parsing for mining aspects and features of products.  In supervised approaches, various kinds of models were applied, such as HMM (Jin and Ho, 2009), SVM (Wu et al, 2009) and CRFs (Li et al, 2010). They regarded it as a sequence labeling task, where several classical models were used, such as CRFs (Li et al, 2010) and SVM (Wu et al, 2009).
Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. This method is called dual decomposition (DD) (Rush et al, 2010). In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. It is a slight variation of the proof given by Rush et al (2010). Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). To find the minimum value, we can use a subgradient method (Rush et al 2010). The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. We follows the formulation by Rush et al (2010). An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. Alternatively, one can employ dual decomposition (Rush et al, 2010). AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). However, if it does not converge or we stop early, an approximation must be returned: following Rush et al (2010) we used the highest scoring output of the parsing submodel over all iterations.  However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010).
Another popular task in SMT is domain adaptation (Foster et al, 2010). In addition, discriminative weighting methods were proposed to assign appropriate weights to the sentences from training corpus (Matsoukas et al, 2009) or the phrase pairs of phrase table (Foster et al, 2010). Domain knowledge also has the potential to improve open-text applications such as summarization (Ceylan et al 2010) and machine translation (Foster et al., 2010). Yasuda et al (2008) and Foster et al (2010) ranked the sentence pairs in the general-domain corpus according to the perplexity scores of sentences, which are computed with respect to in-domain language models. Our main technical contributions are as follows: Additionally to perplexity optimization for linear interpolation, which was first applied by Foster et al (2010), we propose perplexity optimization for weighted counts (equation 3), and a modified implementation of linear interpolation. Matsoukas et al (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al (2010) extend this approach by weighting individual phrase pairs. Foster et al (2010) combine the two, applying linear interpolation to combine the instance weighted out-of-domain model with an in-domain model. Note that both data sets have a relatively high ratio of in-domain to out-of-domain parallel training data (1:20 for DE? EN and 1:5 for HT? EN); Previous research has been performed with ratios of 1:100 (Foster et al 2010) or 1:400 (Axelrod et al 2011). We expand on work by (Foster et al 2010) in establishing translation model perplexity minimization as a robust baseline for a weighted combination of translation models. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) as well as the linear mixture model of (Foster et al, 2010) for conditional phrase-pair probabilities over IN and OUT. Our technique for setting ? m is similar to that outlined in Foster et al (2010). For efficiency and stability, we use the EM algorithm to find ?, rather than L-BFGS as in (Foster et al., 2010). Foster et al (2010), however, uses a different approach to select related sentences from OUT. Foster et al (2010) propose a similar method for machine translation that uses features to capture degrees of generality. As in (Foster et al, 2010), this approach works at the level of phrase pairs. The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010). Foster et al (2010) do not mention what percentage of the corpus they select for their IR-baseline, but they concatenate the data to their in-domain corpus and report a decrease in performance. Foster et al (2010) further perform this on extracted phrase pairs, not just sentences. To address the first shortcoming, we adapt and extend some simple but effective phrase features as the input features for new DNN feature learning, and these features have been shown significant improvement for SMT, such as, phrase pair similarity (Zhao et al, 2004), phrase frequency, phrase length (Hopkins and May, 2011), and phrase generative probability (Foster et al, 2010), which also show further improvement for new phrase feature learning in our experiments.
 Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al, 2010). Our system is an extension of Stanford's multi-pass sieve system, (Raghunathan et al, 2010) and (Lee et al, 2011), by adding novel constraints and sieves. In contrast, (Raghunathan et al, 2010) proposed a rule based model which obtained competitive result with less time. We made three considerable extensions to the Raghunathan et al (2010) model. Please see (Raghunathan et al, 2010) for more details. The core of our coreference resolution system is an incremental extension of the system described in Raghunathan et al (2010). Proper Head Word Match: This sieve marks two mentions headed by proper nouns as coreferent if they have the same head word and satisfy the following constraints: Not i-within-i same as Raghunathan et al (2010). The candidate antecedents for the pronoun are ordered based on a notion of discourse salience that favors syntactic salience and document proximity (Raghunathan et al, 2010). By matching the performance of the DT system in the first two rows of the table, the AC system proves that it can successfully learn the relative importance of the deterministic sieves, which in (Raghunathanet al, 2010) and (Lee et al, 2011) have been manually ordered using a separate development dataset. Chen built upon the sieve architecture proposed in Raghunathan et al (2010) and added one more sieve - head match - for Chinese and modified two sieves. We incorporate lexicalized feature sets into two different coreference architectures: Reconcile (Stoyanov et al, 2010), a pairwise coreference classifier, and Sieve (Raghunathan et al, 2010), a rule-based system. There is no polynomial-time dynamic program for inference in a model with arbitrary entity-level features, so systems that use such features typically rely on making decisions in a pipelined manner and sticking with them, operating greedily in a left-to-right fashion (Rahmanand Ng, 2009) or in a multi-pass, sieve-like manner (Raghunathan et al, 2010). Compared with machine learning methods, (Raghunathan et al, 2010) proposed rule-base models which have been witnessed good performance. This fact explains a new trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles (Raghunathan et al, 2010). This is an interesting observation because pronominal anaphora problem has been reported with much higher results on other domains (Raghunathan et al, 2010), and also on other bio data (hsiang Lin and Liang, 2004). Our multi-sieve approach is different from (Raghunathan et al 2010) in several respects: (a) our sieves are machine-learning classifiers, (b) the same pair of mentions can fall into multiple sieves, (c) later sieves can override the decisions made by earlier sieves, allowing to recover from errors as additional evidence becomes available. (Raghunathan et al 2010) recorded the best result on CoNLL 2011 shared task. The ordering should be such that (a) maximum amount of information is injected at early stages (b) the precision at the early stages is as high as possible (Raghunathan et al 2010). Third, while our division to sieves may resemble witchcraft, it is motivated by the intuition that mentions appearing close to one another are easier instances of co-ref as well as linguistic insights of (Raghunathan et al 2010).
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective.
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6.  An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al).  In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings. The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions.
USR is the weakly supervised system of Naseem et al (2010).  Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc.  Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction.  Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather.  Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules.  In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish.
Eisenstein et al (Eisenstein et al, 2010) show promising results in identifying an author's geographic location from micro-blogs, but the locations are coarse-grained and rely on a substantial message history per-source. For example, Eisenstein et al (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. Our dataset is derived from prior work in which we gathered the text and geographical locations of 9,250 microbloggers on the website twitter.com (Eisenstein et al, 2010). Many of these terms are non-standard; while space does not permit a complete glossary, some are defined in Table 4 or in our earlier work (Eisenstein et al, 2010). Mangoville and M2 are clubs in New York; fasho and coo were previously found to be strongly associated with the West Coast (Eisenstein et al., 2010). Eisenstein et al (2010) collected about 380,000 tweets from Twitter's official API. While the above approaches discretize the continuous surface of the earth, Eisenstein et al (2010) predict locations based on Gaussian distributions over the earth's surface as part of a hierarchical Bayesian model. Following Eisenstein et al (2010), we consider all tweets of a user concatenated as a single document, and use the earliest collected GPS-assigned location as the gold location. Eisenstein et al (2010) use a latent variable model to predict geolocation information of Twitter users, and investigate geographic variations of language use. Eisenstein et al (2010) investigate questions of dialectal differences and variation in regional interests in Twitter users using a collection of geotagged tweets. Eisenstein et al (2010) evaluate their geographic topic model by geolocating USA-based Twitter users based on their tweet content. Performance is measured both on geo tagged Wikipedia articles (Overell, 2009) and tweets (Eisenstein et al, 2010). As a second evaluation corpus on a different domain, we use the corpus of geotagged tweets collected and used by Eisenstein et al (2010). For example, Eisenstein et al (2010) use Gaussian distributions to model the locations of Twitter users in the United States of America. For the Twitter dataset, an additional parameter is a threshold on the number of feeds each word occurs in: in the preprocessed splits of Eisenstein et al (2010), all vocabulary items that appear in fewer than 40 feeds are ignored. Eisenstein et al (2010) used a fixed Twitter threshold of 40. Geographical region knowledge has also been considered in topic models (Eisenstein et al, 2010).
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores.  Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t.  For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work.  Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). 
One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3.  Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters).  The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011).
Socher et al (2011a) and Socher et al (2011b) present a framework based on recursive neural net works that learns vector space representations for multi-word phrases and sentences.        More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al, 2011) and for sentiment domain adaptation (Glo rot et al, 2011). We should emphasize that the features induced from the addressee's utterance are unique to this task and are hardly available in the related tasks that predicted the emotion of a reader of news articles (Lin and HsinYihn, 2008) or personal stories (Socher et al, 2011). Analogous to our prediction task, Lin and Hsin Yihn (2008) and Socher et al (2011) investigated predicting the emotion of a reader from the text that s/he reads. Unlike Socher et al (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Recursive Autoencoder (Socher et al, 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically.  To do this, we use two unsupervised recursive autoencoders (RAE) (Socher et al, 2011b), one for the source phrase and the other for the target phrase. More details can be found in (Socher et al, 2011b). By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al, 2011), topic classification (Klementiev et al, 2012) or word-word similarity (Mitchell and Lapata, 2008). Some previous work on classifying snippets include using pre-defined polarity reversing rules (Moilanen and Pulman, 2007), and learning complex models on parse trees such as in (Nakagawa et al., 2010) and (Socher et al, 2011).  Socher et al (2011) introduce a semi-supervised approach that uses recursive autoencoders to learn the hierarchical structure and sentiment distribution of a sentence.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models.  This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr.  
In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. The corpora used in the experiments comes from a cross-lingual Textual Entailment dataset presented in (Negri et al, 2011), and provided by the task organizers. Defining "entailment" is quite difficult when dealing with expert annotators and still more with non-experts, as was noted by Negri et al. (2011). Afterwards, the creation of CLTE corpus by using Mechanical Turk is described on (Negri et al, 2011) and a corpus freely available for CLTE is published (Castillo, 2011). The dataset provided by the organizers consists of 500 CLTE pairs translated to four languages following the crowdsourcing-based methodology proposed in (Negri et al., 2011). The dataset was created following the crowdsourcing methodology proposed in (Negri et al, 2011), which consists of the following steps. Two datasets were provided by the organization of SemEval 2012 (Negri et al, 2011): a training set and a test set, both composed by a set of 500 pairs of sentences. There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination (Negri et al, 2011). The values of the parameters were chosen based on the CLTE development dataset (Negri et al, 2011) and were as follows.
 RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hyper graph-MERT (Kumar et al, 2009) both of which depend on hyper graph sampling. Additionally, we present Joshua's implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation.  It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. We cast MT tuning as pairwise ranking (Herbrich et al, 1999, inter alia), which Hopkins and May (2011) applied to MT. Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison. We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011). Feature weights were re-tuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Hopkins and May (2011) presented a method that uses a binary classifier. Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. Following Hopkins and May (2011), we used the following parameters for the sampling task: For each sentence, the decoder generates the 1500 best candidate translations (k= 1500), and the sampler samples 5000 pairs (n= 5000).  Like Hopkins and May (2011), we optimize ranking in n-best lists, but learn parameters in an online fashion. Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the n-bests other than those in ORACLE. Hopkins and May (2011) applied a MERT-like procedure in Alg 1 in which Equation 4 was solved to obtain new parameters in each iteration. Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations.  Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments.  Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model.
We employ the method of Ritter et al (2011) to tokenise messages, and use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens.  Our second component - chunker - is taken from (Ritter et al, 2011), which also comes with a model trained on Twitter data and shown to perform better on noisy data such as user comments. The chunker from (Ritter et al., 2011) relies on its own POS tagger, however, in our structural representations we favor the POS tags from the CMU Twitter tagger and take only the chunk tags from the chunker. For example, Ritter et al (2011) develop a system that exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. We have also used a named entity tagger trained specifically on the Twitter data (Ritter et al, 2011) to directly extract named entities from tweets. Firstly, a named entity recognizer (Ritter et al, 2011) is employed to identify named entities. The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al, 2011), retrieval (Subramaniam et al, 2009), summarization (Liu et al, 2011a), sentiment analysis (Celikyilmaz et al, 2010), etc. We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al, 2011). Using the UW Twitter NLP tools (Ritter et al, 2011). To study the diversity of named entities (NEs) in retweets, we used UW Twitter NLP Tools (Ritter et al., 2011) to extract NEs from RT-data. We then standardized variants (i.e. "fb" as a variant of "Facebook"), and manually categorized them against the 10-class schema defined by Ritter et al. (2011). Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets: RITTER (the 10% test split of the data in Ritter et al (2011) used in Derczynski et al (2013)), the test set from Foster et al.  
Reverb (Fader et al 2011) is a state-of-the-art open domain extractor that targets verb-centric relations, which have been shown in Banko and Etzioni (2008) to cover over 70% of open domain relations. Literature on automatic relation discovery (Fader et al, 2011) has shown that verbal phrases uncover a large fraction of binary predicates while reducing the amount of noisy phrases that do not denote any relations. To this end, we used a random sample from the large scale web-based ReVerb corpus (Fader et al, 2011), comprising tuple extractions of predicate templates with their argument instantiations. Two example systems implementing this paradigm are TEXTRUN NER (Yates et al, 2007) and REVERB (Fader et al, 2011). The propositions are usually produced by an extraction method, such as TextRunner (Banko et al, 2007) or ReVerb (Fader et al, 2011). To that end, we applied the methods on a set of one billion extractions (generously provided by Fader et al (2011)) automatically extracted from the ClueWeb09 web crawl, where each extraction comprises a predicate and two arguments.  To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al, 2011), a large scale publicly available web based open extractions data set, containing about 15 million unique template extractions. In terms of the latter, Cai and Yates (2013) and Berant et al (2013) applied pattern matching and relation intersection between Freebase relations and predicate argument triples from the ReVerb OpenIE system (Fader et al, 2011). We compare OLLIE to two state-of-the-art Open IE systems: (1) REVERB (Fader et al2011), which uses shallow syntactic processing to identify relation phrases that begin with a verb and occur between the argument phrases; 2 (2) WOEparse (Wuand Weld, 2010), which uses bootstrapping from entries in Wikipedia info-boxes to learn extraction pat terns in dependency parses. We say a schema is a textual schema if it has been extracted from free text, such as the Nell (Carlson et al, 2010) and ReVerb (Fader et al, 2011) extracted databases. MATCHER uses an API for the ReVerb Open IEsystem (Fader et al, 2011) to collect I (rT), for each rT. We obtained 155,409 positive instances from the English sentences using an off-the-shelf relation extraction system, ReVerb (Fader et al., 2011). The REVERB extractor (Fader et al 2011) on the ClueWeb09 Web corpus found over 1.4 billion noun phrases participating in textual relationships, and a sizable portion of these noun phrases are entities. Zhang and Weld (2013) is based on REVERB (Fader et al, 2011), which uses a regular expression on part-of-speech tags to produce the extractions. For convenience, we identify part-whole relations in Rule 12 based on the output produced by ReVerb (Fader et al 2011), an open information extraction system. Fader et al (2011) utilizes a confidence function. (Fader et al (2011) found that this set covers 69% of their corpus). In this paper, we present an approach for learning to map questions to formal queries over a large, open-domain database of extracted facts (Fader et al, 2011). We performed an end-to-end evaluation against a database of 15 million facts automatically extracted from general web text (Fader et al, 2011).
Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5.  Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used.
 Li et al (2012) and Bohnet and Nivre (2012) use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts. In parsing, Bohnet and Nivre (2012) and Bohnet et al (2013) propose a model for joint morphological analysis, part-of speech tagging and dependency parsing using a Usingeval.pl from Buchholz and Marsi (2006).  Bohnet and Nivre (2012)'s transition-based system jointly performs POS tagging and dependency parsing, which shows higher accuracy than ours. Bohnet and Nivre (2012) introduced a transition-based system that jointly performed POS tagging and dependency parsing. 
 Data-Oriented Parsing (DOP)'s methodology is to calculate weighted derivations, but as noted in (Bod, 2003), it is the highest ranking parse, not derivation, that is desired. Goodman's transform, in combination with a range of heuristics, allowed Bod (2003) to run the DOP model on the Penn Treebank WSJ benchmark and obtain some of the best results obtained with a generative model. Zuidema (2006a) shows that also the estimator (Bod, 2003) uses is biased and inconsistent, and will, even in the limit of infinite data, not correctly identify many possible distributions over trees. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). Our best performing model is more accurate than all these previous models except (Bod, 2003). Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003).   Similarly, (Bod, 2003) changes the way frequencies fi are counted, with a similar effect.  My approach is closely related to work in statistical parsing known as Data-Oriented Parsing (DOP), an empirically highly successful approach with labeled recall and precision scores on the Penn Tree Bank that are among the best currently obtained (Bod, 2003). We approximated the most probable parse as follows (following (Bod, 2003)). This result is only slightly higher than the highest reported result for this test-set, Bod's (.907) (Bod,2003).  This assumption is in consonance with the principle of simplicity, but there are also empirical reasons for the shortest derivation assumption: in Bod (2003) and Hearne and Way (2006), it is shown that DOP models that select the preferred parse of a test sentence using the shortest derivation criterion perform very well. But equally important is the fact that this new DOP* model does not suffer from a decrease in parse accuracy if larger subtrees are included, whereas the original DOP model needs to be redressed by a correction factor to maintain this property (Bod 2003). Of course, it is well-known that a supervised parser's f-score decreases if it is transferred to another domain: for example, the (non-binarized) WSJ-trained DOP model in Bod (2003) decreases from around 91% to 85.5% f score if tested on the Brown corpus. A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003). This subtree probability is redressed by a simple correction factor discussed in Goodman (2003: 136) and Bod (2003).
We have explored using different settings for the seed set size (Steedman et al, 2003). Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup.   This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. (Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere).  Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing.  Bootstrapping was applied to syntax learning by Steedman et al (2003). Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set.  Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training.
  We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set.  For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag.  We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003).  Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. As Clark (2003) points out, many-to-1 accuracy has several defects.  Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems.
Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003).  When compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al, 1994). Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger. The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). Here we use the Maximum Entropy models described in Curran and Clark (2003). Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran and Clark (2003) tagger. The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger. We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C & C tagger (Curran and Clark, 2003). The modern Bible is tagged using the C & C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches. The C & C tagger (Curran and Clark, 2003) was trained on the Wall Street Journal texts in the Penn Treebank and then used to tag the NET Bible (the source text). However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate: over 97% for WSJ text (Curran and Clark,2003). Part-of-speech (POS) tagging is done using the C & C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C & C maximum entropy NER tagger (Curran and Clark, 2003b). We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm (Curran and Clark, 2003). It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003).
Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator.   Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words.  Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a).
 With respect to the use of Wikipedia as a resource for natural language processing tasks, the work that is most closely related to ours is perhaps the name entity disambiguation algorithm proposed in (Bunescu and Pasca, 2006), where an SVM kernel is trained on the entries found in Wikipediafor ambiguous named entities.  The first approach in this line was Bunescu and Pasca (2006), who measure similarity between the textual context of the NE mention and the Wikipedia categories of the candidate.   Based on the aforementioned resources of information, we follow the method presented in (Bunescu and Pasca, 2006) to build a dictionary called ViDic.   (Bunescu and Pasca, 2006) showed that external information from Wikipedia can improve the disambiguation performance. After the task of EL was initiated with Wikipedia-based works on entity disambiguation, in particular by Cucerzan (2007) and Bunescu and Pasca (2006), numerous systems have been developed, encouraged by the TAC 2009 KB population task (McNamee and Dang, 2009). This is addressed by various methods, such as setting a threshold of minimal similarity for an entity selection (Bunescu and Pasca, 2006), or training a separate binary classifier to judge whether the returned top candidate is the actual denotation (Zheng et al, 2010).    We then employ standard entity linking techniques including string matching, prominence priors (Fader et al2009), and context matching (Bunescu and Pasca, 2006) to link the noun phrase subjects into Wikipedia.  Culotta et al (2006) deal with learning contextual patterns for extracting family relation ships from Wikipedia.  
Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation.    In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. The basic concept of the approach has been described by Matusov et al (2006).  (Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. 
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time.  McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. McDonald and Pereira (2006) define this as a second-order Markov assumption. McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively.
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006).     SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features.  The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). PTFs have been defined in (Moschitti, 2006a). Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK).  Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules.   Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement: (i) better possibility to exploit semantic smoothing since, e.g., trivially STK only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e.g. see experiments and motivation in (Moschitti, 2006a).
We have also tested a more complex version of e, with ei scores obtained from release 1.0 of Senti-WordNet (Esuli and Sebastiani, 2006b). Esuli and Sebastiani (2006) used the method to cover objective (N) cases. These terms and prior knowledge of their polarity could be used as features in a supervised classification framework to determine the sentiment of the opinionated text (E.g., (Esuli and Sebastiani, 2006)). Another evaluation resource (Esuli and Sebastiani, 2006) is resorted to in order to recover the evaluation values of all the hypernyms for a particular verb. Esuli and Sebastiani (2006) did a classification experiment for creating lexica for opinion mining, for instance, and the importance of lexical information for event extraction in Biomedical texts has been addressed in Fillmore et al (2006). Esuli and Sebastiani (2006) also address this problem testing three different variants of a semi-supervised method, and classify the input into positive, negative or neutral. The importance of neutral category is also discussed in other literatures (Esuli and Sebastiani, 2006). Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al, 2006), sentiment classification of phrases (Wilson et al, 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a).
The unigrams used for sentence/headline classification were learned from WordNet (Fellbaum, 1998) dictionary entries using the STEP system described in (Andreevskaia and Bergler, 2006b). Since sentiment-bearing words in English have different degree of centrality to the category of sentiment, we have constructed a measure of word centrality to the category of positive or negative sentiment described in our earlier work (Andreevskaiaand Bergler, 2006a).  At lexical level, Andreevskaia and Bergler (2006) exploit an algorithm for extracting sentiment-bearing adjectives from the WordNet based on fuzzy logic. Moreover, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. In addition, Andreevskaia and Bergler (2006) show that the performance of automatic annotation of subjectivity at the word level can be hurt by the presence of subjectivity-ambiguous words in the training sets they use. Researchers have found this at various levels of analysis, including the manual annotation of phrases (Takamura et al, 2006), sentiment classification of phrases (Wilson et al, 2005), sentiment tagging of words (Andreevskaia and Bergler, 2006b), and sentiment tagging of word senses (Esuli and Sebastiani, 2006a). Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other. Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category membership/centrality scores and tags in Andreevskaia and Bergler (2006).  A similar method is presented in (Andreevskaia and Bergler, 2006) where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. 
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.  Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy.  Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information.
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006).   Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001).
McDonald (McDonald, 2006) independently proposed a new machine learning approach. However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph. McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference.  Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006).  One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006).  Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words.  Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora. Both these systems reported results outperforming previous systems such as McDonald (2006). For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm.
In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006). The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006). The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects. However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006). This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard - unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data. Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale. The two automatic metrics used in the evaluations, NIST2 and BLEU3, have been shown to correlate well with expert judgments (Pearson's r= 0.82 and 0.79 respectively) in the SUMTIME domain (Belz and Reiter, 2006). When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices. it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006). Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005).
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. NA indicates scores that were not calculated by Birke and Sarkar (2006). Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training.  Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). Birke and Sarkar (2006) also used WSD. Birke and Sarkar (2006) requires WordNet. Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover. Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set.
Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathy and Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al, 2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). Another work uses both the syntactic and the lexical fixedness of VNICs in order to distinguish them from non-idiomatic ones, and eventually to extract them from corpora (Fazly and Stevenson, 2006). To measure fixedness, we use statistical measures of lexical, syntactic, and overall fixedness that we have developed in a previous study (Fazly and Stevenson, 2006), as well as some new measures we introduce here. The small amount of previous work on the identification of syntactic fixedness (Wermter and Hahn (2004), Fazly and Stevenson (2006)) has either focused on a single variation variety, or has only been evaluated for combinations of a small preselected list of words, presumably due to noise. Fazly and Stevenson (2006) propose a measure for detecting the syntactic fixedness of English verb phrases of the same variety as us. Fazly and Stevenson (2006) combine information about syntactic and lexical fixedness (i.e., estimated degree of compositionality) into one measure. In particular, Fazly and Stevenson (2006) look at the correlation between syntactic fixedness (in terms of e.g. passivisation, choice of determiner type and pluralisation) and non-compositionality of verb-noun compounds such as shoot the breeze. In their work on automatically identifying idiom types, Fazly and Stevenson (2006) - henceforth FS06 - show that an idiomatic VNC tends to have one (or at most a small number of) canonical form (s), which are its most preferred syntactic patterns. Fazly and Stevenson (2006) combine information about syntactic and lexical fixedness (i.e., estimated degree of compositionality) into one measure. In a previous study (Fazly and Stevenson, 2006), the authors came up with a dozen possible syntactic forms for verb-object pairs (based on passivization, determiner, and object pluralization) and used a corpus based statistical measure to determine the canonical form (s). Fazly and Stevenson (2006) use statistical measures of syntactic behaviour to gauge whether a verb and noun combination is likely to be a idiom. Certainly it would be worth combining the preferences with other measures, such as syntactic fixedness (Fazly and Stevenson, 2006). Nonetheless, the mentioned characteristics are useful indicators to distinguish literal and idiomatic expressions (Fazly and Stevenson, 2006). Fazly and Stevenson (2006) use lexical and syntactic fixedness as partial indicators of noncompositionality. Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. Combining our semantics-based approach with other extraction techniques such as the syntactic fixedness measure proposed by Fazly and Stevenson (2006) might improve the results significantly.
We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006).   A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006).  The former approach can produce higher performance: the evaluation of Giuliano et al (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%.  In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). Giuliano et al (2006) use no syntactic information.  In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)). We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts.
They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. PPR2 (Agirre et al, 2010): The system uses the UMLS meta thesaurus as a lexical knowledge graph and executes the Personalized PageRank, a state-of-the-art graph-based method, on the graph (Agirre and Soroa, 2009). Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank.  In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out.
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. (Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). LDA model has also been applied to WSI (Brody and Lapata, 2009). Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity. In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009).  Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007).
 One early approach, suggested by Kay (1987) and later pursued in different variants by Kiraz (1994, 2000) among others, was to, instead of modeling morphology along the more traditional finite-state transducer, modeling it with a n-tape automaton, where tapes would carry precisely this interleaving that is called for in Semitic interdigitation. Kay (1987) devised a framework with which each of the auto segmental tiers is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form. A strictly finite-state mechanism has a number of problems in covering natural language morphology, as has been recognized earlier (e.g. Kay (1987)). Kay (1987) proposes a framework with which each of the auto segmental tiers is assigned a tape in a multi-tape finite state machine, with an additional tape for the surface form.
Several lexical representation formalisms addressing these desiderata have been proposed, e.g. DATR [Evans and Gazdar 1989a, 1989b, 1990]; LRL [Copestake, 1992]; [Russell et al 1991]. Analyses in Network Morphology are implemented in DATR, a formal language for representing lexical knowledge designed and implemented by Roger Evans and Gerald Gazdar (Evans and Gazdar, 1989). A well-known formalism following this approach is DATR [Evans and Gazdar, 1989]. DATR was originally introduced by Evans and Gazdar (1989a; 1989b) as a simple, non monotonic language for representing lexical inheritance hierarchies. The original publications on DATR sought to provide the language with (1) a formal theory of inference (Evans and Gazdar, 1989a) and (2) a model-theoretic semantics (Evans and Gazdar, 1989b). The problem of constructing an explicit theory of infhrence for DATR was originally addressed in (Evans and Gazdar, 1989a). Consider for example the following rule of inherence, adapted from (Evans and Gazdar, 1989a). This fulfills one of the original objectives of the DATR programme, as set out in (Evans and Gazdar, 1989a; Evans and Gazdar, 1989b), to provide the language with an explicit theory of inference.
In this section, we will outline the approach to the translation of non-local re-entrances proposed in Kaplan et al (1989). We have shown in this paper that the approach to transfer between feature structures introduced in Kaplan et al 1989 can be exploited to deal with the translation of anaphoric dependeneins. The first one applies semantic transfer model via the methodology similar to the lexical functional grammar (Kaplan et al., 1989) and it is develop with the intention of public use.  Kaplan et al (1989) present a framework for translation based on the description and correspondence concepts of Lexical Functional Grammar (Kaplan and Bresnan, 1982). Kaplan et al (1989) present a framework for translation based on the description and correspondence concepts of Lexical-Functional Grammar (Kaplan and Bresnan, 1982). Kaplan et al (1989) suggest that this architecture can provide a formal basis for specifying complex source-target translation relationships in a declarative fashion that builds on monolingual grammars and lexicons that are independently motivated and theoretically justified. Kaplan et al (1989) offer several examples to illustrate the effectiveness of this approach to translation. Kaplan et al (1989) discussed such differences in embedding and offered two alternative analyses that rely only on codescriptive specifications.
The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al, 1999).  It has been shown in (Mikheev et al, 1999) that a NE Recognition system performs reasonably well for most classes even without gazetteers. It is common practice in NER to utilize the discourse level to disambiguate items in non predictive contexts (see e.g. Mikheev et al, 1999). Mikheev et al (1999) exploit label consistency information within a document using relatively ad hoc multi-stage labeling procedures. To date, Named Entity Recognition (NER) has only used gazetteers as evidence that a text span could be some kind of place name (LOCATION), even though their finite nature makes lists of names of limited use for classification (Mikheev et al, 1999). Other work has supported the use of gazetteers in general but has found that lists of only moderate size are sufficient to provide most of the benefit (Mikheev et al, 1999). To investigate the role of gazetteers in NER, Mikheev et al (1999) combine grammar rules with maximum entropy models and vary the gazetteer size. The internal and external evidence is being used by all NERC systems such as LTG (Mikheev et al 1999), FASTUS (Hobbs et al 1997), Proteus (Yangarber, Grishman, 1998). In such cases all tokens forming an NE and all their combinations are stored in the dynamic lexicon (Mikheev et al 1999:5). Since many tagging systems utilise gazetteers of known entities, some research has focused on their automatic extraction from the web (Etzioni et al, 2005) or Wikipedia (Toral et al, 2008), although Mikheev et al (1999) and others have shown that larger NE lists do not necessarily correspond to increased NER performance.  Mikheev et al (1999) and Finkel et al (2004) incorporate label consistency information by using ad hoc multi-stage labeling procedures that are effective but special-purpose.
The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization. This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. The word classes for the class-based features are trained using the mkcls tool (Och, 1999). We use the publicly available implementation MKCLS (Och, 1999) to train this model. The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). This feature implements a 5-gram language model of target statistical classes (Och, 1999). This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems. As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. This approach was shown to give the best results in (Och, 1999). Practically, we can use word alignment as used in (Och, 1999). First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). For the unsupervised tags, we used clustered word classes obtained using the mkcls software, which implements the approach of Och (1999). Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999).
(Tjong Kim Sang and Veenstra, 1999) have presented three variants of this tagging representation.  (Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task. We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999). There are four kinds of chunk tags in the CoNLL-1999 dataset, namely IOB1, IOB2, IOE1, and IOE2 (Tjong Kim Sang and Veenstra, 1999). The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for base noun phrase chunking. Tjong Kim Sang and Veenstra (1999) describes in detail the IOB schemes. We used the JNLPBA-2004 training data, which is a set of tokenized word sequences with IOB2 (Tjong Kim Sang and Veenstra, 1999) protein labels. From now on, we shall refer to the Chunk tag of a word as its IOB value (IOB was named by Tjong Kim Sang and Jorn Veeenstra (Tjong Kim Sang and Veenstra, 1999) after Ratnaparkhi (Ratnaparkhi, 1998)). The training data was converted to use the IOB2 phrase model (Tjong Kim Sang and Veenstra, 1999). The mention encoding is the IOB2 encoding presented in (Tjong Kim Sang and Veenstra, 1999) and introduced by (Ramshaw and Marcus, 1994) for the task of base noun phrase chunking. Tjong Kim Sang calls this method as IOB1 representation, and introduces three alternative versions - IOB2, IOE1 and IOE2 (Tjong Kim Sang and Veenstra, 1999). To transform the problem into a classification task, we use the IOB2 classification scheme (Tjong Kim Sang and Veenstra, 1999). We encoded the opinionated expression brackets using the IOB2 encoding scheme (Tjong Kim Sang and Veenstra, 1999) and trained the model using the metod by Collins (2002).
Others have exploited the automatic transfer of some already existing annotated resource in a different medium or language (such as the translingual projection of part-of-speech tags, syntactic bracketing and inflectional morphology in Yarowsky et al (2001), requiring no direct supervision in the foreign language). That includes approaches such as direct model transfer (Zeman and Resnik, 2008) and annotation projection (Yarowsky et al, 2001). In particular, prior work on translingual part-of-speech tagger projection via parallel bilingual corpora (e.g. Yarowsky et al, 2001) has been limited to inducing part-of-speech taggers in second languages (such as French or Czech) that only assign tags at the granularity of their source language (i.e. 49 the Penn Treebank-granularity distinctions from English). Yarowsky et al (2001) performed early work in the cross-lingual projection of part-of-speech tag annotations from English to French and Czech, by way of word-aligned parallel bilingual corpora. Our cross-lingual POS tag projection process is similar to Yarowsky et al (2001). We used the methods of Yarowsky et al (2001) to develop a core part-of-speech tagger for French, based only on the projected core tags, and used this as a basis for fine-grained tags. It is important because a word-aligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation (Och et al., 1999), (Tillmann and Xia, 2003), (Koehn et al., 2003, sec. 3), or for projecting linguistic annotation across languages (Yarowsky et al, 2001). Early studies of cross-lingual annotation projection were accomplished for lexically-based tasks; for example part-of-speech tagging (Yarowsky and Ngai, 2001), named-entity tagging (Yarowsky et al, 2001), and verb classification (Merlo et al, 2002). Of significant interest is the porting of annotations across languages: for example, Yarowsky et al 2001 present a method for automatic tagging of English and the projection of the tags to other languages; however, these tags do not include semantics. Morphological analyzers, noun-phrase chunkers, POS taggers, etc., have also been developed for resource deficient languages by exploiting translated or parallel text (Yarowsky et al, 2001). Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al, 2001), our work shows that they can be exploited forvery specific problems that arise in deep linguistic analysis (see Section 4). Yarowsky et al (2001) introduced a new method for developing a Part-of-Speech tagger by projecting tags across aligned corpora. However, in our research study, new challenges arose because our RRL data are automatically annotated, which is different from what has been reported in the research works we have mentioned before, i.e. (Yarowsky et al., 2001) and (Klementiev and Roth, 2006), where gold annotated data were used.  Yarowsky et al (2001) describe a successful method consisting of (i) automatic annotation of English texts, (ii) cross language projection of annotations onto target language texts, and (iii) induction of noise-robust taggers for the target language. Although potentially useful as a proxy for semantic equivalence, automatically induced alignments are often noisy, thus leading to errors in annotation projection (Yarowsky et al., 2001). Although phrase based approaches to SMT tend to be robust to word alignment errors (Lopez and Resnik, 2006), improving word-alignment is still useful for other NLP research that is more sensitive to alignment quality, e.g., projection of information across parallel corpora (Yarowsky et al, 2001). In alignment work, the foundational work is Yarowsky et al's induction of projections across aligned corpora (Yarowsky et al, 2001), most successfully adapted to cross-linguistic syntactic parsing (Hwa et al, 2005). 
For this reason, we compute an unweighted entity-constrained mention F-measure (Luo, 2005) and report all contrastive experiments with this metric. But the metric has a systematic bias for systems generating fewer entities (Bagga and Baldwin, 1998) - see Luo (2005). Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al, 1995), B 3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), CEAF m (Luo, 2005), and BLANC (Recasens and Hovy, 2011). However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B 3 metric. We report recall, precision, and F1 for MUC (Vilain et al, 1995), B3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005). Three runs have been submitted for the SemEval task 1 on Coreference Resolution (Recasens et al, 2010), optimizing Corry's performance for BLANC (Recasens and Hovy, in prep), MUC (Vilain et al, 1995) and CEAF (Luo, 2005). We have collected a number of runs on the development data to optimize the performance level for a particular score: BLANC (Recasens and Hovy, in prep), MUC (Vilain et al, 1995) or CEAF (Luo, 2005). We report results in terms of recall (R), precision (P), and F-score (F) by employing the mention-based B3 metric (Bagga and Baldwin, 1998), the entity-based CEAF metric (Luo, 2005), and the pairwise F1 (PW) metric. Category Evaluation Measures set mapping purity, inverse purity, F-measure pair counting rand index, Jaccard Coefficient, Folks and Mallows FM entropy entropy, mutual information, VI, V editing distance editing distance co reference resolution MUC (Vilain et al,1995), B-Cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) Table 3. This is similar to Luo (2005) where a Bell tree is used to score and store the searching path. CEAF (Luo, 2005): For a similarity function between predicted and true clusters, CEAF scores the best match between true and predicted clusters using this function. We use the similarity function from Luo (2005). In coreference resolution, typical performance measure functions include MUC (Vilain et al, 1995), Rand index (Rand, 1971), B-CUBED (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). In all our experiments, we use two popular performance measures, B-CUBED Fmeasure (Bagga and Baldwin, 1998) and CEAF F measure (Luo, 2005), to evaluate the co reference resolution result. To evaluate our system we use CEAF (Luo, 2005) and B3 (Bagga and Baldwin, 1998). We compare the end clustering quality across a variety of thresholds and for various system flavors using three metrics: MUC (Vilain et al 1995), B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005).   Both parameters were empirically adjusted on the development set for the evaluation measure used in this shared task: the unweighted average of MUC (Vilain et al, 1995), B3 (Bagga and Baldwin, 1998) and entity-based CEAF (Luo, 2005). Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer (Vilain et al., 1995) and the CEAF scorer (Luo, 2005).
For parameter optimization for the word alignment task, Taskar, Simon and Klein (Taskar et al, 2005) used a large margin approach by factoring the structure level constraints to constraints at the level of an alignment link. It should be noted that previous word-alignment experiments such as Taskar, Simon and Klein (Taskar et al, 2005) have been done with very large datasets and there is little word-order variation in the languages involved. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative mod els, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). Similarly, Taskar et al (2005) cast word alignment as a maximum weighted matching problem and propose a framework for learning word pair scores as a function of arbitrary features of that pair. We followed the work in (Taskar et al, 2005) and split the original test set into 347 test examples, and 100 training examples for parameters tuning. Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al, 2005) use a one-to-one constraint, where words in either sentence can participate in at most one link. Though Tsochantaridis et al (2004) provide several ways to incorporate loss into the SVM objective, we will use margin re-scaling, as it corresponds to loss usage in another max-margin alignment approach (Taskar et al, 2005). To create a matching alignment solution, we reproduce the approach of (Taskar et al, 2005) within the framework described in Section 4.1: 1. We use the same feature representation as (Taskar et al, 2005), with some small exceptions. The first baseline, matching is the matching SVM described in Section 4.2.1, which is a re-implementation of the state-of-the art work in (Taskar et al, 2005). The first thing to note is that our Matching baseline is achieving scores in line with (Taskar et al, 2005), which reports an AER of 0.107 using similar features and the same training and test sets. Our work borrows heavily from (Taskar et al, 2005), which uses a max-margin approach with a weighted maximum matching aligner. (Taskar et al, 2005) cast the problem of alignment as a maximum weight bipartite matching problem, where nodes correspond to the words in the two sentences. Dice Coefficient of the source word and the target word (Taskar et al, 2005). Commitments are then sent to a Commitment Selection module, which uses a weighted bipartite matching algorithm first described in (Taskar et al, 2005b) in order to identify the commitment from the t which features the best alignment for each commitment extracted from the h. Following Commitment Extraction, we used an word alignment technique first introduced in (Taskar et al, 2005b) in order to select the commitment extracted from t (henceforth, ct) which represents the best alignment for each of the commitments extracted from h (henceforth, ch). As with (Taskar et al, 2005b), we use the large-margin structured prediction model. Less general approaches based on matching have been proposed in (Matusov et al, 2004) and (Taskar et al., 2005). Permutation space methods include weighted maximum matching (Taskar et al, 2005), and approximations to maximum matching like competitive linking (Melamed, 2000). (Taskar et al, 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set.
(Moore, 2005) has proposed an approach which does not impose any restrictions on the form of model features. LLR and CLP are the word association statistics used in Moore's work (Moore, 2005). A variation of this feature was used by (Moore, 2005) in his paper. In fact, LLR can still be used for extracting positive associations by filtering in a pre-processing step words with possibly negative associations (Moore, 2005). Furthermore, to ensure that only positive association counts, we set the probability to zero if p (x, y) < p (x) p (y), where the probabilities are estimated using relative frequencies (Moore, 2005). We take advantage of this, building on our existing framework (Moore, 2005), to substantially reduce the alignment error rate (AER) we previously reported, given the same training and test data. As in our previous work (Moore, 2005), we train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. Firstly, as denoted by Moore (2005), one needs to tune numerous parameters in order to optimize the results for a particular alignment task, which can be very time consuming.  d is an absolute discount parameter as in (Moore, 2005). (Moore, 2005) uses an averaged perceptron for training with a customized beam search. 2) Conditional link probability (Moore, 2005). For example, Moore (2005) uses statistics like log-likelihood-ratio and conditional likelihood-probability to measure word associations; Liu et al (2005) and Taskar et al (2005) use results from IBM Model 3 and Model 4, respectively. d is a discounting constant which is set to 0.4 following Moore (2005). Moore (2005) proposes a similar framework, but with more features and a different search method. In order to obtain the word alignment satisfying the ITG constraint, Wu (1997) propose a DPalgorithm, and we (Chao and Li, 2007) have transferred the constraint to four simple position judgment procedures in an explicit way, so that we can incorporate the ITG constraint as a feature into a log linear word alignment model (Moore, 2005). These models are roughly clustered into two groups: generative models, such as those pro posed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). Unfortunately, as Moore (2005) points out, it is usually difficult to extend a given generative model with feature functions without changing the entire generative story. Moore (2005) likewise uses this example to motivate the need for models that support arbitrary, overlapping features. 
This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment. The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al (2011). We experimented with two different supervised aligners: a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al, 2011). In each language, the rule extraction was performed using approximately 1.2M sentence pairs aligned using a maxent aligner (Ittycheriah and Roukos, 2005) trained using a variety of domains (Europarl, computer manuals) and a maximum entropy parser for English (Ratnaparkhi, 1999). To quantify this effect, we learn reordering rules using three sets of alignments: HMM alignments, alignments from a supervised MaxEnt aligner (Ittycheriah and Roukos, 2005), and hand alignments. Parallel sentences were first word-aligned using a MaxEnt aligner (Ittycheriah and Roukos, 2005). As our word aligner (Ittycheriah and Roukos, 2005) can introduce errors in extracting Tree-to-String rules, we use a small hand-aligned data set CE16K, which consists of 16K sentence pairs, to get relatively clean rules, free from alignment errors. A discriminatively trained 1-to-N model with feature functions specifically designed for Arabic was presented in (Ittycheriah and Roukos, 2005). Ittycheriah and Roukos (2005) trained a discriminative model on a corpus of ten thousand word aligned Arabic-English sentence pairs that outperformed a GIZA++ baseline. The gold standard alignments we use here are part of the IBM Arabic-English aligned corpus (IBMAC) (Ittycheriah and Roukos, 2005). Since the IBMAC gold alignments we use are not marked as such, AER reduces to 1 F-score (Ittycheriah and Roukos, 2005): Pr= |A? S||A| Rc= |A? S| |S| AER= 1? 2PrRc Pr+Rc where A links are proposed and S links are gold. Ittycheriah and Roukos (2005) used only the top 50 sentences in IBMAC test data. The two results are not comparable because: (a) Ittycheriah and Roukos (2005) used additional gold aligned data that was not released and (b) they use an additional 500K sentences from the LDC UN corpus for Giza training that was created by adapting to the source side of the test set - the details of such adaptation were not provided and thus it is not clear how to replicate them to compare fairly. For word alignments we use the Maximum Entropy aligner described in (Ittycheriah and Roukos, 2005) that is trained using hand aligned training data. To find the most likely alignment we use the same algorithm as in (Ittycheriah and Roukos, 2005) since the structure of the model is unchanged. The data in this corpus is automatically aligned using a technique presented in (Ittycheriah and Roukos, 2005). Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data. On our test set, (Tillmann and Zhang, 2005) reports a BLEU score of 37.8 and (Ittycheriah and Roukos, 2005) reports a BLEU score of 48.0. We assume that each sentence pair in the training corpus is word-aligned (e.g. using a MaxEnt aligner (Ittycheriah and Roukos, 2005) or an HMM aligner (Ge, 2004)). 
A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others.  Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2. Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005).
Popescu and Etzioni (2005) achieved high-precision opinion phrases extraction by using relaxation labeling. (Popescu and Etzioni, 2005) and (Qiu et al, 2011) designed syntactic patterns to perform this task. Popescu and Etzioni (2005) proposed a relaxed labeling approach to utilize linguistic rules for opinion polarity detection.   A more NLP-oriented approach is proposed in (Popescu and Etzioni 2005), where noun phrases are extracted from online user reviews.  Popescu and Etzioni (2005) investigated the same problem. The dictionary-based method utilizes Wikipedia (Popescu and Etzioni, 2005) to find an entry page for a phrase or a single term in a query.  In (Popescu and Etzioni, 2005), Popescu and Etzioni not only analyzed polarity of opinions regarding product features but also ranked opinions based on their strength. Popescu and Etzioni (2005) present a method that identifies product features for using corpus statistics, WordNet relations and morphological cues. The relevance ranking and extraction was then performed with different statistical measures: Pointwise Mutual Information (Popescu and Etzioni, 2005), the Likelihood Ratio Test (Yi et al, 2003) and Association Mining (Hu and Liu, 2004).   Unlike most previous work on polarity classification, which has largely focused on exploiting adjective-noun (AN) relations (e.g., Dave et al (2003), Popescu and Etzioni (2005)), we hypothesized that subject-verb (SV) and verb-object (VO) relations would also be useful for the task. Common solutions to this problem involve clustering with the help of knowledge-rich methods, involving manually-constructed rules, semantic hierarchies, or both (e.g., Popescu and Etzioni 2005, Fahrni and Klenner 2008). In the IE setting, Popescu and Etzioni (2005) extract frequent terms, and cluster them into aspects.  Popescu and Etzioni applied relaxation labeling to polarity identification (Popescu and Etzioni, 2005).
Wilson et al (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. For example, AbuJbara et al (2013) and Jochim and Schutze (2012) find the list of polar words from Wilson et al (2005) to be useful, and neither study lists dependency relations as significant features. In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al, 2005).  Further, there are analyses (Wiebe et al, 2005) and experiments (Wilson et al, 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). We use a manually constructed polarity lexicon (Wilson et al, 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral).  PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005).  Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). We also used two datasets for the evaluation purpose: the MPQA (Wilson et al, 2005) and IQAPs (Marneffe et al, 2010) datasets. In this work we use MPQA (Wilson et al, 2005). The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters (Wilson et al, 2005), which are used for identifying opinion roots, modifiers and negation words. They use the Opinion Finder lexicon (Wilson et al, 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. To generate the initial explanations, one can use an off-the shelf sentiment classifier such as OpinionFinder2 (Wilson et al, 2005).
(Choi et al, 2005)) and topics of opinions (Stoyanov and Cardie, 2008). We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). The features we use (Table 5) are mostly inspired by Choi et al (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). Choi et al (2005) and Choi et al (2006) explore conditional random fields, Wiegand and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence.
Recent results on the SPORTS and FINANCE gold standard dataset (Koeling et al 2005) show that domain WSD can achieve accuracy in the 50 60% ballpark when a state-of-the-art algorithm such as Personalized PageRank is paired with a distributional approach (Agirre et al 2009) or with semantic model vectors acquired for many domains (Navigli et al 2011). We also experimented with the gold standard produced by Koeling et al (2005). We first test the proposed method over the tasks of predominant sense learning and sense distribution induction, using the WordNet-tagged dataset of Koeling et al (2005), which is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE). We first notice that, despite the coarser-grained senses of Macmillan as compared to WordNet, the upper bound WSD accuracy using Macmillan is comparable to that of the WordNet-based datasets over the balanced BNC, and quite a bit lower than that of the two domain corpora of Koeling et al (2005). The relative occurrence of unlisted/unclear senses in the datasets of Koeling et al (2005) is comparable to UKWAC.
In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding.
We use three base models for dependency parsing: MST parser (McDonald et al 2005), Maltparser (Nivre et al 2006), and the ensemble parser of Surdeanu and Manning (2010).    A detailed description of the Chu Liu Edmonds algorithm for MSTs is available in McDonald et al (2005). We test these two techniques on English-Czech MT outputs using our own reimplementation of the MST parser (McDonald et al, 2005) named RUR1 parser. We have reimplemented the MST parser (McDonald et al, 2005) in order to provide for a simple insertion of the parallel features into the models. The set of monolingual features used in RUR parser follows those described by McDonald et al (2005). The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011).   See Georgiadis (2003) for a detailed algorithmic proof, and McDonald et al (2005) for an illustrative example. They introduce maximum spanning tree (MST) parsing (McDonald et al, 2005) into phrase-based translation. For domain adaptation, we show an error reduction of up to 7.7% when adapting the second-order projective MST parser (McDonald et al 2005) from newswire to the QuestionBank domain. For dependency parsing we utilize the second-order projective MST parser (McDonald et al 2005) with the gold-standard POS tags of the corpus. Base is the second-order, projective dependency parser of McDonald et al (2005). We consider two different approaches to learning a temporal dependency parser: a shift-reduce model (Nivre, 2008) and a graph-based model (McDonald et al, 2005). Dependency tree parsing as the search for the maximum spanning tree in a directed graph was proposed by McDonald et al (2005c). The formulation works by defining in McDonald et al (2005a). The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al 2005) to use ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function.
Alm et al (2005) and Francisco and Gervas (2006) worked on fairy tales. They later use this corpus to construct a reasonably accurate classifier for emotional states of sentences (Alm et al., 2005). The second is the frequency with which the character is associated with emotional language - their emotional trajectory (Alm et al 2005). These tools and resources have been already used in a large number of applications, including expressive text-to-speech synthesis (Alm et al, 2005), tracking sentiment timelines in on line forums and news (Balog et al, 2006), analysis of political debates (Carvalho et al, 2011), question answering (Oh et al., 2012), conversation summarization (Carenini et al., 2008), and citation sentiment detection (Athar and Teufel, 2012).
However, this method does not work for realworld datasets such as PASCAL RTE (Dagan et al., 2006), because of the knowledge bottleneck: it is often the case that the lack of sufficient linguistic knowledge causes failure of inference, thus the system outputs "no entailment" for almost all pairs (Bos and Markert, 2005). On PASCAL RTE datasets, strict logical inference is known to have very low recall (Bos and Markert, 2005), so on-the-fly knowledge is crucial in this setting. Bos and Markert (2005) proposes features from a model builder; Raina et al (2005) proposes an abduction process; Tatu and Moldovan (2006) shows handcrafted rules could drastically improve the performance of a logic-based RTE system. NutCracker (Bos and Markert, 2005) is a system based on logical representation and automatic theorem proving, but utilizes only WordNet (Fellbaum, 1998) as a lexical knowledge resource.
In our model, we adopted the subtree kernel method for the shortest path dependency kernel (Bunescu and Mooney, 2005). It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the in formation required to assert a relationship between them (Bunescu and Mooney, 2005). The respective dependency parse tree is included through following the shortest dependency path hypothesis (Bunescu and Mooney, 2005), by using the syntactical and dependency information of edges (e) and vertices (v). It has been shown in previous work on relation extraction that the shortest path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). The information in the shortest path between two entities in a dependency tree can be used to assert whether a relationship exists between them (Bunescu and Mooney, 2005). 
We use OpinionFinder (Wilson et al, 2005) which employs negative and positive polarity cues.  Sentiment lexicons such as SentiWordNet (Baccianella et al (2010)) and OpinionFinder (Wilson et al (2005a)) show low agreement rate with human, which is somewhat as expected: human judges in this study are labeling for subtle connotation, not for more explicit sentiment. Opinionfinder (Wilson et al, 2005) is a system for mining opinions from text. The list of polarity words that we use in this component has been taken from the OpinionFinder system (Wilson et al, 2005).  For CRF-style features, we consider the string representation of the current word, its part-of speech, and a dictionary-derived feature, which is based on a subjectivity lexicon provided by Wilson et al (2005).  We use Opinion Finder (Wilson et al 2005a) to identify words with positive or negative semantic orientation.  Opinionfinder (Wilson et al, 2005a) is a system for mining opinions from text. This component uses the publicly available tool, opinion finder (Wilson et al., 2005a), as a framework for polarity identification.   For this reason, we employ OpinionFinder (Wilson et al 2005a), which has a pre-trained classifier for annotating the phrases in a sentence with their contextual polarity values. We use OpinionFinder (Wilson et al, 2005a) to identify polarized words and their polarities.  We begin with a seed set combining PARADIGM+ (Jo and Oh, 2011) with 'strongly subjective' adjectives from the OpinionFinder lexicon (Wilson et al, 2005), yielding 1342 seeds.
 We also used the phi2 score (Gale and Church, 1991) as a word association model, and as a POS-tags association model.   Motivated by the need to reduce on the memory requirement and to insure robustness in estimation of probability, Gale and Church (1991) proposed an alternative algorithm in which probabilities are not estimated and stored for all word pairs. When trained with a corpus only one-tenth the size of the corpus used in Gale and Church (1991), the algorithm aligns over 80% of word pairs with comparable precision (93%). These methods often involve using a statistic such as ?2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. We produce an initial alignment using the same algorithm described in Section 3, except we maximize summed phi2 link scores (Gale and Church, 1991), rather than alignment probability.  (Gale and Church, 1991) has used the ?2 statistics as the correspondence level of the word pairs and has showed that it was more effective than the mutual information. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al, 1999) and many others. The first cost function is the correlation measure (cf the use of phi2 in Gale and Church (1991)) computed as follows:= (bc ad )x/ (a+ b) (c+ d) (a+ c) (b+ d) where a =nv -n~, i~v b =nw, y c= Nnvnw +nw, v d =nwnw, v N is the total number of bi texts, nv the number of bi texts in which V appears in the target, nw the number of bi texts in which W appears in the source, and nw, y the number of bi texts in which W appears in the source and V appears in the target. They are used in multilingual NLP as a basis for the creation of translation models (Brown et .al., 1990), lexical acquisition (Gale and Church, 1991) as well as for cross-language information retrieval (Chen and Nie, 2000). Gale and Church (1991) do not follow the EM model, but rather find French translations of English words using a phi2-like measure of association. A wide variety of ways of LTP estimation have been proposed in the literature of computational linguistics, including Dice coefficient (Kay and Roscheisen 1993), mutual information, phi2 (Gale and Church 1991b), dictionary and thesaurus Table 1. Gale and Church (1991) made what may be the first application of word association to word alignment. Third, it makes the correct judgment on Gale and Church's well known chambre-communes problem (Gale and Church, 1991). Many previous efforts have used a similar methodology but were only able to focus on word to word correspondences (Gale and Church, 1991). The algorithm creates an initial alignment using search, constraints, and summed phi2 correlation-based scores (Gale and Church, 1991). In this work, the sentence level alignment algorithm given in (Gale and Church,1991) has been used for applying segment constraint.
To evaluate the parsing performance, we use the three standard ways to measure the performance: unlabeled (i.e., hierarchical spans) and labeled (i.e., nuclearity and relation) F-score, as defined by Black et al (1991).  We evaluated our parser using the standard PARSEVAL measures (Black et al, 1991): labelled precision, labelled recall, and labelled F-measure (Prec., Rec., and F1, respectively), which are based on the number of non-terminal items in the parser's output that match those in the gold-standard parse. The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al, 1991) score that is indicative of the intrinsic performance of the parser. Parse trees are commonly scored with the PARSEVAL set of metrics (Black et al, 1991). Final testing was carried out on section 00, and the PARSEVAL measures (Black et al, 1991) were used to evaluate the performance. The performance is assessed using labeled recall and labeled precision as defined by the standard Parseval metric (Black et al, 1991). Consequently, they relaxed standard PARSEVAL (Black et al, 1991) to treat EDITED constituents like punctuation: adjacent EDITED constituents are merged, and the internal structure and attachment of EDITED constituents is not evaluated. Nevertheless, we agree with the widespread sentiment that dependency-based evaluation of parsers avoids many of the problems of the traditional Parseval measures (Black et al, 1991), and to the extent that the Stanford dependency representation is an effective representation for the tasks envisioned, it is perhaps closer to an appropriate task based evaluation than some of the alternative dependency representations available. In particular, metrics like attachment score for dependency parsers (Buchholz and Marsi, 2006) and Parseval for constituency parsers (Black et al, 1991) suffer from being an average over a highly skewed distribution of different grammatical constructions. Table 3 shows the results of 1st-level partial parsing and full parsing, using the PARSEVAL evaluation methodology (Black et al 1991) on the UPENN Chinese Tree Bank of 100k words developed by Univ. of Penn. The standard PARSEVAL metric (Black et al., 1991) counts labeled nonempty brackets: items are (X, i, j) for each nonempty nonterminal node, where X is its label and i, j are the start and end positions of its span. At their time, each of these models improved the state-of-the-art, bringing parsing performance on the standard test set of the Wall-Street-Journal to a performance ceiling of 92% F1-score using the PARSEVAL evaluation metrics (Black et al, 1991). PARSEVAL measures (Black et al, 1991) are used to evaluate a parser's phrase-structure trees against a gold standard. After the release of the Penn Treebank (PTB) (Marcus et al, 1993) and the PARSEVAL metrics (Black et al, 1991), some new corpus based syntactic parsing techniques were explored in the English language. Empty categories were and still are routinely pruned out in parser evaluations (Black et al, 1991). Measured by the ParsEval metric (Blacketal., 1991), the parser accuracy stands at 80.3% (F score), with a precision of 81.8% and a recall of 78.8% (recall). The parameters lambda i and rho are tuned by the Powell's method (Powell, 1964) on a development set, using the F1 score of PARSEVAL (Black et al, 1991) as objective. As accuracy metric we used the standard PAP, SEVAI, scores (Black et al 1991) to compare a proposed parse P with tile corresponding correct tree bank parse T as follows. A comparison of unlexicalised PCFG parsing (Ku?bler, 2005) trained and evaluated on the German NEGRA (Skut et al, 1997) and the TuBa D/Z (Telljohann et al, 2004) tree banks using LoPar (Schmid, 2000) shows a difference in parsing results of about 16%, using the PARSEVAL metric (Black et al, 1991).
History-based parsing uses features of the parsing history to predict the next parser action (Black et al,1992). Many current parsers fall into the class of history based grammars (Black et al, 1992). History-based feature models for predicting the next parser action (Black et al, 1992).  History-based parsing models rely on features of the derivation history to predict the next parser action (Black et al, 1992).
Fortunately, we have found in (Gale et al, 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions. We originally designed the experiment in Gale et al (1992) to test the hypothesis that multiple uses of a polysemous word tend to have the same sense within a common discourse. We followed the well-known postulate (Gale et al, 1992) that all occurrences of a word in the same discourse tend to have the same sense (one sense per discourse), in order to decrease the annotator workload. In order to improve the bootstrapping performance, we use the heuristic one tag per domain for multi word NE in addition to the one sense per discourse principle [Gale et al 1992]. Apply the 'one sense per discourse' principle [Gale et al1992] for each disambiguated location name to propagate the selected sense to its other mentions within a document. It has been claimed (by Gale et al 1992) on the basis of corpus analysis that to a very large extent a word keeps the same meaning throughout a text. This is a reasonable restriction supported by empirical evidence (see also (Gale et al 1992)). For example, if two NPs are the same string in a given document, then it is more likely than not that they have the same semantic subtype according to the 'one sense per discourse' hypothesis (Gale et al., 1992). In this case we cannot accept the general assumption of one sense per discourse (Gale et al, 1992), because words such as line, large in English or kljuch in Russian can function in the same discourse in a totally different sense. In the original one sense per discourse study, Gale et al (1992b) considered a sample of 9 polysemous English words.  While these numbers are not as high as the 94% agreement reported by Gale et al (1992b) in their empirical study, they still strongly support the one translation per discourse hypothesis. If more than one match of the same term is found in a document, we assume one sense-per-discourse (Gale et al, 1992) and jointly extract features for all matches of the term. Addressing word sense disambiguation, Gale et al (1992) introduced the idea of a word sense located at the discourse-level and observed a 93 strong one-sense-per-discourse tendency, i.e. several occurrences of a polysemous word form have a tendency to belong to the same semantic class within one discourse. Nonetheless, there are a non-negligible number of cases in which the one sense per discourse assumption (Gale et al, 1992) does not hold. This effect of "meaning consistency" also known as the principle of "one sense per discourse" has been applied in word sense disambiguation with quite some success (Gale et al., 1992). This property tends to hold for correct classifiers (Gale et al, 1992a), at least for homonyms.   
Some clusters of studies have used common test suites, most notably the 2094-word line data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). For this preliminary experiment, we used the "line" dataset of a word sense disambiguation task (Leacock et al, 1993). Further details can be found in the Leacock et al (1993).  Others have used common test suites such as the 2094-word line data of Leacock et al (1993). In particular, we use subsets of the line data (Leacock et al, 1993) and the English lexical sample data from the SENSEVAL-2 comparative exercise among word sense disambiguation systems (Edmonds and Cotton, 2001).
It is well known that polysemous words usually have only one sense when used as part of a collocation or technical term (Yarowsky 1993). The procedure for inducing these semantic frames is as follows: 1. apply dependency parsing to a raw corpus and extract predicate-argument structures for each verb from the automatic parses, 2. merge the predicate-argument structures that have presumably the same meaning based on the assumption of one sense per collocation (Yarowsky, 1993) to get a set of initial frames, and 3. apply clustering to the initial frames based on the Chinese Restaurant Process (Aldous, 1985) to produce verb-specific semantic frames. For this merge, we assume one sense per collocation (Yarowsky, 1993) for predicate argument structures.  Much of the research in this area has been compromised by the fact that researchers have focussed on lexical ambiguities that are not true word sense distinctions, such as words translated differently across two languages (Gale, Church, and Yarowsky, 1992) or homophones (Yarowsky, 1993).    Following Yarowsky (1993), who explicitly addresses the use of collocations in the WSD work, we adopt his definition, adapted to our purpose: A collocation is a co-occurrence of two words in a defined relation.  These methods consist of simple rules that can reliably assign a sense to certain word categories: one sense per collocation (Yarowsky, 1993), and one sense per discourse (Gale et al, 1992). In the early nineties two famous papers claimed that the behavior of word senses in texts adhered to two principles: one sense per discourse (Gale et al, 1992) and one sense per collocation (Yarowsky, 1993). In order to analyze and compare the behavior of several kinds of collocations (cf. Section 3), Yarowsky (1993) used a measure of entropy as well as the results obtained when tagging heldout data with the collocations organized as decision lists (cf. Section 4). Compared to Yarowsky (1993), who also took into account grammatical relations, we only share the content-word-to-left and the content-word-to-right collocations. It is not clear how the smoothing technique proposed in (Yarowsky, 1993) could be extended to away ambiguities. Collocations for fine-gained word-senses are sensibly weaker than those reported by Yarowsky (1993) for two-way ambiguous words. This paper shows that the one sense per collocation hypothesis is weaker for fine grained word sense distinctions (e.g. those in WordNet): from the 99% precision mentioned for 2-way ambiguities in (Yarowsky, 1993) we drop to 70% figures. A well-known issue in the WSD area is the one sense per collocation claim (Yarowsky, 1993) stating that the word meanings are strongly associated with the particular collocation in which the word is located. We admit here that, while we have been aware of the fact for long time, only after the dissemination of the closely related hypotheses of one sense per discourse (Gale, Church and Yarowsky 1992) and one sense per collocation (Yarowsky 1993), we are able to articulate the hypothesis of one tokenization per source. Yarowsky (1993) indicated that the objects of verbs play a more dominant role than their subjects in WSD and nouns acquire more stable disambiguating information from their noun or adjective modifiers.
This is due, in part, to the availability of public hand-tagged material, e.g. SemCor (Miller et al, 1993) and the DSO collection (Ng & Lee, 1996). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al, 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. We base our experiments on SemCor (Miller et al, 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al, 1993) and their manual translations into Romanian. Existing hand annotated corpora like SemCor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all words track of the last Senseval competition (Snyder and Palmer, 2004). To train the classifiers for the all-words task we just used Semcor (Miller et al, 1993). This is roughly comparable with most frequent sense figures in standard annotated corpora such as Semcor (Miller et al, 1993) and the Senseval/Semeval data sets, which suggests that diversity may not play a major role in the current Google ranking algorithm. Cor text collection (Miller et al, 1993), a subset of the Brown corpus manually tagged with WordNet senses (37,176 sentences in 352 newspaper articles). To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model (HMM), based on that of Collins (2002), on the manually annotated Semcor corpus (Miller et al, 1993). The Semcor corpus (Miller et al, 1993), a fraction of the Brown corpus (Kucera and Francis, 1967) which has been manually annotated with Wordnet synset labels. Most of current all-words generic supervised WSD systems take SemCor (Miller et al, 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples. This includes sense ranks in WordNet, SemCor statistics (Miller et al, 1993), and similarity scores and rankings in Lin's resources. Unless specified otherwise, we use WordNet 1.7.1 (Milleret al, 1990) and the associated sense annotated SemCor corpus (Miller et al, 1993) (translated to WordNet 1.7.1 by Rada Mihalcea). ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al, 1993). Coarse-grained English All-Words LexPar and SynWSD were trained on an 1 million words corpus comprising the George Orwell's 1984 novel and the SemCor corpus (Miller et al, 1993). Then, we present a detailed comparison of their performance on SemCor (Miller et al, 1993). A Times and SemCor corpora (Milleretal., 1993), and used to generate a training corpus, with manually-annotated positive and negative examples of part-whole relations. We are using a subset of the SemCor texts (Miller et al, 1993) - five randomly selected files covering different topics: news, sports, entertainment, law, and debates - as well as the data set provided for the English all words task during SENSEVAL-2. We contrast the performance of first sense heuristics i) from SemCor (Miller et al, 1993) and ii) derived automatically from the BNC following (McCarthy et al, 2004) and also iii) an upper-bound first sense heuristic extracted from the test data.
In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al, 1994). The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al, 1994) for predictability. The Switchboard (Marcus et al, 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-crafted LFG is successfully scaled to parse the Penn-II treebank (Marcus et al, 1994) with discriminative (log linear) parameter estimation techniques. Figure 1: The tree structure for an imperative sentence part-of-speech is the WP in Penn Treebank (Marcus et al, 1994) POS tag set) or is an adverb (WRB). We will be reporting on results using PropBank (Kingsbury et al, 2002), a 300k-word corpus in which predicate argument relations are marked for part of the verbs in the Wall Street Journal (WSJ) part of the Penn Tree Bank (Marcus et al, 1994). We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank (Marcus et al, 1994) section 2-21. Note that the tag set used by ltpos is the Penn Treebank tag set (Marcus et al, 1994). The use of PCFG is tied to the annotation principles of popular tree banks, such as the Penn Treebank (PTB) (Marcus et al, 1994), which are used as a data source for grammar extraction. A second approach involves querying gold standard treebanks such as the Penn Treebank (Marcus et al, 1994) and Tiger Treebank (Brantset al, 2004) to determine the frequency of certain phenomena. In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Brown portion of the Penn Treebank. Bangalore et al (2001) investigate the effect of training size on performance while using grammars automatically extracted from the Penn II Treebank (Marcus et al, 1994) for generation. In the Penn Treebank (PTB) (Marcus et al, 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges. This results in several aspects that distinguish the MH treebank from, e.g., the WSJ Penn tree bank annotation scheme (Marcus et al, 1994). Also, the MH tree bank is much smaller than the ones for, e.g., English (Marcus et al, 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the all subtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). The idea is to augment the Penn Treebank (Marcus et al, 1994) constituent labels with the semantic role labels from the PropBank (Palmer et al, 2005), and generate a rich training corpus. Note that the Berkeley parser is trained on the Penn treebank (Marcus et al, 1994) yet the HPSG parser is trained on the HPSG tree bank (Miyao and Tsujii, 2008). We conducted two experiments on Penn Treebank II corpus (Marcus et al, 1994). The PennTreebank II (Marcus et al, 1994) marks subjects (SBJ), logical objects of passives (LGS), some reduced relative clauses (RRC), as well as other grammatical information, but does not mark each constituent with a grammatical role. The same idea was used by Magerman (1995), who developed the first "head table" for the Penn Treebank (Marcus et al, 1994), and Collins (1996), whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words.
Also (Miller et al, 1994) tagging semantically SemCor by hand, measure an error rate around 10% for polysemous words. First, the similarity and relatedness measures used in the system may rely on SemCor data (Miller et al., 1994). The sense-tagged corpus SEMCOR, prepared by (Miller et al, 1994), contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. However, as reported in (Miller et al, 1994), there are not enough training examples per word in SP. Among the existing sense-tagged corpora, the SEMCOR corpus (Miller et al, 1994) is one of the most widely used. It is a balanced corpus and it has more than 200K words that are manually sense tagged as a product of the semantic concordance (SemCor) effort using WordNet [Miller et al 1994]. Among the few currently available manually sense-annotated corpora for WSD, the SEMCOR (SC) corpus (Miller et al, 1994) is the most widely used. Bentivogli et al (2004) proposed an approach to create an Italian sense tagged corpus (MultiSemCor) based on the transference of the annotations from the English sense tagged corpus SemCor (Miller et al, 1994), by means of word alignment methods. (Miller et al, 1994) found that automatic assignment of polysemous words in Brown Corpus to senses in WordNet was 58% correct with a heuristic of most frequently occurring sense. The widely used SEMCOR (SC) corpus (Milleret al, 1994) is one of the few currently available manually sense-annotated corpora for WSD. We gathered training examples from parallel corpora, SEMCOR (Miller et al, 1994), and the DSO corpus. The SEMCOR corpus (Miller et al, 1994) is one of the few currently available, manually sense annotated corpora for WSD. The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al 1994, Ng & Hian 1996, and Grishman et al 1994) and for performing disambiguation (Resnik 1995 and Leacock et al). SEMCOR (Miller et al 1994) leaves these uses untagged. Here attested frequencies from SemCor (Miller et al, 1994) are used, so all ancestors are considered. Based on the lexicalized grammars, Bikel (2000) attempts at combining parsing and word sense disambiguation in a unified model, using a subset of SemCor (Miller et al, 1994).
Also, Ratnaparkhi et al (1994) conducted human experiments with a subset of their corpus. Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model. Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. The system achieved 89.0% on a similarly modified Ratnaparkhi et al (1994) dataset. They used a version of the Ratnaparkhi et al (1994) dataset that had all words lemmatized and all digits replaced by @. Their final system had 85.0% precision and 91.8% recall on the Ratnaparkhi et al (1994) dataset. The best performing systems for many tasks in natural language processing are based on supervised training on annotated corpora such as the Penn Treebank (Marcus et al, 1993) and the prepositional phrase dataset first described in (Ratnaparkhi et al, 1994). We did not use the PP data set described by (Ratnaparkhi et al, 1994) because we are using more context than the limited context available in that set (see below). For English, the average human performance on pp-attachment for the (v, n1, p, n2) problem formulation is just 88.2% when given only the four head-words, but increases to 93.2% when given the full sentence (Ratnaparkhi et al, 1994). A benchmark dataset of 27,937 such quadruples was extracted from the Wall Street Journal corpus by Ratnaparkhi et al (1994) and has been the basis of many subsequent studies comparing machine learning algorithms and lexical resources. Ratnaparkhi et al (1994) trained a maximum entropy model on (v, n1, p, n2) quadruples extracted from the Wall Street Journal corpus and achieved 81.6% accuracy. The maximum entropy approach of Ratnaparkhiet al (1994) uses the mutual information clustering algorithm described in (Brown et al, 1992). For our experiments we use the Wall Street Journal dataset created by Ratnaparkhi et al (1994). Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). Ratnaparkhi et al (1994) created a benchmark dataset of 27,937 quadruples (v, n1, p, n2), extracted from the Wall Street Journal. It has been used in a variety of difficult classification tasks such as part-of-speech tagging (Ratnaparkhi, 1996), prepositional phrase attachment (Ratnaparkhi et al, 1994) and named entity tagging (Borthwick et al, 1998), and achieves state of the art performance. A year later (Ratnaparkhi et al, 1994) published a supervised approach to the PP attachment problem. (Ratnaparkhi et al, 1994) used 20,801 tuples for training and 3097 tuples for evaluation. The difference in noun attachments between these two sets is striking, but (Ratnaparkhi et al, 1994) do not discuss this (and we also do not have an explanation for this). But it makes obvious that (Ratnaparkhi et al, 1994) were tackling a problem different from (Hindle and Rooth, 1993) given the fact that their baseline was at 59% guessing noun attachment (rather than 67% in the Hindle and Rooth experiments).
The Penn-style treebank for GENIA, created by Tateisi et al (2005), currently contains 500 abstracts. More detail, the tokenized text was done by GENIA tools, and the syntactic analyses was created by the McClosky-Charinak parser (McClosky and Charniak, 2008), trained on the GENIA Treebank corpus (Tateisi et al, 2005), which is one of the most accurate parsers for biomedical documents. Table 1 lists three corpora in the biomedical domain that are annotated with deep syntactic structures; CRAFT (described below), GENIA (Tateisi et al, 2005), and Penn BIOIE (Bies et al, 2005). We used the Stanford parser (Klein and Manning, 2003), and also a variant of the Stanford parser (i.e., Stanford-Genia), which was trained on the GENIA treebank (Tateisi et al, 2005) for biomedical text. We created the data set by building on the annotation of the GENIA Event corpus (Kim et al, 2008), making use of the rich set of annotations already contained in the corpus: term annotation for NEs and other entities (Ohta et al, 2002), annotation of events between these terms, and treebank structure closely following the Penn Treebank scheme (Tateisi et al, 2005). We first converted the gold standard annotation of the GENIA treebank (Tateisi et al, 2005) into a dependency representation using the Stanford parser tools (de Marneffe et al, 2006) and then determined the shortest paths in the dependency analyses connecting each relevant entity with each NE. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al, 2005). These expected benefits drive the development of domain-specific resources, such as the GENIA treebank (Tateisi et al., 2005), and parser domain adaption (Hara et al, 2007), which are of clear importance in parsing research, but of largely unconfirmed impact on practical systems. The parser was trained using 8,000 sentences from the GENIA Treebank (Tateisi et al, 2005), which contains abstracts of papers taken from MEDLINE, annotated with syntactic structures. For preprocessing, all the sentences in the Bio scope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al, 2005), which is a bracketed corpus in (almost) PTB style. Our biomedical data comes from the GENIA treebank (Tateisi et al, 2005), a corpus of abstracts from the Medline database. The task dataset consists of new annotations for the GENIA corpus (Kim et al, 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al, 2009) and the syntactic annotation (Tateisi et al, 2005) of the corpus. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA tree bank (Tateisi et al., 2005), and also applied a bidirectional part-of speech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA tree bank as a preprocessor. The BioNLP-09 shared task involved documents contained also in the GENIA treebank (Tateisi et al., 2005), creating an opportunity for direct study of intrinsic and task-oriented evaluation results. For comparison and evaluation, the texts in the GENIA treebank (Tateisi et al, 2005) are converted to the various formats as follows. The Penn-style treebank for GENIA, created by Tateisi et al (2005), currently contains 500 abstracts. The native dependency parsers were re-trained on the GENIA Treebank (Tateisi et al, 2005) conversions. The data sets for the COREF task are produced based on three resources: MedCO coreference annotation (Su et al, 2008), Genia event annotation (Kim et al, 2008), and Genia Treebank (Tateisi et al., 2005). In contrast, the GENIA Treebank Corpus (Tateisi et al, 2005) is estimated to have no imperative sentences and only seven interrogative sentences (see Section 5.2.2).
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. The experiments of closed tests on the second SIGHAN Bakeoff (Emerson, 2005) show that the joint model significantly outperforms the baseline models of both generative and discriminative approaches. The corpora provided by the second SIGHAN Bakeoff (Emerson, 2005) were used in our experiments. For more detailed information on the corpora, refer to Emerson (2005). Four training and testing corpora were used in the second bakeoff (Emerson, 2005), including the Academia Sinica corpus (AS), the Hong Kong City University Corpus (CU), the Peking University Corpus (PK) and the Microsoft Research Corpus (MR). We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al (2006) for comparison. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, successfully conducted four prior word segmentation bakeoffs, in 2003 (Sproat and Emerson, 2003), 2005 (Emerson, 2005), 2006 (Levow, 2006) and 2007 (Jin and Chen, 2007), and the bakeoff 2007 was jointly organized with the Chinese Information Processing Society of China (CIPS). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. For detailed info. of the corpora and these scores, refer to (Emerson, 2005). SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, conducted two prior word segmentation bakeoffs, in 2003 and 2005 (Emerson, 2005), which established benchmarks for word segmentation against which other systems are judged. As an evidence, the CWS evaluation campaign, the Sighan Bakeoff (Emerson, 2005) has been held four times since 2004. These specs were used in the second Sighan Bakeoff (Emerson, 2005). The current state-of-the-art segmentation software developed by (Low et al, 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. We used the data provided by the second SIGHAN Bakeoff (Emerson, 2005) to test the two segmentation models. In the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model. The data used was the Microsoft Research Beijing corpus from the Second International Chinese Word Segmentation Bakeoff (Emerson, 2005), and we used the same train/test split used in the competition. We did not explicitly test the utility of CRF-type features for improving recall on out-of-vocabulary items, but we note that in the Bakeoff, the model of Tseng et al (2005), which was very similar to our CRF-only system (only containing a few more feature templates), was consistently among the best performing systems in terms of test OOV recall (Emerson, 2005). After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as 'Achilles' that consists of four modules mainly: Regular expression extractor, dictionary-based N-gram segmentation, CRF-based subword tagging (Zhang et al, 2006), and confidence-based segmentation. We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. The second benchmark that we adopted is the SIGHAN Bakeoff-2005 dataset (Emerson, 2005) for Chinese word segmentation.
In ME model, the following features (Jin Kiat Low et al, 2005) are selected: a) cn (n=-2, -1, 0, 1, 2) b) cncn+1 (n=-2, -1, 0, 1) c) c-1c+1 where cn indicates the character in the left or right position n relative to the current character c0. The current state-of-the-art segmentation software developed by (Low et al, 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. 3) New feature templates were added, such as the templates that were used in representing numbers, dates, letters etc. (Low et al., 2005). Those for the 4-tag set, adopted from (Xue, 2003) and (Low et al., 2005), include C−2, C−1, C0, C1, C2, C−2C−1, C−1C0, C−1C1, C0C1 and C1C2. Third, the post processing method (Low et al, 2005) is employed to enhance the unknown word segmentation.  We add a new feature, which also used in maximum entropy model for word segmentation task by (Low et al, 2005), to the feature templates for CRF model while keep the other features same as (Zhao et al, 2006). 3) New feature templates were added, such as templates used in representing numbers, dates, letters etc. (Low et al., 2005). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). We use the maximum entropy segmenter of (Low et al, 2005) to segment the Chinese part of the FBIS corpus.  Our Chinese word segmenter is a modification of the system described by Low et al (2005), which they entered in the 2005 Second International Chinese Word Segmentation Bakeoff. Much of this can be attributed to the value of using an external dictionary and additional training data, as illustrated by the experiments run by Low et al (2005) with their model. It should be noted that in our testing during development, even when we strove to create a system which matched as closely as possible the one described by Low et al (2005), we were unable to achieve scores for the 2005 bakeoff data as high as their system did. Especially, character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). We utilized four of the five basic feature templates suggested in (Low et al, 2005), described as follows: Cn (n=-2, -1, 0, 1, 2), CnCn+ 1 (n=-2, -1, 0, 1), Pu (C0)? T (C-2) T (C-1) T (C0) T (C1) T (C2) where C refers to a Chinese character. See detail description and the example in (Low et al, 2005). Especially, the character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). Low et al (2005) introduce an external dictionary as features of a discriminative model.
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). (Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively.  They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching.  Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. English-Chinese: For training we used the LDC Sinorama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005).  For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005).  Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005).
Projects involving learner corpora in analyzing and categorizing learner errors include NICT Japanese Learners of English (JLE), the Chinese Learners of English Corpus (Gamon et al, 2008) and English Taiwan Learner Corpus (or TLC) (Wible et al, 2003). Most of these methods use large corpora of well-formed native English text to train statistical models, e.g. (Han et al., 2004), (Gamon et al., 2008) and (De Felice and Pulman, 2008). In our implementation we approach these tasks in a two-step approach as proposed in (Gamon et al, 2008). Our error correction system implements a correction validation mechanism as proposed in (Gamon et al., 2008). We build from Dickinson et al. (2010) in two main ways: first, we implement a presence-selection pipeline that has proven effective for English preposition error detection (cf. Gamon et al., 2008). We may also include heuristic-based filters, such as the ones implemented in Criterion (see Leacock et al, 2010), as well as a language model approach (Gamon et al, 2008). Gamon et al (2008) introduce a system for the detection of a variety of learner errors in non native English text, including preposition errors. Knight and Chander (1994) and Gamon et al (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification algorithm of choice. Training data are normally drawn from sizeable corpora of native English text (British National Corpus for DeFelice and Pulman (2007, 2008), Wall Street Journal in Knight and Chander (1994), a mix of Reuters and Encarta in Gamon et al (2008, 2009).  Gamon et al (2008) used word-based language models to detect and correct common ESL errors, while Leacock and Chodorow (2003) used part-of-speech bigram language models to identify potentially ungrammatical two-word sequences in ESL essays. For example, Tetreault and Chodorow (2008), Gamon et al (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. T&C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. Gamon et al (2008) train a decision tree model and a language model to correct errors in article and preposition usage. While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al, 2006) and prepositions (Gamon et al, 2008), (Felice and Pullman, 2009), there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). Gamon et al (2008) worked on a similar approach using only tagged trigram left and right contexts: a model of prepositions uses serves to identify preposition errors and the Web provides examples of correct form.  Gamon et al (2008) and Gamon (2010) use a combination of classification and language modeling.
Wu (1997) and Alshawi et al (2000) showed statistical models based on syntactic structure. Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al (2000). For a different approach that is based on dependency tree transformations, see Alshawi et al (2000). Yamada and Knight (2000, 2001) and Alshawi et al (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. Alshawi et al (2000) and Hwa et al (2005) explore transfer of deeper syntactic structure: dependency grammars. Other statistical machine translation systems such as (Wu, 1997) and (Alshawi et al, 2000) also produce a tree given a sentence. The latter are small and simple (Alshawi et al, 2000): tree nodes are words, and there need be no other structure to recover or align. However, the binary-branching SCFGs used by Wu (1997) and Alshawi et al (2000) are strictly less powerful than STSG. Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. (Alshawi et al, 2000) represents each production in parallel dependency trees as a finite-state transducer. Along similar lines, Alshawi et al (2000) treat translation as a process of simultaneous induction of source and target dependency trees using head transduction; again, no separate parser is used. Although hybrid approaches, such as dependency grammars augmented with phrase-structure information (Alshawi et al., 2000), can do re-ordering easily. Alshawi et al (2000) also presented a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers. Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer. In a somewhat related manner, Alshawi et al (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. It is described in Alshawi et al (2000b). Wu (1997) and Alshawi et al (2000) used unsupervised learning on parallel text to induce syntactic analysis that was useful for their respective applications in phrasal translation extraction and speech translation, though not necessarily similar to what a human annotator would select. (Alshawi et al, 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. The input to our algorithm is a corpus consisting of pairs of sentences related by an hierarchical alignment (Alshawi et al, 2000).
Melamed (2000) proposed statistical translation models to improve the techniques of word alignment by taking advantage of preexisting knowledge, which was more effective than a knowledge-free model. A typical case is the indirect association problem (Melamed, 2000), as shown in Figure 2 in which we want to translate the term s1 (s=s1). To reduce such errors and enhance the reliability of the estimation, a competitive linking algorithm, which is extended from Melamed's work (Melamed, 2000), is developed to determine the most probable translations. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR (s1, s2)= |LCS (s1, s2) |max (|s1|, |s2|) where LCS (s1, s2) is the longest common subsequence of s1 and s2, and |s| is the length of s. We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000). The utility of the constraint for parallel corpora has already been evaluated by Melamed (2000). Melamed (2000) has already established that most source words in parallel corpora tend to translate to only one target word. A direct association, as defined in (Melamed, 2000), is an association between two words (in this setting found by the TI+Cue method) where the two words are indeed mutual translations. A translation model is induced between phonemes in two word lists by combining the maximum similarity alignment with the competitive linking algorithm of Melamed (2000). One good overview is Melamed (2000). Chen et al (2008) used Competitive Linking Algorithm (CLA) (Melamed, 2000) to align the words to construct confusion network. CLA-based: Chen et al (2008) used competitive linking algorithm (CLA) (Melamed, 2000) to build confusion network for hypothesis regeneration. This view of alignment as graph matching is not, in itself, new: Melamed (2000) uses competitive linking to greedily construct matchings where the pair score is a measure of word-to-word association, and Matusov et al (2004) find exact maximum matchings where the pair scores come from the alignment posteriors of generative models. With just this feature on a pair of word tokens (which depends only on their types), we can already make a stab at word alignment, aligning, say, each English word with the French word (or null) with the highest Dice value (see (Melamed, 2000)), simply as a matching-free heuristic mode. As observed in Melamed (2000), this use of Dice misses the crucial constraint of competition: a candidate source word with high association to a target word may be unavailable for alignment because some other target has an even better affinity for that source word. In particular, the first alignment model we will present has already been described in (Melamed, 2000). As previously mentioned, this model is mostly identical to one already proposed in (Melamed, 2000). Additionally, as already argued in (Melamed, 2000), there are ways to determine the boundaries of some multi-words phrases (Melamed, 2002), allowing to treat several words as a single token. In order to compare the efficiency of the BP procedure to a more simple one, we reimplemented the Competitive Link Algorithm (abbreviated as CLA from here on) that is used in (Melamed, 2000) to train an identical model. 
This paper describes the parameters and tagsets (analogous to "Dialogue Act" tagging (Stolcke et al2000)), which they and three other groups have developed for this highly specialised domain. These are similar to some of the Dialogue Act labels used in NLP work: Stolcke et al's (2000) agreement, response acknowledgement, summarize, or VERBMOBIL's suggest, confirm, clarify (Jekat et al1995). Tremendous amounts of work has focused on this aspect (Stolcke et al, 2000). Stolcke et al (2000) used HMMs as a general model of discourse with an application to speech acts (or dialog acts) in conversations. These notions have been particularly fruitful in the dialog community, where dialog act tagging is a major topic of research; to cite just one prominent example: (Stolcke et al, 2000). Stolcke et al (2000) point out that the use of dialogue acts is a useful first level of analysis for describing discourse structure. We chose 12 tags by manually labelling the dialogue corpus using tags that seemed appropriate from the 42 tags used by Stolcke et al (2000) based on the Dialog Act Markup in Several Layers (DAMSL) tag set (Core and Allen, 1997). A super set of dialogue acts that covers all domains would necessarily be a large number of tags (at least the 42 identified by Stolcke et al (2000)) with many tags not being appropriate for other domains. Although this is higher than the results from 13 recent studies presented by Stolcke et al (2000) with accuracy ranging from 40% to 81.2%, the tasks, data, and tag sets used were all quite different, so any comparison should be used as only a guideline. Another main approach to robust dialogue processing has been statistical models for identifying dialogue acts (e.g., Stolcke et al (2000)). For instance, Stolcke et al (2000) explore n-gram models based on transcribed words and prosodic information for SWBD-DAMSL dialogue acts in the Switchboard corpus (Godfrey et al, 1992). The work of Stolcke et al (2000) found benefits to using Markov sequence models and prosodic features in addition to word features, but those benefits were relatively small, so for simplicity our experiments here use only word features and classify utterances in isolation. In this sort of situations, tag categories are often collapsed when running experiments so as to get meaningful frequencies (Stolcke et al, 2000). (Stolcke et al, 2000) reports an impressive 71% accuracy on transcribed Switchboard dialogues, using a tag set of 42 DAs. (Stolcke et al, 2000) employs a combination of HMM, neural networks and decision trees trained on all available features (words, prosody, sequence of DAs and speaker identity). Work in dialogue act tagging is also relevant, as it seeks to describe the actions and moves with which speakers display these types of positioning (Stolcke et al, 2000). This task involves defining the role of each contribution based on its function (Stolcke et al, 2000). As (Stolcke et al., 2000) report good accuracy (87%) for statement vs. question classification on manual Switchboard transcripts, such coarse-grained information might be reliably available. They model engagement using manually coded dialogue acts based on the SWBDL-DAMSL scheme (Stolcke et al, 2000). Classification over the Switchboard corpus has been demonstrated using Decision Trees (Verbree et al, 2006), Memory-Based Learning (Rotaru, 2002) and Hidden Markov Models (HMM) (Stolcke et al, 2000).
Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result. Teahan (Teahan et al, 2000) has successfully applied escape method D to segment Chinese text. Treating each character individually as in (Teahan et al, 2000) requires a large amount of training data in order to calculate all the probabilities in the tables, as well as a large amount of table space and time to lookup data from the tables. Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuur mans, 2001) and supervised learning (Teahan et al, 2000). For example, the N-gram generative language modeling based approach of Teahan et al (2000) does not use domain knowledge. In addition, compared to simple models like n-gram language models (Teahan et al, 2000), another shortcoming of CRF-based segmenters is that it requires significantly longer training time. This approach reduces to the cross-entropy/compression-based approach of (Teahan et al 2000). Second, the probabilistic models used in these methods (e.g. Teahan et al, 2000) are trained on a segmented corpus which is not always available. This is in contrast to supervised word segmentation algorithms (e.g., Teahan et al, 2000), which are typically used for segmenting text in documents written in languages that do not put spaces between their words like Chinese.
Vieira and Poesio (2000), extending this class to include only and a few others, make use of them in identifying discourse-new definite descriptions. Vieira and Poesio (2000) describe heuristics for processing definite descriptions in news text. We make use of the classification of bridging references proposed by Vieira and Poesio (2000). Markert et al (2003) used the Web and the construction method to extract information about hyponymy used to resolve other-anaphora (achieving an f value of around 67%) as well as the BDs in the Vieira-Poesio dataset (their results for these cases were not better than those obtained by (Vieira and Poesio, 2000)). DDs and PNs in associative relations account for 27% of all NPs in the test data, which is almost double the number of bridging cases (associative plus coreferent cases where head nouns are not the same) reported for newspaper texts in Vieira and Poesio (2000). One common approach involves the design of heuristic rules to identify specific types of (non) anaphoric NPs such as pleonastic pronouns (e.g., Paice and Husk (1987), Lappin and Leass (1994), Kennedy and Boguraev (1996), Denber (1998)) and definite descriptions (e.g., Vieira and Poesio (2000)). While the resolution of pronominal anaphora and tracking of named entities is possible with good accuracy, the resolution of definite NPs (having a common noun as their head) is usually limited to the cases that Vieira and Poesio (2000) call direct coreference, where both coreferent mentions have the same head. Vieira and Poesio (2000) proposed an algorithm for definite description (DD) resolution that incorporates a number of heuristics for detecting discourse-new descriptions. Vieira and Poesio (2000) proposed an algorithm for definite description resolution that incorporates a number of heuristics for detecting discourse-new (henceforth: DN) descriptions. (Vieira and Poesio, 2000) is run, which attempts to find an head-matching antecedent within a given window and taking premodification into account. The baseline algorithm without DN detection incorporated in GUITAR described above (i.e., only the direct anaphora resolution part of (Vieira and Poesio, 2000)). A major obstacle in the resolution of definite noun phrases with full lexical heads is that only a small proportion of them is actually anaphoric (ca. 30% (Vieira and Poesio, 2000)). However, R. Vieira and M. Poesio have recently shown in (Vieira and Poesio, 2000) that such an exhaustive search is not needed, because many noun phrases are not anaphoric at all - about 24365 of definite NPs in their corpus have no prior referents. In the system by Vieira and Poesio (2000), for example, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora. We note that around 80% of the definite NPs are anaphoric in our corpus, instead of the 50% presented in (Vieira and Poesio, 2000) for news paper texts. The Vieira/Poesio algorithm (Vieira and Poesio, 2000) attempts to classify each definite description as either direct anaphora, discourse-new, or bridging description. Uryupina (2003) and Vieira and Poesio (2000) also take capital and low case letters into account. A more fine-grained distinction is made by Bean and Riloff (1999) and Vieira and Poesio (2000) to distinguish restrictive from non-restrictive post modification by ommitting those modifiers that occur between commas, which should not be classified as chain starting. The precision and recall results obtained by these classifiers - tested on MUC corpora - are around the eighties, and around the seventies in the case of Vieira and Poesio (2000), who use the Penn Treebank. For the discourse-new classification task, the model's most important feature is whether the head word of the NP to be classified has occurred previously (as in Ng and Cardie (2002) and Vieira and Poesio (2000)).
With such a concept hierarchy as well as semantic relations with a precisely defined signature, we can for example overcome annotation problems of intensionality and predication as discussed in (van Deemter and Kibble, 2000). In line with (Krahmer and Piwek, 2000) and (van Deemter and Kibble, 2000) this is in our view a too strict definition of anaphora so that we propose a more relation-based classification of anaphoric and bridging relations. The core scheme is in principle identical with the MUC coreference scheme and is restricted to the annotation of coreference in the sense of (van Deemter and Kibble, 2000). Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. In contrast, they do not fall under the identity relation in OntoNotes, which follows the linguistic understanding of coreference according to which nominal predicates and appositives express properties of an entity rather than refer to a second (coreferent) entity (van Deemter and Kibble, 2000). In order to be linguistically accurate (van Deemter and Kibble, 2000), we distinguish between referring and attributive NPs: while the first point to an entity, the latter express some of its properties. Proposals for annotating coreference such as (Hirschman, 1998) have been motivated by work on Information Extraction, hence the notion of coreference used is very difficult to relate to traditional ideas about anaphora (van Deemter and Kibble, 2000). Although often treated together, anaphoric pronoun resolution differs from coreference resolution (van Deemter and Kibble, 2000). As van Deemter and Kibble (2000) point out, however, the result is rather ad hoc; the IDENT relation as defined by the instructions doesn't capture any coherent definition of coreference. see (van Deemter and Kibble, 2000) for some problems with the choices made in MUCCS. Coreference has been defined by (van Deemter and Kibble, 2000) as the relation holding between linguistic expressions that refer to the same extra linguistic entity. It suffers however from a number of problems (van Deemter and Kibble, 2000), chief among which is the fact that the one semantic relation expressed by the scheme, ident, conflates a number of relations that semanticists view as distinct: besides COREFERENCE proper, there are IDENTITY ANAPHORA, BOUND ANAPHORA, and even PREDICATION. Following van Deemter and Kibble (2000), we define a coreference relation to hold between two NPs just in case they refer to the same extra-linguistic referent in the real world. These arguments give support to van Deemter and Kibble (2000), who argue that it is problematic that the annotation strategy assumed in MUC-6 and MUC-7 goes beyond a mark-up of coreference. NP coreference is related to the task of anaphora resolution, whose goal is to identify an antecedent for an anaphoric NP (i.e., an NP that depends on another NP, specifically its antecedent, for its interpretation) [see van Deemter and Kibble (2000) for a detailed discussion of the difference between the two tasks].
Goldsmith (2001b; 2001a), inspired by de Marcken's (1995) thesis on minimum description length, attempts to provide both a list of morphemes and an analysis of each word in a corpus. (Goldsmith, 2001b), endeavor to learn to analyze the morphology of the language at hand in the manner of a linguist. Obviously the program needs to be systematically tested on multiple lexica from different languages, but these results strongly suggest that it is possible to model the acquisition of morphology as a component of learning to generate language directly, rather than to treat computational learning as the acquisition of linguistic theory as several current approaches do, e.g. (Goldsmith, 2001b). For example, the objective of the influential Linguistica program is 'to produce an output that matches as closely as possible the analysis that would be given by a human morphologist' (Goldsmith, 2001). We also experimented with Linguistica (Goldsmith, 2001), training on a large corpus, but results were worse than with Morfessor. Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwateret al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). In (Goldsmith, 2001) a recursive structure is proposed, such that stems can consist of a sub-stem and a suffix. Moreover, it performs better than a state-of-the-art morphology learning algorithm, Linguistica (Goldsmith, 2001), when evaluated on Finnish data. The new category-learning algorithm is compared to two other algorithms, namely the baseline segmentation algorithm presented in (Creutz, 2003), which was also utilized for initializing the segmentation in the category-learning algorithm, and the Linguistica algorithm (Goldsmith, 2001). Consider, for instance, the model of morphology described in Goldsmith (2001). Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001). Then, for each stem, the set of all affixes with which it appears (its signature, (Goldsmith, 2001)) is collected. The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). It is time consuming and, as Goldsmith (2001) explains, leaves difficult decisions of what constitutes a morpheme to on-the-fly subjective opinion. The evaluation should also be done with respect to well known systems like Linguistica (Goldsmith, 2001) or the morphological analyzer of Bernhard (2006). We compare our system with Linguistica (Goldsmith 2001), and discuss the advantages of the probabilistic paradigm over Linguistica's signature representation. These models include the signature of (Goldsmith 2001), the conflation set of (Schone and Jurafsky 2001), the paradigm of (Brent et. al. 2002), and the inflectional class of (Monson 2004). We compare the probabilistic paradigm to the signature model of (Goldsmith 2001). In this section, we compare our system with Linguistica (Goldsmith 2001), a freely available program for unsupervised discovery of morphological structure.
This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected.
Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001)). Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition.  The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank. We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search. In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999). The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights.  Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word. A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning. At the end one has a beam-width's number of best parses (Roark, 2001). To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses. The n-best lists were provided by Brian Roark (Roark, 2001).  Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001).  For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags). We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time. In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here. At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures.
Stevenson and Wilks (2001) presented a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999). Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning process.  This is also shown by Stevenson and Wilks (2001), who used the Longman Dictionary of Contemporary English (LDOCE) as sense inventory.  However, there has been no direct comparison of which knowledge sources are the most useful or whether combining a variety of knowledge sources, a strategy which has been shown to be successful for WSD in the general domain (Stevenson and Wilks, 2001), improves results.  In the hybrid approaches that have been explored so far, deep knowledge, like selectional preferences, is either pre-processed into a vector representation to accommodate machine learning algorithms, or used in previous steps to filter out possible senses e.g. (Stevenson and Wilks, 2001). We refer to this method as stacking, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). POS tags of the focus word itself are also included, to aid sense disambiguations related to syntactic differences (Stevenson and Wilks, 2001). Prior research (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)) suggests that use of both syntactic and lexical features will improve disambiguation accuracies.
(Merlo and Stevenson, 2001) use grammatical features (acquired from corpora) to classify verbs into three semantic classes: unergative, unaccusative, and object-drop. there has been some work on the acquisition of thematic roles, (e.g., Merlo and Stevenson, 2001). However, those works targeted a small subset of Levin classes, and a limited number of monosemous verbs; for example, Merlo and Stevenson (2001) studied three classes and 59 verbs, and Joanis et al (2008) focused on 14 classes and 835 verbs. The value of this feature is computed by the method of Merlo and Stevenson (2001). As a cue to this alternation, Merlo and Stevenson (2001) create a bag of head nouns for each of the two potentially alternating slots, and compare them. The method used by Merlo and Stevenson (2001) has the advantage of directly capturing similarity between slots (in terms of use of identical nouns [lemmas]), but fails to generalize over the nouns, lending itself to sparse data problems. Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulteim Walde, 2006 for German semantic verb classes. In particular, Merlo and Stevenson (2001) present a classification experiment which bears similarities to ours. The results reported in Tables 2 and 3 are significantly higher than those of Merlo and Stevenson (2001). Merlo and Stevenson (2001) present a method for verb classification which relies only on distributional statistics taken from corpora in order to train a decision tree classifier to distinguish between three groups of intransitive verbs. This paper presents experiments in automatic classification of the animacy of unseen Norwegian common nouns, inspired by the method for verb classification presented in Merlo and Stevenson (2001).  Based on our observation and previous studies (Merlo and Stevenson, 2001), we assume that each ILC has a distinct frequency distribution of roles on different grammatical slots. (Merlo and Stevenson, 2001) approximates diathesis alternations by hand-selected grammatical features. Merlo and Stevenson (2001) use corpus-based thematic role information to identify and classify unergative, unaccusative, and object-drop verbs. For open, the first three words in either position are the same. For the verb play, on the other hand, classified as an "object-drop" verb by Merlo and Stevenson (2001), we would expect overlap between the subject of transitive and intransitive uses. Merlo and Stevenson (2001) presented an automatic classification of three types of English intransitive verbs, based on argument structure and heuristics to thematic relations. This was the approach taken by Merlo and Stevenson (2001), who worked with a Decision Tree and selected linguistic cues to classify English verbs into three classes: unaccusative, unergative and object-drop. Based on our observation and previous studies (Merlo and Stevenson, 2001), we assume that each Levin class has a distinct frequency distribution of roles on different grammatical slots. In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes.
Decision tree algorithms were used for reference resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al (2001, C5.0). It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain. Soon et al (2001) use twelve features (see Table 1). Soon et al (2001) include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data. Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs. However, reference resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure (Soon et al, 2001). While the second best system (Bjorkelund and Farkas, 2012) followed the widely used baseline of Soon et al (2001), the winning system (Fernandes et al, 2012) proposed the use of a tree representation. For instance, the popular pairwise instance creation method suggested by Soon et al (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3). By using a simple co-reference resolution tool adapted from (Soon et al, 2001), we add all the mentions referring to the target into the extended target set. Instances are created following Soon et al (2001). Following Ng & Cardie (2002), our baseline system reimplements the Soon et al (2001) system. We start with a baseline system using all the features from Soon et al (2001) that were not removed in the feature selection process (i.e. DISTANCE). It supports both local (Soon et al (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al (2001) with an extended feature set. PAIRWISE: as in the work by Soon et al (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al, 2001) and (Ng and Cardie, 2002a). We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al, 2001). Soon et al (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al (2001). The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001).
Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). Pevzner and Hearst (2002, pp. 3-4) explain Pk well: a window of size k — where k is half of the mean manual segmentation length — is slid across both automatic and manual segmentations. Pevzner and Hearst (2002, pp. 5-10) identified that Pk: i) penalizes false negatives (FNs)2 more than false positives (FPs); ii) does not penalize full misses within k units of a reference boundary; iii) penalize near misses too harshly in some situations; and iv) is sensitive to internal segment size variance. To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task: the pk metric (Beeferman et al, 1999), WindowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. Pevzner and Hearst (2002) highlighted several problems of the Pk metric. Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels).
Therefore, van Deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations. Recent extensions address some of its shortcomings, such as negated and disjoined properties (van Deemter, 2002) and an account of salience for generating contextually appropriate shorter REs (Krahmer and Theune, 2002). The Context-Sensitive extension (Krahmer and Theune, 2002) is able to generate referring expressions for the most salient entity in a context; the Boolean Expressions algorithm (van Deemter, 2002) is able to derive expressions containing boolean operators, as in the cup that does not have a handle; and the Sets algorithm (van Deemter, 2002) extends the basic approach to references to sets, as in the red cups. Recently, algorithms have been applied to the identification of sets of objects rather than individuals [Bateman 1999, Stone 2000, Krahmer, v. Erk, and Verweg 2001], and the repertoire of descriptions has been extended to boolean combinations of attributes, including negations [van Deemter 2002]. This is because these operators appear only in embedded boolean combinations [van Deemter 2002], which are the basis for building larger varieties of expressions [Horacek 2004]. Subsequent work on referring expression generation has expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter, 2002), explored different search algorithms for finding the minimal description (e.g., Horacek (2003)) and offered different representation frameworks like graph theory (Krahmer et al, 2003) as alternatives to AVMs. Transformation Rules: In connection with reference to sets, it has been proposed to use the Q-M algorithm (McCluskey,) to find the shortest formula equivalent to a given input formula (van Deemter, 2002). GRE has been dominated by Dale and Reiter's (1995) Incremental Algorithm (IA), one version of which, generalised to deal with non-disjunctive plural references, is shown in Algorithm 1 (van Deemter, 2002). Such a description would be returned by a generalised version of Algorithm 1 proposed by van Deemter (2002). Unlike van Deemter (2002), we only focus on disjunction, leaving negation aside. Evaluation results showed that these principles are on the right track, with significantly better performance over a previous model (van Deemter, 2002). For instance, the classical Dale and Reiter algorithms compute purely conjunctive formulas; van Deemter (2002) extends this language by adding the other propositional connectives, whereas Dale and Haddock (1991) extends it by allowing existential quantification. Although we agree with van Deemter (2002) and others that the careful use of negation and disjunction can improve REs, these connectives must not be overused. Recently, algorithms have also been developed to the identification of sets of objects rather than individuals (Bateman 1999, Stone 2000, Krahmer, v. Erk, and Verweg 2001), and the repertoire of descriptions has been extended to boolean combinations of attributes, including negations (van Deemter 2002). An exception to this method is the work by Paraboni and van Deemter (2002) who use hierarchical object representations to refer to parts of a book (figures, sections., etc.).
Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. We follow the approach by Clark and Weir (2002) to create the test data. Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community.
Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. Generally speaking, these SRL approaches use a two-stage architecture: i) argument identification; ii) argument classification, to solve the task as a derivation of Gildea and Jurafsky's pioneer work (Gildea and Jurafsky, 2002). The features are listed as follows: Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky, 2002). Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al, 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word; head word, combination features of frame evoking word; phrase type, and combination features of voice; phrase type.  Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia).
We pursue a methodology based on Teufel and Moens (2002) where sentences are classified according to their argumentative role. Our methodology builds and extends the Teufel and Moens (Teufel and Moens, 2002) approach to automatic summarisation. It is also similar conceptually to content selection algorithms that have been used for text summarization (Teufel and Moens, 2002) and text generation (Sauper and Barzilay, 2009), both of which rely on finding highly-relevant passages within source texts. This approach to summarization was inspired by the process described in (Teufel and Moens, 2002). That work focused on the summarization of scientific articles to describe a new work in a way which rhetorically situates that work's contribution within the context of related prior work. The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al, 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. This subset of the corpus is similar in size to the corpus reported in (Teufel and Moens, 2002): the T & M corpus consists of 80 conference articles while ours consists of 40 HOLJ documents. Ultimately, human supervision may be required as in Teufel and Moens (2002), however we can make some observations about the automatic annotation methods above. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. For instance, Teufel and Moens (2002) summarize scientific articles by selecting rhetorical elements that are commonly present in scientific abstracts. Argumentation has typically been studied in relation to summarization (Teufel and Moens, 2002). Teufel and Moens (2002) present a coding scheme for scientific argumentation in research articles that is designed for automatic summarization of human-authored text. This requires adaptation of the high-level features used in AZ (Teufel and Moens, 2002) to chemistry. Previous work on rhetorical analysis of scientific articles focus on either; 1) hierarchical discourse relations between sentences (e.g. Mann and Thompson, 1987), 2) genre analysis within a descriptive framework (e.g. Swales 1990), or 3) ZI in a flat structure and a statistical evaluation of the annotation scheme from a machine learning perspective (e.g. Teufel and Moens, 2002). We follow the lines of (Teufel and Moens, 2002) and apply ZI to the domain of biology. Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers. As these schemes are too fine grained for abstracts (some of the categories do not appear in abstracts at all), we adopt a reduced version of AZ which integrates seven categories from (Teufel and Moens, 2002) and (Mizuta et al, 2005) those which actually appear in abstracts. We know from Teufel and Moens (2002) that verb tense and voice should be useful for recognizing statements of previous work, future work and work performed in the paper. These 16 categories refer to the dimension that is discussed under problem structure in (Teufel and Moens, 2002), rather than to exclusively rhetorical zones and are viewed as types of topics. It has been successfully applied for text content such as news articles, scientific papers (Teufel and Moens, 2002) that follow a discourse structure.
Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table. Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003).
Identifying sets of objects originally followed the incremental algorithm (Dale and Reiter 1995), as in (Bateman 1999), (Stone 2000) and (Krahmer et al 2003), with limited coverage, since only few attributes typically apply to all intended referents and to none of the potential distractors. For the first step, attribute selection, we use a version of the Graph-based REG algorithm of Krahmer et al (2003). (in line with the human preference for basic level values; cf. Krahmer et al 2003. We are currently exploring whether Krahmer et als (2003) graph-based approach to GRE is able to provide a better coverage of the data: this algorithm provides the ability to make use of different search strategies and weighting mechanisms when adding properties to a description, and such a mechanism might be used, for example, to counterbalance the Relational Algorithm's heavy bias towards the relations in this domain. The major exceptions here may be (a) van Deemter's (2002) algorithm for sets; recall that we excluded from the human data used here 16 references that involved sets; and, as noted above, (b) Krahmer et als (2003) graph-based approach to GRE, which may perform better than the Relational Algorithm on descriptions using relations. They distinguish three types of preciseness, i.e. precise, imprecise, or very imprecise pointing, and integrate pointing into the graph-based algorithm proposed by [Krahmer et al, 2003]. Algorithms for the generation of referring expressions commonly use this as a starting point, proposing that properties are organized in some linear order (Dale and Reiter, 1995) or weighted order (Krahmer et al, 2003) as input. Similarly, one of the strengths of the Graph-Based Algorithm (Krahmer et al, 2003) is its ability to generate expressions that involve relations between objects, and these include spatial ones (next to, on top of, etc.). In addition, we plan to experiment with assigning costs to planning operators in a metric planning problem (Hoffmann, 2002) in order to model the cognitive cost of an RE (Krahmer et al, 2003) and compute minimal-cost instruction sequences. (Krahmer et al, 2003) describe an approach to GRE in which a cost function guides search for a suitable description, and show that some existing GRE algorithms fit into this framework.  Recent work has formalised NLG algorithms for referring expression generation in terms of algorithms for finding an appropriate subgraph of a graph representing the domain knowledge [Krahmer et al, 2003]. The Graph based algorithm (Krahmer et al, 2003), for example, searches for the cheapest description for a target, and distinguishes cheap attributes (such as color) from more expensive ones (orientation). The use of vertex facts indicates that the representation is inspired by the Graph approach to referring expression generation [Krahmer et al, 2003]. We use the Graph-based algorithm of Krahmer et al (2003) for attribute selection. For attribute selection, we use the graph-based algorithm of Krahmer et al (2003), one of the highest scoring attribute selection methods in the TUNA 2008 Challenge (Gatt et al (2008), table 11). Although Krahmer et al claim that their method can handle n-ary relations (Krahmer et al, 2003), they provide node tails. A possible direction would be to enhance the algorithm proposed by Krahmer et al (Krahmer et al., 2003). The graph-based REG algorithm (Krahmer et al, 2003), for example, models preferences in terms of costs, where cheaper is more preferred. In the graph-based algorithm (Krahmer et al, 2003), which we refer to as Graph, information about domain objects is represented as a labelled directed graph, and REG is modeled as a graph-search problem.
We use a DP-based beam search procedure similar to the one presented in (Tillmann and Ney, 2003). The same beam-search pruning as described in (Tillmann and Ney, 2003) is used. Related works either deal with reordering in general as (Kanthak et al, 2005) or deal with local reordering as (Tillmann and Ney, 2003). Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. A monotone decoder similar to (Tillmann and Ney, 2003) with a trigram language model is set up for translations. However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al, 1999), and (Tillmann and Ney, 2003). The phrase-based decoder we use is inspired by the decoder described in (Tillmann and Ney, 2003) and similar to that described in (Koehn, 2004). The beam search algorithm attempts to find the translation (i.e., hypothesis that covers all source words) with the minimum cost as in (Tillmann and Ney, 2003) and (Koehn, 2004). The distortion cost is added to the log-linear mixture. The word reorderings that are explored by the search algorithm are controlled by two parameters s and w as described in (Tillmann and Ney, 2003). Machine Translation Performance using the NIST 2005 Bleu scorer scribed in (Tillmann and Ney, 2003). A beam search decoder similar to phrase-based systems (Tillmann and Ney, 2003) is used to translate the Arabic sentence into English. For details, see (Tillmann and Ney, 2003). Our implementation of a monotone-at-punctuation reordering constraint (Tillmann and Ney,2003) requires that all input words before clause separating punctuation have be translated, be forewords afterwards are covered. In (Tillmann and Ney, 2003), a beam-search algorithm used for TSP is adapted to work with an IBM-4 word-based model and phrase-based model respectively. Dynamic-programming based beam search algorithms are discussed for both word-based and phrase-based models by Tillmann and Ney (2003) and Tillmann (2006). In (Tillmann and Ney, 2003) and (Tillmann, 2006), the authors modify a certain Dynamic Programming technique used for TSP for use with an IBM4 word-based model and a phrase-based model respectively. For further details see e.g. (Tillmann and Ney, 2003). Investigations on the IBM constraints (Berger et al, 1996) for single-word based statistical machine translation can be found e.g. in (Tillmann and Ney, 2003). The paper contains the following original contributions: 1) the DP-based decoding algorithm in (Tillmann and Ney, 2003) is extended in a formal way to handle phrases and a novel pruning strategy with increased translation speed is presented 2) a novel alignment algorithm is presented that computes a phrase alignment efficiently in the case that it is consistent with an underlying word alignment. In these algorithms, a shortest-path search is carried out in one pass over some input along a specific 'direction': in speech recognition the search is time synchronous, the single-word based search algorithm in (Tillmann et al., 1997) is (source) position-synchronous or left-to-right, the search algorithm in (Niessen et al., 1998) is (target) position-synchronous or bottom-to-top, and the search algorithm in (Tillmann and Ney, 2003) is so-called cardinality-synchronous.
Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003).  The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). So we have the Internet: it is immense, free, easily accessible and can be used for all manner of language research (Kilgarriff and Grefenstette, 2003). Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003).
Approaches such as harvesting parallel corpora from the web (Resnik and Smith, 2003) address the creation of data. Besides, there are also some other types of methods for mining parallel corpora from the web such as the work in (Resnik, 1998), (Resnik and Smith, 2003) and (Zhang et al, 2006). Unfortunately, parallel corpora are not readily available in large quantities, except for a small subset of the world's languages (see Resnik and Smith (2003) for discussion), therefore limiting the potential use of current SMT systems. For example, Resnik and Smith (2003) propose mining the web to collect parallel corpora for low-density language pairs. Most existing studies, such as Nie (1999), Resnik and Smith (2003) and Shi (2006), mine parallel web documents within bilingual web sites first and then extract bilingual sentences from mined parallel documents using sentence alignment method. Resnik and Smith (2003) exploit the similarities in URL structure, document structure and other clues for mining the Web for parallel documents. Earlier work on corpus collection from the web (e.g. (Resnik and Smith, 2003)) gave some hope that reasonably large quantities of parallel text could be found on the web, so that a bitext collection could be built for interesting language pairs(with one member of the pair usually being English) relatively cheaply. At the other end of the spectrum, Resnik and Smith (2003) search the Web to detect web pages that are translations of each other. Resnik and Smith (2003) use a similar idea of candidates and filters in their STRAND system. These approaches are in essence a combination of the usage of lexico-syntactic pattens conveying a certain relation of interest such as in (Hearst, 1992), (Charniak and Berland, 1999), (Iwanska et al, 2000) or (Poesio et al, 2002) with the idea of using the web as a big corpus (Resnik and Smith, 2003), (Grefenstette, 1999), (Keller et al, 2002). STRAND (Resnik and Smith, 2003) is a system that acquires document pairs in parallel translation automatically from the Web. Due to its significant growth, the WWW has become an attractive database for different systems applications as, machine translation (Resnik and Smith, 2003), question answering (Kwok et al, 2001), commonsense retrieval (Matuszek et al, 2005), and so forth. Another approach to retrieving relevant documents involves the collection of relevant document URLs from the WWW (Resnik and Smith, 2003). This is for instance the case of PTMINER (Chen and Nie, 2000) and STRAND (Resnik and Smith, 2003), two systems that are intended to mine parallel documents over the Web. Resnik and Smith (2003) develop a method for gathering parallel corpora from the web. Resnik and Smith (2003) employ the Web as parallel corpora to provide bilingual sentences for translation models. (Resnik and Smith, 2003) show that parallel corpora for a variety of languages can be harvested on the Internet. Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine translation. Starting from nothing other than a set of language codes, our extension of the STRAND algorithm (Resnik and Smith,2003) identifies potentially parallel documents using cues from URLs and document content. Our system is based on the STRAND algorithm (Resnik and Smith, 2003).
Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. See Keller and Lapata (2003) for more issues. The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). We also use Keller and Lapata (2003)'s approach to obtaining web-counts. Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web.  Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5).  The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems.  NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003).
This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings. Reranking of n-best lists has recently become popular in several natural language problems, including parsing (Collins, 2003), machine translation (Och and Ney, 2002) and web search (Joachims, 2002). Collins (2003) uses both Markov Random Fields and boosting, Och and Ney (2002) use a maximum entropy ranking scheme, and Joachims (2002) uses a support vector approach. All the COL03 systems are results obtained using the restriction of the output of Collins (2003) parser. My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres. The corpora are first parsed using Collins's parser (Collins, 2003) with the boundaries of all the entity mentions kept. To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information. We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) - both sentence-final and sentence-initial - could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth. The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). Collins (2003) proposes to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies. A formal overview of the transformation and its correspondence to (Collins, 2003)'s models is available at (Hageloh, 2007). In a second set of experiments we use an unlexicalized head driven baseline a la (Collins, 2003) located on the (0, 0, 0) coordinate. In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a la (Collins,2003). This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it can not be collapsed into any one of the previous two. Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. Motivated by Collins syntactic parsing models (Collins, 2003), we consider the generation process for a hybrid sequence from an MR production as a Markov process. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al (2008).
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. To be particular, the method used by McCarthy and Carroll (2003) is formula (6). For example: McCarthy and Carroll (2003) use Li and Abe's method in a word sense disambiguation setting; Schulteim Walde et al (2008) use their MDL approach as part of a system for syntactic and semantic subcategorisation frame learning; Shutova (2010) deploys Resnik's method for metaphor interpretation. Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003).
To determine whether semantic restrictions are being violated, domain information from ontologies/thesauri such as WordNet could be used and/or statistical techniques as used by Mason (2004). Previous work on automated metaphor detection includes Fass (1991), Martin (1990), and Mason (2004). The CorMet system (Mason, 2004) dynamically mines domain specific corpora to find less frequent usages and identifies conceptual metaphors. Mason (2004) shows how statistical analysis can automatically detect and extract conventional metaphors from corpora, though creative metaphors still remain a tantalizing challenge.  By finding semantic differences between the selectional preferences, it can articulate the higher-order structure of conceptual metaphors ((Mason, 2004), p. 24), finding mappings like LIQUID -> MONEY. CorMet (Mason, 2004) is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from Google queries. The CMI system described in this paper is informed largely by CorMet (Mason, 2004).   Our method is different from automated work on metaphor recognition such as (Mason, 2004) and (Gedigian et al, 2006) in that it includes nouns as parts of speech.
We follow the notation of Di Eugenio and Glass (2004). To assess this classification task we also used the kappa statistics which yielded KCo=0.922 (following (Eugenio and Glass, 2004) we report Kas KCo, indicating that we calculate K a la Cohen (Cohen, 1960). Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement. But Di Eugenio and Glass (2004) have found that this interpretation does not hold true for all tasks. Ever since its introduction in general (Cohen, 1960) and in computational linguistics (Carletta, 1996), many researchers have pointed out that there are quite some problems in using κ (e.g. (Di Eugenio and Glass, 2004)), one of which is the discrepancy between p0 and κ for skewed class distribution.
In addition, Niessen and Ney (2004) decompose German words into a hierarchical representation using lemmas and morphological tags, and use a MaxEnt model to combine the different levels of representation in the translation model. There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). Niessen and Ney (2004) used morphological decomposition to get better alignments.   Niessen and Ney (2004) describe an approach for translation from German to English that combines verbs with associated particles, and also reorders questions. Niessen and Ney (2004) have used morphological decomposition to improve alignment quality. For the second kind, Niessen and Ney (2004) used morpho-syntactic information for translation between language pairs with scarce resources. (Niessen and Ney, 2004) describe a method that combines morphologically split verbs in German, and also reorders questions in English and German. Among these were cases that can be handled such as separable prefix verbs like 'aufzeigten' ('pointed out') (Niessen and Ney, 2000) or adjective compounds such as 'multidimensionale' ('multi dimensional').
Wiebe et al (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. Hedging is sometimes classed under the umbrella concept of subjectivity, which covers a variety of linguistic phenomena used to express differing forms of authorial opinion (Wiebe et al, 2004). This phenomenon, together with others used to express forms of authorial opinion, is often classified under the notion of subjectivity (Wiebe et al, 2004), (Shanahan et al, 2005). In contrast to the findings of Wiebe et al ((Wiebe et al, 2004)), who addressed the broader task of subjectivity learning and found that the density of other potentially subjective cues in the context benefits classification accuracy, we observed that the co-occurence of speculative cues in a sentence does not help in classifying a term as speculative or not. Following Wiebe et al (2004) we apply a unique feature. On the other hand, Wiebe et al (2004) have noted that hap ax legomena (terms that only appear once in a collection of texts) are good signs for detecting subjectivity. Accurate automatic analysis of these aspects of language will augment existing research in the fields of sentiment (Pang et al, 2002) and subjectivity analysis (Wiebe et al, 2004), but assessing the usefulness of analysis algorithms leveraging the Appraisal framework will require test data. We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al, 2004), the Polarity data set created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al, 2005). This paper is also not concerned with subjectivity (Wiebe et al, 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. Subjectivity lexicon (Wiebe et al, 2004) is a resource that annotates words with tags like parts-of-speech, prior polarity, magnitude of prior polarity (weak/strong), etc. For example, detecting subjective sentences, expressions, and other opinionated items in documents representing certain press categories (Wiebe et al, 2004) and measuring strength of subjective clauses (Wilson et al, 2004). Wiebe et al (2004) focused on the detection of subjective language such as opinions, evaluations, or emotions in text. More generally, (Wiebe et al 2004) and subsequent work focused on the analysis of subjective language in narrative text, primarily news. There have been attempts on tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). Wiebe et al (2004) proposed that whether a sentence is subjective or objective should be discriminated according to the adjectives in it. We believe that this relaxation can be done in that particular case, as adjectives are much more likely to convey opinions a priori than verbs (Wiebe et al 2004).  First, we investigate the utility of applying a UNIQUE feature (Wiebe et al, 2004) where low frequency words below a threshold are replaced with the token UNIQUE. This group includes two features that have been employed in various SSA studies. Unique: Following Wiebe et al (2004), we apply a UNIQUE (Q) feature: We replace low frequency words with the token UNIQUE. In addition to Appraisal Theory, subjectivity annotation of text in context has also been performed in Yu and Hatzivassiloglou (2003), Bruce and Wiebe (1999), and Wiebe et al (2004).
Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) to all 5-grams. The retrieval results are then used as input to a standard SMT pipeline to train translation models, starting from unsupervised induction of word alignments (Och and Ney, 2000) to phrase-extraction (Och and Ney, 2004) and phrase-based decoding (Koehn et al, 2007). Based on these word-alignments, we extract phrases by applying the grow-diag-final-and heuristic and using Och and Ney (2004)'s phrase extraction algorithm as implemented in Moses (Koehn et al, 2007). Our phrase-based SMT system is similar to the alignment template system described in Och and Ney (2004). Various feature functions (Och and Ney, 2004) are then computed over the entries in the phrase table. Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004). Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link. This discussion, which is partly based on Section 4.1.2 of (Och and Ney, 2004), means that the lexical translation probability pw (f|e) is another probability estimated using the word translation probability w (f|e). As described above, our base system is a phrase based statistical MT system, similar to that of Och and Ney (2004). Galley and Manning (2008) propose an algorithm that begins by running standard phrase extraction (Och and Ney, 2004) without a phrase-length limit, noting the corners of each phrase found. We consider the standard phrase-based approach to MT (Och and Ney, 2004). Breadth-first beam search algorithm of Och and Ney (2004). We assume familiarity with these operations, which are described in detail in (Och and Ney, 2004). Here we refer to Bilingual Phrase (BP) as the bilingual phrases used by Och and Ney (2004). The extraction of SSRs is similar to the well known phrase extraction algorithm (Och and Ney, 2004). For example, given a source phrase f and a target phrase e, the phrase pair (f, e) is said to be consistent (Och and Ney, 2004) with the alignment if and only if: (1) there must be at least one word in side one phrase aligned to a word inside the other phrase and (2) no words inside one phrase can be aligned to a word outside the other phrase. Och and Ney (2004) describe a phrase-extract algorithm for extracting phrase pairs from a sentence pair annotated with a 1-best alignment. The methods for calculating relative frequencies (Och and Ney, 2004) and lexical weights (Koehn et al, 2003) are also adapted for the weighted matrix case. First, many-to-many word alignments are induced by running a one-to-many word alignment model, such as GIZA++ (Och and Ney, 2003), in both directions and by combining the results based on a heuristic (Och and Ney, 2004).
Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. Furthermore, the work in this paper relates to Bikel (2004)'s work. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion. They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing.     We used Bikel's reimplementation of Collins' parsing model 2 (Bikel, 2004). Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance. We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney,2000). For English we use the Bikel parser default head word rules (Bikel, 2004). 
Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. In our work, we included all features described in (Collins and Koo, 2005). We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. There is also work on discriminative models for parse reranking (Collins and Koo, 2005). These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN).
To some extent, function labels overlap with semantic role labels as defined in PropBank (Palmer et al, 2005). Our results confirm the findings in (Palmer et al, 2005). For our experiments on semantic role labeling we used PropBank annotations (Palmer et al, 2005). Example of Semantic Role Labeling from the PropBank dataset (Palmer et al, 2005). We focus our experimental study on the semantic role labeling problem (Palmer et al, 2005): being able to give a semantic role to a syntactic constituent of a sentence, i.e. annotating the predicate argument structure in text. FrameNet (Baker et al, 1998) and the Proposition Bank (Palmer et al, 2005), or PropBank for short, are the two main systems currently developed for semantic role-labeling annotation. The inter-annotator agreement for PropBank reported in (Palmer et al, 2005) is above 0.9 in terms of the Kappa statistic (Sidney and Castellan Jr., 1988). Recently, large corpora have been manually annotated with semantic roles in FrameNet (Fillmore et al, 2001) and PropBank (Palmer et al, 2005). Instead, we resort to Semantic Role Labeling (Palmer et al, 2005) to provide more lexicalized and semantic constraints to select the candidates. In the first step, we adopt the definitions found in PropBank (Palmer et al, 2005), defining our own frame sets for verbs not in Prop Bank, such as phosphorylate. As proposition banks are semantically annotated versions of a Penn-style tree bank, they provide consistent semantic role labels across different syntactic realizations of the same verb (Palmer et al, 2005). In addition to these syntactic structures, it was also annotated with predicate-argument structures (WSJ proposition bank) by Palmer et al (2005). Propbank (Palmer et al., 2005): This is a semantic annotation of the Wall Street Journal section of Penn Treebank-2. Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al 2010), PropBank (Palmer et al 2005) and the word-sense part of OntoNotes (Weischedel et al 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al 2008)). More recent lexical resources have been built as semantic concordances from the very beginning (PropBank (Palmer et al 2005), OntoNotes word senses (Weischedel et al 2011)). Two other, somewhat different, lexical resources have to be mentioned to complete the picture: FrameNet (Ruppenhofer et al 2010) and PropBank (Palmer et al 2005). The overall IAA measured on verbs was 94% (Palmer et al 2005). For our experiments, we use an enhanced version of the CCGbank (Hockenmaier and Steedman, 2007) - a corpus of CCG derivations derived from the Penn Treebank - with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008). The PropBank project (Palmer et al,2005) is another popular resource related to semantic role labeling. Thus, Narayanan and Harabagiu (2004) apply the argument-predicate relationship from PropBank (Palmer et al, 2005) together with the semantic frames from FrameNet (Baker et al, 1998) to create an inference mechanism to improve QA.
To evaluate the grammaticality of our generated summaries, following common practice (Barzilay and McKeown, 2005), we randomly selected 50 sentences from original conversations and system generated abstracts, for each dataset. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from "meme" tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the frame work used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non-overlapping summary sentence. This is the other way around compared to the English dependency such as in Barzilay and McKeown (2005). Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). Recent abstractive approaches, such as sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009) and sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009) have focused on rewriting techniques, without consideration for a complete model which would include a transition to an abstract representation for content selection. The work of (Barzilay and McKeown, 2005) on sentence fusion shows an example of re-using the same syntactical structure of a source sentence to create a new one with a slightly different meaning. The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. In our experiments, dependency parsing is accomplished with Minipar (Lin, 1998) and alignment is done using a bottom-up tree alignment algorithm (Barzilay and McKeown, 2005) modified to account for the shallow semantic role labels produced by the parser. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009), and a generation based approach that could be called sentence splitting (Genest and Lapalme, 2011). Sentence fusion is a text-to-text generation application, which given two related sentences, outputs a single sentence expressing the information shared by the two input sentences (Barzilay and McKeown 2005). Barzilay and McKeown (2005) argue convincingly that employing such a fusion strategy in a multidocument summarization system can result in more informative and more coherent summaries. In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005).
Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two. The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set. Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio of source and target sentence has to be smaller than 2.
Its applications include word sense disambiguation, text summarization and information retrieval (Budanitsky and Hirst, 2006). In addition, those methods that are based on hierarchical, taxonomically structured resources are generally better suited for measuring semantic similarity than relatedness (Budanitsky and Hirst, 2006). In particular, WordNet based measures are well known to be better suited to measure similarity than relatedness due to its hierarchical, taxonomic structure (Budanitsky and Hirst, 2006). The semantic distance can be got by the usage of lexicon, such as WordNet (Budanitsky and Hirst, 2006). Semantic relatedness can denote every possible relation between two concepts, unlike semantic similarity, which typically denotes only certain hierarchical relations (like hypernymy and synonymy) and is often computed using hierarchical networks like WordNet (Budanitsky and Hirst, 2006). Budanitsky and Hirst (2006) provide an extensive survey of these measures. Budanitsky and Hirst (2006) also point out an important distinction, between relatedness and similarity. Following Budanitsky and Hirst (2006), we consider two frames as similar if they are linked via is-a like relations (e.g. GETTING and COMMERCE BUY), while as related if any relation stands between them (e.g. causation between KILLING and DEATH). We also experiment with the Jiang and Conrath's (Jiang and Conrath, 1997) measure which relies only on the is-a hierarchy, but proved to be the best WordNet-based measure in the task of ranking words (Budanitsky and Hirst, 2006). A direct comparison to the word ranking task, suggests that ranking frames is harder than words, not only for humans (as reported in Section 3.2), but also for machines: Budanitsky and Hirst (2006) show that measures for ranking words get much closer to the human upper-bound than our measures do, confirming that frame relatedness is a fairly complex notion to model. Following Budanitsky and Hirst (2006), we estimate the WordNet sense similarity using the method proposed by Jiang and Conrath (1997). For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs.  SCM and SPE capture the two most important parameters of measuring semantic relatedness between terms (Budanitsky and Hirst, 2006), namely path length and senses depth in the used thesaurus. The reader can consult Budanitsky and Hirst (2006) to confirm that all the other measures of semantic relatedness we compare to, do not follow the same pattern as the human ratings, as closely as our measure of relatedness does (low y values for small x values and high y values for high x). There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, model semantic relatedness by computing the shortest path between two concepts in the knowledge base and/or by looking at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). There is a large body of work on using WordNet to compute measures of lexical similarity (Budanitsky and Hirst, 2006). The ability to determine semantic relatedness between terms is useful for a variety of nlp applications, including word sense disambiguation, information extraction and retrieval, and text summarisation (Budanitsky and Hirst, 2006).
Veale (2004) used WordNet to answer 374 multiple-choice SAT analogy questions, achieving an accuracy of 43%, but the best corpus-based approach attains an accuracy of 56% (Turney, 2006). The template we use here is similar to Turney (2006), but we have added extra context words before the X and after the Y. Turney (2006) also selects patterns based on the number of pairs that generate them, but the number of selected patterns is a constant (8000), independent of the number of input word pairs. Turney (2006) used a corpus-based algorithm. The best previous result is an accuracy of 56.1% (Turney, 2006). The average senior high school student achieves 57% correct (Turney, 2006). PairClass generates probability estimates, whereas Turney (2006) uses a cosine measure of similarity. The automatically generated patterns in PairClass are slightly more general than the patterns of Turney (2006). The morphological processing in PairClass (Minnen et al, 2001) is more sophisticated than in Turney (2006). Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpus based approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. This aspect is most striking for ukWaC where the coverage is low and by only utilizing the single-occurrence sub-vectors we obtain a performance of 38.2% correct answers (the comparable "attributional" models reported in Turney, 2006, have an average performance of 31%). Vector-based distributional similarity methods have proven to be a valuable tool for a number of tasks on automatic discovery of semantic relatedness between words, like synonymy tests (Rapp, 2003) or detection of analogical similarity (Turney, 2006). On a structural level, the prediction of meta alternations shows a clear correspondence to analogy prediction as approached in Turney (2006) (carpenter:wood is analogous to mason:stone, but not to photograph:camera). We solve SAT analogies with a simplified version of the method of Turney (2006). We use this space to measure "relational" similarity (Turney, 2006) of concept pairs, e.g., finding that the relation between teachers and handbooks is more similar to the one between soldiers and guns, than to the one between teachers and schools. The Attr cells summarize the performance of the 6 models on the wiki table that are based on "attributional similarity" only (Turney, 2006). In particular, we need to develop a backoff strategy for unseen pairs in the relational similarity tasks, that, following Turney (2006), could be based on constructing surrogate pairs of taxonomically similar words found in the CxLC space. Some of the recent work on this problem includes that of Butnariu et al (2009), Girju (2007), Girju et al (2005), Kim and Baldwin (2005), Nakov (2008), Nastase et al (2006), Turney (2006), and Saghdha and Copestake (2009). The distinction between lexical and relational similarity for word pair comparison is recognised by Turney (2006) (he calls the former attributional similarity), though the methods he presents focus on relational similarity. Turney (2006) describes a method (Latent Relational Analysis) that extracts subsequence patterns for noun pairs from a large corpus, using query expansion to increase the recall of the search and feature selection and dimensionality reduction to reduce the complexity of the feature space.
See (Chiang, 2007) for more details. In this section, we express the hierarchical phrase based extraction technique of (Chiang, 2007) as an extraction program. For instance, (Chiang, 2007) lists six criteria that he uses in practice to restrict the generation of Hiero rules. Hierarchical Phrase-based Machine Translation, proposed by Chiang (Chiang, 2007), uses a general non-terminal label X but does not use linguistic information from the source or the target language. The hierarchical phrase-based model (Chiang, 2007) makes an advance of statistical machine translation by employing hierarchical phrases, which not only uses phrases to learn local translations but also uses hierarchical phrases to capture reorderings of words and subphrases which can cover a large scope. We evaluate the distribution of these rules in the same way as Chiang (2007). So we follow the method described in Chiang (2007) to filter the rule set except that we allow two nonterminals to be adjacent. We compared our loose decoder and tight decoder with our in-house hierarchical phrase-based decoder (Chiang, 2007) and the tree-to-string decoder (Liu et al., 2006). Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a subtranslation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right. The complexity of this dynamic programming algorithm for g-gram decoding is O(2nn 2 |V | g−1 ) where n is the sentence length and |V | is the English vocabulary size (Huang and Chiang, 2007). This is also the case with other syntax-based models like Hiero or GHKM: language model integration overhead is the most significant factor that causes syntax-based decoding to be slow (Chiang, 2007). Following previous work (Chiang, 2007), we assume a constant number of English translations for each foreign word in the input sentence, so |V|= O (n). The work of Watanabe et al (2006) is closest in spirit to ours: they also design an incremental decoding algorithm, but for the hierarchical phrase-based system (Chiang, 2007) instead. A Hiero grammar (Chiang, 2007) is an SCFG with only one type of nonterminal symbol, traditionally labeled X. Chiang (2007) gives reasonable heuristic choices for these parameters when extracting a Hiero grammar, and Lopez (2008) confirms some of them (maximum rule span of 10, maximum number of sourceside symbols at 5, and maximum number of nonterminals at 2 per rule). Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed. Venugopal et al (2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007). The first decoder, Hiero Cube Pruning (HCP), is a k-best decoder using cube pruning implemented as described by Chiang (2007). For the HCP system, MET is done following Chiang (2007). A language model was incorporated using cube pruning (Huang and Chiang, 2007), using a 200 best limit at each node during LM integration.
For the identification and labeling steps, we train a maximum entropy classifier (Berger et al, 1996) over sections 02-21 of a version of the CCGbank corpus (Hockenmaier and Steedman, 2007) that has been augmented by projecting the Propbank semantic annotations (Boxwell and White, 2008). We used the CCGbank-style dependency output of the parser (Hockenmaier and Steedman,2007), which is a directed graph of head-child relations labelled with the head's lexical category and the argument slot filled by the child. CCGbank (Hockenmaier and Steedman, 2007) is a corpus of CCG derivations that was semiautomatically converted from the Wall Street Journal section of the Penn treebank. For example, declarative sentences are S [dcl], wh-questions are S [wq] and sentence fragments are S [frg] (Hockenmaier and Steedman, 2007). Bos et al (2009) created a CCGbank from an Italian dependency tree bank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). The resource used for building wide-coverage CCG parsers of English is CCGbank (Hockenmaier and Steedman, 2007), a version of the Penn Treebank in which each phrase-structure tree has been transformed into a normal-form CCG derivation. Our experiments were performed using CCGBank (Hockenmaier and Steedman, 2007), which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). CCGbank was derived from the PTB, and so it might be considered that converting back to the PTB would be a relatively easy task, by essentially reversing the mapping Hockenmaier and Steedman (2007) used to create CCGbank. In our experience, trying to add number and animacy agreement constraints to a grammar induced from the CCGbank (Hockenmaier and Steedman, 2007) turned out to be surprisingly difficult, as hard constraints often ended up breaking examples that were working without such constraints, due to exceptions, sub-regularities and acceptable variation in the data. For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. We agree; however, our ultimate motivation is to use this work to tackle bootstrapping from very small tag dictionaries or dictionaries obtained from linguists or resources other than a corpus, and for tag sets that are more ambiguous (e.g., super tagging for CCGbank (Hockenmaier and Steedman, 2007)). This logical form can be expressed in many ways; we will focus on the dependency representation used in CCGbank (Hockenmaier and Steedman, 2007). All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. The grammar is automatically extracted from a version of the CCGbank (Hockenmaier and Steedman, 2007) with Propbank (Palmer et al, 2005) roles projected onto it (Boxwell and White, 2008). Hockenmaier and Steedman (2007) showed that a CCG corpus could be created by adapting the Penn Treebank (Marcus et al, 1993). We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al, 2004) and LFG (Cahill et al, 2002). CCGbank (Hockenmaier and Steedman, 2007) extends this grammar with a set of type-changing rules, designed to strike a better balance between sparsity in the category set and ambiguity in the grammar.
Clark and Curran (2007) show that both accurate and highly efficient parsing is possible using a CCG. From a parsing perspective, the C & C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al, 2009). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. Following Clark and Curran (2007), we assume that each input word has been assigned a POS-tag (from the Penn Treebank tag set) and a set of CCG lexical categories. Clark and Curran (2007) gives a more precise definition. We ran the C & C parser using the normal-form model (we reproduced the numbers reported in Clark and Cur ran (2007)), and copied the results of the hybrid model from Clark and Curran (2007), since the hybrid model is not part of the public release. The numbers for C & C are for the hybrid model, copied from Clark and Curran (2007). The numbers for the normal-form model are evaluated by running the publicly available parser, while those for the hybrid dependency model are from Clark and Curran (2007). Following Hockenmaier (2003), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman, 2007), rather than defining the combinatory rule schema manually as in Clark and Curran (2007). When it is reasonable to assume that the input sentence for the grammaticality improvement system is sufficiently fluent, a list of candidate lexical categories can be assigned automatically to each word via super tagging (Clark and Curran, 2007) on the input sequence. The average number of lexical categories per word drops to 1.3 when equals 0.075, which is the value used for parsing newspaper text in Clark and Curran (2007). However, compared to the 93% lexical category accuracy of a CCG parser (Clark and Curran, 2007), which also uses a level of 0.075 for the majority of sentences, the accuracy of our grammaticality improvement system is much lower. We parsed both corpora using the C & C parser (Clark and Curran, 2007) as we employ both GR and POS information in our learning method. We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art PennTreebank (PTB) parser. Examples of this approach include Riezler et al (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007). The formalism-based parser we use is the CCG parser of Clark and Curran (2007), which is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. The CCG parser has been extensively evaluated elsewhere (Clark and Curran, 2007), and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees. Since this short paper reports a small, focused research contribution, we refer readers to Clark and Curran (2007) and Petrov and Klein (2007) for details of the two parsers. The schemas were developed by manual inspection using section 00 of CCGbank and the PTB as a development set, following the oracle methodology of Clark and Curran (2007), in which gold standard derivations from CCGbank are converted to the new representation and compared with the gold standard for that representation. While high quality syntactic parsers are able to efficiently annotate large quantities of English text (Clark and Curran, 2007), existing approaches to query do not work on the same scale.
Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. We adopt Barzilay and Lapata (2008)'s entity based local coherence model to represent a document by an entity grid, and extract local transitions among entities in continuous discourse constituents. A prominent example is the entity-based model by Barzilay and Lapata (2008). Adapted from the introduction to Barzilay and Lapata (2008). We follow Barzilay and Lapata (2008) and use the Fisher Sign test. The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al, 1995). Their local model of discourse coherence is based on the entity-grid (Barzilay and Lapata, 2008), as well as on the lexicalized IBM model (see Section 4.6 above); we have experimented with both, and showed that they have a minimal effect on grading performance with the FCE dataset. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Barzilay and Lapata (2008) introduce the entity grid as a method of representing the coherence of a document. In Barzilay and Lapata (2008), an entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). This set is larger than the set that was used in Experiment 1 of Barzilay and Lapata (2008), which consists of 400 documents in two English subcorpora on earthquakes and accidents respectively. Barzilay and Lapata (2008) found that grammatical role improves performance in this task for an English corpus. The results we obtain are higher than the results for the English corpora of Barzilay and Lapata (2008) (87.2% on the Earthquakes corpus and 90.4% on the Accidents corpus), but this is probably due to corpus differences as well as the availability of perfect coreference information in our experiments. Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. We show that our multiple-rank model outperforms B & L's basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference + and Coreference -. Two evaluation tasks for Barzilay and Lapata (2008)'s entity-based model are sentence ordering and summary coherence rating. Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents).
These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results.  Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task.  This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars.
Toutanova et al (2008) presented a re-ranking model to jointly learn the semantic roles of multiple constituents in the SRL task. We also compare with the multi parse system of (Toutanova et al, 2008) which uses a global joint model using multiple parse trees. Consider the following examples, due to Toutanova et al (2008): (3) [Temporal The day] that [arg0 the ogre] [Predicate cooked] [arg1 the children] is still remembered. The complex SRL architectures proposed (usually combining local and global, i.e. joint, models of argument classification, e.g. (Toutanova et al., 2008)) require a large number of annotated examples. Most of the CoNLL 2005 systems show a significant performance drop when the tested corpus, i.e. Brown, differs from the training one (i.e. Wall Street Journal), e.g. (Toutanova et al, 2008). Learning from richer linguistic descriptions of more complex structures is proposed in (Toutanova et al, 2008). In (Toutanova et al, 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. This approach effectively introduces a new step in SRL, also called Joint Re-ranking, (RR), e.g. (Toutanova et al., 2008) or (Moschitti et al., 2008). Toutanova et al (2008), Johansson and Nugues (2008), and Bjorkelund et al (2009) presented importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis. While there are a number of existing tools for performing these tasks based on the linguistic context (e.g., Toutanova et al, 2008, Erk and Pado, 2006), their performance is only moderate (e.g., Agirre et al 2007). Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). Similar to Toutanova et al (2008), we propose to use global role ordering preferences but in a generative model in contrast to their discriminative one. Toutanova et al (2008) currently have the best performing SRL system on the Brown corpus test set with an F1 score of 68.81 (80.8 for the WSJtest). Indeed, when SRL systems use gold standard parses, they tend to perform extremely well (Toutanova et al, 2008).   These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al,2008). Toutanova et al (2008) and Punyakanok et al (2008) presented a re-ranking model and an integer linear programming model respectively to jointly learn a global optimal semantic roles assignment. Reranking has become a popular technique for solving various structured prediction tasks, such as phrase-structure (Collins, 2000) and dependency parsing (Hall, 2007), semantic role labeling (Toutanova et al 2008) and machine translation (Shen et al 2004). For instance, the confusion matrix in (Toutanova et al, 2008) indicates that their model scores 99.5% accuracy on this task.
We follow the approach in (Punyakanok et al, 2008) in framing the SRL problem as a two-stage pipeline: identification followed by labeling. In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). Finally, the system described in (Punyakanok et al., 2008) uses a joint inference model to resolve discrepancies between multiple automatic parses. First, others (Punyakanok et al, 2008, e.g.), have found that different parsers have different error patterns, and so using multiple parsers can yield complementary sources of correct information. The results for gold standard parses are comparable to the winning system of the CoNLL 2005 shared task on semantic role labeling (Punyakanok et al, 2008). These annotations were obtained by automatically semantic role labelling portions of CHILDES with the system of Punyakanok et al (2008) before roughly hand-correcting them (Connor et al, 2008). Punyakanok et al. (2008) and Toutanova et al. (2008) used global inference to ensure that the predictions across all arguments of the same predicate are coherent. We re-implemented the system of Punyakanok et al (2008), which we briefly describe here, to serve as our baseline verb semantic role labeler. The verb SRL system of Punyakanok et al (2008) consists of four stages: candidate generation, argument identification, argument classification and inference. ILPs have since been used successfully in many NLP applications involving complex structures - Punyakanok et al. (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al. (2009) for dependency parsing and several others. Compared to the 76.29% F1 score reported by Punyakanok et al (2008) using single parse tree predictions from the parser, our system obtained 76.22% F1 score on section 23 of the Penn Treebank. As a technical point, this defines one inference problem per sentence, rather than per predicate as in the verb SRL system of Punyakanok et al (2008). Following Das et al (2014) and Punyakanok et al (2008) we use the log-probability of the local classifiers as a score in an integer linear program (ILP) to assign roles subject to hard constraints described in 5.4 and 5.5. Here, too, the embedding model barely misses the performance of the best baseline, but we are at par and sometimes better than the single parser setting of a state-of-the-art SRL system (Punyakanok et al, 2008). Directed edges link verbs to the head words of semantic role labeling arguments produced by (Punyakanok et al, 2008). Punyakanok et al (2008) formulate a variety of constraints on argument configurations. It also has been shown to produce state-of-the-art results on many natural language applications (Punyakanok et al, 2008). Following the joint inference procedure in (Punyakanok et al, 2008), we want to select a label for each argument such that the total score is maximized subject to some constraints. Our baseline SRL model is an implementation of (Punyakanok et al, 2008) which was the top performing system in CoNLL 2005 shared task. Due to space constraints, we omit the details of the system and refer readers to (Punyakanok et al, 2008).
It is based on the transition-based dependency parsing paradigm (Nivre, 2008). If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). For a more detailed presentation of this subject, we refer the reader to Nivre (2008). Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). Transition-based: An implementation of the transition-based dependency parsing framework (Nivre, 2008) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations. This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing.
Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556).  Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008).
The authors later explored the difference between prior and contextual polarity (Wilson et al, 2009): words that lose polarity in context, or whose polarity is reversed because of context. Other studies such as Na et al (2004), Ding et al (2008), and Wilson et al (2009) also explore negation shifting and achieve some improvements. Wilson et al (2009) use conjunctive and dependency relations among polarity words. In the research on recognizing contextual polarity done by Wilson et al (2009) a rich prior-polarity lexicon and dependency parsing technique were employed to detect and analyze subjectivity on phrasal level, taking into account all the power of context, captured through such features as negation, polarity modification and polarity shifters. Wilson et al (2009) proposed a two-step approach to classify word polarity out of context firstly, and then to classify word polarity in context with a wide variety of features. Domain: Following (Wilson et al, 2009), we apply a feature indicating the domain of the document to which a sentence belongs. This is significantly different from the previous input structure methods, which consider the linguistic structure as heuristic rules (Ding and Liu, 2007) or input features for classification (Wilson et al 2009). Currently, our input sentiment list exists only of prior sentiment values, however work by Wilson et al (2009) has advanced the notion of contextual polarity lists. For the task of classifying the polarity of a given expression, there has been fairly extensive work on suitable classification features (Wilson et al, 2009). The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al (2009). The problem of polarity classification has been studied in detail by Wilson et al (2009), who used a set of carefully devised linguistic features.  Wilson et al (2009) show that modalities as well as negations are good cues for opinion identification. Wilson et al (2009) also consider negators and in addition distinguish between positive polarity shifters and negative polarity shifters since they only reverse a particular polarity type. Among the few research efforts in this direction, Wilson et al (2009) use a list of modal words. Our treatment of negation goes beyond the approaches of (Wilson et al, 2009) (Taboada et al., 2011) and (Liu and Seneff, 2009) since we propose a specific treatment for negative polarity items and for multiple negatives. Features previously found to be useful for detecting phrase-level contextual polarity (Wilson et al, 2009) are also included. The polarity of each word in arguments is derived from Multi-perspective Question Answering Opinion Corpus (MPQA) (Wilson et al, 2009). The task has been extended to allow sentences to be annotated as displaying both positive and negative sentiment (Wilson et al, 2009) or indicating the degree of intensity (Thelwall et al, 2010). More recently, Wilson et al (2009) distinguish prior and contextual polarity, and thus describe a method to phrase-level sentiment analysis.
For a more extensive survey on paraphrasing methods, see Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). There are various data-driven approaches to this NLP-task (Madnani and Dorr, 2010), but they usually focus on lexical paraphrases and do not address the problem of sentence splitting, either. Madnani and Dorr (2010) survey a variety of data driven paraphrasing techniques, categorizing them based on the type of data that they use. It could be applied to other data driven paraphrasing techniques (see Madnani and Dorr (2010) for a survey). Paraphrase generation, on the other hand, has been an area of active research and the related work has been thoroughly surveyed in Androutsopoulos and Malakasiotis (2010) as well as in Madnani and Dorr (2010). A much more detailed discussion on the use of paraphrases and ways to extract them is given in (Madnani and Dorr, 2010). See Madnani and Dorr (2010) for a good paraphrasing survey. More comprehensive surveys of data-driven paraphrasing techniques can be found in Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). As Madnani and Dorr (Madnani and Dorr, 2010) suggested, it would be beneficial to the research community to develop a standard, shared evaluation that would act to catalyze further advances and encourage more meaningful comparative evaluations of such approaches moving forward. they assume that the same paraphrase generation resources (Madnani and Dorr, 2010), for example paraphrasing rules, that some abstractive sentence compressors (including ours) use always produce acceptable paraphrases, which is not the case as discussed below. Paraphrase generation methods that operate both on a single monolingual corpus or on parallel corpus are discussed by Madnani and Dorr 2010. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. Because equivalence is the most fundamental semantic relationship, techniques for generating and recognizing paraphrases play an important role in a wide range of natural language processing tasks (Madnani and Dorr, 2010). Data-driven paraphrase acquisition techniques can be categorized by the type of data that they use (Madnani and Dorr,2010). A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). Sub-sentential paraphrases can be acquired from text pairs expressing the same meaning (Madnani and Dorr, 2010). Over the years, paraphrase acquisition and generation have attracted a wealth of research works that are too many to adequatly summarize here: (Madnani and Dorr, 2010) presents a complete and up to-date review of the main approaches. The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010), the inherent polysemy of such linguistic units and the pragmatic constraints on their uses make it impossible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007). The acquisition of sub-sentential paraphrases has attracted a lot of attention recently (Madnani and Dorr,2010).
We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure. Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1).  Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010).
 In addition to conveying information about one's own mental state, pragmatic principles and rules, such as those we have presented, may be deployed to reason about the intentions and beliefs of others (Perrault and Allen, 1980). Indirect speech acts are studied by Clark (1979), Perrault and Allen (1980), Allen and Perrault (1980) and Asher and Lascarides (2003), who identify a wide range of factors that govern how speakers convey their intended messages and how hearers seek to uncover those messages from uncertain and conflicting signals. This is computationally much less complex than classical speech act planning (Perrault and Allen, 1980), in which the intended physical effect comes at the end of a long chain of inferences.
For the sake of comparison, let us consider the following example which Pereira 1981 uses in order to illustrate Extraposition Grammar. A number of nonconcatenative grammar formalisms has been put forward, such as head-wrapping grammars (HG) (Pollard, 1984), extra position grammars (XG) (Pereira, 1981). Other non-concatenative formalisms are head-wrapping grammars (HG) (Pollard, 1984), extra position grammars (XG) (Pereira, 1981) and various exotic forms of tree adjoining grammar (Kroch and Joshi, 1986). Whereas head grammars provide for an account of verb fronting and cross-serial dependencies, Pereira, introducing extraposition grammars in (Pereira, 1981), is focused on displacement of noun phrases in English. XP -* NP While XG allows for elegant accounts of cross-serial dependencies and topicalization, it seems again hard to simultaneously account for verb and noun movement, especially if the bracketing constraint introduced in (Pereira, 1981), which requires that XG derivation graphs have a planar representation, is not relaxed. 
In addition to the cases of left-linear and right linear grammars discussed in Section 3, our algorithm is exact in a variety of interesting cases, including the examples of Church and Patil (1982), which illustrate how typical attachment ambiguities arise as structural ambiguities on regular string sets. In contrast, our method will produce an exact FSA for many interesting grammars generating regular languages, such as those arising from systematic attachment ambiguities (Church and Patil, 1982). As the NP length increases, the number of possible binary trees (parses) increases with the Catalan numbers (Church and Patil, 1982). Coordination ambiguity is under-explored, despite being one of the three major sources of structural ambiguity (together with prepositional phrase attachment and noun compound bracketing), and belonging to the class of ambiguities for which the number of analyses is the number of binary trees over the corresponding nodes (Church and Patil, 1982), and despite the fact that conjunctions are among the most frequent words. The number of possible binary-branching parses of a sentence is defined by the Catalan number, an exponential combinatoric function (Church and Patil, 1982), so dynamic programming is crucial for efficiency.
As a discourse progresses, an adequate discourse model must represent the relevant entities, and the relationships between them (Grosz and Sidner, 1986), A speaker may then felicitously refer anaphorically to an object (subject to focusing or centering constraints (Grosz et al, 1983, Sidner 1981, 1983, Brennan et al 1987)) if there is an existing DE representing it, or if a corresponding DE may be directly inferred from an existing DE. It was reported that the lexical chains closely correlated to the intentional structure (Grosz and Sidner, 1986) of the texts, where the start and end of chains coincided with the intention ranges. The most well-known of work from this period is that of Mann and Thompson (1988), Grosz and Sidner (1986b), Moore and Moser (1996), Polanyi and van den Berg (1996), and Asher and Lascarides (2003). That is, as Grosz and Sidner (1986b) argued several years ago, the sentences may only be related by their communicative intentions one sentence intended to draw the reader's attention to the specific error that was made (so that the reader knows what was mis-stated), the other intended to correct it. This belief that different aspects of discourse would be related, is what led Grosz and Sidner (1986b) to propose a theory that linked what they called the intentional structure of discourse, with its linguistic structure and with the reader or listener's cognitive attentional structure. As Grosz and Sidner (Grosz and Sidner, 1986) pointed out, cue phrases such as now and well serve to indicate a topic change. Indeed, the COLLAGEN architecture, like that of the Queen's Communicator, manages discourse using a focus stack, a classical idea in the theory of discourse structure (Grosz and Sidner, 1986). This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Examples of such models and their implementation are the information state-update approach (an implemented system is described in (Larsson, 2002)), or more linguistically oriented approaches like the adjacency-pair models or intentional models such as GROSZ and SIDNER's (see (Grosz and Sidner, 1986)). DAR presupposes the discourse structure described by Grosz and Sidner (1986). In order to cover coreference over discourse segments the centering model was extended by a stack mechanism (Grosz and Sidner, 1986). Since a more general theory to referring expressions is needed, an extension is presented by Grosz and Sidner (1986). This message constitutes the primary communicative or dis course goal (Grosz and Sidner, 1986) of the graphic and captures its main contribution to the overall dis course goal of the entire document. Therefore, it is appropriate to close the initial summary with propositions from the computational class so that the whole graphic is in the user's focus of attention (Grosz and Sidner, 1986). structure of discourse segment purposes in the view of (Grosz and Sidner, 1986). This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Recognizing the structure of text is an essential task in text understanding. [GroszandSidner, 1986]. Third, the knowledge of textual structure helps to interpret the meaning of entities in a text (Grosz and Sidner 1986). Grosz and Sidner (1986) proposed a theory of discourse structure to account for why an utterance was said and what was meant by it. We start by discussing the work of Grosz and Sidner (1986), which ties speaker's intentions to linguistic structure.
 Similar to the work of Lang (1974) and Tomita (1987) extending LR parsers for arbitrary CFGs, the LR parsers for TAGs can be extended to solve by pseudo-parallelism the conflicts of moves. We developed a set of augmented context free grammar rules for general English syntactic analysis and the analyzer is implemented using Tomita LR parsing algorithm (Tomita, 1987). The RASP parser is a generalized LR parser which builds a non-deterministic generalized LALR parse table from the grammar (Tomita, 1987). a graph-structured stack (Tomita, 1987) was used to efficiently represent ambiguous index operations in a GIG stack. A context-free backbone is automatically derived from the unification grammar1 and a generalized or non-deterministic LALR(1) table is constructed from this backbone (Tomita, 1987). The parsers create parse forests (Tomita, 1987) that incorporate subtree sharing (in which identical sub-analyses are shared between differing superordinate analyses) and node packing (where sub analyses covering the same portion of input whose root categories are in a subsumption relationship are merged into a single node). In this parser, the LALR (1) technique (Aho, Sethi Ullman, 1986) is used, in conjunction with a graph-structured stack (Tomita, 1987), adapting for unification-based parsing Kipps's (1989) Tomita-like recogniser that achieves polynomial complexity on input length through caching. A forest (Tomita, 1987) compactly encodes an exponential number of parse trees.
Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (Hobbs and Shieber, 1987) would not be difficult. Hobbs and Shieber (1987) extend this formalism to support operators (such as not) and present an enumeration algorithm that is more efficient than the naive wrapping approach. The sentence has 42 readings (Hobbs and Shieber, 1987), and it is easy to imagine how the number of readings grows exponentially (or worse) in the length of the sentence. In summary, however, the analysis is slightly more restrictive than that of Hobbs and Shieber (1987), making predictions regarding the scope of topicalized or wh-moved constituents, relative scope of embedded quantifiers, and possibly even syntactic structure of complex NPs. For example, in processing (22) (adapted from Hobbs and Shieber 1987), which Park 1995 claims to have only four readings, rather than the five predicted by their account, such a system can build both readings for the S/NP every representative of three companies saw and decide which is more likely, before building both compatible readings of the whole sentence and similarly resolving with respect to statistical or contextual support: (22) Every representative of three companies aw some sample. [Hobbs and Shieber 1987] presented an algorithm to generate quantifier scopings from a representation of predicate-argument relations and the relations of grammatical subordination. An algorithm for generating all possible quantifier scopings was detailed by Hobbs and Shieber (1987).
The algorithm is essentially the same as [DeRose, 1988].  This is well-described in for example (DeRose 1988).  As we said at the outset, we don't necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to/in and verbs associated with a following infinitive marker to/to. Kallgren (1996) gives a more covering description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS (Church 1988) and VOLSUNGA (DeRose 1988). The latter approach was pioneered by Stolz et al (1965) and Bahl and Mercer (1976), and became widely known through the work of e.g. Church (1988) and DeRose (1988).
Forward and backward shifts correspond to introducing the consequence or preparatory phases of events [Moens and Steedman, 1988]. Every frame is represented as an example sentence, a syntactic structure and a semantic structure containing semantic predicates and their arguments and temporal information in a way similar to Moens and Steedman (1988). For example, Sue played the piano is nonculminated, while Sue played the sonata signifies a culminated event (this example comes from Moens and Steedman (1988)). The perfect is analyzed by using the notion of a nucleus (Moens and Steedman, 1988) to account for the inner structure of an eventuality.  Presuming a tripartite event structure (Moens and Steedman, 1988) consisting of a preparation phase (dynamic phase theta dyn), a culmination point (boundary tau) and a consequent state (static phase phi stat), there are three possibilities for aspect to select. We decompose each event E into a tripartite structure in a manner similar to Moens and Steedman (1988), introducing a time function for each predicate to specify whether the predicate is true in the preparatory (during (E)), culmination (end (E)), or consequent (result: (E)) stage of an event. Sony Corp. has heavily promoted the Video Walkman since the product's introduction last summer, (2) but Bob Gerson, video editor of This Week in Consumer Electronics, says (3) Sony conceives of 8 mm as a family of products, camcorders and VCR decks, SE classification is a fundamental component in determining the discourse mode of texts (Smith, 2003) and, along with aspectual classification, for temporal interpretation (Moens and Steedman, 1988). Moens and Steedman (1988), Smith (1991), Pustejovsky (1995), and others argue that it is a defining property of telic events. In addition, each predicate has a time function to show at what stage of the event the predicate holds true, in a manner similar to the event decomposition of Moens and Steedman (1988). Moens and Steedman (1988) describes temporal expressions relating to changes of state. Consider the following examples, from Ritchie (1979) and Moens and Steedman (1988). An obvious first step, which we are currently working on, is to include a linguistically motivated temporal ontology (Moens and Steedman,1988), which will be separate from the existing do main ontology. It is suggested that when clauses, by contrast, do not implicate inceptiveness; indeed they do not have any special temporal implicatures (cf. Moens and Steedman 1988). However, it can be argued that when does have a. The latter would accord with a theoretical distinction that has been made between post posed when expressing a purely temporal relation between the two clauses, and proposed when ex pressing a contingent relation between them (Moens and Steedman, 1988). Each semantic predicate in VerbNet also include a time function specifying whether the predicate is true in the preparatory (during (E)) ,culmination (end (E)), or consequent (result (E)) stage of an event, in a tripartite event structure is similar to that of Moens and Steedman (1988), which allows us to express the semantics of classes of verbs like change of state verbs whose description requires reference to a complex event structure.   The subdivision is achieved by means of three features proposed by [Bennett et al, 1990] following the framework of [Moens and Steedman, 1988]. The subdivision is achieved by means of three features proposed by [Bennett et al., 1990] following the framework of [Moens and Steedman, 1988] (in the spirit of [Dowty, 1979] and [Vendler, 1967]): +-dynamic (i.e., events vs. states, as in the Jackendoff framework), +-telic (i.e., culminative events (transitions) vs. nonculminative events (activities)), and +-atomic (i.e., point events vs. extended events).
More independent of this approach are suggestions made by Moens and Steedmalt (1988) and by Webber (1988). We define a temporal segment to be a fragment of text that does not exhibit abrupt changes in temporal focus (Webber, 1988).    However, previous research has found that demonstrative anaphors rarely refer to NPs, while it rarely refers to discourse segments (Webber (1988a)).  Webber (Webber, 1988) improved upon the above work by specifying rules for how events are related to one another in a discourse and Sing and Sing defined semantic constraints through which events can be related (Sing, 1997). Consider discourse (1), from (Webber 1988). 
The results of these studies have important applications in lexicography, to detect lexico-syntactic regularities (Church and Hanks). In (Church and Hanks, 1990) the significance of an association (x, y) is measured by the mutual information I (x, y) ,i.e. the probability of observing x and y together, compared with the probability of observing x and y independently. We also measured the syntagmatic association of neighbour a and neighbour b, with a mutual information measure (Church and Hanks, 1990), computed from the cooccurrence of two tokens within the same paragraph in Wikipedia. On the other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al, 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis (SSA) (Hassan and Mihalcea, 2011), Pointwise Mutual Information (PMI) (Church and Hanks, 1990), PMI-IR (Turney, 2001), Second Order PMI (Islamand Inkpen, 2006), Hyperspace Analogues to Language (Burgess et al, 1998) and distributional similarity (Lin, 1998) employ probabilistic approaches to decode the semantics of words. In this equation, pmi (i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g. consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximum PMI score between all patterns and tuples. To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts. Like (Church and Hanks, 1990), we used mutual information to measure the cohesion between two words. We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using point wise mutual information (PMI) (Church and Hanks, 1990). Collocation has been applied successfully to many possible applications (Church et al, 1989). Collocations were extracted according to the method described in (Church and Hanks, 1990) by moving a window on texts. We use pointwise mutual information (PMI) (Church and Hanks, 1990) to measure the strength of the association between x and y, which is defined as follows PMI (x, y)= log (P (x, y) P (x) P (y)). The information content of this set is defined as mutual information I (F (w)) (Church and Hanks, 1990). PMI scores have been widely used in previous studies to measure association between words (Church and Hanks (1990)). Computational linguists have demonstrated that a word's meaning is captured to some extent by the distribution of words and phrases with which it commonly co-occurs (Church and Hanks, 1990). Following a very long tradition in computational linguistics (Church and Hanks, 1990), we use cooccurrence statistics for words in certain contexts to hypothesise a meaningful connection between the words. Early approaches to MWEs identification concentrated on their collocational behavior (Church and Hanks, 1990). Church and Hanks (1990) suggested pointwise mutual information: PMI (wi ,wj)= log Pr (wi ,wj) Pr (wi) Pr (wj), showing linguistically appealing results using contexts defined by fixed width n-gram windows, and syntactic dependencies derived from automatically parsed corpora. Therefore, we propose a second baseline where pairs are rated according to their Pointwise Mutual Information (PMI) (Church and Hanks, 1990), which measures the statistical association between two words. Collocation: Collocations were extracted from a seven million word sample of the Longman English Language Corpus using the association ratio (Church and Hanks, 1990) and outputted to a lexicon. Word collocation Various collocation metrics have been proposed, including mean and variance (Smadja, 1994), the t-test (Church et al, 1991), the chi-square test, point wise mutual information (MI) (Church and Hanks, 1990), and binomial log likelihood ratio test (BLRT) (Dunning, 1993).
(Shieber et al., 1990) introduces a head-driven algorithm for generating from logical forms. Whereas Shieber et al (1990) have discussed similar techniques in the context of semantic head-driven generation, we are concerned here with parsing. We view the linking relation not simply as a filter to increase efficiency within the domain of syntactic analysis; this aspect is stressed by Shieber (1985) and other investigators such as Bouma (1991), but rather as a device for the top-down predictive instantiation of information, as Shieber et al (1990) have shown for semantic-head-driven generation. The previously proposed semantic-head-driven methods run into problems if none of the daughter constituents in the syntactic semantic rule schemata of a grammar fits the definition of a semantic head given in [Shieber et al, 1990]. For the phrase structure tree rooted with, there is no leaf which would fulfill the definition of a semantic head given in [Shieber et al, 1990] or [van Noord, 1993]. Shieber et al (1990) show that a top-down evaluation strategy will fail for rules such as vP-*vp x, irrespective of the order of evaluation of the right-hand side categories in the rule. It is important to compare the generation strategy presented here with Semantic-head-driven generation [Shieber et al 1990, van Noord 1990] which is a direct generation algorithm from logical form encodings. Unlike deduction-based approaches to natural language generation in computational linguistics (e.g., Shieber et al 1990),. The algorithm in fact follows a head-driven node expansion, or search through the grammar, (as in Shieber et al, 1990), with the head of the most recently expanded node being selected for the next expansion (in step 2 of the algorithm above), until a leaf node is produced. Regular feature grammars can also be compiled into generators using a version of the Semantic Head Driven algorithm (Shieber et al, 1990). In this sense, it is not unlike (Shieber et al, 1990)'s semantic-head-driven generation. As shown in detail in (Shieber et al, 1990), top down generators can fail to terminate on certain grammars because they lack the lexical information necessary for their well-foundedness. The transformation is straightforward to define in its general form, and the transformed grammars can be readily compiled into efficient generators by standard feature grammar generator-compiler algorithms like Semantic Head-Driven Generation (Shieber et al, 1990). One standard approach to sentence generation from predicate/argument structure (like the semantic-head-driven generation in (Shieber et al., 1990)) involves a simple algorithm. Our general method is to take as inputs to the process various communicative goals of the system, expressed as logical forms, and use them to construct a single new logical form to be input to Gemini's Semantic Head-Driven Generation algorithm (Shieber et al, 1990), which produces strings for Festival speech synthesis. The end result of our selection and aggregation module (see section 6.2) is a fully specified logical form which is to be sent to the Semantic-Head Driven Generation component of Gemini (Shieber et al, 1990).  Given an off-line optimization of the order in which the right-hand side categories in the rules of a logic grammar are processed (Minnen et al., 1996) the resulting processing behavior can be considered a generalization of the head corner generation approach (Shieber et al, 1990). In generation, examples of such extended processing strategies are head corner generation with its semantic linking (Shieber et al, 1990) or bottom-up (Earley) generation with a semantic filter (Shieber, 1988). Generation with the resulting grammar can be compared best with head corner generation (Shieber et al, 1990).
Parallel texts have recently received considerable attention in machine translation (e.g., Brown et al 1990). They are used in many applications, e.g. word prediction (Bickel et al, 2005), speech recognition (Rabiner and Juang, 1993), machine translation (Brown et al, 1990). probability P (tw|dw) can be learned from the training corpus using statistical translation model (Brown et al, 1990). Systems for automatic translation between languages have been divided into transfer-based approaches, which rely on interpreting the source string into an abstract semantic representation from which text is generated in the target language, and statistical approaches, pioneered by Brown et al (1990), which estimate parameters for a model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text. This decomposition into two knowledge sources is known as the source-channel approach to statistical machine translation (Brown et al, 1990). Giza++ (Och and Ney,2003) is used to induce, based on statistical principles (Brown et al, 1990), an automatic word alignment of SMS tokens with their normalized counterparts. The noisy channel model approach is being successfully applied to various natural language processing (NLP) tasks, such as speech recognition (Jelinek, 1985), spelling correction (Kernighan et al, 1990), machine translation (Brown et al, 1990). This approach is a generalization of the source channel approach (Brown et al, 1990). However, we can learn to attribute some similarity between (Brown et al, 1990) and the second publication using the text in (Marcu and Wong, 2002). Now we can attribute some similarity between the (Brown et al, 1990) and (Marcu and Wong, 2002) publication since they contain similar keywords. In such translation, given a source language text, S, the translated text, T, in the target language that maximizes the probability P (T |S) is selected as the most appropriate translation, T best, which is represented as (Brown et al, 1990). This approach is a generalization of the source channel approach (Brown et al, 1990). Akhmatova and Dras (2007) described a two-fold probabilistic approach to recognizing entailment, that in its turn was based on the well-known noisy channel model from Statistical Machine Translation (Brown et al., 1990). The second method proposed by RALI is based on a dynamic programming scheme which uses a score function derived from a translation model similar to that of (Brown et al, 1990). The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation (Brown et al, 1990). But for other tasks, such as machine translation (Brown et al, 1990), the chief merit of unlabeled data is simply that nothing else is available. Brown et al (1990) gradually increased learning difficulty using a series of increasingly complex models for machine translation. This approach is a generalization of the source channel approach (Brown et al, 1990). For example, when considering whether to align two words in the IBM models (Brown et al, 1990), one can not easily include information about such features as orthographic similarity (for detecting cognates), presence of the pair in various dictionaries, similarity of the frequency of the two words, choices made by other alignment systems on this sentence pair, and so on. Statistical machine translation was introduced by work at IBM [Brown et al, 1990, 1993].
Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. (Xiong et al, 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. An algorithm for computing lexical chains was first given by (Morris and Hirst, 1991) using the Roget's Thesaurus (Kirkpatrick, 1998). Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. Global context where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an en tire text (Morris and Hirst, 1991). The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991). Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text.
Computational approaches are mostly concerned with inferring implicitly expressed metonymic relations in English texts (Fass 1991). In his program, (Fass 1991) makes use of formal definitions of several kinds of metonymic relations. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in metonymy resolution such as Hobbs et al1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon.  Fass (Fass, 1991) uses selectional preference violation technique to detect metaphors. Wilks (1978) advocates that the typically hard constraints that define a literal semantics should instead be modeled as soft preferences that can accommodate the violations that arise in metaphoric utterances, while Fass (1991) builds on this view to show how these violation scan be repaired to thus capture the literal intent be hind each metaphor. One of the first attempts to identify and interpret metaphorical expressions in text automatically is the approach of Fass (1991). Fass (1991) developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly. Another system ,met* (Fass, 1991), is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. For example, 'the rock is becoming brittle with age' (Reddy, 1969, p. 242), has "a literal interpretation when uttered about a stone and a metaphorical one when said about a decrepit professor emeritus" (Fass, 1991, p. 54). One of the first attempts to identify and interpret metaphorical expressions in text automatically is the approach of Fass (1991). Fass (1991) developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly. Almost simultaneously with the work of Fass (1991),.  A similar path-finding methodology for deriving metonymies was used by the met* system (Fass, 1991), in which connections between the sense frames of textual concepts are retrieved from a lexicon of the size of 500 word senses. He drank his glass. (Fass, 1991). 
 The work of Pustejovsky [Pustejovsky, 1991] is related in its attempt o reduce the size and complexity of individual lexical entries. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in metonymy resolution such as Hobbs et al1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. The term quale (plural :qualia) is borrowed from Pustejovsky (1991). To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon (Pustejovsky, 1991). CoreLex is based on a different theory than Levin's (that of the generative lexicon (Pustejovsky, 1991)), but does provide a compatible decompositional meaning representation for nouns. Generative Lexicon (Pustejovsky, 1991), for example have been proposed to facilitate computationally precise description of natural language syntax and semantics. The idea that noun meaning involves event-based description has been particularly emphasized by J. Pustejovsky (1991). This semantic framework demonstrates that the association between nominal constituents and underlying predicative relation in root compounds is not arbitrary: it involves conceptual mechanisms that are triggered in other linguistic phenomena such as type coercion (Pustejovsky, 1991), anaphora (Fradin, 1984) or adjectival constructions (Bouillon and Viegas, 1993). Such compounds illustrate the notion of co-compositionality (Pustejovsky 1991). Pustejovsky (1991) refers to this kind of relation as co specification i.e. like verb can select for its argument type, an argument also can select its associated predicates. The associated predicate information is included in the qualia structure of a lexical item (Pustejovsky, 1991). I declare that Korean common nouns have both the RESTR (ICTION) for normal semantics and the QUALIA-ST (RUCTURE), which in turn has the AGENTIVE and TELIC attributes, adopting the basic idea from Pustejovsky (1991). The VPs with the verbs start or finish (see Pustejovsky, 1991) can also be accounted for using the qualia structure. The vast space between these two extremes can still be explained in terms of compositional principles with mechanisms from GLT such as type coercion and sub selection (Pustejovsky, 1991, 1993). The Generative Lexicon Theory (GLT) (Pustejovsky, 1991, 1994c) can be said to take advantage of both linguistic and conceptual approaches, providing a framework which arose from the integration of linguistic studies and of techniques found in AI. The basic sense originates derived usages, which are more or less constrained and limited, via metonymy, metaphor, slight sense-shiftings or co-composition (Pustejovsky, 1991, 1995). Finally, SEMANTIC COLLOCATIONS like long book derive their particular meaning from the recovery in context of parameters for events and other entities (Pustejovsky, 1991). We are investigating the use of principles of the Generative Lexicon (Pustejovsky 1991) for that purpose. Qualia structures have been originally introduced by (Pustejovsky, 1991) and are used for a variety of purposes in natural language processing (NLP).
Those relations between the sense and its defining words are reflected in semantic dusters that are termed categorical, functional, and situational clusters in McRoy (1992). Moreover, those relations have been shown to be very effective knowledge sources for WSD (McRoy 1992) and interpretation of noun sequences (Vanderwende 1994). Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of (McRoy, 1992), that full-fledged lexical ambiguity resolution should combine several information sources and techniques.   (McRoy, 1992) was one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP.     We propose a tagger that makes use of several types of information (dictionary definitions, parts-of-speech, domain codes, selectional preferences and collocates) in the tradition of McRoy (McRoy, 1992) although, the information sources we use are orthogonal, unlike the sources he used, making it easier to evaluate the performance of the various modules. This approach as been recently used by (McRoy, 1992) and (Mahesh and Beale, 1996). There are a variety of methods for combining multiple knowledge sources (linguistic cues) (McRoy, 1992). McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information.
The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992).
On the other hand, the thesaurus-based method of Yarowsky (1992) may suffer from loss of information (since it is semi-class-based) as well as data sparseness since H Classes used in Resnik (1992) are based on the WordNet taxonomy while classes of Brown et al (1992) and Pereira et al (1993) are derived from statistical data collected from corpora. For all languages we use Brown clustering (Brown et al, 1992) to construct a log (C)+ C feature vector where the first log (C) elements indicate which mer gable cluster the word belongs to, and the last C elements indicate the cluster identity. We use the word clusters computed by Candito and Crabbe (2009) using Percy Liang's implementation of the Brown unsupervised clustering algorithm (Brown et al, 1992).  One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al, 1992). Agglomerative clustering algorithm by Brown et al (1992) is used for this purpose. Formally, as mentioned in Brown et al (1992), let C be a hard clustering function which maps vocabulary V to one of the K clusters. Note this is different from the likelihood estimation of Brown et al (1992). The features were all derived from the publicly available clusters produced by running the Brown clustering algorithm (Brown et al, 1992) over the BLLIP corpus with the Penn Treebank sentences excluded. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al 1992). As a state of-the-art clustering method, we consider Brown clustering (Brown et al 1992) as implemented in the SRILM-toolkit (Stolcke, 2002). Methods that use bigrams (Brown et al, 1992) or trigrams (Martin et al, 1998) cluster words considering as a word's context he one or two immediately adjacent words and employ as clustering criteria the minimal loss of average information and the perplexity improvement respectively. Brown et al (1992) also proposed a window method introducing the concept of semantic stickiness of two words as the relatively frequent close occurrence between them (less than 500 words distance). We find that the oldest system tested (Brown et al, 1992) produces the best prototypes, and that using these prototypes as input to Haghighi and Klein's system yields state of-the-art performance on WSJ and improvements on seven of the eight non-English corpora. The systems are as follows:1 [brown]: Class-based n-grams (Brown et al,1992). We found that the oldest system (Brown et al, 1992) yielded the best prototypes, and that using these prototypes gave state-of-the-art performance on WSJ, as well as improvements on nearly all of the non-English corpora.  The idea was introduced by Brown et al (1992) and used in different applications, including speech recognition, named entity tagging, machine translation, query expansion, text categorization, and word sense disambiguation. To this end, the Brown algorithm (Brown et al, 1992) is applied to pairwise word co-occurrence statistics based on different definitions of word co-occurrence. In order to cluster lexical items, we use the algorithm proposed by Brown et al (1992), as implemented in the SRILM toolkit (Stolcke, 2002).
Moore and Pollack (1992) gave an example of a simple discourse. Moore and Pollack (1992) note that Rhetorical Structure Theory conflates the informational (the information being conveyed) and intentional (the effects on the reader's beliefs or attitudes) levels of discourse. Moore and Pollack (1992) have already shown that different high-level intentions yield different RS-trees. For example, for the text shown in (1 I) below, which is taken from (Moore and Pollack, 1992), one may argue from an informational perspective that A3 is a CONDITION for B3. However, the results are trees of Rhetorical Structure Theory (RST) (Mann and Thompson, 1986), and the classifiers rely on well-formedness constraints on RST trees which are too restrictive (Moore and Pollack, 1992). Moore and Pollack (1992) argue that both informational (semantic) and intentional relations can hold between clause simultaneously and independently. Finally, sometimes more than one relation can hold between two given units (Moore and Pollack, 1992).
An historical account of this empirical renaissance is provide in [Church and Mercer, 1993]. However, using more data usually leads to better results, or how Church and Mercer (1993) put it more data are better data?. which has commonly been used in the recent statistical NLP research (Church and Mercer, 1993).  Many of the possible cooccurrences are not observed even in a very large corpus (Church and Mercer, 1993). Much recent research in the field of natural language processing (NLP) has focused on an empirical, corpus-based approach (Church and Mercer, 1993).
Briscoe and Carroll (1993) observed that half of the parse failures were caused by inaccurate sub categorization information in the lexicon. If, instead, this procedure returns a list of several possible actions with corresponding probabilities, we can then parse with a model similar to the probabilistic LR models described by Briscoe and Carroll (1993), where the probability of a parse tree is the product of the probabilities of each of the actions taken in its derivation.  work by Briscoe and Carroll (1993) on statistical parsing uses an adapted version of the system which is able to process tagged input, ignoring the words in order to parse sequences of tags. Finally, we observe that there are also trainable stochastic shift-reduce parser models (Briscoe and Carroll, 1993), which are theoretically related to shift-reduce parsing, but operate in a highly non deterministic fashion during parsing. Following (Briscoe and Carroll, 1993), conflict resolution is based on contextual information extracted from the so called Instantaneous Description or Configuration. The probability function can be obtained on the basis of a treebank, as proposed by (Briscoe and Carroll, 1993). The model by (Briscoe and Carroll, 1993) however incorporated a mistake involving lookahead, which was corrected by (Inui et al, 2000). One important assumption that is made by (Briscoe and Carroll, 1993) and (Inui et al, 2000) is that trained probabilistic LR parsers should be proper. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al 1985), specially constructed definition parsers (e.g., Wilks et al 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). Our second method of acquiring verb grammatical relations uses the statistical parser developed by Briscoe and Carroll (1993, 1997) which is an extension of the ANLT grammar development system which we used for our deep grammatical analysis as reported in Section 3 above.
The problem of low counts (i.e. linguistic patterns that were never, or rarely found) has not been analyzed appropriately inmost papers, as convincingly demonstrated in [Dunning, 1993]. Since then, a variety of statistical methods have been proposed to measure bi-gram association, such as Log-likelihood (Dunning, 1993). The likelihood ratio tests (Dunning, 1993) is used for this purpose. Each element of the resulting vector was replaced with its log-likelihood value (see Definition 10 in Section 2.3) which can be considered as an estimate of how surprising or distinctive a co-occurrence pair is (Dunning, 1993). Many previous studies have shown that the log-likelihood ratio is well suited for this purpose (Dunning, 1993). It can be expected that the log-likelihood ratio produces an accurate ranking of word pairs that highly correlates with human judgment (Dunning, 1993), although there are other measures which come close in performance (e.g. Rapp, 1998). It is known that PMI gives undue importance to low frequency events (Dunning, 1993), therefore the evaluation considers only pairs of genes that occur at least 5 times in the whole corpus. The measures2 - Mutual Information (Church and Hanks, 1989), the log-likelihood ratio test (Dunning, 1993), two statistical tests: t-test and chi square-test, and co-occurrence frequency - are applied to two sets of data: adjective-noun (AdjN) pairs and preposition-noun-verb (PNV) triples, where the AMs are applied to (PN,V) pairs. For instance, there is a widely held belief that and are inferior to other measures because they overestimate the collocativity of low-frequency candidates (cf. the remarks on the chi square measure in (Dunning, 1993)). We tried two feature reduction methods: a simple count cutoff, and selection of the top n features in terms of log likelihood ratio (Dunning, 1993) with the target values. The LLR measurement measures stochastic dependency between two such random variables (Dunning, 1993), and is known to be equal to Mutual Information that is linearly scaled by the size of the corpus (Moore, 2004). As an alternative for determining the probability of a positive association using P (PMI& gt; 0), we calculate LLR and assume that approximately LLR with one degree of freedom (Dunning, 1993). Many statistical metrics have been proposed, including point wise mutual information (MI) (Church et al 1990), mean and variance, hypothesis testing (t-test, chi square test, etc.), log-likelihood ratio (LR) (Dunning, 1993),statistic language model (Tomokiyo, et al 2003), and so on. Given a contextual word cw that occurs in the paragraphs of bc, a log-likelihood ratio (G2) test is employed (Dunning, 1993), which checks if the distribution of cw in bc is similar to the distribution of cw in rc. Since it was first introduced to the NLPcommunity by Dunning (1993), the G2 log-likelihood-ratio statistic has been widely used in statistical NLP as a measure of strength of association, particularly lexical associations. Dunning (1993) gives the formula for the statistic we are calling G2 in a form that is very compact, but not necessarily the most illuminating. As the strength of relevance between a target compound noun t and its co-occurring word r, the feature value of r, w (t, r) is defined by the log likelihood ratio (Dunning, 1993) as follows. We then ranked the collected query pairs using log likelihood ratio (LLR) (Dunning, 1993), which measures the dependence between q1 and q2 within the context of web queries (Jones et al, 2006b). The starting point is the log likelihood ratio (Dunning 1993). Although log identifies collocations much better than competing approaches (Dunning 1993) in terms of its recall, it suffers from its relatively poor precision rates.
Labeling of sentence boundaries is a necessary prerequisite for many natural language processing (NLP) tasks, including part-of-speech tagging (Church, 1988), (Cutting et al, 1991), and sentence alignment (Gale and Church, 1993), (Kay and RSscheisen, 1993). For each aligned pair of text chunks, perform the sentence alignment method of Gale and Church (1993). For each aligned text chunk pair, we perform sentence alignment using the algorithm of Gale and Church (1993). Gale and Church (1993) based their align program on the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences. Previous investigations can be found in works such as (Gale and Church, 1993). Sentences have been aligned using the length-based dynamic programming approach of Gale and Church (1993) enhanced with a small number of lexical and non-alphabetic anchors. The algorithm used to align English-Inuktitut sentences is an extension of that presented in Gale and Church (1993). Following a suggestion in Gale and Church (1993), the alignment was aided by the use of additional anchors that were available for the language pair. All alignments that occurred in the first two sentences of each paragraph were marked as hard boundaries for the Gale and Church (1993) program as provided in their paper. For comparison, the Gale and Church (1993) program, which did not make use of additional anchors, had poorer results over our corpus. We ran a sentence alignment algorithm (Gale and Church, 1993) for each pair of English and Chinese stories. Among approaches that are unsupervised and language independent, (Brown et al, 1991) and (Gale and Church, 1993) use sentence-length statistics in order to model the relationship between groups of sentences that are translations of each other. A hybrid approach is presented in (Gale and Church, 1993) whose basic hypothesis is that longer sentences in one language tend to be translated into longer sentences in the other language, and shorter sentences tend to be translated into shorter sentences. We performed robust alignment based on sentence lengths as in (Gale and Church, 1993). Gale and Church (1993) proposed a dynamic programming algorithm for the sentence-level alignment of translations that exploited two facts: the length of translated sentences roughly corresponds to the length of the original sentences and the sequence of sentences in translated text largely corresponds to the original order of sentences. we used a slightly modified version of CharAlign described by Gale and Church (1993). The alignment of sentences can be done sufficiently well using cues such as sentence length (Gale and Church, 1993) or cognates (Simardetal., 1992). A second pass aligns the sentences in a way similar to the algorithm described by Gale and Church (1993), but where the search space is constrained to be close to the one delimited by the word alignment. In our case, the score we seek to globally maximize by dynamic programming is not only taking into account the length criteria described in (Gale and Church, 1993) but also a cognate-based one similar to (Simard et al, 1992). This is in line with previous findings that length difference can help predict alignment (cf. ,e.g., Gale and Church, 1993).
As the result, we conclude that if we do not resolve PP attachment problem (Hindle and Rooth, 1993), to the expected extent, we will not extract he maximal noun phrases. PP-attachment is known to depend on the object of the preposition (Hindle and Rooth, 1993). For prepositions, we consider only cases in which the parent is a noun or a verb and the child is a noun (this corresponds to the cases considered by Hindleand Rooth (1993) and others). We therefore analyze the errors made on the development set to determine whether the difficult remaining cases for parsers correspond to the Hindle and Rooth (1993) style PP attachment classification task. Hindle and Rooth (1993) were the first to show that a corpus-based approach to PP attachment ambiguity resolution can lead to good results. After the initial effort by Hindle and Rooth (1993), it has become clear that this area needs statistical methods in which an easy integration of many information sources is possible. One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993).  There are multiple approaches ranging from purely statistical (Ratnaparkhi, 1998), to hybrid approaches that take into account the lexical semantics of the verb (Hindle and Rooth, 1993). Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. Examples of this work include a system by Liu et al (1990), and experiments by Hindle and Rooth (1993), and Resnik and Hearst (1993). It is also the case that we thought PP attachment might be improved because of the increased coverage of preposition noun and preposition-verb combinations that work such as (Hindle and Rooth, 1993) show to be so important. In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993). Hindle and Rooth (1993) used a partial parser to extract (v, n, p) tuples from a corpus, where p is the preposition whose attachment is ambiguous between the verb v and the noun n. Training on unambiguous cases is similar in spirit to (Hindle and Rooth, 1993). The pioneering work in this area was that of Hindle and Rooth (1993). Web frequencies were reliable enough and did not need smoothing for (i), but for (ii), smoothing using the technique described in Hindle and Rooth (1993) led to better recall. Pattern (5) is motivated by the observation that if n1 is a pronoun, this suggests a verb attachment (Hindle and Rooth, 1993). The standard method of solving the PP-attachment problem is based on collocation extraction (cf., e.g., (Hindle and Rooth, 1993)). Early work, including (Hindle and Rooth, 1993), concentrated on lexical associations, later also using word net information.
Word forms are treated as instances of one and the same word if either their actual or normalised forms are equal (Kay and Roscheisen,1993). The morphology algorithm proposed by Kay and Roscheisen (1993) is applied for splitting potential suffixes and prefixes and for obtaining the normalised word forms. Kay and Roscheisen (1993) tried lexical methods for sentence alignment. Several automatic sentence alignment approaches have been proposed based on sentence length (Brown et al, 1991) and lexical information (Kay and Roscheisen, 1993).
Smadja (1993) illustrates this by presenting 8 different ways of referring to the Dow Jones index, among which only 4 are used. Smadja (1993) classifies them according to syntactic function while Sag et al (2002) classify them according to flexibility. Among early work on developing methods for MWE identification, there is that of Smadja (1993). Therefore, sublanguage techniques such as Sager (1981) and Smadja (1993) do not work. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Future work will include: (i) applying the method to retrieve other types of collocations (Smadja,1993).  It is true that various term extraction systems have been developed, such as Xtract (Smadja 1993). Schone & Jurafsky's results indicate similar results for log-likelihood & T-score, and strong parallelism among information-theoretic measures such as ChiSquared, Selectional Association (Resnik 1996), Symmetric Conditional Probability (Ferreira and Pereira Lopes, 1999) and the Z-Score (Smadja 1993). One aspect of VPCs that makes them difficult to extract (cited in ,e.g., Smadja (1993)) is that the verb and particle can be non-contiguous. Note, this is the same as the maximum span length of 5 used by Smadja (1993). One of the earliest attempts at extracting 'interrupted collocations' (i.e. non-contiguous collocations, including VPCs), was that of Smadja (1993). Lastly, collocations are domain-dependent (Smadja 1993) and language-dependent. There have been many statistical measures which estimate co-occurrence and the degree of association in previous researches, such as mutual information (Church 1990, Sporat 1990), t-score (Church 1991), dice matrix (Smadja 1993, 1996).   Smadja (Smadja 1993) proposed a statistical model by measuring the spread of the distribution of co occurring pairs of words with higher strength. This is an example of a collocation ,i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). In terms of practical MWE identification systems, a well known approach is that of Smadja (1993), who uses a set of techniques based on statistical methods, calculated from word frequencies, to identify MWEs in corpora. It would therefore be interesting to conduct this study on a larger scale, using more general MWE definitions such as automatically learned collocations (Smadja, 1993) or verb-noun constructions (Diab and Bhutada, 2009).
Since Brent (1993) a considerable amount of research focusing on large-scaled automatic acquisition of subcategorization frames (SCF) has met with some success not only in English but also in many other languages. Apply some statistical tests such as the Binomial Hypothesis Test (Brent, 1993) and log likelihood ratio score (Dunning, 1993) to SCCs to filter out false SCCs on the basis of their reliability and likelihood. We also experimented with a method suggested by Brent (1993) which applies the binomial test on frame frequency data.  Since (Brent 1993) began to use the method, most researchers have agreed that the BHT results in better precision and recall with SCF hypotheses of high, medium and low frequencies. (Brent 1993) estimated pe according to the acquisition system's performance.   Following studies on automatic SCF extraction (Brent, 1993), we apply a statistical test (Binomial Hypothesis Test) to the unfiltered-Levin-SCF to filter out noisy SCFs, and denote the resulting SCF set as filtered-Levin SCF. Other works describe systems that induce structures from corpora, but they use tagged corpora (Brill, 1993), or grammatical informations (Brent, 1993). Our evaluator has two parts: the Binomial Hypothesis Test (Brent, 1993) and a back-off algorithm (Sarkar and Zeman, 2000). The final component assesses the frames encountered by the parser by using the same model as (Brent, 1993), with the error rate set empirically. (Brent, 1993) relies on local morphosyntactic cues (such as the -ing suffix, except where such a word follows a determiner or a preposition other than to) in the untagged Brown Corpus as probabilistic indicators of six different predefined subcategorisation frames. (Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative subcategorization frequency for individual predicates. In his seminal work, Brent (1993) already pointed out that the cues occur in contexts that were not aimed at. (Brent, 1993) uses regular patterns. Thus, Brent (1993) only creates hypotheses on the basis of instances of verb frames that are reliably and unambiguously cued by closed class items (such as pronouns) so there can be no other attachment possibilities. The foundational work of (Brent, 1993) was based on plain text (2.6 million words of the Wall StreetJournal (WSJ, 1994)).
The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown's Model 2 (Brown et al, 1993), modified and extended to deal with robustness issues. Recent work in machine translation has evolved from the traditional word (Brown et al, 1993). The IBM Model 1 (Brown et al, 1993) and hidden Markov model (HMM) (Vo gel et al, 1996) are used to estimate the alignment. A special NULL word is typically used when learning word alignment (Brown et al, 1993). For instance, the most relaxed IBM Model-1, which assumes that any source word can be generated by any target word equally regardless of distance, can be improved by demanding a Markov process of alignments as in HMM-based models (Vogel et al, 1996), or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models (Brown et al, 1993). It can be applied to complicated models such IBM Model-4 (Brown et al, 1993). We shall take HMM-based word alignment model (Vogel et al., 1996) as an example and follow the notation of (Brown et al, 1993). The word alignment was trained with six iterations of IBM model 1 (Brown et al 1993). The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. In (Brown et al, 1994), the authors proposed a method to integrate the IBM translation model 2 (Brown et al, 1993) with an ASR system. We rescore the ASR N -best lists with the standard HMM (Vogel et al, 1996) and IBM (Brown et al, 1993) MT models. In (Brown et al, 1993), three alignment models are described that include fertility models, these are IBM Models 3, 4, and 5. Word based models have a long history in machine translation, starting with the venerable IBM translation models (Brown et al, 1993) and the hidden Markov model (Vogel et al, 1996). Originally introduced as a byproduct of training statistical translation model since (Brown et al, 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The IBM models (Brown et al, 1993) benefit from a one-to-many constraint. The statistical machine translation framework (SMT) formulates the problem of translating a sentence from a source language S into a target language T as the maximization problem of the conditional probability: TM LM =argmaxT p (S|T) p (T), (1) where p (S|T) is called a translation model (TM), rep resenting the generation probability from T into S, p (T) is called a language model (LM) and represents the likelihood of the target language (Brown et al, 1993). We then train IBM models (Brown et al, 1993) using the GIZA++ package (Och and Ney, 2000). Statistical machine translation (SMT) was originally focused on word to word translation and was based on the noisy channel approach (Brown et al., 1993). lexical translation models (Brown et al, 1993) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We defined a feature set which includes: length ratio and length difference between source and target sentences, lexical probability scores similar to IBM model 1 (Brown et al, 1993), number of aligned/unaligned words and the length of the longest aligned word sequence.
Of the 1600 IBM sentences that have been parsed (those available from the Penn Treebank [Marcus et al, 19931), only 67 overlapped with the IBM-manual treebank that was bracketed by University of Lancaster. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides annotations for the arguments and relation senses of one hundred pre-selected discourse connectives over the news portion of the Penn Treebank corpus (Marcus et al, 1993). Marcus et al (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for part-of-speech annotation. Agirre et al (2008) applied two state-of-the-art tree bank parsers to the sense tagged subset of the Brown corpus version of the Penn Treebank (Marcus et al, 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PP attachment. These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al, 1993). We have used Sections 02-21 of CCG bank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al, 1993), as training data for the newspaper domain. The English sentences were parsed using a state-of-the-art statistical parser (Charniak, 2000) trained on the University of Pennsylvania Treebank (Marcus et al, 1993). The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al, 1993 ) with a layer of discourse annotations. These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al (1993). This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al, 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency. In the Penn Treebank (Marcus et al, 1993), null elements, or empty categories, are used to indicate non-local dependencies, discontinuous constituents, and certain missing elements. This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus (Marcus et al 1993). The sentences included in the gold standard were chosen at random from the BNC, subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB (Marcus et al, 1993). We use the Penn WSJ treebank (Marcus et al, 1993) for our experiments. The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al, 1993) consisting of 10948 sentences and 259104 words. For instance, about 38% of verbs in the training sections of the Penn Treebank (PTB) (Marcus et al, 1993) occur only once the lexical properties of these verbs. The third corpus was Section 23 of the Wall Street Journal data in the Penn Treebank (Marcus et al, 1993). We have observed in several experiments that the number of SuperARVs does not grow significantly as training set size in creases; the moderate-sized Resource Management corpus (Price et al, 1988) with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set (Marcus et al, 1993). In one experiment, it has to be performed on the basis of the gold-standard, assumed-perfect POS taken directly from the training data, the Penn Treebank (Marcus et al, 1993), so as to abstract from a particular POS tagger and to provide an upper bound. Our chunks and functions are based on the annotations in the third release of the Penn Treebank (Marcus et al, 1993).
Pustejovsky confronted with the problem of automatic acquisition more extensively in [Pustejovsky et al 1993]. We inferred the bracketing by modifying an algorithm initially proposed by Pustejovsky et al (1993). The bracketing problem for noun-noun-noun compounds has been investigated by Liberrnan (1992), Pustejovsky et al (1993). (Pustejovsky et al., 1993) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency.   Finally, (Pustejovsky et al, 1993) present an interesting framework for the acquisition of semantic relations from corpora not only relying on statistics, but guided by theoretical lexicon principles. The best known early work on automated unsupervised NC bracketing is that of Lauer (1995) who introduces the probabilistic dependency model for the syntactic disambiguation of NCs and argues against the adjacency model, proposed by Marcus (1980), Pustejovsky et al (1993) and Resnik (1993). Pustejovsky et al (1993) show how statistical techniques, such as mutual information measures can contribute to automatically acquire lexical information regarding the link between a noun and a predicate.  For example, Pustejovsky et al (1993) use generalized syntactic patterns for extracting qualia structures from a partially parsed corpus. Concerning relatedness measure, additional corpus-based measures such as Web-basedmeasures (Cimiano and Wenderoth, 2007) or measures based on syntactic relations (Pustejovsky et al, 1993) could appear to be useful for improving the ranking of the extracted relations.
We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993). Word features are introduced primarily to help with unknown words, as in (Weischedel et al 1993). Weischedel's group (Weischedel et al, 1993) examines unknown words in the context of part-of-speech tagging. in (Weischedel et al, 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos, its capitalisation feature and its ending. For words that were unknown in our subtree set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993). For words that were unknown in the training set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model. The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al 1993) and concentrates on the simple noun phrase. Weischedel et al (1993) combine several heuristics in order to estimate the token generation prob ability according to various types of information. In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, given the words, is out put (Weischedel et al, 1993). In addition to the ending, Weischedel et al (1993) exploit capitalisation.   The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case.
Empirical studies on the disambiguation of cue phrases (Hirschberg and Litman, 1993) have shown that just by considering the orthographic environment in which a discourse marker occurs, one can distinguish between sentential and discourse usages in about 80% of cases. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). In the literature, there is still no consistent definition for discourse markers (Hirschberg and Litman 1993). This is due to the fact that, similar to the question of cue phrase polysemy (Hirschberg and Litman 1993), many Chinese discourse markers have both discourse senses and alternate sentential senses in different utterances. Annotation includes two stages: first, we allowed two coders to choose 'explanation' relations signaled by because using (Hirschberg and Litman, 1993)'s 3-way classification. Many discourse segmentation techniques (e.g. Hirschberg and Litman, 1993) as well as some topic segmentation algorithms rely on cue words and phrases.  We use a list of cue phrases identified by Hirschberg and Litman (1993). Perhaps the most studied cue for discourse structure are lexical cues, also called cue phrases, which are defined as follows by Hirschberg and Litman (1993). For example Hirschberg and Litman (1993) found that into national phrasing and pitch accent play a role in disambiguating cue phrases, and hence in helping determine discourse structure. our own implementation of a feature concerning the presence of certain cue words (Hirschberg and Litman, 1993). Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993).
Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions. Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions. We adopt the problem formulation of Merialdo (1994), in which we are given a dictionary of possible tags for each word type. Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years Merialdo (1994). We also did not consider morphological analyzers as a form of type supervision, as suggested by Merialdo (1994). Advanced students might also want to read about a modern supervised trigram tagger (Brants, 2000), or the mixed results when one actually trains trigram taggers by EM (Merialdo, 1994). For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM. Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. Unsupervised Part-of-Speech Tagging Since the work of Merialdo (1994), the HMM has been the model of choice for unsupervised tagging (Banko and Moore, 2004). EM was first used in POS tagging in (Merialdo, 1994) which showed that except in conditions where there are no labeled training data at all, EM performs very poorly. We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type. Previous results on unsupervised POS tagging using a dictionary (Merialdo, 1994) on the full 45-tag set. The basic engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). We can surmise from the log-likelihood plot that the drop in accuracy is not due to the optimization being led astray, but probably rather due to the complex relationship between likelihood and task specific evaluation metrics in unsupervised learning (Merialdo, 1994). Work on type-supervision goes back to (Merialdo, 1994), who introduced the still standard procedure of using a bigram Hidden Markov Model (HMM) trained via Expectation Maximization. Such work has for instance been based on hidden Markov models (Merialdo, 1994). (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g. Merialdo (1994)). Merialdo (1994) reports an accuracy of 86.6% for an unsupervised word-based HMM,. We adopt the common problem formulation for this task described by Merialdo (1994).
Hence, the most important single construct of the centering model is the ordering of the list of forward-looking centers (Walker et al, 1994).   In this paper, I will review and assess the recent centering approach to the interpretation of Japanese zero pronouns (Walker et al 1994) as a case study.  The BFP-algorithm (c.f. Walker et al (1994)) consists of three basic steps: 1 GENERATE possible Cb-Cf combinations. 2 FILTER by constraints, e.g., contra-indexing, sortal predicates, centering rules and constraints. 3 RANK by transition orderings. Walker et al (1994) proposed forward center ranking for Japanese.  
see Kaplan and Kay (1994) for an exposition of the mathematical basis. We assume that the reader is familiar with the basic concepts of finite state transducers (FST hereafter), finite state devices that map between two regular languages U and L (Kaplan and Kay, 1994). Two-level formal lists based on that introduced by (Koskenniemi, 1983) (see also (Ritchie et al, 1992) and (Kaplan and Kay, 1994)) are widely used in practical NLP systems, and are deservedly regarded as something of a standard. Unlike arbitrary regularization, same-length regular n-relations are closed under intersection and complementation , because a theorem tells us that they correspond to regular languages over of symbols (Kaplan and Kay, 1994, p. 342). (Kaplan and Kay, 1994) express CR rules by the relation. This expression is an expansion of Restrict in (Kaplan and Kay, 1994, p. 371). The Xerox calculus includes the composition, ignore, and substitution operator discussed by Kaplan and Kay (1994) and the priority-union operator of Kaplan and Newman (1997). This can be seen as an application of the ignore operator of Kaplan and Kay (1994), where E* is being ignored. Rules are compiled into finite-state transducers and merged using transducer composition (Kaplan and Kay, 1994).  The context-dependent rewrite algorithm used is that of Mohri and Sproat (1996), and see also Kaplan and Kay (1994). In particular, constructing an OT grammar step-by-step as the composition of a set of transducers, akin to rewrite rule com position in (Kaplan and Kay, 1994), has offered the attractive possibility of simultaneously modeling OT parsing and generation as a natural consequence of the bi-directionality of finite-state transducers. An algorithm for compilation into transducers was provided by Kaplan and Kay (1994). Back referencing has been implicit in previous research, such as in the batch rules of Kaplan and Kay (1994). Previous algorithms for compiling rewrite rules into transducers have followed Kaplan and Kay (1994) by introducing special marker symbols (markers) into strings in order to mark off candidate regions for replacement. it will be helpful to have at our disposal a few general tools, most of which were described already in Kaplan and Kay (1994). For batch context-dependent rules, the context of the application for all rules is determined at once before their application (Kaplan and Kay, 1994). Phonological rewrite-rules (Kaplan and Kay, 1994), two-level rules (Koskenniemi 1983), syntactic disarnbiguation rules (Karlsson et al 1994, Koskenniemi, Tapanainen, and Voutilainen 1992), and part-of-speech assignment rules (Brill 1992, Roche and Schabes 1995) are examples of replacement in context of finite-state grammars. Kaplan and Kay (1994) describe a general method representing a replacement procedure as finite-state transduction. 
we calculated the bunsetsu-based accuracies of our model and KNP (Kurohashi and Nagao, 1994) on the rst 100 sentences of the test corpus. Kurohashi and Nagao (1994) used a similar data structure in their rule-based method. Popular parsers are Cabo Cha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which we redeveloped to analyze formal written language expressions such as that in newspaper articles. Sentences with coordinate structures are also difficult to parse (Kurohashi and Nagao, 1994). Kurohashi and Nagao (1994) proposed a method to detect conjunctive structures by calculating similarity scores between two sequences of bunsetsus. If the current modifier bunsetsu is a distinctive key bunsetsu (Kurohashi and Nagao, 1994, page 510), these features are triggered.  The state space of our model resembles that of Kurohashi and Nagao's Japanese coordination detection method (Kurohashi and Nagao, 1994). We use a syntactic parser KNP (Kurohashi and Nagao, 1994) to divide the ASR results into the phrases. An input sentence is parsed using the Japanese parser, KNP (Kurohashi and Nagao, 1994). while Sassano (2004) used richer features which are not used in the proposed method, such as features for conjunctive structures based on Kurohashi and Nagao (1994). We first process them with the Japanese morphological analyzer, JUMAN (Kurohashi et al., 1994). We have crawled 218 million web pages over three months, May July in 2007, by using the Shim Crawler, and then converted these pages into web standard format data with results of a Japanese parser, KNP (Kurohashi and Nagao, 1994), through our conversion tools. A large raw corpus is parsed by the Japanese parser, KNP (Kurohashi and Nagao, 1994b), and reliable predicate argument examples are extracted from the parse results.  Kurohashi and Nagao proposed a similarity-based method to resolve both of the two tasks for Japanese (Kurohashi and Nagao, 1994). Kurohashi and Nagao proposed a Japanese parsing method that included coordinate structure detection (Kurohashi and Nagao, 1994). Therefore, we use a method to calculate the similarity between two whole coordinate conjuncts (Kurohashi and Nagao, 1994). A similarity value between two bunsetsus is calculated on the basis of POS matching, exact word matching, and their semantic closeness in a thesaurus tree (Kurohashi and Nagao, 1994). We classified coordination keys into 52 classes ac cording to the classification proposed by (Kurohashiand Nagao, 1994).
our model incorporates the text-level of anaphora resolution, a shortcoming of the original SG approach that has recently been removed (Lappin and Leass, 1994), but still is a source of lots of problems. We have used the same weights, listed in table 2, proposed by Lappin and Leass (1994).  Lappin and Leass (1994) extracted rules from the output of the English Slot Grammar (ESG) (McCord, 1993). Only one of the four is explicitly aimed at personal-pronoun anaphora RAP (Resolution of Anaphora Procedure) (Lappin and Leass, 1994). In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Lappin and Leass (1994), for example, use several heuristics to filter out expletive pronouns, including a check for patterns including modal adjectives. We implemented a coreference resolution tool using a shallow rule-based approach inspired by Lappin and Leass (1994) and Bontcheva et al (2002). The new tool combines Hobbs algorithm (Hobbs, 1978) and the Resolution of Anaphora Procedure (RAP) algorithm (Lappin and Leass, 1994). (Lappin and Leass, 1994) describe several syntactic heuristics for reflexive, reciprocal and pleonastic anaphora, among others. Definiteness (Lappin and Leass, 1994). Non-prepositional NP (Lappin and Leass, 1994). Pleonastic (Lappin and Leass, 1994). Syntactic Parallelism (Lappin and Leass, 1994).  Other works, however, distinguish between restrictions and preferences (e.g. Lappin and Leass (1994)). Lappin and Leass (1994) describe an algorithm for pronominal anaphora resolution that achieves a high rate of correct analyses (85%). More recently, Kennedy and Boguraev (1996) propose an algorithm for anaphora resolution that is actually a modified and extended version of the one developed by Lappin and Leass (1994). Lappin and Leass (1994) has also been implemented in our system and an accuracy of 64% was attained. Implementation of constraints and preference scan be based on empirical insight (Lappin and Leass, 1994).
Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora (Brown et al., 1990), (Brown et al., 1991), (Brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (Yamabana et al., 1998) and 3) un-tagged mono-language corpora in the target language (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998). Dagan and Itai (1994) have also addressed the lexical selection problem from the TL point of view. We used two measurements, applicability and precision (Dagan and Itai 1994), to evaluate the performance of our method. In addition, (Dagan and Itai, 1994) and (Li, 2002) propose using two monolingual corpora for word sense disambiguation. The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90's, the most representative works being (Brown et al, 1991), (Gale et al, 1992), and (Dagan and Itai, 1994). Note that, unlike Dagan and Itai (1994), we give no consideration to statistical confidence as we are after 100% recall, whatever the cost to precision. It may, therefore, be desirable to apply a dynamic threshold on the discriminative ratio (cf. (Dagan and Itai, 1994)) to accept only those translation pairs with sufficiently high statistical confidence, for example. However, most of previous research has focused on using multilingual resources typically used in SMT systems to improve WSD accuracy Dagan and Itai (1994). For example, Dagan and Itai (1994) carried out WSD experiments using monolingual corpora, a bilingual lexicon and a parser for the source language. Dagan and Itai (1994) indicate that two languages are more informative than one; an English corpus is very helpful in disambiguating polysemous words in Hebrew text. The work of (Dagan and Itai, 1994) has also successfully used WSD to improve the accuracy of machine translation. Dagan and Itai (1994) proposed an approach to WSD using monolingual corpora, a bilingual lexicon and a parser for the source language. The first method is based on a decision threshold (Dagan and Itai, 1994): the algorithm rejects decisions taken when the difference of the maximum likelihood among the competing senses is not big enough. As in (Dagan and Itai, 1994), we adjusted the measure to the amount of evidence. (Dagan and Itai, 1994) explicitly suggests performing word sense disambiguation in the target language (English in the article) with the goal of resolving ambiguity in the source language (Hebrew).
A similar approach as been adopted in generation (Bateman, 1997), (Bateman et al., 1991) and in machine translation most notably in (Dorr, 1994). Using these simple, language agnostic measures allows one to look for divergence types such as those described by Dorr (1994). However, real bitexts generally do not exhibit parse-tree isomorphism, whether because of systematic differences between how languages express a concept syntactically (Dorr, 1994). We suggest that alignment constraints such as this one can be used to define most of the possible syntactic divergences between languages (Dorr, 1994),. Theoretically, it is well-known that two languages often do not express the same meaning in the same way (Dorr, 1994). One of the reasons for this difference is due to the different language pairs under study; (Meyers et al, 1998) deals with two languages that are closely related syntactically (Spanish and English) while we are dealing with languages that syntactically are quite divergent, Korean and English (Dorr, 1994). We suggest that alignment constraints such as this one can be used to define most of the possible syntactic divergences between languages (Dorr, 1994), and that only a handful of them are necessary for two given languages (we have identified 11 general alignment constraints necessary for Korean to English transfer so far). Dorr (1994) categorizes sources of syntactic divergence between languages. Yet, we can not preclude divergence from translational correspondence; on the contrary, it occurs routinely and to a certain extent systematically (Dorr, 1994). However, structural divergences between languages (Dorr, 1994) which are due to either systematic differences between languages or loose translations in real corpora pose a major challenge to syntax-based statistical MT. Dorr (1994) presents some major lexical-semantic divergence problems applicable in this scenario: (a) Thematic Divergence In some cases, although there exists semantic parallelism, the theme of the English sentence captured in the subject changes into an object in the Urdu sentence. Of particular relevance to MT is the issue of structural divergence (Dorr, 1994). English to French Lexical-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998). The syntax based statistical approaches have been faced with the major problem of pervasive structural divergence between languages, due to both systematic differences between languages (Dorr, 1994) and the vagaries of loose translations in real corpora. However, previous work in machine translation leads us to believe that transferring the correlations between syntax and semantics across languages would be problematic due to argument structure divergences (Dorr, 1994).
This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy to calculate exact prefix probabilities using the algorithm of Stolcke (1995). Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al (1999). Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition. Two of these, P(X) and S(X) are just the prefix and suffix probability distributions for the symbol(Stolcke, 1995): the probabilities that the string derived from begins (or ends) with a particular tag. Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. Therefore, we use an Earley-style probabilistic parser, which outputs Viterbi parses (Stolcke, 1995). A third approach is to calculate the forward probability (Stolcke, 1995) of the sentence using a PCFG. The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley's algorithm. In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995). Monolingual parsing with unary productions is fairly straightforward (Stolcke,1995), however in the transductive setting these rules can licence infinite insertions in the target string. An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995) . Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al (1999), and Stolcke (1995). These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). A probabilistic Earley parser can retrieve all possible derivations at (Stolcke, 1995).
The centering model (Grosz et al, 1995) focuses on the resolution of inter-sentential anaphora. Possible strategies for treating sentence-level anaphora within the centering framework are processing sentences linearly one clause at a time (as suggested by Grosz et al (1995)). To summarize the results of our empirical evaluation, we claim that our proposal based on functional criteria leads to substantively better results for languages with free word order than the linear approach suggested by Grosz et al (1995) and the two approaches which prefer inter-sentential or intra-sentential ntecedents. Crucial for the evaluation of the centering model (Grosz et al, 1995) and its applicability to naturally occurring discourse is the lack of a specification concerning how to handle complex sentences and internal sentential anaphora. One popular model, the centering model (Grosz et al, 1995), uses a ranking of discourse entities realized in particular sentence sand computes transitions between adjacent sentences to provide insight in the felicity of texts. Linguists have also studied various aspects of text flow like the centering theory (Grosz et al, 1995) among the most influential contributions. Most models attempting to capture local coherence between sentences were based on or inspired by centering theory (Grosz et al, 1995), which postulated strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference. Finally, Poesio examined the hypothesis that finding the anchor of a BD involves knowing which entities are the CB and the CP in the sense of Centering (Grosz et al, 1995). Centering Theory (CT) characterises the local coherence of a text on the basis of the discourse entities in a text and the way in which they are introduced (Grosz et al, 1995). Centering Theory (CT, Grosz et al 1995) is an entity-based theory of local coherence, which claims that certain entities mentioned in an utterance are more central than others and that this property constrains a speaker's use of certain referring expressions. Centering Theory (Grosz et al, 1995) has been an influential framework for modelling entity coherence in computational linguistics in the last two decades. Entity-based theories of discourse (e.g., (Grosz et al, 1995)) claim that a coherent text segment tends to focus on a specific entity. Summarised below are some issues specific to anaphora resolution in spoken dialogues (see also Byron and Stent (1998) who mention some of these problems in their account of the Centering model (Grosz et al, 1995)). Byron and Stent (1998) present extensions of the centering model (Grosz et al, 1995) for spoken dialogue and identify several problems with the model.  The main new aspect of the markup scheme, especially as far as our studies of salience were concerned, are the elements used to annotate potential utterance sin the sense of Centering (Grosz et al, 1995). Our model is inspired by Centering (Grosz et al, 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences. Grosz et al (1995) give the following example of the Am SHIFT pattern. Centering's Rule 1 states that if any element of the previous utterance's forward looking center list is realized in the current utterance as a pronoun, then the backward looking center must be realized as a pronoun as well (Grosz et al, 1995). pronouns involve local focusing while full lexical forms involve global focusing (Grosz et al, 1995).
In (Brill, 1995) a system of rules which uses both ending-guessing and more morphologically motivated rules is described. Brill (Brill, 1995) outlines a transformation-based learner which learns guessing rules from a pre-tagged training corpus. The other tagger was the rule-based tagger of Brill (Brill, 1995). The task is typically addressed as a sequential tagging problem; one notable exception is the work of Brill (1995), who proposed non-sequential transformation-based learning. For example, Dojchinova and Mihov (2004) mapped their initial tag set of 946 tags to just 40, which allowed them to achieve 95.5% accuracy using the transformation-based learning of Brill (1995), and 98.4% accuracy using manually crafted linguistic rules. We used Brill's tagger (Brill, 1995) and Memory-Based Shallow Parser (Daelemansetal., 1999) to analyze English sentences. A third, clean-up pass is then performed to partially disambiguate the identified WordNet glosses with Brill's part-of-speech tagger (Brill, 1995), which performs with up to 95% accuracy, and eliminates errors introduced into the list by part-of-speech ambiguity of some words acquired in pass 1 and from the seed list. According to (Brill, 1995), a Transformation-Based Error-Driven learning application is defined by 1. The initial annotation scheme 2. The space of allowable transformations 3. The iterative algorithm for choosing a transformation sequence. For example, in a part-of-speech tagging task, the initial an notation may assign each token its most likely tag without any regard to context (Brill, 1995). The addition of a look-ahead searcher has been suggested (Brill, 1995), but we have not seen it implemented in a research context, likely due to the fact that a straightforward implementation of the concept would at minimum square the amount of time required for training. Many alternatives suggest themselves to expand the options, including maximum entropy models, which have been previously successfully applied to, inter alia, sentence boundary detection (Reynar and Ratnaparkhi, 1997), and transformation-based learning, as used in part-of-speech tagging and statistical parsing applications (Brill, 1995). TBL is a machine learning approach that has been employed to solve a number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (Brill, 1995). The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratna park hi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al, 2003). A class sequence example Transformation-based learning is a symbolic machine learning method, introduced by (Eric Brill, 1995). There has been a modest amount of previous work on improving probabilistic decision lists, as well as a fair amount of work in related fields, especially in transformation-based learning (Brill, 1995). For instance, in part-of-speech tagging (Brill, 1995), when the tag of one word is changed, it changes the answers to questions for nearby words. First, since probabilistic decision lists are probabilistic analogs of TBLs, we compared to TBL (Brill,1995). Partly inheriting from (Brill, 1995), we applied error-driven learning to filter prefixes in Sp and suffixes in Ss. In line with our assumption of raw text to extract over, we use the Brill tagger (Brill, 1995) to automatically tag the WSJ, rather than making use of the manual POS annotation provided in the Penn Treebank. The expanded set of results are summarised in Table 1, for Transformation Based Learning (TBL) (Brill, 1995).
This score is used, for instance, in the collocation compiler XTract (Smadja, 1993) and in the lexicon extraction system Champollion (Smadja et al, 1996). Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al, 1996). This highlights the need for finding multi-word translation correspondences. Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences (Kupiec, 1993), fixed flexible collocations (Smadja et al, 1996). Our preliminary finding supports the work on collocation by Smadja et al (1996) in that gapped sequences are also an important class of multi-word translations. Smadja et al (1996) limits to find French translation of English collocation identified by his Xtract system. The relationship between pointwise Mutual Information and the Dice coefficient is also discussed in (Smadja et al, 1996). A number of corpus-based methods to extract bilingual lexicons have been proposed (Smadja et al,1996).  Commonly used association measures are the Mutual Information (Fano, 1961) and the Dice factor (Smadja et al 1996).  Our method is similar to that of Smadja et al (1996), except that we incorporate lexical-level information into the association-based method. Our MWE translation extraction method is similar to the two-phase approach proposed by Smadja et al (1996). As noted by Smadja et al (1996), this two-step approach drastically reduces the search space. However, translations of collocated context words in the source word sequence create noisy candidate words, which might cause incorrect extraction of target translations by naive statistical correlation measures, such as the Dice coefficient used by Smadja et al (1996). Bound-length N-gram correspondences include (Kupiec, 1993) where NP recognizers are used to extract translation units and (Smadja et al, 1996) which uses the Extract system to extract collocations. Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al, 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al, 1996) and induction of NLP-tools (Kuhn, 2004). Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al, 1996)). Smadja et al (1996) proposed a statistical association measure of the Dice coefficient to deal with the problem of collocation translation. Some methods make alignment suggestions at an intermediate level between sentence and word 271 and word (Smadja, 1992; Smadja et al, 1996).
The approach made use of a maximum entropy model (Berger et al, 1996) formulated from frequency information for various combinations of the observed features. This is not a issue for the MaxEnt classifier as it can deal with arbitrary overlapping features (Berger et al, 1996). We performed feature selection by incrementally growing a log-linear model with order0 features f (x,yt) using a forward feature selection procedure similar to (Berger et al, 1996). For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a "maximum entropy" model (Berger et al., 1996) or a log-linear model. Berger et al (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument. A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al, 1996). For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al, 1996). The log-linear model (LLM), or also known as maximum-entropy model (Berger et al, 1996), is a linear classifier widely used in the NLP literature. We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al, 1996). Maximum entropy classification (MaxEnt) is a technique which has proven effective in a number of natural language processing applications (Berger et al, 1996). Berger et al (1996) propose to estimate the weight wi of the candidate feature fi, while assuming that the weights of features in S stay constant. the gain-informed selection method proposed by Berger et al (1996) still recalculates the weights of all the candidate features during every cycle. The maximum entropy approach (Berger et al, 1996) is known to be well suited to solve the classification problem. The NLU uses a maximum-entropy model (Berger et al, 1996) to classify utterances as one of the user SAs using shallow text features. we use the general technique of choosing the maximum entropy (maxent) distribution that estimates the average of each feature over the training data (Berger et al, 1996). Uses Maximum Entropy (Berger et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). An especially well-founded framework for doing this is maximum entropy (Berger et al, 1996). It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following exponential form (Berger et al 1996). Since its introduction to the Natural Language Processing (NLP) community (Berger et al, 1996), ME-based classifiers have been shown to be effective in various NLP tasks. Maximum entropy (ME) models have been used in bilingual sense disambiguation, word reordering, and sentence segmentation (Berger et al, 1996),.
As advocated by Carletta (1996), we have used the Kappa coefficient (Siegel and Castellan, 1988) as a measure of coder agreement. Following the works of Carletta (1996) and Artstein and Poesio (2008), there is an increasing consensus within the field that in order to properly gauge the reliability of an annotation effort, chance-corrected measures of inter-annotator agreement should be used. (Cohen, 1960, introduced to computational linguistics by Carletta, 1996) and pi (Scott, 1955). The reliability of the annotation was evaluated using the kappa statistic (Carletta, 1996). (Carletta 1996) is another method of comparing inter-annotator agreement. We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common. To measure inter-annotator agreement, we compute Cohen's Kappa (Carletta, 1996) from the two sets of annotations. Obtained percent agreement of 0.988 and coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations. P (A) is the observed agreement between annotators and P (E) is the probability of agreement due to chance (Carletta, 1996). Annotation was highly reliable with a kappa (Carletta, 1996) of 3. With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation. To test the reliability of the annotation, we first considered the kappa statistic (Siegel and Castellan, 1988) which is used extensively in empirical studies of dis course (Carletta, 1996). Inter-annotator agreement was sufficient (κ = 0.77 on average (Carletta, 1996)). In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance (Carletta, 1996), was also determined. Following the suggestions in (Carletta, 1996), Core et al. consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement. In order to test the reliability of our annotation, we double coded about 18% of the data, namely 21 sub-dialogues comprising 213 pronouns, on which we computed the Kappa coefficient (Carletta, 1996). As reported elsewhere the resulting Kappa statistics (Carletta, 1996) over the annotated data yields 0.7. Since co-reference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e.g., Kappa (Carletta, 1996)) can be used for coreference scoring (see Popescu-Belis et al (2004)). The reliability for the two annotation tasks (statistics (Carletta, 1996)) was of 0.94 and 0.90 respectively. From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen's kappa was obtained.
[Sproat et al, 1996] built a word uni gram model using the Viterbi re-estimation whose initial estimates were derived from the frequencies in the corpus of the strings of each word in the lexicon. [Sproat et al, 1996] wrote lexical rules for each productive morphological process, such as plural noun formation, Chinese personal names, and transliterations of foreign words. We used a simple greedy algorithm described in [Sproat et al, 1996]. [Sproat et al, 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus. Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al, 1996]. Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al, 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese. In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996). One example of such approaches is Sproat et al (1996), which is based on weighted finite-state transducers (FSTs). Sproat et al (1996) also studied such problems (with the same example) and uses weighted FSTs to deal with the affixation. As described in Sproat et al (1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name. Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al, 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting. Chinese According to Sproat et al (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases.   Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996). Another difference from our model is the rule-based sub-model, which uses a dictionary-based forward maximum match method described by Sproat et al (1996). As discussed elsewhere (Sproat et al, 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese. The actual implementation of the weighted finite state transducer by Sproat et al (1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use. what Sproat et al (1996) implemented was simply a token unigram scoring function. First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al, 1996).
 These strategies can be seen as transactions made up of conversational games (Carletta et al., 1997). Each dialogue is divided into short, clearly defined dialogue acts Initiations I and Acknowledgments A based on the top of the hierarchy given in Carletta et al (1997). This is an empirically very well-founded distinction: the ICSI-MR group have provided some inter-annotator agreement figures (Carletta et al, 1997) for a very similar task and report a kappa of 0.79. As indicated in Table 9, the unanimous agreement of just 16.6% and 19.5% in the ad hoc and categorization tasks respectively is low: the agreement data has Kappa (Carletta et al 1997) of .38 for ad hoc and .29 for categorization 4. To put Table 8 in perspective, note that expert human coders achieved= 0.83 on DA classification for MapTask, but also had available the speech source (Carletta et al, 1997). We also compared the confusion matrix from (Carletta et al, 1997) with the confusion matrix we obtained for our best result on MapTask (FLSA using Game+ Speaker). The same type of structures is also used in the analysis of dialogues, e.g. (Carletta et al., 1997). Another common agreement metric is the kappa coefficient which normalises token level accuracy by chance, e.g. Carletta et al (1997). Other tagging schemes, such as the Maptask scheme (Carletta et al., 1997), are also too general for our purposes. For dialog act labeling, we built models from our corpus and from the Maptask (Carletta et al, 1997). This kind of multi-level assessment corresponds to that described and used in Carletta et al, (1997). Many such models focus on other aspects of a dialog such as coordinated activities ,i.e. turn-taking and grounding, (Traum and Hinkelman, 1992) and regular patterns in the dialog (Carletta et al, 1997) rather than the domain-specific information communicated by participants. Note that we are interested in the process of designing a domain-specific tag set from the definitions of task, subtask, and concept provided by the framework, not in the process of using an existing tag set to annotate data (see for example (Carletta et al, 1997)). Again, according to the work of Carletta et al (1997), a minimum kappa score of 0.67 is required to draw tentative conclusions. This position was taken by other computational linguists as well (Carletta et al, 1997, p. 25). For example, Carletta et al (1997) computed agreement on a coarse segmentation level that was constructed on the top of finer segments, by determining how well coders agreed on where the coarse segments started, and, for agreed starts, by computing how coders agreed on where coarse segments ended. Redo our experiments on other corpora, such as Map Task (Carletta et al., 1997). The monologue side has been annotated with discourse relations, using an adaptation of the annotation guidelines of Carlson and Marcu (2001), whereas the dialogue side has been marked up with dialogue acts, using tags inspired by the schemes of Bunt (2000), Carletta et al. (1997) and Core and Allen (1997).
The performance with respect to identifying sentence boundaries appears to be close to that of systems aimed at identifying only sentence boundaries (Palmer and Hearst, 1997), whose accuracy is in the range of 99%. This similarity is computed by applying in the style of Hearst (1997) a cosine-based metric on the morphed segments. Foltz et al's (1998) approach is in line with the earlier TextTiling method that identifies subtopic structure in text (Hearst, 1997). Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997). Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al, 2003). The best known algorithm based on this idea is TextTiling (Hearst, 1997). reference segmentation from a coder should not be trusted, given that inter-annotator agreement is often reported to be rather poor (Hearst, 1997, p. 54). (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean kappa scores were calculated by comparing a coder's segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53-54). Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings. This category choice is similar to those chosen by Hearst (1997, p. 53), who computed chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt). The best known algorithm based on this idea is TextTiling (Hearst, 1997). Automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarisation to anaphora resolution (Hearst, 1997). While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). As in (Hearst, 1997), we segment each document into several 'mini-documents', each one devoted to a single topic, and then to perform location and topic-based clustering over the (now larger) set of mini-documents. So far, it has been used mainly in the context of automatic text segmentation, where a change in vocabulary is often the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al, 1998)). In this study, we use Galley et al's (2003) LCSeg algorithm, a variant of TextTiling (Hearst, 1997). To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. Compared to the task of segmenting expository texts reported in Hearst (1997) with a 39.1% chance of each paragraph end being a target topic boundary, the chance of each speaker turn being a top-level or sub-topic boundary in our ICSI corpus is just 2.2% and 0.69%. For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al, 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows.
Passonneau and Litman (1997) describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie. We turned the segmentation task into a classification task by using boundaries between dialogue acts as one class and non-boundaries a the other (see Passonneau and Litman (1997) for a similar practice). By using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have a higher accuracy than methods using a single marker alone (Passonneau and Litman, 1997). Even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (PassonneauandLitman, 1997). These referential and lexical features build on the work of Passonneau and Litman (1997), who use them in discourse segmentation. First, if non-canonicals are indicators of attentional stack pops, they should be more likely at segment boundaries; hence, we expect an increased presence of cue words (Passonneau and Litman, 1997) on non-canonicals compared to canonicals. In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries. This makes it different from others using surface features like (Passonneau and Litman, 1997). Automatic discourse segmentation, as shallow annotation of discourse structure, also provides a testing grounds for linguistic theories of discourse (Passonneau and Litman, 1997) and provides a natural unit of measure in linguistic corpora (Biber et al, 2004). Human annotators demonstrate frequent disagreement about the number of segments and exactly where the transitions between segments occur, while still demonstrating statistically significant agreement (Passonneau and Litman, 1997).
for a recent elaboration of these concepts see Mohri, 1997 [13]). Weighted finite-state transducers have found recent favor as models of natural language (Mohri, 1997). Application of cascades of weighted string transducers (WSTs) has been well-studied (Mohri,1997). We recall previous formal presentations of WSTs (Mohri, 1997) and note informally that they may be represented as directed graphs with designated start and end states and edges labeled with input symbols, output symbols, and weights. There exist a general weighted determinization and weight pushing algorithms that can be used to create a deterministic and pushed automaton equivalent to an input word or phone lattice (Mohri, 1997). To apply the rules defining the classes to the input corpus, we just need to compose the automaton with a and project the result on the output: X can be made stochastic using a pushing algorithm (Mohri, 1997).  The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997]. Sample XFST code Finite-state transducers are robust and time and space efficient (Mohri, 1997). This two-way power of the finite-state transducer (Mohri, 1997) has significantly reduced the amount of efforts to build the HUMT system. Another very important and powerful strength of finite-state transducers, they can be composed together to build a single transducer that can perform the same task that could be done with help of two or more transducers when applied sequentially (Mohri, 1997), not only allows us to build a direct Hindi Urdu transducer, but also helps to divide difficult and complex problems into simple ones, and has indeed simplified the process of building the HUMT system. Equivalence is efficiently tested by pushing the (deterministic) automata to canonicalize their arc labels and then testing unweighted equivalence (Mohri, 1997). However, the computational consequences of this can be lessened by lazy evaluation techniques (Mohri, 1997) and we believe that this finite state approach to constructing semantic representations is viable for a broad range of sophisticated language interface tasks. The combined WFST will be more efficient by optimizing with determinization, minimization and pushing algorithms of WFSTs (Mohri, 1997). Regular languages, especially generated by deterministic finite state automata are widely used in Natural Language processing, for various different tasks (Mohri, 1997). Not all PFAs can be determinized, as discussed by (Mohri, 1997). Numerous examples of the utility of word lattices come from the field of finite state automata, language modeling, speech recognition, parsing and machine translation (Mohri, 1997, inter alia). O-TRIE is a deterministic right-branching trie encoding (Leermakers, 1992) with weights pushed left (Mohri, 1997). Additionally, with I-tries, only the top-level intermediate rules have probability less than 1, while for O-tries, one can back-weight probability as in (Mohri, 1997). For example transducer determinization (Mohri, 1997) uses a set of pairs of states and weights.
The work reported in Wu (1997), which uses an inside-outside type of training algorithm to learn statistical context free transduction. The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntax based machine translation systems such as Wu (1997). In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al (2006) and Chiang (2007) are not expressive enough to do that.  Bilingual Bracketing [Wu 1997] is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment. In [Wu 1997], the Bilingual Bracketing PCFG was introduced, which can be simplified as the following production rules. More suitable ways could be bilingual chunk parsing, and refining the bracketing grammar as described in [Wu 1997]. Among the grammar formalisms successfully put into use in syntax based SMT are synchronous context-free grammars (SCFG) (Wu, 1997). Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6) in the length of the sentence (Wu, 1997). Although this is less than the O (n6) complexity of exact ITG (In version Transduction Grammar) model (Wu, 1997), a quintic algorithm is often quite slow. In this respect it resembles Wu's bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrase based philosophy. Bilingual bracketing methods were used to produce a word alignment in (Wu, 1997). We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy significantly higher than previous known methods. (Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O (n6) as opposed to the monolingual O (n3) time. The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). Wu (1997) demonstrated that for pairs of sentences that are less than 16 words, the ITG alignment space has a good coverage over all possibilities. Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997).
One could also expand the lexicon, by adapting algorithms for analyzing unknown words (e.g., Mikheev, 1997). We have used LTPOS (Mikheev, 1997), which performed the task almost error less. Step 6 contains a call to the other main LT TTT program, LTOPS (Mikheev, 1997), which performs both sentence identification and POS tagging. The search for such rules has previously been conducted in the context of supervised part-of-speech tagging (Mikheev, 1997). Tagging and chunking is done by a standard tagger and chunker, LTPos (Mikheev,1997). The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence identifier (Mikheev, 1997). PoS tagging can be performed using LTPOS (Mikheev, 1997). A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997). the possible part(s)-of-speech of unknown words (Mikheev,1997). The other main LT TTT program is ltpos, a statistical combined part-of-speech (POS) tagger and sentence boundary disambiguation module (Mikheev, 1997). The next stage in the linguistic analysis module performs noun group and verb group chunking using fsg match with the specialised hand-written rule sets which were the core part of LT CHUNK (Finch and Mikheev, 1997). The stripping-recoding rules could be manually encoded, mined from a monolingual corpus usinga learning method such as (Mikheev, 1997), or supplied by a source terminology extraction system that handles morphological variations. Following Mikheev (1997), we therefore adjust reliability using lower confidence limit statistics. The identification of sentence boundaries, mark-up of sentence elements and POS tagging is done by the statistical program lt pos (Mikheev, 1997). Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead. The prime public domain examples of such implementations include the Trigrams'n'Tags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997). The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word 'guesser' (Mikheev, 1997). Mikheev (1997) suggested a guessing-rule technique, based on prefix morphological rules ,suffix morphological rules, and ending-guessing rules. Transitions to the normal and pivotal stage occur when an estimator of the relative frequency is high enough, for example by taking the lower bound of the confidence interval (Mikheev,1997). Lexical knowledge (for unknown words) and the word lemma (for known words) provide, w.h.p, one sided error (Mikheev, 1997).
On the other hand, as Abney (1997) points out, the context-sensitive dependencies that unification-based constraints introduce render the relative frequency estimator suboptimal. Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework. Abney (1997) proposes a gradient ascent, based upon a Monte Carlo procedure for estimating E0 (fj). Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). As Abney (1997) shows, we can not use relatively simple techniques such as relative frequencies to obtain a model for estimating derivation probabilities in attribute-value grammars. PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997). Simple PCFG based models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney,1997).  Unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible UBG readings do not sum to the value 1, a problem which is discussed intensively by Eisele (1994), Abney (1997). We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars. For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). In particular, when we apply Feature-Based LTAG (FBLTAG), the above probability is no longer consistent because of the non-local constraints caused by feature unification (Abney, 1997). Such constraints are known 83 to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). As is well known (Abney, 1997), DAG-like dependencies can not in general be modeled with a generative approach of the kind taken here. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations. Abney (1997) pointed out that the non-context free dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of log linear models for defining probability distributions over the parses of a unification grammar. The following section reviews stochastic unification grammars (Abney, 1997) and the statistical quantities required for efficiently estimating such grammars from parsed training data (Johnson et al, 1999). (Abney, 1997) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures.  Abney gives fuller details (Abney, 1997).
Two of the fundamental components of a natural language communication are word sense discovery (Jones, 1986) and word sense disambiguation (Ide and Veronis, 1998). Ide and Veronis (1998) present a very concise survey of the history of ideas used in word sense disambiguation; for a recent survey of the state-of-the-art one can refer to (Navigli, 2009). Several types of information are useful for WSD (Ide and Veronis 1998). The NLP field has gone through a very long tradition of algorithms designed for solving this problem (Ide and Veronis, 1998). Several efforts have been made to develop automatic WSD systems that can provide accurate sense tagging (Ide and Veronis, 1998), with a current emphasis on creating manually sense-tagged data for supervised training of statistical WSD systems, as evidenced by SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Edmonds and Cotton, 2001). Following Ide and Veronis (1998) we distinguish between data and knowledge-driven word sense disambiguation. Historically, after work on WSD had overcome so called early doubts (Ide and Veronis, 1998) in the 1960s, it was applied to various NLP tasks, such as machine translation, information retrieval, content and grammatical analysis and text processing. In general, following Ide and Veronis (1998) the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches. A great deal of research on WSD has been done over the past decade (Ide and Veronis, 1998). Regardless of the technique that is used for WSD, the most important part of the process is the context in which the word appears (Ide and Veronis 1998). WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks (Ide and Veronis, 1998). Following Ide and Veronis (1998) we can distinguish between data and knowledge-driven word sense disambiguation (WSD). After work on WSD had overcome so-called early doubts (Ide and Veronis, 1998) in the 1960s, it was applied to various NLP tasks, such as machine translation ,information retrieval, content and grammatical analysis and text processing. In general, following Ide and Veronis (1998) the various WSD approaches of the past can be divided into two types, i.e., data and knowledge-based approaches. Ide and Veronis (1998) argue that word sense ambiguity is a central problem for many established HLT applications (for example Machine Translation, Information Extraction and Information Retrieval). A review of methods for word sense disambiguation is presented by Ide and colleagues (Ideand Veronis, 1998). Unsupervised methods using dictionaries and corpora were proposed for monolingual WSD (Ide and Veronis, 1998).
Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on information content. Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. It was first suggested by Leacock et al (1998). Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy. Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc. The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes. To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). The method we applied is based on the monosemous relatives of the target words (Leacock et al, 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. This method is inspired in (Leacock et al, 1998).
Such items have also been called bridging anaphors (Poesio and Vieira, 1998). Bridging anaphora, as in (2a) above, have received much attention, see e.g. Asher and Lascarides (1998) or Poesio and Vieira (1998). Associative anaphora (e.g., Poesio and Vieira, 1998) and indirect anaphora (e.g., Murata and Nagao, 2000) are virtually the same phenomena that this paper is concerned with. A serious problem when working with bridging reference sis the fact that subjects, when asked for judgments about bridging references in general, have a great deal of difficulty in agreeing on which expressions in the corpus are bridging references, and what their anchors are (Poesio and Vieira, 1998). Things are different for associative anaphora, see (Poesio and Vieira, 1998). Poesio and Vieira (1998) carried out corpus studies indicating that in corpora like the Wall Street Journal portion of the Penn Treebank. Index1 refers to the sequential numbering of definite descriptions; Index2 refers to the sequential numbering of noun phrases; and Code refers to their classification, according to discourse status (Poesio and Vieira, 1998). Poesio and Vieira (1998) found that of the 1,400 definite descriptions in their corpus, only about 50% were subsequent mention or bridging references, whereas 50% were first mentions. this was confirmed by Poesio and Vieira (1998). The annotation scheme proposed by Poesio et al (Poesio and Vieira, 1998) is a product of a corpus based analysis of definite description (DD) use showing that more than 50% of the DDs in their corpus are discourse new or unfamiliar. In fact, it has been shown that the agreement of subjects annotating bridging (Poesio and Vieira, 1998) or discourse (Cimiano,2003) relations can be too low for tentative conclusion to be drawn (Carletta, 1996). Poesio and Vieira (1998) report lower human agreement on more fine-grained classifications of definite descriptions. the results of Poesio and Vieira (1998) indicated that this type of an notation could be highly unreliable. One of our aims was to continue the work on bridging references annotation and interpretation in (Poesio and Vieira, 1998), which showed that marking up bridging references is quite hard. Only perhaps one in four or five NPs are markable (Poesio and Vieira, 1998). as Poesio and Vieira (1998) show, only about 2/3 of definite descriptions which are anaphoric have the same head as their antecedent. In previous work (Poesio and Vieira, 1998) we reported the results of corpus annotation experiments in which the subjects were asked to classify the uses of definite descriptions in Wall Stree Journal articles. The final configuration of the system was arrived at on the basis of an extensive evaluation of the heuristics using the corpus annotated in our previous work (Poesio and Vieira, 1998). In our corpus study (Poesio and Vieira, 1998) we found that our subjects did better at ideutifying discourse-new descriptions all together. This model aims to resolve both coreferent and associative (also called bridging (Poesio and Vieira, 1998)) cases of nonpronominal anaphora.
  After extracting the argument heads of the target slots of each verb (e.g., the intransitive subject and the transitive object for the causative alternation), she then determined their selectional profiles using a minimum description length tree cut model (Li and Abe, 1998). The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. W ealso plan to compare the results to the tree cut algorithm reported in (Li and Abe, 1998), which allows different levels to be identified for different subtrees. Our selectional preference model relies on Li and Abe (1998), applying the MDL principle to determine selectional preferences of verbs and their arguments, by means of a concept hierarchy ordered by hypernym/hyponym relations. Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them.   In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). Initially our project began as an application of the closely related MDL approach of Li and Abe (1998), but was hindered by sparse data. Li and Abe (1998) used a tree cut model over WordNet, based on the principle of MinimumDescription Length (MDL).  McCarthy determines the sense profile of a verb/slot pair using a minimum description length tree cut model over the frequency-populated hierarchy (Li and Abe, 1998). We suspect the problem is two-fold, arising from the dependence of her method on tree cut models (Li and Abe, 1998). The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus (Li and Abe, 1998). This leads to the notion of "cutting" the hierarchy at one or more positions (Li and Abe, 1998). Li and Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length.
The (Caraballo and Charniak, 1998) article considers a number of different figures of merit for ordering the agenda, and ultimately recommends one that reduces the number of edges required for a full parse into the thousands. (Caraballo and Charniak, 1998) and [Gold98] use a figure which indicates the merit of a given constituent or edge, relative only to itself and its children but independent of the progress of the parse we will call this the edge's independent merit (IM).   Again it is reminiscent of a best-first parser (Caraballo and Charniak, 1998) in the use of an agenda and a chart, but is fundamentally different due to the fact that there is no input order.  Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. Best-first parsers deal with this by allowing an upward propagation, which updates such edges' scores (Caraballo and Charniak, 1998). To situate our results, the FOMs used by (Caraballo and Charniak, 1998) require 10K edges to parse 96% of these sentences, while BF requires only 6K edges. Therefore, we were less concerned with improving efficiency, and more with the properties of this algorithm, which we consider a baseline method upon which more sophisticated techniques such as best-first parsing (Caraballo and Charniak, 1998). Similar to a best-first parser (Caraballo and Charniak, 1998), the highest scored hypothesis is expanded first. For efficiency reasons, we use a coarse-to-fine pruning scheme like that of Caraballo and Charniak (1998). It might be used to rapidly compute approximate outside-probability estimates to prioritize best-first search (e.g., Caraballo and Charniak, 1998). A quite different approach to parsing efficiency is taken in Caraballo and Charniak (1998). The results cited in Caraballo and Charniak (1998) can not be compared directly to ours, but are roughly in the same equivalence class. The standard methods of the best analysis selection (Caraballo and Charniak, 1998) usually use simple stochastic functions independent on the peculiarities of the underlying language. For instance, (Caraballo and Charniak, 1998) presents and evaluate different figures of merit in the context of best-first chart parsing. Caraballo and Charniak (1998) present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string.
 For examples of work in producing abstract-like summaries, see Radev and McKeown (1998), which combines work in information extraction and natural language processing. Radev and McKeown (1998) point out when summarizing interesting news events from multiple sources, one can expect reports with contradictory and redundant information. Notable systems are SUMMONS (Radev and McKeown, 1998). While template-based representations have been proposed for information merging in the past (Radev and McKeown, 1998), they considered only domain-specific scenarios. the main previous body of work on biographical summarization is that of (Radev and McKeown 1998). This is particularly problematic in the case of multi-document summarization, where sentences extracted from related documents are very likely to express similar information in different ways (Radev and McKeown, 1998). Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998). Since data producing these summaries can be sourced in different documents, summary fusion techniques as proposed in (Radev and McKeown, 1998) can be employed. Since (Radev and McKeown, 1998) describes the summary fusion mechanisms, Class 3 of questions can be reduced in this paper to Class 2, which deals with the processing of the template. (Radev and McKeown, 1998) and (Harabagiu and Lacatusu, 2004) define agreement (when two sources report the same information), addition (when a second source reports additional information), contradiction (when two sources report conflicting information). Recently, advanced QA systems defined relationships (equivalence, contradiction, ...) between Web page extracts or texts containing possible answers in order to combine them and to produce a single answer (Radev and McKeown, 1998), (Harabagiu and Lacatusu, 2004), (Webber et al., 2002). These include comparing templates filled in by extracting information using specialized, domain specific knowledge sources from the document, and then generating natural language summaries from the templates (Radev and McKeown, 1998). Similarly, Radev and McKeown (1998) used IE combined with Natural Language Generation (NLG) in their SUMMON system.
Knight and Graehl (1998) proposed a Japanese-English transliteration method based on the mapping probability between English and Japanese katakana sounds. The first is machine transliteration (Knight and Graehl, 1998), in which names and technical terms are translated across languages with different sound systems. Knight and Graehl (1998) used a bilingual English-katakana dictionary, a katakana-to-English phoneme mapping, and the CMU Speech Pronunciation Dictionary to create a series of weighted finite-state transducers between English words and katakana that produce and rank transliteration candidates. Compared with the previous work on katakana to-English transliteration, these accuracies do not look particularly high: both Knight and Graehl (1998) and Bilac and Tanaka (2004) report accuracies above 60% for 1-best transliteration. We should emphasize that this is due to the difficulty of our test data, and that we have tested against a baseline that has been shown to outperform Knight and Graehl (1998). phoneme based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al2007), and paraphrasing (Barzilay and Lee, 2003). Our terminology is based on studies on machine transliteration (Knight and Graehl, 1998). Transliteration (Knight and Graehl, 1998) is a mapping from one system of writing into another, automation of which has been actively studied be tween English and other languages such as Arabic, Chinese, Korean, Thai, and Japanese. Knight and Graehl (1998) model the transliteration of Japanese syllabic katakana script into English with a sequence of finite-state transducers. Unlike the transducers proposed in (Stalls and Knight, 1998) and (Knight and Graehl, 1998) no attempt is made to model the pronunciation of words. The seen test is similar to tests run in (Knight and Graehl, 1998) and (Stalls and Knight, 1998) where the models could not produce any words not included in the language. Lee and Chang (2003) detect transliterations with a generative noisy channel transliteration model similar to the transducer presented in (Knight and Graehl, 1998). One area is the generative transliteration modeling (Knight and Graehl, 1998), which studies how to convert a word from one language to another using statistical models. For example Knight and Graehl (1998) use the lexicon frequency. If source and target languages have different phoneme sets, alignment between the different phonemes is also required (Knight and Graehl, 1998). Instead of using the lexicon frequency (Knight and Graehl, 1998) or Web statistics (Qu and Grefenstette, 2004), we propose validating transliteration pairs according to the alignment distance D between the aligned English graphemes and Chinese phonemes (see equations (2) and (5)). We use the problem formulation of Knight and Graehl (1998). phoneme based method (Knight and Graehl, 1998) makes use of phonetic correspondence to generate the transliteration. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998).
Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). See Johnson (1998) for a presentation of the transform/de transform paradigm in parsing. First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998).  These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars.  Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998).  The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent's label. To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998).
Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates (Michel Simard et al., 1992) or from a seed translation lexicon. The latter approach uses other filtering parameters: maximum point ambiguity level, point dispersion and angle deviation (Melamed, 1999, pp. 115-116). Melamed (1999) also filtered candidate correspondence points obtained from orthographic cognates. This approach is similar to Melamed (1999) but, in contrast, it is statistically supported and uses no heuristics. We use a similarity function proposed in (Contractor et al, 2010) which is based on Longest Common Subsequence Ratio (LCSR) (Melamed, 1999). The former includes string edit distance (Wagner and Fischer, 1974), longest common subsequence ratio (Melamed, 1999), and measures based on shared character n-grams (Brew and McKelvie, 1996). Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dicecoefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter's stemmer (Porter, 1980), Morpha (Minnen et al,2001), and CST's lemmatiser (Dalianis and Jonge 3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. Other popular measures include Dice's Coefficient (DICE) (Adamson and Boreham, 1974), and the length-normalized measures Longest Common Subsequence Ratio (LCSR) (Melamed, 1999), and Longest Common Prefix Ratio (PREFIX) (Kondrak,2005). We adopt an improved definition (suggested by Melamed (1999) for the French-English Canadian Hansards) that does not over-propose shorter word pairs. Our labelled set is then generated from pairs with LCSR 0.58 (using the cutoff from Melamed (1999)). A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999). The Korean-English parallel data was collected from news websites and sentence aligned using two different tools described by Moore (2002) and Melamed (1999).  Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string. Our work is based on a modification of the SIMR bitext mapping algorithm (Melamed, 1999). For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or - alternatively - Dan Melamed's GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999).
These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in Tree Adjoining Grammar models (Bangalore and Joshi,1999). Supertags are the elementary structures of Lexicalized Tree Adjoining Grammars (LTAGs) (Bangalore and Joshi, 1999). We used the supertagger (Bangalore and Joshi, 1999) to supertag each word in the corpus. Insertion of further information such as supertags (Bangalore and Joshi, 1999) or word stems can also be beneficial for further processing. The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns rich descriptions (supertags) that impose complex constraints in a local context. The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). Bangalore and Joshi (1999) indicated that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enabled effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing. As methodologies deriving well-formedness of a sentence we use super tagging (Bangalore and Joshi, 1999) with lightweight dependency analysis (LDA) (Bangalore, 2000), link grammars (Sleator and Temperley, 1993) and a maximum entropy (ME) based chunk parser (Bender et al, 2003). Supertagging (Bangalore and Joshi, 1999) uses the Lexicalized Tree Adjoining Grammar formalism (LTAG). We used the supertagger of Bangalore and Joshi (1999). This is a variant of the approach above, but using super tags (Bangalore and Joshi, 1999) instead of PoS tags. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). It was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999). We condition prosody not only on word strings and their parts-of-speech but also on richer syntactic information encapsulated in the form of Supertags (Bangalore and Joshi, 1999). In addition to the POS tags, we also annotate the utterance with Supertags (Bangalore and Joshi, 1999). Supertags have been successfully applied to guide parsing in symbolic frameworks such as Lexicalised Tree-Adjoning grammar (Bangalore and Joshi, 1999). The parser uses a two-stage system, first employing a super tagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. We automatically annotate a user's utterance with super tags (Bangalore and Joshi, 1999). The second direction concerns experiments on supertagging (Bangalore and Joshi, 1999) followed by a parsing stage the tagging stage associates to each word a supertag. Bangalore and Joshi (1999), Clark and Curran (2004) and Matsuzaki et al (2007) show that by using a supertagger before (CCG and HPSG) parsing, the space required for discriminative training is drastically reduced.
Using functional centering (Strube and Hahn, 1999) to rank the CFs led to no improvements, because of the almost perfect correlation in our domain be tween subject hood and being discourse-old. For example, Strube and Hahn (1999) introduce Functional Centering, a variant of Centering Theory which utilizes information status distinctions between hearer-old and hearer-new entities. Violations of CHEAPNESS (Strube and Hahn, 1999), COHERENCE and SALIENCE (Kibble and Power, 2000). work has also been done on adapting the centering model to other, freer word order languages such as German (Strube and Hahn, 1999). Finally, the scripts determine whether CBn is the same as CPn, known as the principle of cheapness (Strube and Hahn, 1999).  Arguably, the free word-order of German arguably leads to a clearer distinction between grammatical function, surface order, and in formation status (Strube and Hahn, 1999). Thus the size of the annotated data (3,115 personal pronouns1, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English co referential expressions in (Ng and Cardie, 2002)) .In the experiments, systems only looked for single NP antecedents. In general, however, the required knowledge sources are lacking, so they must be hand-coded and can only be applied in restricted domains (Strube and Hahn, 1999). Strube and Hahn (1999) extend the context to more than the last sentence, but switch preference order between the last and the current sentence so that an antecedentes determined in the last sentence, whenever possible. Strube (1998) and Strube and Hahn (1999) argue that the information status of an antecedent is more important than the grammatical role in which it occurs. Moreover, in order to compare our proposal with Centering approach, Functional Centering by Strube and Hahn (1999) has also been implemented. Finally, a transition is considered to satisfy the CHEAPNESS constraint (Strube and Hahn, 1999) if Cb (ui)= Cp (ui).
 Figure 3 presents the deductive inference rules (Goodman, 1999) for our generation algorithm. We presume that readers are familiar with declarative descriptions of inference algorithms, as well as with semiring parsing (Goodman, 1999). Goodman (1999) shows how a parsing logic can be combined with various semirings to compute different kinds of information about the input. Under the various derivation semirings (Goodman, 1999). To the best of our knowledge, Logic CT is the first published translation logic to be compatible with all of the semirings catalogued by Goodman (1999). Borrowing terms from parsing semirings (Goodman, 1999), a packed forest is composed of additive forest nodes and multiplicative forest nodes. Goodman (1999) augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure. This is often done using back pointers, but can also be accomplished by representing the most probable proof for each theorem in its entirety as part of the semiring value (Goodman, 1999). This is similar to the k-best semiring defined by Goodman (1999). Goodman (1999) handles this situation more carefully, though our version is more likely to be used in practice for both the Viterbi proof and k-best proof semirings. Solvers have been proposed by Goodman (1999), by Klein and Manning (2001) using a hypergraph representation, and by Eisner et al (2005).  PTREE still propagates in O (n3) time: simply change the first-order parser's semiring (Goodman, 1999) to use max instead of sum. However, a more general framework to specify these algorithms is semiring-weighted parsing (Goodman, 1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. Deductive logic (Pereira and Warren, 1983), extended with semirings (Goodman, 1999), is an established formal ism used in parsing. we write an inference rule with antecedents on the top line and consequent on the second line, following Goodman (1999) and Shieber et al (1995). This suggests a useful generalization: semiring-weighted deduction (Goodman, 1999). Goodman (1999) describes semirings for the Viterbi derivation, k-best Viterbi derivations, derivation forest, and number of paths.
The bias introduced by TMEMs is a practical alternative to finding optimal translations, which is NP-complete (Knight, 1999). However, as phrase-based decoding usually casts translation as a string concatenation problem and permits arbitrary permutation, it proves to be NP-complete (Knight, 1999). In (Knight,1999) it was proved that the Exact Decoding problem is NP-Hard when the language model is a bigram model. Note that our results for decoding are sharper than that of (Knight, 1999). However allowing reordering in translation is computationally expensive and in some cases even provably NP-complete (Knight, 1999). In theory, this process can be reduced to the Traveling Salesman Problem and thus requires an exponential time algorithm (Knight, 1999). Part of the complexity arises from the expressive power of the translation model: for example, a phrase or word-based model with full reordering has exponential complexity (Knight, 1999). This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word's translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999). Both of the two steps are very time-consuming due to the exponential number of translation rules and the complex nature of machine translation as an NP-hard search problem (Knight, 1999). If arbitrary re-orderings are allowed, the search problem is NP-complete (Knight, 1999). Under certain restrictions, both algorithms handle MT-related problems efficiently that are generally NP complete (Knight, 1999). In the general case, no efficient search algorithm exists to search all word or phrase reorderings (Knight, 1999). (Knight, 1999) shows that the decoding problem for SMT as well as some bilingual tiling problems are NP-complete, so no efficient algorithm exists in the general case. Investigation of the computational complexity of translation models has started in (Knight, 1999) for word-to-word models. Knight (1999) has shown that even for a simple form of statistical MT models, the decoding problem is NP-complete. in particular (Knight, 1999) has shown that any TSP instance can be mapped to a sub-case of a word-based SMT model, demonstrating NP-hardness of the decoding task. As already mentioned, the similarity between SMT decoding and TSP was recognized in (Knight, 1999), who focussed on showing that any TSP can be reformulated as a sub-class of the SMT decoding problem, proving that SMT decoding is NP-hard. Knight (1999) has shown the problem to be NP-complete.  It has been known that phrase-based decoding should be constrained to some extent not only for transferring the NP-hard problem (Knight,1999) into a tractable one in practice but also for improving translation quality.
This paper considers differences in texts in the well known Penn TreeBank (hereafter, PTB) and in particular, how these differences show up in the Penn Discourse TreeBank (Prasad et al, 2008). Genre differences at the level of discourse in the PTB can be seen in the manual annotations of the Penn Discourse TreeBank (Prasad et al, 2008). The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides annotations for the arguments and relation senses of one hundred pre-selected discourse connectives over the news portion of the Penn Treebank corpus (Marcus et al, 1993). The PDTB adopts a lexically grounded approach to discourse relation annotation (Prasad et al, 2008). The set of discourse connectives is taken from the Penn Discourse Treebank (Prasad et al, 2008), thus creating a list of 240 potential connectives. Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al, 2008) benefits the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. The Penn Discourse Treebank (Prasad et al, 2008) (see Section 3.1 below) includes around 100 connective types, but the exact number varies across studies, 194 depending on the discourse theory used to classify them. One of the very few available discourse annotated corpora is the Penn Discourse Treebank (PDTB) in English (Prasad et al, 2008). One of the most important resources for discourse connectives in English is the Penn Discourse Treebank (Prasad et al, 2008). The last example in Table 1 is a sentence from the Penn Discourse Treebank (Prasad et al., 2008). One of the few available discourse annotated corpora in English is the Penn Discourse Treebank (PDTB) (Prasad et al, 2008). The Penn Discourse Treebank (PDTB) (Prasad et al, 2008) is such a corpus which provides a discourse-level annotation on top of the Penn Treebank, following a predicate argument approach (Webber, 2004). Later, with the release of manually annotated corpus, such as Penn Discourse Treebank 2.0 (PDTB) (Prasad et al, 2008), recent studies performed implicit discourse relation recognition on natural (i.e., genuine ) implicit discourse data (Pitler et al, 2009) (Lin et al,2009) (Wang et al, 2010) with the use of linguistically informed features and machine learning algorithms.  PDTB (Prasad et al, 2008) is the largest hand annotated corpus of discourse relation so far. It is also considered to be the anchor of discourse relations, in the sense of the Penn Discourse Treebank (PDT) (Prasad et al, 2008). The goal is not only to annotate the data, but also to compare the representation of these relations in the Prague Dependency Treebank with the annotation done at the Penn Treebank, which was carried out at University of Pennsylvania (Prasad et al., 2008). As described e.g. in Mladova et al. (2009), the annotation framework that we use is based on the knowledge obtained from studying various other systems, especially the Penn Discourse Treebank (Prasad et al., 2008), but naturally it has been adjusted to specific needs of the Czech language and PDT. To attempt an answer to this question, we utilized the end-to-end discourse parser proposed by Lin et al (2010) to extract PDTB-styleddiscourse relations (Prasad et al, 2008) from RT data. In the realm of discourse annotation, the Penn Discourse TreeBank (PDTB) (Prasad et al, 2008) separates itself by adopting a lexically grounded approach.
Using the MUC co reference scoring algorithm (see Vilain et al 1995). Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al, 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANC gold (Recasens and Hovy, 2011). For evaluation of coreference relations, we calculated re call and precision based on the MUC score (Vilain et al, 1995). For evaluation, Vilain et al (1995)'s scoring algorithm was adopted to compute the recall and precision of the whole co reference resolution. We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al 1995) for the coreference resolution task. For tests on the MUC data, we report both F-measure using the official MUC score (Vilain et al, 1995 ) and ECM-F. Three metrics have been commonly used for evaluating coreference performance over an unrestricted set of entity types: i) The link based MUC metric (Vilain et al, 1995), ii) The mention based B-CUBED metric (Bagga and Baldwin, 1998) and iii) The entity based CEAF (Constrained Entity Aligned F-measure) metric (Luo, 2005).  So, the evaluation programs, in an obvious extension of what was pro posed in (Vilain et al, 1995) for identity. For coreference resolution, MUC (Vilain et al 1995), B-CUBED (Bagga and Baldwin, 1998) and CEAF-E (Luo, 2005) are used for evaluation. Another possible choice is the MUC F-measure (Vilain et al., 1995). For coreference resolution, we report the performance in terms of recall, precision, and F1-measure using the commonly-used model theoretic MUC scoring program (Vilain et al, 1995). Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two coreference scoring programs: the MUC scorer (Vilain et al., 1995). We report recall, precision, and F1 for MUC (Vilain et al, 1995). Vilain et al (1995) introduced the link-based MUC evaluation metric for the MUC-6 and MUC 7 coreference tasks.  Results are shown in Table 2 (Duplicated Soon Baseline) where performance is reported in terms of recall, precision, and F-measure using the model theoretic MUC scoring program (Vilain et al, 1995). We report in the following tables the MUC score (Vilain et al, 1995). MUC (Vilain et al, 1995).
In addition to these finite state pattern approaches, a variant of Brill rules has been applied to the problem, as outlined in (Aberdeen et al, 1995). The typical machine learning approaches for English NE are transformation-based learning [Aberdeen et al 1995]. Each document in the collection to be summarized is processed by a sentence tokenizer, the Alembic part-of-speech tagger (Aberdeen et al 1995). Many research groups are making progress toward efficient customization, such as BBN (Weischedel, 1995), NYU (Grishman, 1995), SRI (Appelt et al, 1995), SRA (Krupka, 1995), MITRE (Aberdeen et al, 1995), UMass (Fisher et al, 1995) ... etc.
For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001).
The approach is a fully automated variant of the example selection algorithm introduced in Harabagiu et al (2001). Similar observations are made by Harabagiu et al (2001), who point out that intelligent selection of positive instances can potentially minimize the amount of knowledge required to perform coreference resolution accurately. Examples of such scoring functions include the Dempster Shafer rule (see Kehler (1997) and Bean and Riloff (2004)) and its variants (see Harabagiu et al (2001) and Luo et al (2004)). To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., Harabagiu et al (2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)). Vieira & Poesio (2000), Harabagiu et al (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, other and definite NP anaphora, and MUC-style coreference resolution. These measures are not specifically developed for coreference resolution but simply taken off-the-shelf and applied to our task without any specific tuning i.e. in contrast to Harabagiu et al. (2001), who weight WordNet relations differently in order to compute the confidence measure of the path. In numerous articles the usefulness of this data and software ensemble has been demonstrated (e.g., for word sense disambiguation (Patwardhan et al, 2003), the analysis of noun phrase conjuncts (Hogan, 2007), or the resolution of coreferences (Harabagiu et al, 2001)). Harabagiu et al (2001) use paths through Wordnet, using not only synonym and is-a relations, but also parts, morphological derivations, gloss texts and polysemy, which are weighted with a measure based on the relation types and number of path elements. For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion of hard training instances that may complicate the acquisition of an accurate coreference model. Results presented in Harabagiu et al (2001) are higher than those reported here, but assume that all and only the noun phrases involved in coreference relationships are provided for analysis by the coreference resolution system. We also plan to investigate previous work on common noun phrase interpretation (e.g. Sidner (1979), Harabagiu et al (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems. In (Harabagiu et al, 2001), the path patterns in WordNet are utilized to compute the semantic consistency between NPs.
This would not rule out the possibility that bigram presence is as equally useful a feature as unigram presence; in fact, Pedersen (2001) found that bigrams alone can be effective features for word sense disambiguation. Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Pedersen, 2001). Such descriptions can be found in (Pedersen, 2001b) or (Pedersen, 2002). The frustration with models that lack an intuitive interpretation led to the development of decision trees based on bigram features (Pedersen, 2001a). The supervised lexical sample system that participated in SENSEVAL-3 is the Duluth3 (English) or Duluth8 (Spanish) system as used in SENSEVAL 2 (Pedersen, 2001b). In (Pedersen, 2001a) we introduced the use of decision trees based strictly on bigram features. The former approach relies on previously acquired linguistic knowledge, and the latter uses techniques from statistics and machine learning to induce models of language usage from large samples of text (Pedersen, 2001). We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). We also obtained salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001). Salient bigrams: Salient bigrams within the abstract with high log-likelihood scores, as described by Pedersen (2001). They are derived from the Duluth systems that participated in SENSEVAL-2, and which are more fully described in (Pedersen, 2001b). However, both (Pedersen, 2001a) and (Lee and Ng, 2002) show that different learning algorithms produce similar results and that the use of appropriate features may dramatically improve results. We also obtain salient bigrams in the context, with the methods and the software described in (Pedersen, 2001). The recent work of Pedersen (2001a) and Zavrel et al (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set. In SENSEVAL-2, the various Duluth systems (Pedersen, 2001b) attempted to investigate whether features or learning algorithms are more important. It should be noted that the experiments for the SENSEVAL-2 and SENSEVAL-1 data using unigrams and bigrams are re-implementations of (Pedersen, 2001a), and that our results are comparable. (Pedersen, 2001b) compares decision trees, decision stumps and a Naive Bayesian classifier to show that bigrams are very useful in identifying the intended sense of a word. Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001).
Work in statistically parsing conversational speech (Charniak and Johnson, 2001) has examined the performance of a parser that removes edit regions in an earlier step. A study by Charniak and Johnson (2001) shows that one can identify and remove edits from transcribed conversational speech with an F-score of about 78, with roughly 95 Precision and 67 recall. Readability studies have shown that disfluencies (fillers and speech repairs) may be deleted from transcripts without compromising meaning (Jones et al, 2003), and deleting repairs prior to parsing has been shown to improve its accuracy (Charniak and Johnson, 2001). Earlier work had already made this claim regarding speech repairs and argued that there was consequently little value in syntactically analyzing repairs or evaluating our ability to do so (Charniak and Johnson, 2001). Our division of the corpus follows that used in (Charniak and Johnson, 2001). These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001].  These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001]. We include the distributions with punctuation is to match with the baseline system reported in [Charniak and Johnson 2001], where punctuation is included to identify the edited regions. We take as our baseline system the work by [Charniak and Johnson 2001]. In the rest of the section, we first briefly introduce the boosting algorithm, then describe the method used in [Charniak and Johnson 2001], and finally we contrast our improvements with the baseline system. In [Charniak and Johnson 2001], identifying edited regions is considered as a classification problem, where each word is classified either as edited or normal. We relax the definition for rough copy, because more than 94% of all edits have both reparandum and repair, while the rough copy defined in [Charniak and Johnson 2001] only covers 77.66% of such instances. Descriptions of the 18 conditioning variables from [Charniak and Johnson 2001] 182 rough copy if their corresponding major categories match. Since the original code from [Charniak and Johnson 2001] is not available, we conducted our first experiment to replicate the result of their baseline system described in section 3. We used the exactly same training and testing data from the Switchboard corpus as in [Charniak and Johnson 2001]. Edit disfluency detection systems that rely exclusively on word-based information have been presented by Heeman et al (Heeman et al, 1996) and Charniak and Johnson (Charniak and Johnson, 2001). Based on the convention from Shriberg (1994) and Charniak and Johnson (2001), a disfluent spoken utterance is divided into three parts: the reparandum, the part that is repaired; the interregnum, which can be filler words or empty; and the repair/repeat, the part that replaces or repeats the reparandum. Charniak and Johnson (2001) and Kahn et al (2005) have shown that improved edit region identification leads to better parsing accuracy they observe a relative reduction in parsing f-score error of 14% (2% absolute) between automatic and oracle edit removal. The features used here are grouped according to variables, which define feature sub-spaces as in Charniak and Johnson (2001) and Zhang and Weng (2005).
There is work on lexicon induction using string distance or other phonetic/orthographic comparison techniques, such as Mann and Yarowsky (2001). Moreover, while some techniques (e.g., Mann and Yarowsky (2001)) use multiple languages, the languages used have resources such as dictionaries between some language pairs. Mann and Yarowsky (2001) present a method for inducing translation lexicons based on trasduction modules of cognate pairs via bridge languages. Similarly to Mann and Yarowsky (2001), we show that languages are often close enough to others within their language family so that cognate pairs between the two are common, and significant portions of the translation lexicon can be induced with high accuracy where no bilingual dictionary or parallel corpora may exist. Mann and Yarowsky (2001) applied the stochastic transducer of Ristad and Yianilos (1998) for inducing translation lexicons between two languages, but found that in some cases it offered no improvement over Levenshtein distance. LLW stands for Levenshtein with learned weights, which is a modification of RY proposed by Mann and Yarowsky (2001). As mentioned in (Mann and Yarowsky, 2001), it appears that there are significant differences between the pronunciation task and the cognate identification task. Mann and Yarowsky (2001) saw little improvement over Edit Distance when applying this transducer to cognates, even when filtering the transducer's probabilities into different weight classes to better approximate Edit Distance. For example, Mann and Yarowsky (2001) define a word pair (e, f) to be cognate if they are a translation pair (same meaning) and their Edit Distance is less than three (same form). The marked difference in the availability of monolingual vs parallel corpora has led several researchers to develop methods for automatically learning bilingual lexicons, either by using monolingual corpora (Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Haghighi et al., 2008) or by exploiting the cross-language evidence of closely related "bridge" languages that have more resources (Mann and Yarowsky, 2001). Mann and Yarowsky (2001) investigated the induction of translation lexicons via bridge languages. Mann and Yarowsky (2001) developed yet another model, which outperformed all other similarity measures. The HMM model of (Mann and Yarowsky, 2001) is of distinctly different design than our PHMM model. Mann and Yarowsky (2001) present a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages. Our work is inspired by Mann and Yarowsky (2001). Mann and Yarowsky (2001) distinguish between static metrics, which are sufficiently general to be applied to any language pair, and adaptive metrics, which are adapted to a specific language pair. Mann and Yarowsky (2001) use variants of Levenshtein distance as a static metric, and a Hidden Markov Model (HMM) and a stochastic transducer trained with the Expectation-Maximisation (EM) algorithm as adaptive metrics. Mann and Yarowsky (2001) consider a word pair as cognate if the Levenshtein distance between the two words is less than 3. This finding is consistent with the results of Mann and Yarowsky (2001), although our experiments show more clear-cut differences. Using the same models, Mann and Yarowsky (2001) induced over 90% of the Spanish-Portuguese cognate vocabulary.
Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental probabilistic Earley parser to compute a metric which correlates with increased reading difficulty. Using the prefix probability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word's prefix probability against this word's prefix probability. Since the introduction of a parser-based calculation for surprisal by Hale (2001), statistical techniques have been become common as models of reading difficulty and linguistic complexity. Surprisal (Hale, 2001) is then a straightforward calculation from the prefix probability. Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory. Hale (2001) pointed out that the ratio of the prefix probabilities. Hale (2001) suggests this quantity as an index of psycholinguistic difficulty. Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log probability of word wt given the preceding words, typically computed using a probabilistic context-free grammar. The original formulation of surprisal (Hale 2001) used a probabilistic parser to calculate these probabilities, as the emphasis was on the processing costs incurred when parsing structurally ambiguous garden path sentences. Hale (2001) reported that a probabilistic Earley parser can make correct predictions of garden-path effects and the subject/object relative asymmetry. Hale (2001) introduced the surprisal metric for probabilistic parsers, which measures the log ratio of the total probability mass at word t1 and word t. Hale (2001) showed that surprisal calculated from a probabilistic Earley parser correctly predicts well 356 known processing phenomena that were believed to emerge from structural ambiguities (e.g., garden paths) and Levy (2008) further demonstrated the relevance of surprisal to human sentence processing difficulty on a range of syntactic processing difficulty phenomena. The surprisal (Hale, 2001) at word wi refers to the negative log probability of wi given the preceding words, computed using the prefix probabilities of the parser. Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence. An alternative is to keep track of all derivations, and predict difficulty at points where there is a large change in the shape of the probability distribution across adjacent parsing states (Hale, 2001). This conclusion is strengthened when we turn to consider the performance of the parser on the standard Penn Treebank test set: the Within model showed a small increase in F-score over the PCFG baseline, while the copy model showed no such advantage.5All the models we proposed offer a broad coverage account of human parsing, not just a limited model on a hand-selected set of examples, such as the models proposed by Jurafsky (1996) and Hale (2001) (but see Crocker and Brants 2000). Although this does not match the interpretation of the experimental results followed in this paper, it leaves open the possibility that the feature could model the data according to a different measure of parsing difficulty, such as surprisal (Hale, 2001).
Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al.  Co-traininghas been successfully applied to various applications, such as statistical parsing (Sarkar, 2001). Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). (Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. Sarkar (2001) and Steedman et al (2003 ) investigated using co-training for parsing. Sarkar (2001) applied co-training to LTAG parsing, in which the super tagger and parser provide the two views. Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text.
Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Approaches to the induction of morphology as presented in e.g. Schone and Jurafsky (2001) or Goldsmith (2001) show that the morphological properties of a small subset of languages can be induced with high accuracy, most of the existing approaches are motivated by applied or engineering concerns, and thus make assumptions that are less cognitively plausible. Schone and Jurafsky (2001) select words with frequency higher than 5 to induce morphological segmentation. Presupposing input driven learning, it has been shown in the literature that initial segmenations into words (or word-like units) is possible with unsupervised methods (e.g. Brent and Cartwright (1996)), that induction of morphology is possible (e.g. Goldsmith (2001), Schone and Jurafsky (2001)) and even the induction of syntactic structures (e.g. Van Zaanen (2001)). These models include the signature of (Goldsmith 2001), the conflation set of (Schone and Jurafsky 2001), the paradigm of (Brent et. al. 2002), and the inflectional class of (Monson 2004). In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus.  Schone and Jurafsky (2001) used latent semantic analysis to find affixes. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. (Goldsmith 2000) presents an unsupervised technique based on the expectation maximization algorithm and minimum description length to segment exactly one suffix per word, resulting in an F-score of 81.8 for suffix identification in English according to (Schone and Jurafsky 2001). (Schone and Jurafsky 2001) proposes an unsupervised algorithm capable of automatically inducing the morphology of inflectional languages using only text corpora. Often trie similarities are used as a first step followed by further processing to identify morphemes (Schone and Jurafsky, 2001). Like Schone and Jurafsky (2001), we build clusters that will have both inflectionally and derivationally related stems and evaluate them with respect to a gold standard of only inflectionally related stems. Schone and Jurafsky (2001) builds on this approach, but adds more ad hoc parameters to handle circumfixation. Further cues for morphological learning are presented in (Schone and Jurafsky, 2001) and (Yarowsky and Wicentowsky, 2000). Schone and Jurafsky (2001) employ distributions over adjacent words (yielding a syntactic distance metric) to improve the precision of their conflation sets. In addition, we measure precision, recall, and F1 as in Schone and Jurafsky (2001). Nevertheless, these metrics give us a point of comparison with Schone and Jurafsky (2001) who, using a vocabulary of English words occurring at least 10 times in a 6.7 million word newswire corpus, report F1 of 88.1 for conflation sets based only on suffixation, and 84.5 for circumfixation.
We use a support vector machine (SVM) based chunker yamcha (Kudo and Matsumoto, 2001) for the chunking process. We used the chunker yamcha (Kudo and Matsumoto,2001), which is based on support vector machines (Vapnik, 1998). On the chunking task, the bagged model also outperforms the models of Kudo and Matsumoto (2001).  We use the chunker YamCha (Kudo and Matsumoto, 2001). We employ the technique of Support Vector Machines (SVMs) (Vapnik, 1998) as the machine learning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (Kudo and Matsumoto, 2001) and named entity chunking (Mayfield et al, 2003). In this paper, we use an SVMs-based chunking tool YamCha (Kudo and Matsumoto, 2001). We use a Support Vector Machines-basedchunker, YamCha (Kudo and Matsumoto, 2001), to extract unknown words from the output of the morphological analysis. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). As regards the basic feature set for Chunking, we followed (Kudo and Matsumoto, 2001), which is the same feature set that provided the best result in CoNLL-2000. With Chunking, (Kudo and Matsumoto, 2001) reported the best F-score of 93.91 with the voting of several models trained by Support Vec tor Machine in the same experimental settings and with the same feature set. The model used to obtain the SVM baseline for concept classification was trained using YamCHA (Kudo and Matsumoto, 2001). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al, 2004). See, for example, NPchunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001). We used YamCha (Kudo and Matsumoto, 2001) as a text chunker, which is based on Support Vector Machine (SVM). Therefore, we introduce a Support Vector Machine (below SVM)-based chunker (Kudo and Matsumoto, 2001) to cover the errors made by the segmenter. Wu et al (2007, 2008) presented an automatic chunk pair relation construction algorithm which can handle so-called IOB1/IOB2/IOE1/IOE2 (Kudo and Matsumoto, 2001) chunk representation structures with either left-to-right or right-to left directions. Sha and Pereira (2003) reported the Kudo and Matsumoto (2001) performance on the NP-Chunking task to be 94.39 and to be the best reported result on this task. His system is an extension of Kudo's chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. His system is an extension of Kudo's chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.
(Yarowsky and Ngai, 2001) aim at pos tagging a target language corpus using English pos tags as well as estimation of lexical priors. Having said this, we follow in principle the algorithm proposed by (Yarowsky and Ngai, 2001) to estimate lexical priors. However, as was noted by (Yarowsky and Ngai, 2001), most words tend to have at most two pos. (Yarowsky and Ngai, 2001) propose the same algorithm as the one proposed here for their estimation of lexical priors, with the exception that they use automatic word alignments rather than our extraction algorithm for finding corresponding words. As for (Yarowsky and Ngai, 2001) estimating lexical priors is merely an intermediate step, they do not report evaluation results for this step. Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al, 2001). Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. This can be seen as a rough approximation of Yarowsky and Ngai (2001). Early studies of cross-lingual annotation projection were accomplished for lexically-based tasks; for example part-of-speech tagging (Yarowsky and Ngai, 2001). The first to explore the idea were Yarowsky and Ngai (2001), who induced a part-of-speech tagger for French and base noun phrase detectors for French and Chinese via transfer from English resources. Yarowsky and Ngai (2001) were the first to propose the use of parallel texts to bootstrap the creation of taggers.  In this case alignments such as English laws (NNS) to Frenchles (DT )lois (NNS) would be expected (Yarowsky and Ngai, 2001).  Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and Ngai, 2001). A technique known as annotation projection (Yarowsky and Ngai, 2001) provides a means to relax this resource bottleneck to some extent. Previous research on resource projection attempts to address these problems by redistributing the parameter values (Yarowsky and Ngai, 2001) or by applying transformation rules (Hwa et al, 851 2002). Following the work of Yarowsky and Ngai (2001) we focus on the task of training a Part-of-Speech (POS) tagger, but we conduct our experiments with the more dissimilar language pair of English Chinese instead of English-French. One method of acquiring a large corpus of automatically POS tagged Chinese data is by projection (Yarowsky and Ngai, 2001). Following Yarowsky and Ngai (2001), we define 12 equivalence classes over the 47 Penn-English Treebank POS tags.
It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). We adopt the scoring function for MSA from Barzilay and Lee (2003). See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al, 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004).
we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003). This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). H03 indicates the model illustrated in (Henderson, 2003). (Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. (Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). In other words, the coarse outside score computed by the algorithm plays the same role as a heuristic in standard A* parsing (Klein and Manning, 2003). The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A* search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. Tsuruoka and Tsujii (2004) explore the frame work developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme.    A specific case of this algorithm is the A* parsing of Klein and Manning (2003) where they achieve significant speed up using carefully designed heuristic functions. We intend to explore other methods for pruning the space and agenda-based parsing, in particular A* parsing (Klein and Manning, 2003), which will allow only the most probable parts of the chart to be built, improving efficiency while still ensuring the optimal derivation is found. In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al (2006), and the tree transducer grammars of Galley et al (2006). parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). see Klein and Manning (2003c) for details. If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). We also experimented with the lexicalized parsing model described in Klein and Manning (2003b).   We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding transformations. 
A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al, 2003). Following phrase-based methods in statistical machine translation (Koehn et al., 2003). Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases. We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system. Consider the lexical model pw (ry|rx), defined following Koehn et al (2003). All conditions use word alignments produced by sequential iterations of IBM model 1, HMM, and IBM model 4 in GIZA++ , followed by 'diag-and' symmetrization (Koehn et al., 2003). We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule "grow-diag-final-and" (Koehn et al., 2003). Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. we achieved results similar to Koehn et al (2003a). Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003). We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003). This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003). This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system's log-linear model is augmented with a set of features specifically suited to the task of transliteration. In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012).  Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al, 2003). Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003). The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010). As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002): GIZA++ implementation of IBM word alignment model 4, the refinement and phrase extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training (Och, 2003), a 5-gram language model with Kneser-Ney smoothing (Kneser and Ney, 1995) trained with SRILM (Stolcke, 2002) on the English side of the training data, and Moses (Koehn et al, 2007) to decode. 
In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. Rouge (Lin and Hovy, 2003) represents another such effort. The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments.  Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. We use the ROUGE (Lin and Hovy, 2003) evaluation measure. Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC.
The joint model used by our bilingual parser is an instance of a stochastic bilingual multi text grammar (2MTG), formally defined by Melamed (2003). This speedup could not, however, be applied to the bilingual parsing algorithm since a split parsing algorithm will preclude inference of certain configurations of word alignments that are allowed by a non-split parser (Melamed, 2003). In more formal work, Melamed (2003) proposes multi text grammars and algorithms for parsing them. Melamed (2003) discussed the applicability of the hook trick for parsing bilexical multi text grammars. The assumption here (following (Melamed, 2003)) is that lexicalization is not considered as just affecting the grammar constant, but that in parsing, every terminal symbol has to be considered as the potential head of every phrase of which it is a part. While our simulation may be significantly slower than a direct implementation of the algorithm (especially when some of the optimizations discussed in (Melamed, 2003) are taken into account), the fact that it is orders of magnitude slower does indicate that our correspondence-guided approach is a promising alternative for an application context in which a word alignment is available. Melamed (2003, 2004) formalized the MT problem as synchronous parsing based on multi text grammars. Melamed (2003) presents algorithms for synchronous parsing with more complex grammars, discussing how to parse grammars with greater than binary branching and lexicalization of synchronous grammars. Variations of SCFGs go back to Aho and Ullman (1972)'s Syntax-Directed Translation Schemata, but also include the Inversion Transduction Grammars in Wu (1997), which restrict grammar rules to be binary, the synchronous grammars in Chiang (2005), which use only a single nonterminal symbol, and the Multi text Grammars in Melamed (2003), which allow independent rewriting, as well as other tree-based models such as Yamada and Knight (2001) and Galley et al (2004). The vehicle for the present guided tour shall be multi text grammar (MTG), which is a generalization of context-free grammar to the synchronous case (Melamed, 2003). This normal form allows simpler algorithm descriptions than the normal forms used by Wu (1997) and Melamed (2003).   Under such an MTG, the logic of word alignment is the one in Melamed (2003)'s Parser A, but without Compose inferences.  These switches correspond to discontinuous constituents (Melamed, 2003) in general bitext parsing. Lexicalization seems likely to help models predict alignment patterns between languages, and has been proposed by Melamed (2003) and implemented by Alshawi et al. However, modeling complete bilingual bilexical dependencies as theorized in Melamed (2003) implies a huge parameter space of O (|V| 2 |T| 2), where |V| and |T| are the vocabulary sizes of the two languages. Melamed (2003) discussed the applicability of the so-called hook trick for parsing bilexical multi text grammars. (2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003).
A third approach, exemplified by Moldovan et al (2003) and Raina et al (2005), is to translate dependency parses into neo-Davidsonian-style quasi logical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al, 1988). The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms - as in Moldovan et al (2003). Most of existing systems on the web produce a set of answers to a question in the form of hyper links or page extracts, ranked according to a relevance score (for example, COGEX [Moldovan et al, 2003]). Our system uses COGEX (Moldovan et al, 2003), a natural language prover originating from OT TER (McCune, 1994). This assumption, also made by other recent abductive approaches (Moldovanetal., 2003), does not hold for several classes of examples. Analysis of results on some RTE examples along without guesses and confidence probabilities inference of (Moldovan et al, 2003) and have proposed a way to capture common cases of this phenomenon. In COGEX (Moldovan et al, 2003), a recent QA system, authors used automated reasoning for QA and showed that it is feasible, effective and scalable. Moldovan et al (2003) describe a method similar to ours. Continuing this work Moldovan et al (Moldovan et al, 2003) built a logic prover for Question Answering. Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks, such as reasoning based on axioms extracted from WordNet (Moldovanetal., 2003). COGEX (Moldovan et al, 2003) uses its logic prover to extract lexical relationships between the question and its candidate answers. a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf. (Moldovan et al., 2003)). WordNet (Fellbaum, 1998) is perhaps the most popular resource and has been employed in a variety of QA-related tasks ranging from query expansion, to axiom-based reasoning (Moldovan et al., 2003), passage scoring (Paranjpe et al, 2003), and answer filtering (Leidner et al, 2004). Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al, 2003). 
We point out that one such approach, recently proposed by Pang et al (2003), also represents paraphrases by lattices, similarly to our method, although their lattices are derived using parse information. Much prior work has used lattices to compactly represent a range of lexical choices (Pang et al,2003). Similar work is described in [Pang et al, 2003], who describe a syntax-based algorithm that builds word lattices from parallel translations which can be used to generate new para phrases.  In a similar vein, Pang et al (2003) used a corpus of alternative English translations of Chinesenews stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.  Evaluation is sped up by using a compact word lattice view for eliciting human judgments, built using the syntactic fusion algorithm of (Pang et al, 2003).  To avoid combinatorial problems, implementing multiple simultaneous substitutions could be done using a lattice, much like in (Pang et al, 2003). In the automatic evaluation of machine translation, paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text (Pang et al, 2003).    Earlier, Barzilay and Lee (2003) and Pang et al (2003) developed approaches to aligning multiple reference translations in order to extract para phrases and generate new sentences. Pang et al (Pang et al, 2003) used parallel monolingual corpora built from news stories that had been independently translated several times to learn lattices from a syntax-based alignment process. Pang et al provide a remedy to this problem by performing alignment on the Charniak parse trees of the clustered sentences (Pang et al, 2003). Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003) and (Pang et al, 2003). Pang et al (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. Pang et al (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. On the basis of this hypothesis, Barzi lay and McKeown (2001) and Pang et al2003) created monolingual parallel corpora from multiple human translations of the same source.
Furthermore, we consider an F-score measure that is adapted from dependency-based parsing (Crouch et al., 2002) and sentence-condensation (Riezler et al,2003). The intrinsic evaluation measures used in our experiments are the well-known BLEU (Papineni et al., 2001) and NIST (Doddington, 2002) metrics, and an F-score measure that adapts evaluation techniques from dependency-based parsing (Crouch et al., 2002) and sentence-condensation (Riezler et al, 2003) to machine translation. We stand in a marked contrast to previous 'grafting' approaches which more or less rely on an ad-hoc collection of transformation rules to generate candidates (Riezler et al, 2003).  Our results show that grammatical relations based F-score (Riezler et al 2003) correlates reliably with human judgements and could thus be used to measure compression performance automatically. Riezler et al (2003) present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions. It is an important and growing field of natural language processing with applications in areas such as transfer based machine translation (Riezler and Maxwell, 2006) and sentence condensation (Riezler et al, 2003). Riezler et al (2003) applied linguistically rich LFG grammars to a sentence compression system. One of the most widely used automatic metrics is the F1 measure over grammatical relations of the gold standard compressions (Riezler et al, 2003). As an automated metric of quality, we compute F-score based on grammatical relations (relational F1, or RelF1) (Riezler et al, 2003). We also report results using F1 computed over grammatical relations (Riezler et al, 2003). The first evaluation is dependency base devaluation same as Riezler et al (2003).
Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (Zhang et al, 2001), SVMs (Kudo and Matsumoto, 2001), CRFs (Sha and Pereira, 2003), Maximum Entropy Model (Collins, 2002), Memory Based Learning (Sang, 2002) and SNoW (Munoz et al., 1999). For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information. the perceptron performance is comparable to that of Conditional Random Field models (ShaandPereira, 2003). The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set. This evaluation was also used in (Sha and Pereira, 2003). The second-order encoding used in our NER experiments is the same as that described in (Sha and Pereira, 2003) except removing IOB-tag of previous position label. CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking (Sha and Pereira, 2003). In recent years discriminative probabilistic model shave been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003). Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set. we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood n i log P (yi|xi) (Sha and Pereira, 2003). For more information on current training methods for CRFs, see Sha and Pereira (2003).  The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete an notations using gradient ascent iterations (Sha and Pereira, 2003). Chunking approach in this paper is closely similar to the work of Sha and Pereira (2003). In the field of English text chunking (Sha and Pereira, 2003), the step 1, 3, and 4 have been studied sufficiently, whereas the step 2, how to select optimal feature template subset efficiently, will be the main topic of this paper. 
SEE allowed the judges to step through predefined units of the model summary (elementary discourse units/EDUs) (Soricut and Marcu, 2003) and for each unit of that summary, mark the sentences in the peer summary that expressed [all (4), most (3), some (2), hardly any (1) or none (0)] of the content in the current model summary unit. Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. Soricut and Marcu (2003) use syntactic features to identify sentence-internal RST structure. The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long sentences. One exception is Marcu's work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). Within Rhetorical Structure Theory (RST), Soricut and Marcu (2003) have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). their relation edges are obtained from the Spade system described in Soricut and Marcu (2003). Soricut and Marcu (2003) also build up RST sentential trees to use in discourse parsing. Though statistical methods have been used to induce such trees (Soricut and Marcu, 2003), they are not used for ordering and other text-structuring tasks. (Soricut and Marcu, 2003) and (Polanyi et al., 2004) implement models to perform discourse parsing. A discourse tree (Soricut and Marcu, 2003). Soricut and Marcu (2003) introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels. Like Soricut and Marcu (2003), they formulate the discourse segmentation task as a binary classification problem of deciding whether a word is the boundary or no-boundary of EDUs. Soricut and Marcu (2003) and Subba and Di Eugenio (2007) use boundary labels, which are assigned to words at the end of EDUs. SPADE is the work of Soricut and Marcu (2003).
Recently, Toutanova et al (2003) presented a supervised conditional Markov Model part-of-speech tagger (CMM) which exploited information coming from both left and right contexts. The algorithms were trained and tested using version 3 of the Penn Treebank, using the training, development, and test split described in Collins (2002) and also employed by Toutanova et al (2003) in testing their supervised tagging algorithm. In the future, we will consider making an increase the context-size, which helped Toutanova et al (2003). We use the Stanford POS Tagger (Toutanova et al, 2003) to tokenize and POS tag English and German sentences. We then obtain their POS N-grams from the Stanford POS tagger (Toutanova et al 2003), and count how many of them have the POS N-gram. We used the Stanford tagger (Toutanova et al, 2003 ) v3.1, with the MEMM model. For the discriminative distortion models, we tag the pre-processed input using the log-linear POS tagger of Toutanova et al (2003). We use the log-linear tagger of Toutanova et al (2003), which gives 96.8% accuracy on the test set. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al (2003), and used the tags to decide if mentions were plural or singular. Text is tagged using the Stanford POS tagger (Toutanova et al, 2003). Decoding is performed with the Viterbi algorithm. We also evaluate state-of-the-art Maximum Entropy taggers: the Stanford Left tagger (Toutanova and Manning, 2000). We then perform POS tagging using the Stanford POS tagger (Toutanova et al, 2003). We tag the source language with the Stanford POS tagger (Toutanova et al, 2003). They surpassed their earlier work in 2003 with acyclic dependency network tagger, achieving 97.2% /89.05% (seen/unseen) (Toutanova et al, 2003). Text was tagged using the Stanford POS tagger (Toutanova et al., 2003). All data is tokenized, POS tagged (Toutanova et al, 2003) and lemmatized, resulting in 341,557 sense definitions and 3,563,649 words. Our system reports an error rate of 2.67% on the standard PTB test set, a relative 3.3% error reduction of the previous best system (Toutanova et al, 2003) by using fewer features. Toutanova et al (2003) reported a POS tagger based on cyclic dependency network.  Compared to previous best result on the same dataset, 2.76% by (Toutanova et al, 2003), our best result shows a relative error reduction of 3.3%.
Different back-off strategies, including different back-off paths as well as combination methods (Bilmes and Kirchhoff, 2003), were tried and here we present the best results. We compare an optimized four-gram, a three gram baseline, and various numbers of cluster sizes using our MCMI method and generalized back off (Bilmes and Kirchhoff, 2003), which, (again) with 500 clusters, achieves an 8.9% relative improvement over the trigram. In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity. Conditional probability distributions are represented as factored language models smoothed using Witten-Bell interpolated back off smoothing (Bilmes and Kirchhoff, 2003), according to the backoff graphs in Fig. Class-based LMs (Brown et al, 1992) or factored LMs (Bilmes and Kirchhoff, 2003) are very similar to our T+C scenario. In (Ji and Bilmes, 2005), for example, an analysis of DA tagging using DBNs is performed, where the models avoid label bias by structural changes and avoid data sparseness by using a generalized backoff procedures (Bilmes and Kirchhoff, 2003). We therefore have developed a procedure that allows us to train generalized backoff models (Bilmes and Kirchhoff, 2003), even when some or all of the variables involved in the model are hidden. Because all variables are observed when training our baseline, we use the SRILM toolkit (Stolcke, 2002), modified Kneser-Ney smoothing (Chen and Goodman, 1998), and factored extensions (Bilmesand Kirchhoff, 2003). We use the SRILM toolkit with extensions (Bilmes and Kirchhoff, 2003) to train, and use GMTK (Bilmes and Zweig, 2002) for decoding. The Factored Language Model (FLM) (Bilmes and Kirchhoff, 2003) offers a convenient view of the input data: it represents every word in a sentence as a tuple of factors. Both approaches are essentially a simple form of a factored language model (FLM) (Bilmes and Kirchhoff, 2003). A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework. This work is related to several existing directions: generative factored language model, discriminative language models, online passive-aggressive learning and confidence-weighted learning. Generative factored language models are proposed by (Bilmes and Kirchhoff, 2003). A more powerful back off strategy is used in factored language models (FLMs) (Bilmes and Kirchhoff, 2003), which view a word as a vector of word features or factors. Another approach is to use the factored language models (FLMs) which are powerful models that combine multiple sources of information and efficiently integrate them via a complex back off mechanism (Bilmes and Kirchhoff, 2003). In addition, the framework exists to integrate language models, such as those described in (Bilmes and Kirchhoff 2003), which takes advantage of the factored representation within Moses. In COMIC, the OpenCC Grealiser uses factored language models (Bilmes and Kirchhoff, 2003) over words and multi modal co articulations to select the highest-scoring realisation licensed by the grammar that satisfies the specification given by the fission module. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel back off (Bilmesand Kirchhoff, 2003) which is used in factored language models. Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
Another example of a loss function in this class is the MTeval metric introduced in Melamed et al (2003). We used the following n-gram-based metrics: BLEU (Papineni et al, 2002), NIST (Dod ding ton, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al, 2003), METEOR (Banerjee and Lavie, 2005). GTM General Text Matching (Melamed et al, 2003) calculates word overlap between a reference and a solution, without double counting duplicate words. Table 2 reports the translation performance as measured by BLEU (Papineni et al,2002), GTM (Melamed et al, 2003) and ME TEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. Melamed et al (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F-measure was as good as BLEU. We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al, 2001), NIST (Dodding ton, 2002), GTM (Melamed et al, 2003), Per (Leusch et al, 2003) , WER (NieBen et al, 2000) and ROUGE (Lin and Och, 2004a). Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al, 2003). Other well-known metrics are WER (NieBen et al, 2000), NIST (Doddington, 2002), GTM (Melamed et al, 2003), ROUGE (Lin and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al, 2006), just to name a few. We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al, 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e= 1, 2) (Melamed et al, 2003), and the METEOR measure (Banerjee and Lavie, 2005). Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. Melamed et al (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. Melamed et al (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al, 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation. For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n= 4) (Papineni et al, 2001), NIST (n= 5) (Lin and Hovy, 2002), GTM F1-measure (e= 1, 2) (Melamed et al, 2003), 1-WER (NieBen et al, 2000), 1-PER (Leusch et al, 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). Melamed et al (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties. each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al, 2003). At this time, the realizations are also scored using the General Text Matcher method (GTM) (Melamed et al, 2003), by comparing them to the original sentence. Melamed et al (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F measure was as good as BLEU. Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used.
Details of the mention detection and coreference system can be found in (Florian et al, 2004). However, Florian et al (2006) used some gazetteers and the output of other Information Extraction (IE) models as additional features, which provided significant gains ((Florian et al, 2004)). We use an information extraction toolkit (Florian et al, 2004) to analyze each event argument. In the case when some in-domain labeled training data is available, we show how to use SCL together with the classifier combination techniques of Florian et al (2004) to achieve even greater performance. In this case, we use classifiers as features as described in Florian et al (2004). In this case, we make use of the out-of-domain data by using features of the source domain tagger's predictions in training and testing the target domain tagger (Florian et al, 2004). Aside from Florian et al (2004), several authors have also given techniques for adapting classification to new domains. This is because, similar to many NLP tasks, good performance has been shown to depend heavily on integrating many sources of information (Florian et al, 2004). Initially, the corpus is automatically annotated with NE types in the source and target languages using NE identifiers similar to the systems described in (Florian et al, 2004) for NE detection. In addition, feature-based integration has been used by Taskar et al (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al (2004), who trained classifiers on auxiliary data to guide named entity classifiers. The performance of many natural language processing tasks, such as shallow parsing (Zhang et al., 2002) and named entity recognition (Florianet al, 2004), has been shown to depend on integrating many sources of information. Good performance in many natural language processing tasks has been shown to depend heavily on integrating many sources of information (Florian et al., 2004). These features were described in (Florian et al, 2004), and are not discussed here. We also note that while Florian et al (2004) and Blitzer et al (2006) observe that including the label of a source classifier as a feature on small amounts of target data tends to improve over using either the source alone or the target alone, we did not observe that for our data. Finally we note that while Blitzer et al (2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al (2004). Florian et al (2004) first train a NE tagger on the source domain, and then use the tagger's predictions as features for training and testing on the target domain. Each instance represents w i, the token under consideration, and consists of 29 linguistic features, many of which are modeled after the systems of Bikel et al (1999) and Florian et al (2004), as described below. For event coreference, we follow the approach to entity coreference detailed in (Florian et al,2004). Florian et al (2004) reports good results on the 2003 ACE task. These features were described in (Florian et al, 2004), and are not discussed here.
Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al, 2004). We use the same 560 sentence subset from the DepBank utilised by Kaplan et al (2004) in their study of parser accuracy and efficiency. Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004). The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al, 2004). Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).  We achieve between 77.68% and 80.24% against the PARC 700 following the experiments in (Kaplan et al, 2004). Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%. Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al, 2004).
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. Graehl and Knight (2004) describe methods for training tree transducers. Such an algorithm is presented by Graehl and Knight (2004). We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)).
This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]. Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. These methods identify regularities in words (Barzilay and Lee, 2004). To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used: the hidden topic Markov model (Gruber et al2007) that models the transitional topic structure; a global model based on the generalised Mallows model (Chen et al2009), and a HMM based content model (Barzilay and Lee, 2004). Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004).
Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)). While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. Aside from counting bigrams, various tasks are attainable using web based models: spelling correction, adjective ordering, compound noun bracketing, countability detection, and so on (Lapata and Keller, 2004). Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004). The results are compared against two state of the art approaches: a supervised machine learning model, Semantic Scattering (Moldovan and Badulescu, 2005), and a web based probabilistic model (Lapata and Keller, 2004). More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. We have experimented with the support vector machines (SVM) model and compared the results against two state-of-the-art models: a supervised model, Semantic Scattering (SS), (Moldovan and Badulescu, 2005), and a web-based unsupervised model (Lapata and Keller, 2004). (Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004). Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models. Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004).  The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004).
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced.
Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score (Och et al., 2004). In (Och et al, 2004), the effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated. Although the improvement on the IWSLT 04 set is only moderate, the results are nevertheless comparable or better to the ones from (Och et al, 2004). As a workaround, parsers can rerank the translated output of translation systems (Och et al, 2004). Och et al (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. Och et al (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. We follow Och et al (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. A typical reranking approach to SMT (Och et al, 2004) uses a 1000 best list. (Och et al, 2004) describe the use of syntactic features in the rescoring step. Many solutions to the reordering problem have been proposed ,e.g. syntax-based models (Chiang, 2005), lexicalized reordering (Och et al, 2004), and tree-to-string methods (Zhang et al,2006). (Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. Oracle BLEU scores computed over k-best lists (Och et al, 2004) show that many high quality hypotheses are produced by first-pass SMT decoding. Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual IR (Schonhofen et al, 2008) or re-score candidate translation outputs (Och et al, 2004). Despite the notational similarities, our approach should not be confused with projected POS models, which use source side POS tags to model reordering (Och et al, 2004). We compared these results against an inverse IBM model 1 but the results were inconclusive which is consistent with the results presented in (Och et al, 2004) where no improvements were achieved using p (e|f). A common approach of integrating new models with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system (Och et al, 2004). There are ten feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions (Och et al. 2004). Such an approach has been taken by Och et al (2004) for integrating sophisticated syntax-informed models in a phrase based SMT system. This method is a straightforward application of the n-best re-ranking approach described in Och et al (2004). Many different feature functions were explored in (Och et al, 2004).
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic. Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004).
This approach has successfully been applied in numerous Natural Language Processing (NLP) tasks including syntactic parsing (Collins and Koo, 2005), semantic parsing (Ge and Mooney, 2006), machine translation (Shen et al, 2004), spoken language understanding (Dinarelli et al, 2012), etc. Shen et al (2004) compared different algorithms for tuning the log-linear weights in a re ranking framework and achieved results comparable to the standard minimum error rate training. (Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. The labels for the SVM are derived as in (Shen et al, 2004), where top 10% of hypotheses by smoothed sentence-BLEU is ranked before the bottom 90%. A perceptron like algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004). Moreover, our ranking model is related to reranking (Shen et al, 2004) in SMT as well. Work on discriminative reranking has been reported before by Och and Ney (2002), Och et al (2004), and Shen et al (2004). Traditionally, n-best rerankers (Shen et al, 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al, 2007). It was also used also in (Shen et al, 2004) to re-rank different candidates of the same hypothesis for machine translation. The same reranking schema has been used also in (Shen et al, 2004) for reranking different candidate hypotheses for machine translation. Our modified training procedure is related to the discriminative re-ranking procedure presented in (Shen et al., 2004). Shen et al (2004) and Och et al (2004) presented approaches to re-rank the output of the decoder using syntactic information.
Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages.  In order to adjust search result presentation to the user's reading ability, we estimate the reading difficulty of each retrieved document using the Smoothed Unigram Model, a variation of a Multinomial Bayes classifier (Collins-Thompson and Callan, 2004). The latter has been found to be more effective as the former when approaching the reading level of subjects in primary and secondary school age (Collins-Thompson and Callan, 2004). Collins-Thompson and Callan (2004) adopted a similar approach and used a smoothed unigram model to predict the grade levels of short passages and web documents. First, a language modeling approach generally gives much better accuracy for Web documents and short passages (Collins-Thompson and Callan, 2004). However, there are other important criteria for the user besides relevance, such as readability (Collins-Thompson and Callan, 2004), novelty (Harman, 2003), and authority (Kleinberg, 1998).  Advanced NLP-based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al, 2010) and German (Bruck, 2008). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. As a learning model, we use unigram language modelling introduced in (Collins-Thompson and Callan, 2004) to model the reading level of subjects in primary and secondary school. In some of the early works on statistical readability assessment, Si and Callan (2001) and Collins-Thompson and Callan (2004) reported the impact of using unigram language models to estimate the grade level of a given text. The widely used Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al, 1975) (as cited in (Collins-Thompson and Callan, 2004)). classifier to better capture the variance in word usage across grade levels (Collins-Thompson and Callan, 2004). Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004). Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004). It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French.
Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given.  Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004).  We compared our system to the freely available Assert system (Pradhan et al, 2004). Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004).    We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. 
We use a phrase-based translation approach as described in (Zens and Ney, 2004). We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities.  In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). (Logic MONOTONE) This is the algorithm of Zens and Ney (2004). The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur.  For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. This finding fails to echo the promising results in the previous study (Zens and Ney,2004). The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes.
Galley et al (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al, 2004) and HPSG trees/forests (Wu et al, 2010). In particular, we implemented the GHKM algorithm as proposed by Galley et al (2004) from word-aligned tree string pairs.  For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al, 2004). Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. From word level alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. algorithm (Galley et al, 2004) to forest-based by introducing non-deterministic mechanism. GHKM (Galley et al, 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al, 2003). Galley et al (2004) describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and Galley et al (submitted) describe probability estimators for those rules. In order to test whether good translations can be generated with rules learned by Galley et al (2004), we created DerivTool as an environment for interactively using these rules as a decoder would. The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-to-string rules according to the GHKM algorithm (Galley et al 2004). Galley et al (2004) alleviate this modeling problem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora, which we review below. We contrast our work with (Galley et al, 2004), highlight some severe limitations of probability estimates computed from single derivations, and demonstrate that it is critical to account for many derivations for each sentence pair. Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al, 2004). Galley et al (2004) present one such formalism (henceforth 'GHKM'). Formally, transformational rules ri presented in (Galley et al, 2004) are equivalent to 1-state x Rs transducers mapping a given pattern (subtree to match in pi) to a right hand side string. In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al, 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests. Typically, by using the GHKM algorithm (Galley et al 2004), translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser.
We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work.  Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture. First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes. Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text. In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). (Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation.
See Peng and McCallum (2004) for more details and further experiments. For this underlying model, we employ a chain structured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction (Peng and McCallum, 2004). Later, CRFs were shown to perform better on CORA, improving the results from the Hmm's token-level F1 of 86.6 to 91.5 with a CRF (Peng and McCallum, 2004). This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA (Peng and McCallum, 2004). In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004).  For example, Peng and McCallum (2004) applied Conditional Random Fields to extract information, which draws together the advantages of both HMM and SVM. This paper mainly focuses on the VP problem, since linguistic features for the HP problem is the general IE topic of much past research (e.g., Peng and McCallum, 2004). Recently an increasing number of research efforts on text mining and IE have used CRF models (e.g., Peng and McCallum, 2004). More recently, Peng and McCallum (2004) applied supervised learning of Conditional Random Field (CRF) sequence models to the problem of parsing the headers of research papers. In our experiments we use the bibliographic citation dataset described in (Peng and McCallum, 2004). In recent years, conditional random fields (CRFs) (Lafferty et al, 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). CORA (Peng and McCallum, 2004) consists of two collections: a set of research paper headers annotated for entities such as title, author, and institution; and a collection of references annotated with BibTeX fields such as journal, year, and publisher. In bibliography entries (Peng and McCallum, 2004), a given field (author, title, etc.) should be filled by at most one substring of the input, and there are strong preferences on the cooccurrence and order of certain fields.  Examples of these models include maximum entropy Markov models (McCallum et al, 2000), Bayesian information extraction network (Peshkin and Pfeffer, 2003), and conditional random fields (Mc Callum, 2003) (Peng and McCallum, 2004). Although we are primarily concerned with unsupervised property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al, 2006) who approached the problem using supervised machine learning techniques and require labeled data.
Miller et al (2004) describe a relevant technique for the latter. We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training).  Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al, 2008). Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008).  (Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. In addition, Miller et al (2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain. One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data.
Extensive research concerning the integration of semantic knowledge into NLP for the English language has been arguably fostered by the emergence of WordNet::Similarity package (Pedersen et al, 2004). Work based on WordNet-like lexical networks for building semantic similarity measures such as (Budanitsky and Hirst, 2006) or (Pedersen et al, 2004) falls into this category. For replacement using semantic similarity measures, we used WordNet::Similarity 2.05 package by Pedersen et al (2004). We use the Lesk (overlap) similarity as implemented by the WordNet::similarity package (Pedersen et al, 2004). SR-AW finds the sense of each word that is most related or most similar to those of its neighbors in the sentence, according to any of the ten measures available in WordNet::Similarity (Pedersen et al, 2004). Then the value of vi is assigned as follows, where the similarity function SIM (ti ,wj) is calculated according to the path measure (Pedersen et al., 2004) using the WordNet. We used the Wordnet::Similarity software package (Pedersen et al, 2004) to calculate the similarity between every two words at first. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn (Pedersen et al, 2004). We use the WordNet word-to-word similarity metrics (Pedersen et al, 2004) and Latent Semantic Analysis (Landauer et al, 2007).  Yet, other resources of semantically-related terms can be beneficial, such as WordNet::Similarity (Pedersen et al, 2004), statistical resources like that of Lin (1998) or DIRECT (Kotlerman et al, 2010), thesauri, Wikipedia (Hu et al., 2009), ontologies (Suchanek et al, 2007) etc. where d(.) is a WordNet based relatedness measure (Pedersen et al, 2004). The thresholds were thoroughly selected depending on our analysis for the WordNet hierarchary and semantic similarity measures (Pedersen et al, 2004). For example, WordNet similarities (Pedersen et al, 2004) or Latent Semantic Analysis over a large corpus are widely used in many applications and for the definition of kernel functions. It is worth mentioning that the LSA similarity measure depends on the selected corpus but it benefits from a higher computation speed in comparison to the construction of the similarity matrix based on the WordNet Similarity package (Pedersen et al, 2004). Additionally, we used the Jiang&Conrath (J&C) distance (Jiang and Conrath, 1997) computed with wn::similarity package (Pedersen et al, 2004) to measure the similarity between T and H. Moreover, Table 4 shows that the computation of the LSA matrix on Wikipedia is faster than using the WordNet similarity software (Pedersen et al, 2004). We use the default configuration of the measure in WordNet::Similarity-0.12 package (Pedersen et al, 2004), and, with a single exception, the measure performed below Gic; see BP in table 1. We use (Pedersen et al., 2004) implementation with a minor alteration. The WordNet::Similarity package provides a flexible implementation of many of these measures (Pedersen et al, 2004).
Another project utilizing morphological analysis for statistical machine translation is described by Lee (2004). Lee (2004), for example, showed that morphological analysis can improve the quality of statistical machine translation for Arabic. We found that our approach of using lemmatization improved both the word alignment and the quality of SMT with a small amounts of training data, and, while much work indicates that MA is useless in training large amounts of data (Lee, 2004), our intensive experiments proved that the chance to get a better MT quality using lemmatization is higher than that without it for large amounts of training data. Often the choice of directionality is motivated by this restriction, and the choice of tokenization style may be designed (Lee, 2004) to reduce this problem. Lee (2004) uses a trigram language model to segment Arabic words.  For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Lee (2004) only changes the word segmentation of the morphologically complex language (Arabic) to induce morphological and syntactic symmetry between the parallel sentences. Another way for determiner deletion is described in (Lee, 2004). For languages with more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)-stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained segmentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes using alignments with a part-of-speech tagged English corpus. Specifically considering Arabic, Lee (2004) investigated the use of automatic alignment of POS tagged English and affix-stem segmented Arabic to determine appropriate tokenizations. Lee (2004) and Zolmann et al (2006) have exploited morphology in Arabic English SMT. While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e.g., Sadat and Habash (2006), Zollmann et al (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al). Lee (2004) uses a morphologically analyzed and tagged parallel corpus for Arabic English SMT. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. Specifically considering Arabic, Lee (2004) investigated the use of automatic alignment of POS-tagged English and affix-stem segmented Arabic to determine appropriate tokenizations. It is identical to the initial tokenization used by Lee (2004). We do not use any additional information to remove specific features using alignments or syntax (unlike, e.g. removing all but one Al+ in noun phrases (Lee, 2004)).
A DP-based beam search procedure identical to the one used in (Tillmann,2004) is used to maximize over all oriented block segmentations. For details see (Tillmann, 2004).  We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in position (s-1,v+1) and then treats the orientation of bp as S. The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). Both Moses and our system are evaluated with and without lexicalized reordering (Tillmann, 2004) . (Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a word bonus model and a tuple-bonus model which compensate for the system preference for short translations. In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al, 2007). The (Tillmann, 2004) paper introduced lexical features for distortion modeling. An other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004). Tillmann (2004) used a lexical reordering model, and Galley et al (2004) followed a syntactic-based model. 
Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. We use AMIRA (Diab et al, 2004) to annotate Arabic and Tree Tagger (Schmid, 1994) to annotate German. Arabic text is then segmented with AMIRA (Diab et al, 2004) according to the ATB scheme. This scheme is compatible with the chunker we use (Diab et al, 2004). For chunking Arabic, we use the AMIRA (ASVMT) toolkit (Diab et al, 2004). In supervised POS tagging, (Diab et al, 2004) achieves high accuracy on MSA with the direct application of SVM classifiers. A comparable work was done by (Diab et al, 2004), where a POS tagging method for Arabic is also discussed. In (Lee et al, 2003), (Diab et al, 2004) and (Habash and Rambow, 2005) three supervised segmentation methods are introduced. In the next subsections we will shortly describe the method of (Diabetal., 2004). (Diab et al, 2004) propose solutions to word segmentation and POS Tagging of Arabic text. L1 uses the simple POS tags advocated by Habash and Rambow (2005) (15 tags); while L2 uses the reduced tag set used by Diab et al (2004) (24 tags). Khoja (2001) first introduced a tagger for Arabic, which has 131 tags, but subsequent work has collapsed the tag set to simplify tagging (Diab et al, 2004). Recently, Diab et al (2004) used SVM based approach for Arabic text chunking. It was also successfully used in Arabic (Diab et al, 2004). The data facilitates machine learned part-of-speech taggers, tokenizers, and shallow parsing units such as chunkers, as exemplified by Diab et al (2004). Diab et al (2004) describe a part-of-speech tagger based on support vector machines that is trained on tokenized data (clitics are separate tokens), reporting a tagging accuracy of 95.5%. Mansour et al (2007) combine a lexicon-based tagger (such as MorphTagger (Bar-Haim et al, 2005)), and a character-based tagger (such as the data-driven ArabicSVM (Diab et al, 2004)), which includes character features as part of its classification model, in order to extend the set of analyses suggested by the analyzer. Diab et al. (2004) used a Support Vector Machine, SVM-based tagger, trained on the Arabic Penn Treebank 1 to tokenize, POS tag, and annotate Arabic base phrases. The tokenization was done using the ASVM Toolkit (Diab et al, 2004). As reported by Habash and Rambow, the first work on Arabic tagging which used a corpus for training and evaluation was the work of Diab et al (2004).
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). Callison-Burch et al (2006) point out three prominent factors. This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. Other proposed methods include paraphrasing (Callison-Burch et al, 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity. Callison-Burch et al (2006) propose the use of paraphrases as a means of dealing with unseen source phrases. Although related to Callison-Burch et al (2006) our method is conceptually simpler and more general. We carried out experiments on small, medium and large scale English-Chinese translation tasks to compare against a baseline PBSMT system, the translation model augmentation of (Callison-Burch et al, 2006) method and the word-lattice-based method of (Du et al., 2010) to show the effectiveness of our novel approach. Compared with translation model augmentation with paraphrases (Callison-Burch et al, 2006), word-lattice-based paraphrasing for PBSMT is introduced in (Du et al, 2010). Callison-Burch et al (2006) used paraphrases of the trainig corpus for translating unseen phrases. Callison-Burch et al (2006) argue that limited amounts of parallel training data can lead to the problem of low coverage in that many phrases encountered at run-time are not observed in the training data and so their translations will not be learned. Callison-Burch et al (2006) proposed a novel method which substitutes a paraphrase for an unknown source word or phrase in the input sentence, and then proceeds to use the translation of that paraphrase in the production of the target-language result. to explain how difficult it is to translate the source-side sentence in three respects: The OOV rates of the source sentences in the test set (Callison-Burch et al, 2006). system performs slightly better (0.36 absolute BLEU points) than the baseline system on the 20K data set, but slightly worse (0.19 absolute BLEU points) than the baseline on the 200K data set, which indicates that the paraphrase substitution method used in (Callison-Burch et al, 2006) does not work on resource-sufficient data sets. Callison-Burch et al (2006) aim to improve MT quality by adding paraphrases in the translation table, while Madnani et al (2007) aim to improve the minimum error rate training by adding the automatically generated paraphrases into the English reference sets. Callison-Burch et al (2006) exploited the existence of multiple parallel corpora to learn paraphrases for Phrase-based MT.  For example, according to Callison-Burch et al (2006), a SMT system with a training corpus of 10,000 words learned only 10% of the vocabulary; the same system learned about 30% with a training corpus of 100,000 words; and even with a large training corpus of nearly 10,000,000 words it only reached about 90% coverage of the source vocabulary. The table augmentation idea is similar to Callison Burch et al (Callison-Burch et al, 2006), but our proposed paradigm does not require using a limited resource such as parallel texts in order to generate paraphrases. This work is most closely related to that of Callison-Burch et al (2006), who also translate source-side paraphrases of the OOV phrases.
Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence: negations, modals, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment. Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment: an alignment stage followed by an entailment stage. Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing: (1) an alignment model aligns terms in the hypothesis to terms in the conversation segment; and (2) an inference model predicts the entailment based on the alignment between the hypothesis and the conversation segment. We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). MacCartney et al (2006) describe a system for doing robust textual inference.
We follow the recent work of (Klementiev and Roth 2006) who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for transliteration.      We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal cooccurrence similarity when ranking the transliteration candidates. This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs. Recently, [Klementiev and Roth, 2006] outlined an approach by leveraging the availability of article aligned news corpora between English and Russian, and tools in English, for discovering transliteration pairs between the two languages, and progressively refining the discovery process. As in [Klementiev and Roth, 2006] no language specific knowledge was used to refine our mining process, making the approach broadly applicable. In this section, we outline briefly the methodology presented in [Klementiev and Roth, 2006], and refer interested readers to the source for details. We start with comparable corpora in English and Tamil, similar in size to that used in [Klementiev and Roth, 2006], and using the English side of this corpora, first, we extract all the NEs that occur more than a given threshold parameter, FE, using a standard NER tool. While we adopted a methodology similar to that in [Klementiev and Roth, 2006], our focus was on mining parallel NE transliteration pairs, leveraging the availability of comparable corpora and a well-trained linear classifier to identify transliteration pairs. However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al, 2009b). Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al, 2008). The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages.
Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al (2006) as features, we achieve an absolute AER of 3.8 on the English French Hansards alignment task - the best AER result published on this task to date. By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5 precision/recall. Liang et al (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006). Concerning the former, we trained an unsupervised model with the Berkeley aligner, an implementation of the symmetric word-alignment model described by Liang et al (2006). The state-of-the-art unsupervised Berkeley aligner (Liang et al, 2006) with default setting is used to construct word alignments. We used version two of the Berkeley alignment model (Liang et al, 2006), with the posterior threshold set at 0.5. The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al., 2006), with the posterior probability to being 0.5. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al (2006), and then trained our model for 5 EM iterations. Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al, 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al (2006). We used two well studied unsupervised aligners, GIZA++ (Och and Ney, 2003) and HMM (Liang et al, 2006) and one supervised aligner, ITG (Haghighi et al, 2009) as representatives in this work. We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al, 2006), and ITG (Haghighi et al, 2009). The HMM aligner used in this work was due to Liang et al (2006). Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al, 2006). As suggested by Liang et al (2006), we can group the distortion parameters into a few buckets. As an additional experiment, we tested the Cross EM aligner (Liang et al, 2006) from the Berkeley Aligner package on the MSR data. Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences. Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed per sentence BLEU (Liang et al, 2006a). We performed word alignment using a cross EM word aligner (Liang et al, 2006).
We should keep in mind that (1) a tree bank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al (2006), and (2) that our tree bank PCFG is binarized as in Klein and Manning (2005) to make results comparable.  For instance, McClosky et al (2006) improved a statistical parser by self-training. Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). previously used for self-training of parsers (McClosky et al, 2006).  We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006).
The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)). Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). Semantic features: semantic class agreement, governing verb and its grammatical role, predicate (Ponzetto and Strube, 2006) For English, the number agreement and gender agreement features can be obtained through the gender corpus provided. Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006).  As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2,..., NPj-1. In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006).
The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006). For arbitrary SCFGs, this is typically accomplished via the synchronous binarization technique of (Zhang et al, 2006). We show that this approach gives us better practical performance than a mature system that binarizes using the technique of (Zhang et al, 2006). For SCFG grammars, (Zhang et al, 2006) provide a scope reduction method called synchronous binarization with quantifiable loss. Their principal objective is to provide a scope reduction method for SCFG that introduces fewer postconditions than (Zhang et al, 2006). However unlike (Zhang et al, 2006), their method only addresses simple grammars. Binarizing the grammars (Zhang et al, 2006) further increases the size of these sets, due to the introduction of virtual nonterminals. Rule size and lexicalization affect parsing complexity whether the grammar is binarized explicitly (Zhang et al, 2006) or implicitly binarized using Early-style intermediate symbols (Zollmann et al, 2006). Synchronous binarization (Zhang et al, 2006) solves this problem by simultaneously binarizing both source and target-sides of a synchronous rule, making sure of contiguous spans on both sides whenever possible. Intuitively speaking, the gaps on the target-side will lead to exponential complexity in decoding with integrated language models (see Section 3), as well as synchronous parsing (Zhang et al, 2006). This representation, being contiguous on both sides, successfully reduces the decoding complexity to a low polynomial and significantly improved the search quality (Zhang et al, 2006). Although according to Zhang et al (2006), the vast majority (99.7%) of rules in their Chinese-English dataset are binarizable, there do exist some interesting cases that are not (see Figure 2 for a real-data example). We used a bottom-up, CKY-style decoder that works with binary xRs rules obtained via a synchronous binarization procedure (Zhang et al, 2006). Most practical non-binary SCFGs can be binarized using the synchronous binarization technique by Zhang et al (2006). Zhang et al (2006) discuss methods for binarizing SCFGs, ignoring the nonbinarizable grammars. A CYK-style decoder has to rely on binarization to preprocess the grammar as did in (Zhang et al, 2006) to handle multi-nonterminal rules. Many solutions to the reordering problem have been proposed, e.g. syntax-based models (Chiang, 2005), lexicalized reordering (Och et al, 2004), and tree-to-string methods (Zhang et al, 2006). Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) and tree parsing generally runs in linear time (Huang et al, 2006). As tree-to-string rules usually have multiple non-terminals that make decoding complexity generally exponential, synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) is a key technique for applying the CKY algorithm to parsing with tree-to-string rules. In our string-to-tree model, for efficient decoding with integrated n-gram LM, we follow (Zhang et al, 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM.
Our work is related to previous work on domain independent unsupervised relation extraction, in particular Sekine (2006), Shinyama and Sekine (2006) and Banko et al (2007). Shinyama and Sekine (2006) apply NER, coreference resolution and parsing to a corpus of newspaper articles to extract two-place relations between NEs. Like Sekine (2006) and Shinyama and Sekine (2006), we concentrate on relations involving NEs, the assumption being that these relations are the potentially interesting ones. Inference rules are an important building block of many semantic applications, such as Question Answering (Ravichandran and Hovy, 2002) and Information Extraction (Shinyama and Sekine, 2006). An alternative paradigm, Open IE, pioneered by the TextRunner system (Banko et al., 2007) and the "preemptive IE" in (Shinyama and Sekine, 2006), aims to handle an unbounded number of relations and run quickly enough to process Webscale corpora. Shinyama and Sekine proposed the "preemptive IE" framework to avoid relation-specificity (Shinyama and Sekine, 2006). Our work focuses on a recent line of exploratory work in the direction of Unrestricted Relation Discovery which is defined as: the automatic identification of different relations in text without specifying a relation or set of relations in advance (Shinyama and Sekine, 2006). Shinyama and Sekine (2006) developed an approach to preemptively discover relations in a corpus and present them as tables with all the entity pairs in the table having the same relations between them. A paradigm related to Open IE is Preemptive IE (Shinyama and Sekine, 2006). However, most algorithms (Hasegawa et al 2004, Shinyama and Sekine, 2006, Chen et. al, 2005) rely on tagging predefined types of entities as relation arguments, and thus are not well-suited for the open domain. Prior work in relation discovery (Shinyama and Sekine, 2006) has investigated the problem of finding relationships between different classes. Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). (Shinyama and Sekine, 2006) rely further on supervised methods, defining features over a full syntactic parse, and exploit multiple descriptions of the same event in newswire to identify useful relations. Shinyama and Sekine (2006) describe an approach to template learning without labeled data. Preemptive IE (Shinyama and Sekine, 2006) is a paradigm related to Open IE that first groups documents based on pairwise vector clustering, then applies additional clustering to group entities based on document clusters. Inference rules are an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006), describing a directional inference relation between two text patterns with variables. The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006).
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). (Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data.
We compared our system's performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006). Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), interalia), databases of simulated card games (Goldwasser and Roth (2013), interalia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia).  In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG. While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic. We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence. For details regarding non-isomorphic NL/MR parse trees, removal of bad links from alignments, and extraction of word gaps (e.g. the token (1) in the last rule of Figure 3), see Wong and Mooney (2006). The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006). We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL. Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. A recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system.
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score.  PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation.
Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006).  Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes.  For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006).
The data we used for our experiments are developed as part of the OntoNotes project (Hovy et al, 2006) and they come from a variety of sources. However, the CoNLL data sets come from OntoNotes (Hovy et al, 2006), where singleton entities are not annotated, and BLANC has a wider dynamic range on data sets with singletons (Recasens and Hovy, 2011). The second set is radically different as it comprised 750 pairs of glosses from OntoNotes 4.0 (Hovy et al, 2006) and WordNet 3.1 (Fellbaum, 1998) senses. OntoNotes (Hovy et al, 2006) is a project that has annotated several layers of semantic information including word senses, at a high inter-annotator agreement of over 90%. Overall, however, this data indicates that the approach suggested by (Palmer, 2000) and that is being adopted in the ongoing OntoNotes project (Hovyet al, 2006) does result in higher system performance. In this work we focus on two datasets of hand-labeled sense groupings for WordNet: first, a dataset of sense groupings over nouns, verbs, and adjectives provided as part of the SENSEVAL-2 English lexical sample WSD task (Kilgarriff, 2001), and second, a corpus-driven mapping of nouns and verbs in WordNet 2.1 to the Omega Ontology (Philpot et al, 2005), produced as part of the ONTONOTES project (Hovy et al, 2006). We then merged the word alignment annotation with the TreeBank and PropBank annotation of Ontonotes4.0 (Hovy et al, 2006), which includes a wide array of data sources like broadcast news, news wire, magazine, web text, etc. Suggestions came from the previous named entity annotation of PERSONs, organizations (GROUP), and LOCATIONs, as well as heuristic lookup in lexical resources - Arabic WordNet entries (Elkateb et al, 2006) mapped to English WordNet, and named entities in OntoNotes (Hovy et al, 2006). Arabic is no exception: the publicly available NER corpora - ACE (Walker et al 2006), ANER (Benajiba et al 2008), and OntoNotes (Hovy et al 2006) - all are in the news domain. In some projects (e.g. OntoNotes (Hovy et al 2006)), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see (Artstein and Poesio, 2008)). The OntoNotes 90% solution (Hovy et al 2006) actually means such a degree of granularity that enables a 90% IAA. The corpus consists of texts of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al, 2006). However, the performance of our model, trained using the OntoNotes corpus (Hovy et al, 2006), fell short of separate parsing and named entity models trained on larger corpora, annotated with only one type of information. We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. Elsewhere, similar, iterative annotation processes have yielded significant improvements in agreement for word sense and coreference (Hovy et al, 2006). We use the English part of the SemEval-2010 CR task data set, a subset of OntoNotes 2.0 (Hovy et al, 2006). Turkers were presented sentences from the test portion of the word sense induction task of SemEval-2007 (Agirre and Soroa, 2007), covering 2,559 instances of 35 nouns, expert-annotated with OntoNotes (Hovy et al, 2006) senses. This latter convention recently seems to be gaining ground in data sets like the Google 1T n-gram corpus (LDC# 2006T13) and OntoNotes (Hovy et al, 2006). In the OntoNotes project (Hovy et al., 2006), annotators use small-scale corpus analysis to create sense inventories derived by grouping together WordNet senses, with the procedure restricted to maintain 90% inter-annotator agreement. For experiments with PropBank, we used the Ontonotes corpus (Hovy et al, 2006), version 4.0, and only made use of the Wall Street Journal documents; we used sections 221 for training, section 24 for development and section 23 for testing.
Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). This group of features are completely identical to those used in Sagae and Lavie (2006a). Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. (Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. To solve this issue, Sagae and Lavie (2006) use a threshold to balance them.  We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here).
Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. The one exception is the size of a cluster (Culotta et al, 2007). Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions.  As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). Many of our features are similar to those described in Culotta et al (2007). Our test set contains the same 107 documents as Culotta et al (2007). For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. Culotta et al (2007) is the best comparable system of which we are aware. Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOST FALSE, MOST-TRUE, and ALL.   We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008).
However, as noted by Johnson et al (2007), this choice of beta leads to difficulties with MAP estimation. Adaptor grammars are a framework for Bayesian inference of a certain class of hierarchical nonparametric models (Johnson et al, 2007b). Adaptor Grammars are formally defined in Johnson et al (2007b), which should be consulted for technical details. There are several different procedures for inferring the parse trees and the rule probabilities given a corpus of strings: Johnson et al (2007b) describe a MCMC sampler and Cohen et al (2010) describe a Variational Bayes procedure. Hidden Markov Models (HMMs), a special case of DBNs, are a classical method for important NLP applications such as unsupervised part-of-speech tagging (Gael et al., 2009) and grammar induction (Johnson et al, 2007) as well as for ASR. Our model is similar to the Adaptor Grammar model of Johnson et al (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar. Our model is similar in this way to the Adaptor Grammar model of Johnson et al (2007a). Given a sample, we can reason over the space of possible trees using a Metropolis-Hastings sampler (Johnson et al, 2007a) coupled with a Monte Carlo integral (Bod, 2003). A natural proposal distribution, p(d|w), is the maximum a posterior (MAP) grammar given the elementary tree analysis of our training set (analogous to the PCFG approximation used in Johnson et al (2007a)). To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al (2007a). This section informally introduces adaptor grammars using unsupervised word segmentation as a motivating application; see Johnson et al (2007b) for a formal definition of adaptor grammars. In adaptor grammars the base distributions HX are determined by the PCFG rules expanding X and the other adapted distributions, as explained in Johnson et al (2007b). Johnson et al (2007a) describe Gibbs samplers for Bayesian inference of PCFG rule probabilities, and these techniques can be used directly with adaptor grammars as well.  The adaptor grammar algorithm described in Johnson et al (2007b) repeatedly resamples parses for the sentences of the training data. This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting. This work was inspired by adaptor grammars (Johnson et al, 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al, 2007b). In the monolingual setting, there is a well known tree sampling algorithm (Johnson et al,2007). However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large.
Modifications of this model are reported in Turner and Charniak (2005) and Galley and McKeown (2007) with improved results. (Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous context free grammar (SCFG) deletion rules. The last sentence compression method we use is the lexicalized Markov grammar-based approach (Galley and McKeown, 2007) with edit word detection (Charniak and Johnson, 2001).  Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown,2007). Galley and McKeown (2007) show improvements to the noisy-channel approach based on rule lexicalization and rule Markovization. We use the sentence pairs available in the Ziff-Davis Tree Alignment corpus (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content.
This work was extended in (Rosti et al, 2007) by introducing system weights for word confidences. The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al, 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al, 2007), the new method improves the BLEU scores significantly. The subnetworks in the latter approach may be weighted by prior probabilities estimated from the alignment statistics (Rosti et al, 2007a). Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination, such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs (Rosti et al, 2007a; Huang and Papineni, 2007). Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al 2007). Sim et al (2007), Rosti et al (2007a), and Rosti et al (2007b) used minimum Translation Error Rate (TER) (Snover et al, 2006) alignment to build the confusion network. Similar to (Rosti et al, 2007a), each word in the hypothesis is assigned with a rank-based score of 1/(1+r), where r is the rank of the hypothesis. The system used in this paper is a variant of the one proposed in Rosti et al (2007a), which we now describe in detail. Meanwhile, we also use a word-level combination framework (Rosti et al, 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result. We also implemented the word-level system combination (Rosti et al, 2007) and the hypothesis selection method (Hildebrand and Vogel, 2008).  The current state-of-the-art is confusion-network-based MT system combination as described by Rosti and colleagues (Rosti et al., 2007a, Rosti et al., 2007b). Bangalore et al (2001) used a multiple string matching algorithm based on Levenshtein edit distance, and later Sim et al (2007) and Rosti et al (2007) extended it to a TER-based method for hypothesis alignment. Similar to (Rosti et al, 2007), each word in the confusion network is associated with a word posterior probability. Various techniques include hypothesis selection from different systems using sentence-level scores, re-decoding source sentences using phrases that are used by individual systems (Rosti et al., 2007a; Huang and Papineni, 2007) and word-based combination techniques using confusion networks (Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b). Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations. For selecting the best skeleton, two common methods are choosing the hypothesis with the Minimum Bayes Risk with translation error rate (TER) (Snover et al, 2006) (i.e., the hypothesis with the minimum TER score when it is used as the reference against the other hypotheses) (Sim et al, 2007) or choosing the best hypotheses from each system and using each of those as a skeleton in multiple confusion networks (Rosti et al, 2007b). Rosti et al (2007) look at sentence-level combinations (as well as word and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. A new search space is constructed from these backbone-aligned outputs, and then a voting procedure or feature-based model predicts a final consensus translation (Rosti et al, 2007).
Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task.  Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys.    Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection.  Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold.
Similarly, Snyder and Barzilay (2007) decompose an opinion across several dimensions and capture the sentiment across each dimension. However, it has been observed that ratings for different aspects can be correlated (Snyder and Barzilay, 2007), e.g., very negative opinion about room cleanliness is likely to result not only in a low rating for the aspect rooms, but also is very predictive of low ratings for the aspects service and dining. Snyder and Barzilay (2007) combine an agreement model based on contrastive RST relations with a local aspect (or target) model to make a more informed overall decision for sentiment classification. Also, it would be interesting to take a closer look at the interactions between aspect and sentiment, especially at a multiple-sentence level (see Snyder and Barzilay 2007). In a multi-aspect setting, however, information about the sentence topic is required to determine the aspect to which a sentiment-bearing word relates (Snyder and Barzilay, 2007). Our first task is a multi-aspect sentiment analysis task, where a system predicts the aspect-specific sentiment ratings (Snyder and Barzilay, 2007). The goal of multi-aspect sentiment classification is to predict a set of numeric ranks that reflects the user satisfaction for each aspect (Snyder and Barzilay, 2007). Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al, 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007). In multi-aspect rating (Snyder and Barzilay, 2007) one finds several distinct aspects such as food or service in a restaurant and then rates them on a fixed linear scale such as 1-5 stars, where all aspects could obtain just 1 star or all aspects could obtain 5 stars independently. We also propose to consider aspects of reviews (Snyder and Barzilay, 2007), and investigate other methods that measure class similarity, such as selecting typical in stances (Zhang, 1992).  
To explore this, we tested our model in conjunction with a recent L2P system that has been shown to predict phonemes with state-of-the-art word accuracy (Jiampojamarn et al, 2007). We would like to use features that look at wide context on the input side, which is inexpensive (Jiampojamarn et al, 2007). Our system employs the many-to-many alignment described in (Jiampojamarn et al, 2007). We use the many-to-many letter phoneme alignment algorithm (Jiampojamarn et al, 2007) to map each letter to multiple phonemes (1-to-2 alignment). The M2M-aligner (Jiampojamarn et al, 2007) is based on the expectation maximization (EM) algorithm. Our input string representation for a candidate pair is formed by first aligning the source and target words using M2M-aligner (Jiampojamarn et al., 2007). DIRECTL+ (Jiampojamarn et al, 2010a) is an online discriminative training system that incorporates joint n-gram features and many-to-many alignments, which are generated by M2M-ALIGNER (Jiampojamarn et al, 2007). Advanced L2P approaches, including the joint n-gram models (Bisani and Ney, 2008) and the joint discriminative approach (Jiampojamarn et al., 2007) eliminate the one-to-one constraint entirely, allowing for linking of multiple letters to multiple phonemes. M2M-aligner (Jiampojamarn et al, 2007) is a many-to-many (M-M) alignment algorithm based on EM that allows for mapping of multiple letters to multiple phonemes. Extensions to this model are possible, for example the use of many-to-many alignments which have been shown to be very effective in letter-to-phoneme alignment tasks (Jiampojamarn et al 2007). In our lexicon, the graphemes and phonemes of each word are aligned according to the method of Jiampojamarn et al (2007). P(ht|ps,pos) is estimated from the frequency of ps, and its alignment with ht, in a version of CELEX in which the graphemic and phonemic representation of each word is many-many aligned using the method of Jiampojamarn et al (2007). In order to extract source-target character mappings, we use m2m-aligner (Jiampojamarn et al, 2007), which implements a forward-backward algorithm to sum over probabilities of possible character sequence mappings, and uses Expectation Maximization to learn mapping probabilities. One of the most popular alignment tools is m2maligner (Jiampojamarn et al, 2007), which is released as an open source software. To establish the substring alignment between katakana and Latin alphabet strings, we use the probabilistic model proposed by (Jiampojamarn et al., 2007). To balance cost and benefit for English-to-Chinese (E2C) transliteration, this work compares the one-stage method with the two-stage one, using additional features of AV (Feng et al, 2004) and M2M-aligner as an initial alignment (Jiampojamarn et al, 2007), to explore where the best investment reward is. For the alignment of supplemental data with candidate outputs, we apply M2M ALIGNER (Jiampojamarn et al, 2007). The second hybrid approach (Jiampojamarn et al, 2007) also extends instance-based classification. In the pipeline approach (Figure 1b), the input word is segmented into letter substrings by an instance-based classifier (Aha et al, 1991), which learns a letter segmentation model from many-to-many alignments (Jiampojamarn et al, 2007). We ignored one-to-one alignments included in the PRONALSYL data sets, and instead induced many-to-many alignments using the method of Jiampojamarn et al (2007).
Similar methods were applied by Matsuzaki et al (2005) and Petrov and Klein (2007) for parsing under a PCFG with nonterminals with latent annotations. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al, 2005). For preprocessing, all the sentences in the Bioscope corpus are tokenized and then parsed using the Berkeley parser (Petrov and Klein, 2007) trained on the GENIA TreeBank (GTB) 1.0 (Tateisi et al, 2005), which is a bracketed corpus in (almost) PTB style. On standard evaluations using both the Penn Tree bank and the Penn Chinese Treebank, our parser gave higher accuracies than the Berkeley parser (Petrov and Klein, 2007), a state-of-the-art chart parser. We use Berkeley PCFG parser (Petrov and Klein, 2007) for all experiments. Note that Algorithm 1 & 2 rely on the use of Berkeley parser (Petrov and Klein, 2007). The Berkeley and BUBS parsers both parse with the Berkeley latent-variable grammar (Petrov and Klein, 2007b), while the Charniak parser uses a lexicalized grammar, and the exhaustive CKY algorithm is run with a simple Markov order-2 grammar. These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech. For the experiments in this paper, we will use the Berkeley parser (Petrov and Klein, 2007) and the related Maryland parser (Huang and Harper,2011). In this paper, we used the Berkeley Parser (Petrov and Klein, 2007) for learning these structures. We used the Berkeley parser (Petrov and Klein, 2007) to train the parsing model for HFE and to parse HFE. The split-merge smooth implementation of (Petrov et al, 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al, 2009) and for many other languages (Petrov and Klein, 2007). We used the Berkeley Parser (Petrov and Klein, 2007) with the standard model they provide for building syntactic parse trees and defined the patterns for extracting various syntactic features from the trees using the Tregex pattern matcher (Levy and Andrew, 2006). For this work, we have re-implemented and enhanced the Berkeley parser (Petrov and Klein 2007) in several ways: (1) developed a new method to handle rare words in English and Chinese; (2) developed a new model of unknown Chinese words based on characters in the word; (3) increased robustness by adding adaptive modification of pruning thresholds and smoothing of word emission probabilities. To obtain parse trees over both sides of each parallel corpus, we used the English and Chinese grammars of the Berkeley parser (Petrov and Klein, 2007).  Chinese and Arabic parses were generated using the Berkeley parser (Petrov and Klein, 2007). An approach found to be effective by Petrov and Klein (2007) for coarse-to-fine parsing is to use likelihood-based hierarchical EM training. This process not only guarantees that the clusters are hierarchical, it also avoids the state drift discussed by Petrov and Klein (2007). A similar observation was reported in the parsing literature, where coarse-to-fine inference with multiple passes of roughly equal complexity produces tremendous speed-ups (Petrov and Klein, 2007).
Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing context sensitive predicate inference rules are based on topic models (except for (Pantel et al, 2007), which was outperformed by later models). In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al (2007), for typing the arguments of paraphrase rules. Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. MI was also recently used for inference-rule SPs by Pantel et al (2007). As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots.  Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). In (Pantel et al, 2007), they augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy. This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path.
We turned to TextRunner (Yates et al, 2007) as a large source of background knowledge about pre-existing relations between nominals. Yates (2009) considers the output from an open information extraction system (Yates et al, 2007) and clusters predicates and arguments using string similarity and a combination of constraints. Two example systems implementing this paradigm are TEXTRUNNER (Yates et al, 2007) and REVERB (Fader et al, 2011). In contrast, when research focuses on any relation, as in TextRunner (Yates et al, 2007), there is no standardized manner for re-using the pattern learned. The online demo of TextRunner (Yates et al, 2007) actually allowed us to collect the arguments for all our semantic relations.
Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores.  We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006). In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words.   While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations.  
Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data).  Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others.   Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. These are the same training, development, and test sets used by Cohen and Smith (2009). Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results.  This has been done successfully in multilingual settings (Cohen and Smith, 2009). Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences. Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009).
Headden III et al (2009) showed that performance could be improved by including high frequency words as well as tags in their model. The final base distribution over CFG-DMV rules (Psh) is inspired by the skip-head smoothing model of Headden III et al (2009). On the |w| ≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009).  This graph indicates that the improvements in the posterior probability of the model are correlated with the evaluation, though the correlation is not as high as we might require in order to use LLH as a model selection criteria similar to Headden III et al (2009). Headden III et al (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al., 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated. We follow an approach similar to the widely-referenced DMV model (Klein and Manning, 2004), which forms the basis of the current state-of-the-art unsupervised grammar induction model (Headden III et al, 2009). An additional point of comparison is the lexicalized unsupervised parser of Headden III et al (2009), which yields the current state-of-the-art unsupervised accuracy on English at 68.8%.  As in the previous section, we find that the presence of linguistic rules greatly reduces this sensitivity: for HDP-DEP, the standard deviation over five randomly initialized runs with the English-specific rules is 1.5%, compared to 4.5% for the parser developed by Headden III et al. (2009) and 8.0% for DMV (Klein and Manning, 2004). In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rule sets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al (2009)) to benefit from their complementary strengths.  In Headden III et al (2009), by using the lexical values with the frequency more than 100 and defining tied probabilistic context free grammar (PCFG) and Dirichlet priors, the accuracy is improved. For better comparison with previous work we implemented three model extensions, borrowed from Headden III et al (2009). We fix λ = 1/3, which is a crude approximation to the value learned by Headden III et al. (2009). Headden III et al (2009) also implement a sort of parameter tying for the E-DMV through a learning a back off distribution on child probabilities.  
Chiang et al (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above. Our baseline decoder contains a large and powerful set of features, which include: 7 sparse feature types, totaling 50k features (Chiang et al., 2009). Potentially, improvements could be gained from using separate weights for individual local models, but this would require an optimization procedure such as MIRA (Chiang et al, 2009), which can handle a larger number of features. Given the current infrastructure, other training methods (e.g., maximum conditional likelihood or MIRA as used by Chiang et al (2009)) can also be easily supported with minimum coding. See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT. Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al (2009). Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). Alternatively, by using the large margin optimizer in (Chiang et al, 2009) and moving it into the for-each loop (lines 4-9), one can get an online algorithm such PMO-MIRA. With the training sentences yi and their simulated confusion sets N (yi) — represented as hypergraphs D(yi)) — we can perform the discriminative training using any of a number of procedures such as MERT (Och, 2003) or MIRA as used by Chiang et al. (2009). Additionally, we use 50,000 sparse, binary-valued features such as "Is the bi-gram 'united states' present in the output?", based on (Chiang et al., 2009). The second one is the MIRA algorithm, first applied for machine translation in (Chiang et al, 2009). In addition, it uses a large number of discriminatively tuned features, which were inspired by Chiang et al (2009) and implemented in a way described in (Devlin 2009). Following (Chiang et al, 2009), we only use 100 most frequent words for word context feature. Such features could include syntactical features (Chiang et al, 2009). For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. The base feature set for all systems was similar to the expanded set recently used for Hiero (Chiang et al, 2009), but with bigram features (source and target word) instead of trigram features (source and target word and neighboring source word). The situation might be improved by using a model that does a better job of both capturing the structure of the source and target sentences and their allowable reorderings, such as a syntactic tree-to-string system that uses contextually rich rewrite rules (Galley et al, 2006), or by making use of larger more fine grained feature sets (Chiang et al., 2009) that allow for better discrimination between hypotheses. This was well understood in previous work, so heuristic filtering was usually applied (Chiang et al, 2009, inter alia). Recent work by (Chiang et al, 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing. Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1).
Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. Xu et al (2009) used a semantic head-based dependency parser for a similar purpose. Reversing the children order (Xu et al, 2009) reconnects is and popular. We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). (Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. The work by Xu et al (2009) is the closest to our approach. Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish. Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. R5 Superset of rules from (Xu et al., 2009).
One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000). Johnson and Goldwater (2009) showed that word segmentation accuracy improves further if the model learns a nested hierarchy of collocations. The starting point and baseline for our extension is the adaptor grammar with syllable structure phonotactic constraints and three levels of collocational structure (5-21), as prior work has found that this yields the highest word segmentation token f-score (Johnson and Goldwater, 2009). Adaptor Grammar model described in section 2.3 and compare it to the baseline grammar with collocations and phonotactics from Johnson and Goldwater (2009). We use the standard hyperparameters values α = 1.0, β = 0.01 and τ = 1.0 and run the sampler for 1000 iterations, but one can use techniques like slice sampling to estimate the hyperparameters (Johnson and Goldwater, 2009). The Dirichlet parameters α are drawn independently from a Γ(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009). Hierarchical Bayes methods have been mainstream in unsupervised word segmentation since the dawn of hierarchical Dirichlet process (Goldwater et al, 2009) and adaptors grammar (Johnsonand Goldwater, 2009). For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample (Johnson and Goldwater, 2009).  This is the same set-up used by Liang and Klein (2009), Goldwater et al (2006), and Johnson and Goldwater (2009). It is substantially simpler than the non-parametric Bayesian models proposed by Johnson et al (2007), which require sampling procedures to perform inference and achieve an F1 of 87 (Johnson and Goldwater, 2009). While in principle, increasing the number of rejuvenation steps and particles will make this gap smaller and smaller, we believe the existence of the gap to be interesting in its own right, suggesting a general difference in learning behaviour between batch and incremental learners, especially given the similar results in Johnson and Goldwater (2009). Furthermore, adaptor grammars have largely been applied to the task of unsupervised structural induction from raw texts such as morphology analysis, word segmentation (Johnson and Goldwater, 2009), and dependency grammar induction (Cohen et al., 2010), rather than constituent syntax parsing. We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sampling on a segmentation problem. We use the standard Brent corpus (Brent and Cartwright,1996), which includes 9,790 unsegmented phonemic representations of utterances of child-directed speech from the Bernstein-Ratner (1987) corpus. Johnson and Goldwater (2009) test three grammars for this segmentation task. GUnigram and GSyllable can be found in Johnson and Goldwater (2009). For example, with GUnigram convergence typically takes 40 iterations with variational inference, while Johnson and Goldwater (2009) ran their sampler for 2,000 iterations, for which 1,000 were for burning in. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al, 2009). Johnson and Goldwater (2009) have proposed a novel method based on adaptor grammars, whose accuracy surpasses the aforementioned methods by a large margin, when appropriate assumptions are made regarding the structural units of a language. Modeling the corpus using hierarchical grammars that can model the input at varying levels (word collocations, words, syllables, onsets, etc.) provide the learner the most flexibility, allowing the learner to build structure from the individual phonemes and apply distributions at each level of abstraction (Johnson and Goldwater, 2009).
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data.  Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010).
In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. Once this is done, one of the learned collections can be used to generate the summary that best approximates this collection, using the greedy algorithm described by Haghighiand Vanderwende (2009). The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighiand Vanderwende, 2009) were defined over single words (unigrams). This model is very similar to the one used by Haghighi and Vanderwende (2009) in the context of text summarization. Effective ways of representing content and ensuring coverage are the subject of ongoing research in the field (e.g., Gillick et al 2009, Haghighi and Vanderwende 2009). The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization. Entity-aspect model is similar with 'HIERSUM' content model proposed by Haghighi and Vanderwende (2009). In this baseline, we directly compare our method with "HIERSUM" proposed by (Haghighi and Vanderwende, 2009). One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. We re-implement the HIERSUM system from Haghighi and Vanderwende (2009), and show that using our objective dramatically improves the content of extracted summaries. This idea was first presented by Daume and Marcu (2006) for their BAYESUM system for query-focused summarization, and later adapted for non-query summarization in the TOPICSUM system by Haghighi and Vanderwende (2009). Haghighi and Vanderwende (2009) presented a version of HIERSUM that models documents as a bag of bigrams, and provides results comparable to PYTHY. These are based on the manual evaluation questions from DUC 2007, and are the same questions asked in Haghighi and Vanderwende (2009). Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009).
Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009). The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). All data was tokenized and lowercased; German compounds were split (Dyer, 2009). Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al, 2011). An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. Dyer (2009) applied this to German using a lattice encoding different segmentations of German words. Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input.
However, little research has been done on how to effectively perform SRL on bi text, which has important applications including machine translation (Wu and Fung, 2009). Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. Wu and Fung (2009) demonstrated the promise of using features based on semantic predicate argument structure in machine translation, using these feature to re-rank machine translation output. Wu and Fung (2009) developed a framework to reorder the output using information from both the source and the target SRL labels. Wu and Fung (2009) used SRL labels for reordering the n-best output of phrase-based translation systems. Recently, Wu and Fung (2009a; 2009b) also show that semantic roles help in statistical machine translation , capitalising on a study of the correspondence between English and Chinese which indicates that 84% of roles transfer directly, for PropBank-style annotations. Recent years have witnessed increasing efforts towards integrating predicate-argument structures into statistical machine translation (SMT) (Wu and Fung, 2009b; Liu and Gildea, 2010). Unfortunately they are usually neither correctly translated nor translated at all in many SMT systems according to the error study by Wu and Fung (2009a). As PAS analysis widely employs global and sentence-wide features, it is computationally expensive to integrate target side predicate argument structures into the dynamic programming style of SMT decoding (Wu and Fung, 2009b). We also want to address another translation issue of arguments as shown in Table 7: arguments are wrongly translated into separate groups instead of a cohesive unit (Wuand Fung, 2009a). Wu and Fung (2009) present a two-pass model to incorporate semantic information to the phrase-based SMT pipeline. As the demand for semantically consistent machine translation rises (Wu and Fung, 2009a), the need for a comprehensive semantic mapping tool has become more apparent. With the current architecture of machine translation decoders, few ways of incorporating semantics in MT output include using word sense disambiguation to select the correct target translation (Carpuat and Wu, 2007) and reordering/reranking MT output based on semantic consistencies (Wu and Fung, 2009b) (Carpuat et al, 2010). Later, Wu and Fung (2009b) used parallel semantic roles to improve MT system outputs. Other statistical translation specific applications we would like to explore include extensions of MT output reordering (Wu and Fung, 2009b) and reranking using predicate-argument mapping, as well as predicate-argument projection onto the target language as an evaluation metric for MT output. (Wu and Fung, 2009) uses PropBank role labels (Palmer et al, 2005) as the basis of a second pass filter over an SMT system to improve the BLEU score from 42.99 to 43.51.  Our approach is inline with Wu and Fung (2009b) who demonstrated that on the one hand 84% of verb syntactic functions in a 50-sentence test corpus projected from Chinese to English, and that on the other hand about 15% of the subjects were not translated into subjects, but their semantic roles were preserved across language. One problem with using BLEU as an evaluation metric is that it is a precision-oriented metric and tends to reward fluency rather than adequacy (see (Wu and Fung, 2009a; Liu and Gildea, 2010)). Concerning the usage of SRL for SMT, Wu and Fung (2009) reported a first successful application of semantic role labels to improve translation quality.
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built.  Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice.
Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. Gamon (2010) also considers missing and extraneous preposition errors. Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set.  The heuristics are based on those used in Gamon (2010) (personal communication). Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). This is a baseline run that represents the language model approach proposed by Gamon (2010). Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error. Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010).
Recent studies have also developed approaches to summarize conversations (Murray and Carenini, 2008) and to model conversation structures (dialogue acts) from online Twitter conversations (Ritter et al, 2010). Ritter et al (2010) extends LMHMM to allow words to be emitted from two additional sources: the topic of current dialogue, or a background LM shared across all dialogues. Ritter et al (2010) finds these alternate sources are important in non-task-oriented domains, where events are diffuse and fleeting. Ritter et al (2010) proposes an evaluation based on rank correlation coefficient, which measures the degree of similarity between any two orderings over sequential data. Ritter et al (2010) limits their dataset by choosing Twitter dialogues containing 3 to 6 posts (utterances), making it tractable to enumerate all permutations. To overcome this, we plan to use an unsupervised learning approach to discover dialogue acts (Ritter et al, 2010). To date, our paper most closely relates to works on semantic role labeling (SRL) on social media (Liu et al., 2010) and conversation modeling (Ritter et al, 2010). On a different ground, Ritter et al (2010) propose a probabilistic model to discover dialogue acts in Twitter conversations and to classify tweets in a conversation according to those acts. Twitter also provides a wealth of user dialog, and a variety of dialog acts have been observed (Ritter et al, 2010) and predicted (Ritter et al, 2011). Examples of such approaches include methods for conversation structure analysis (Ritter et al, 2010) and exploration of geographic language variation (Eisenstein et al, 2010) from Twitter messages. Second, we employ a MIRA-based binary classifier (Ritter et al, 2010) to predict whether a message mentions a concert event. Twitter has previously been proposed as a candidate for modeling conversations, see for example Ritter et al (2010). In the context of Twitter conversations, Ritter et al (2010) suggests using dialogue act tags as a middle layer towards conversation reconstruction. Specifically, we build off the Bayesian block HMMs used by Ritter et al (2010) for modeling Twitter conversations, which will be our primary baseline. We show that M4 increases thread reconstruction accuracy by up to 15% compared to the HMM of Ritter et al (2010), and we reduce variation of information against speech act annotations by an average of 18% from HMM and LDA baselines. Under the block HMM, as utilized by Ritter et al (2010), messages in a conversation flow according to a Markov process, where the words of messages are generated according to language models associated with a state in a hidden Markov model. If it is desired to have a large number of latent topics as is common in LDA, this model could be combined with a standard topic model without sequential dependencies, as explored by Ritter et al (2010). Unsupervised HMMs were applied to conversational data by Ritter et al (2010) who experimented with Twitter conversations. Second, we use the Twitter data set created by Ritter et al (2010). Our work is motivated by the Bayesian HMM approach of Ritter et al (2010) - the model we refer to as the block HMM (BHMM) - and we consider this our primary baseline.
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. (Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). (Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification.
These abstract notions (lexical association, proximity, tendencies towards few or many relations, and allowing for unassociated items) play an important role in many relation-detection tasks (e.g., co-reference resolution, Haghighi and Klein 2010). Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al (2009) and Haghighi and Klein (2010). In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010).  Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. For the ACE05 and ACE05-ALL datasets, we revert to the 'AllPairs' (AP) setting of Reconcile because this gives us baselines competitive with Haghighi and Klein (2010). Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). The specific update methods vary for each set of parameters; for details see Section 4 of Haghighi and Klein (2010). Unlike Haghighi and Klein (2010), no extra data from Wikipedia or Bllip was used, a restriction that was necessary to be eligible for the closed part of the task. Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al, 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). This knowledge could be especially helpful for cross document coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents.  For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. Daume III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference.
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. This methodology is similar to that of Smith et al (2010). Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. Smith et al (2010) extended these previous lines of work in several directions. To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010).
Similar methods to Shen et al (2007) have also been used in Shen and Joshi (2008) and Goldberg and Elhadad (2010). To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. BIUTEE provides state-of-the-art pre-processing utilities: Easy-First parser (Goldberg and Elhadad, 2010), Stanford named-entity-recognizer (Finkel et al, 2005) and ArkRef coreference resolver (Haghighi and Klein, 2009), as well as utilities for sentence splitting and numerical-normalizations.  This has some similarity to Goldberg and Elhadad (2010). We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise "co-reference" vs. "non-coreference" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010).  The Hebrew tagger and parsing models are described in Goldberg and Elhadad (2010). We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser. The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions.  Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). Our system not only outperforms the best single system (Bjorkelund et al., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al., 2013). In the context of dependency parsing, the strategy of delaying arc construction when the current configuration is not informative is called the easy-first strategy, and has been first explored by Goldberg and Elhadad (2010). The parsing approach is based upon the non directional easy-first algorithm recently presented by Goldberg and Elhadad (2010). While theoretically slower, this has a limited impact upon actual parsing times. See Goldberg and Elhadad (2010) for more explanation. The feature set is based off the features used by Goldberg and Elhadad (2010) but has a significant number of extensions.
Recent work in this area includes Velikovich et al (2010), who developed a method for automatically deriving an extensive sentiment lexicon from the web as a whole. We examine two methods for sentiment detection that of Brody and Elhadad (2010) for detecting sentiment in reviews, and that of Velikovich et al (2010) for finding sentiment terms in a giga-scale web corpus. Velikovich et al (2010) constructed a graph where the nodes were 20 million candidate words or phrases, selected using a set of heuristics including frequency and mutual information of word boundaries. On the other hand, the method of Velikovich et al (2010) is based on huge amounts of data, and takes advantage of the abundance of contextual information available in full documents, whereas our domain is closer to that of Brody and Elhadad (2010), who dealt with a small number of candidates and short documents typical to online reviews. Once the graph is constructed, we can use either of the propagation algorithms of Brody and Elhadad (2010) and Velikovich et al (2010), which we will denote Reviews and Web, respectively. Velikovich et al (2010) employed a different label propagation method, as described in Figure 3.  In Velikovich et al (2010), the parameters were tuned on a held out dataset. Top fifteen negative and positive words for the algorithms of Brody and Elhadad (2010) (Reviews) and Velikovich et al (2010) (Web). Although such an assumption played a key role in previous work for the analogous task of learning sentiment lexicon (Velikovich et al, 2010), we expect that the same assumption would be less reliable in drawing subtle connotative sentiments of words.  Velikovich et al (2010) use graph propagation algorithms for constructing a web-scale polarity lexicon for sentiment analysis. A technique named label propagation (Zhu and Ghahramani, 2002) has been used by Rao and Ravichandran (2009) and Velikovich et al (2010), while random walk based approaches, PageRank in particular, have been used by Esuli and Sebastiani (2007).  However in recent years, sentiment lexicons started expanding to include some of those words that simply associate with sentiment, even if those words are purely objective (e.g., Velikovich et al (2010), Baccianellaet al (2010)). In order to collectively induce the visually descriptive words from this graph, we apply the graph propagation algorithm of Velikovich et al (2010), a variant of label propagation algorithms (Zhu and Ghahramani, 2002) that has been shown to be effective for inducing a web-scale polarity lexicon based on word co-occurrence statistics. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al 2010) and unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011). For example, constructing web-derived polarity lexicons (Velikovich et al 2010), top 25 edges were used, and for unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011), top 5 edges were used. A web-derived lexicon (Velikovich et al, 2010) was constructed for all words and phrases using graph propagation algorithm which propagates polarity from seed words to all other words. Recently, (Velikovich et al, 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as 'once in a life time'.
(Cherry and Foster, 2012), which is closer to the usual loss used for max-margin in machine learing. Cherry and Foster (2012) have concurrently performed a similar analysis. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training (Macherey et al, 2008) and batch MIRA (Cherry and Foster, 2012). As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al, 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. Second, we ran the k-best batchMIRA (kbMIRA) (Cherry and Foster, 2012) implementation in Moses. Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). See (Cherry and Foster, 2012) for details on objectives.
   Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012).  A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance.      Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features.   
Dahlmeier and Ng (2012) propose an alternative evaluation scheme which, along with other properties, overcomes this by operating in terms of tokens rather than character offsets. Unlike evaluating grammar error correction systems (Dahlmeier and Ng, 2012), correction detection cannot refer to a gold standard.   In order to overcome this problem, the Max Match (M2) scorer was proposed in (Dahlmeier and Ng, 2012b).   
POS tagger in this work, we note that taggers optimized specifically for social media are now available and would likely have resulted in higher tagging accuracy (e.g. Owoputi et al (2013)). To test this, we train a CRF model (Lafferty et al, 2001) with simple orthographic features and word clusters (Owoputi et al, 2013) on the annotated Twitter data described in Gimpel et al (2011). For NER, we use standard features, including POS tags (from the previous experiments), indicators for hyphens, digits, single quotes, upper/lowercase, 3-character prefix and suffix information, and Brown word cluster features 6 with 2,4,8,16 bit string prefixes estimated from a large Twitter corpus (Owoputi et al, 2013). We use the CMU Twitter Part-of-Speech Tagger (Owoputi et al, 2013) to select only instances in the verb sense. Part-of-speech tags are assigned based on Owoputi et al's tweet POS system (Owoputi et al, 2013).
These vector representations capture interesting linear relationships (up to some accuracy), such as king-man+woman=queen (Mikolov et al, 2013). However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize (Mikolov et al, 2013) and learn nonlinear relationships. Continuous space models have also been used for generating translations for new words (Mikolov et al 2013a) and ITG reordering (Li et al 2013). While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al (2013a) specifically to test predict models. Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large). Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al (2013c) using a skip-gram predict model. Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al (2013c) among 700K words, and we among 300K words.  Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al, 2013b). More recently, Mikolov et al (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. Mikolov et al (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations. The word vectors we use are from Mikolov et al (2013) 13 (Mikolov13), and additional results are also shown using Turian et al (2010) 14 (Turian10). Mikolov et al (2013b) show that pre-trained embeddings can capture interesting semantic and syntactic information such as king-man+woman=queen on English data. These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech. Among the state-of-the-art word embedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al (2013b) and implemented in the word2vec software. Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al, 2013a) trained using the negative-sampling procedure presented in (Mikolov et al, 2013b). We also tried using the sub sampling option (Mikolov et al, 2013b) with BOW contexts (not shown). In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014). Furthermore, their text-based vectors encode very rich information, such as king-man+woman=queen (Mikolov et al, 2013c). This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically.
Related work by Mani and Wilson (2000) focuses only on the core temporal expressions neglecting the temporal information conveyed by prepositions (e.g. Friday vs. by Friday). The main part of the system is a temporal expression tagger that employs finite state transducers based on hand-written rules. A more complex set of temporal expressions as extracted by recent systems (e.g. (Mani and Wilson, 2000)) was tagged. The systems compared against are: GUTime (Mani and Wilson, 2000), a widely used, older rule-based system. Although Ahn et al (2007) compared their results with those presented by Mani and Wilson (2000), they went on to point out that, for a variety of reasons, the numbers they provided were not really comparable. Most closely relevant to the work described in the present paper are the approaches described in (Baldwin, 2002), (Jang et al, 2004) and (Mani and Wilson, 2000). In the system presented in (Mani and Wilson, 2000), weekday name interpretation is implemented as part of a sequence of interpretation rules for temporal expression interpretation more generally. GUTime (Mani and Wilson, 2000) presents an older but widely used baseline. As a result, many timex interpretation systems are a mixture of both rule-based and machine learning approaches (Mani and Wilson, 2000).  The GUTime tagger, developed at Georgetown University, extends the capabilities of the TempEx tagger (Mani and Wilson, 2000). The system of Mani and Wilson (2000) goes further in using separate sets of hand-crafted rules for recognition and normalization and in separating out several disambiguation tasks. Mani and Wilson (2000) and Ahn et al (2005b) also perform limited semantic class disambiguation. (Mani and Wilson, 2000) use a heuristic method for this task, while (Ahn et al, 2005b) use a machine learned classifier. The feature TEMPEX recorded the number of temporal expressions in each clause, as returned by a temporal expression tagger (Mani and Wilson, 2000). A natural way to go about update summarization would be extracting temporal tags (dates, elapsed times, temporal expressions...) (Mani and Wilson, 2000) or to automatically construct the timeline from documents (Swan and Allan, 2000). In terms of hand-coded approaches, (Mani and Wilson 2000) used a baseline method of blindly propagating TempEx time values to events based on proximity, obtaining 59.4% on a small sample of 8,505 words of text. The use of machine learning techniques — mainly statistical — for this task is a more recent development, either alongside the traditional hand-grammar approach to learn to distinguish specific difficult cases (Mani and Wilson, 2000), or on its own (Hacioglu et al., 2005). Mani and Wilson (2000) worked on news and introduced an annotation scheme for temporal expressions, and a method for using explicit temporal expressions to assign activity times to the entirety of an article. Mani and Wilson (2000) attribute over half the errors of their baseline method to propagation of an incorrect event time to neighboring events. It was cited in (Mani and Wilson 2000) as achieving a .83 F-measure against hand-annotated data.
(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. Ngai and Yarowsky (2000) used an ensemble based on bagging and partitioning for active learning for base NP chunking. In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker.
Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language's inflectional parts of speech, and the canonical suffixes for each part of speech. MED has been applied to the morphology induction problem by other researchers (such as Yarowsky and Wicentowski, 2000). For the future, we expect improvements could be derived by coupling this work, which focuses primarily on inducing regular morphology, with that of Yarowsky and Wicentowski (2000), who assume some information about regular morphology in order to induce irregular morphology.  Yarowsky and Wicentowski (2000) propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations). Yarowsky and Wicentowski (2000) report an accuracy of over 99% for their best model and a test set of 3888 pairs. From the point of view of the evaluation of the algorithm, we should design an assessment scheme that would make our experimental results more directly comparable to those of Yarowsky and Wicentowski (2000), Schone and Jurafsky (2000) and others. The development of the WordFrame model was motivated by work originally presented in Yarowsky and Wicentowski (2000). The supervised morphological learner presented in Yarowsky and Wicentowski (2000) modeled lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes. In Yarowsky and Wicentowski (2000), the end-of-string model is trained from inflection-root pairs acquired through unsupervised methods. We propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented in (Yarowsky and Wicentowski, 2000), (Baroni et al, 2002) and (Wicentowski, 2002). Yarowsky and Wicentowski (2000) have developed a system that learns such rules given a preliminary morphological hypothesis and part of speech tags. Similarly as Yarowsky and Wicentowski (2000), we assume that, in any language, vowels are more mutable in inflection than consonants, thus for example replacing a for i is cheaper that replacing s by r. Yarowsky and Wicentowski (2000) use similar statistics to identify words related by inflection, but they gather their counts from a much smaller corpus. Yarowsky and Wicentowski (2000) propose an interesting algorithm that employs four similarity measures to successfully identify the most probable root of a highly irregular word. Lemmatization was performed using an existing trie-based supervised models for English, and a combination of supervised and unsupervised methods (Yarowsky and Wicentowski, 2000) for all the other languages. Yarowsky and Wicentowski (2000) present a corpus-based approach for morphological analysis of both regular and irregular forms based on four models including: relative corpus frequency, context similarity, weighted string similarity and incremental retraining of inflectional transduction probabilities.
At ACL 2000, Brill and Moore (2000) introduced a new error model, allowing generic string-to-string edits. Brill and Moore (2000) present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions. Brill and Moore (2000) showed that adding a source language model increases the accuracy significantly. The error model LTR was trained exactly as described originally by Brill and Moore (2000). In baseline speller we use a substring-based error model P dist (q0|q1) described in (Brill and Moore, 2000), the error model training method and the hypotheses generator are similar to (Duan and Hsu, 2011). Brill and Moore (2000) characterise the error model by computing the product of operation probabilities on slice-by-slice string edits. The next class of approaches applied the noisy channel model to correct single word spelling errors (Kernighan et al., 1990), (Brill and Moore, 2000). Neither do we require pairs of misspelled names and their correct spellings for learning the error model unlike (Brill and Moore, 2000) or large-coverage general purpose lexicon for unlike (Cucerzan and Brill, 2004) or pronunciation dictionaries unlike (Toutanova and Moore, 2002). Brill and Moore (2000) learn misspelled-word to correctly-spelled-word similarities for spelling correction. For example, Brill and Moore (2000) combine a character-based alignment with the Expectation Maximization (EM) algorithm to develop an improved probabilistic error model for spelling correction. The largest step towards an automatically trainable spelling system was the statistical model for spelling errors (Brill and Moore, 2000). Contrary to Brill and Moore (2000), we observe that user edits of ten have both left and right context, when editing a document. The perfect score function calculates the probability of a suggestion given the misspelled word (Brill and Moore, 2000). In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al (2001). One interesting future avenue to consider is to use the edit distance functions in our current model to select a subset of query-candidate pairs that are similar in terms of these functions, separately for the surface and Romanized forms, and use this subset to align the character strings in these query-candidate pairs as described in Brill and Moore (2000), and add the edit operations derived in this manner to the term variation identification classifier as features. Brill and Moore (2000) introduced a model that worked on character sequences, not only on character level, and was conditioned on where in the word the sequences occurred. The approximate string matching algorithm we suggest is essentially that of Brill and Moore (2000), a modified weighted Levenshtein distance, where we allow error operations on character sequences as well as on single characters. It would have been possible to use a fast trie implementation (Brill and Moore, 2000), however. (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al, 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. The second is a slightly modified version of the spelling correction model of Brill and Moore (2000).
One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). Banko et al (2000) propose a bag-of-words model for headline generation. Following Banko et al (2000), we approximated the length distribution with a Gaussian. This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. Banko et al (2000) uses beam search to identify approximate solutions. In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003).
IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003). We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). The Alignment Error Rate (AER) introduced by Och and Ney (2000b) measures the fraction of links by which the automatic alignment differs from the reference alignment. The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). The GIZA++ toolkit (Och and Ney, 2000a) was used for training the IBM-3 models (as in (Och and Ney, 2000b)). Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b). The performance of the four decoders was measured with respect to the alignments provided by human experts (Och and Ney, 2000b). Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). The approach was first presented by Brown et al (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003). In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences.  Table 2 compares the results of our algorithm with the results in (Och and Ney, 2000), where an HMM model is used to bootstrap IBM Model 4. This demonstrates that we are competitive with the methods described in (Och and Ney, 2000).  The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004).
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem: the non-argument tree can be adjoined onto a node (say, VP), positioning itself in between the VP's children, which is not possible with TAGs. Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. 
See (Gildea and Jurafsky, 2000) for some promising initial work in applying statistical techniques to the FrameNet database to automatically label frame elements. While a machine learning approach is used in (Gildea and Jurafsky, 2000) to determine general semantic roles, we used a simple rule-based traversal of the parse tree instead, which could also reliably determine the generic agent and patient role of a sentence, and this suffices for our current purpose. Many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicate argument recognition tasks, such as semantic roles. Gildea and Jurafsky (2000, 2002) describe a statistical approach for semantic role labelling using data collected from FrameNet. (Pado et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles.  Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). Since direct assignment of role labels to instances fails due to the preponderance of unlabelled instances, which make up 86.7% of all instances, we follow Gildea and Jurafsky (2000) in splitting the task into two sequential subtasks: first, argument recognition decides for each instance whether it bears a semantic role or not; then, argument labelling assigns a label to instances recognised as role-bearers. There has been some related work on using the frame of FrameNet for reasoning (Chang et al, 2002) and also on the automatic annotation of English texts with regard to the relevant frames (Gildea and Jurafsky, 2000) and frame elements. To our knowledge, Gildea and Jurafsky (2000) is the only work that uses FrameNet to build a statistical semantic classifier. Gildea and Jurafsky (2000) describe a system that uses completely syntactic features to classify the Frame Elements in a sentence. We extend Gildea and Jurafsky (2000)'s initial effort in three ways. Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000). Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000). Gildea and Jurafsky (2000) use 36995 training, 4000 development, and 3865 test sentences. As a further analysis, we have examined the performance of our base ME model on the same test set as that used in Gildea and Jurafsky (2000). Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. (Gildea and Jurafsky, 2000) proposed a statistical approach based on FrameNet I data for annotation of semantic roles. The features used for training the labeler are a subset of the features used by Gildea and Jurafsky (2000), Xue and Palmer (2004), and Pradhan et al (2004).
 Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000).  Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. (Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)): Heuristic 1: Whenever the first feedback loop requires the addition of the main verb of the question as a query keyword, generate all verb conjugations as well as its nominalizations. Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. In the work of Moldovan et al (2000), all why-questions share the single answer type reason.
However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus. Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a). Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)). Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b). Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001). As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods.
Some works, such as Barzilay and McKeown (2001), have acquired paraphrasing knowledge automatically. Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences.  Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines forword sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source. Jacquemin (1999) and Barzilay and McKeown (2001) identify phrase level paraphrases, while Lin and Pantel (2001) and Shinyama et al (2002) acquire structural paraphrases encoded as templates. Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al, 2007) (SMT method), and that of Murata et al (2004) (Mrt method). In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point. A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al, 2004). We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a co training-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (11 2) equivalent translation pairs.). Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. We find that most paraphrases extracted using the method of Barzilay and McKeown (2001) are quite short. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al 2004) and DIRT (Lin and Pantel 2001). To generate dialogue sentences for a corresponding discourse structure we are adapting the approach to paraphrasing of Barzilay and McKeown (2001).  The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours.
   That is, we parse all sentences using the Charniak's parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence. The model presented by Charniak (Charniak, 2001) identifies both syntactic structural and lexical dependencies that aid in language modeling. The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice, while the second stage rescores these candidate edges using a lexicalized syntactic language model (Charniak, 2001). These parses are then rescored using a lexicalized syntactic model (Charniak, 2001). These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent (Charniak, 2001). The second stage parser used is a modified version of the Charniak language modeling parser described in (Charniak, 2001). Charniak (Charniak, 2000) developed a state-of-the-art statistical CFG parser and then built an effective language model based on it (Charniak, 2001). Our system embeds Phramer2 (used for minimum error rate training, decoding, decoding tools), Pharaoh (Koehn, 2004) (decoding), Carmel 3 (helper for Pharaoh in n-best generation), Charniak's parser (Charniak, 2001) (language model) and SRILM4 (n-gram LM construction). We used Charniak's parser as an additional LM (Charniak, 2001) in reranking. Hall and Johnson (2003) use a best-first probabilistic context free grammar (PCFG) to parse the input lattice, pruning to a set of local trees (candidate partial parse trees), which are then passed to a version of the parser of Charniak (2001) for more refined parsing. The parses were automatically produced by the parser of Charniak (2001). The current best-performing models, in terms of WER, for the HUB-1 corpus, are the models of Roark (2001), Charniak (2001) (applied to n-best lists by Hall and Johnson (2003)), and the SLM of Chelba and Jelinek (2000) (applied to n-best lists by Xu et al (2002)). Hall (2003) is a lattice-parser related to Charniak (2001). The difference in WER between our parser and those of Charniak (2001) and Roark (2001) applied to word lists may be due in part to the lower PARSEVAL scores of our system. Another contributing factor to the accuracy of Charniak (2001) is the size of the training set: 20M words larger than that used in this work. We use a syntax-based language model which was originally developed for use in speech recognition (Charniak, 2001) and later adapted to work with a syntax-based machine translation system (Charniaket al, 2001). Noun noun (adjective noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak, 2001) (for XWN we used the gold parse trees).
Given the fine-grained syntactic and semantic analysis of the HPSG grammar and its robustness (through SNLP integration), we decided to use the semantic representation (MRS, see (Copestake et al, 2001)) as additional input for IE. The semantic representations used are flat semantic representations in the sense of [Copestake et al, 2001] and the semantic parameters (that is, the semantic indices representing the missing arguments of the semantic functors) are represented by unification variables. Ge and Mooney (2009) extracts semantic representations using syntactic structures while Copestake et al (2001) develops algebras for semantic construction within grammars. Recent work in semantic construction for HPSG (Copestake et al., 2001) supports our conjecture: the examples discussed there are compatible with our simplification. The original Grammar Matrix consisted of types defining the basic feature geometry, types associated with Minimal Recursion Semantics (e.g., (Copestake et al, 2001)), types for lexical and syntactic rules, and configuration files for the LKB grammar development environment (Copestake, 2002) and the PET system (Callmeier,2000). The Redwoods tree bank provides deeper semantics expressed in the Minimum Recursion Semantics formalism (Copestake et al, 2001), but in the present experiments we have not explored this fully. In future work we will compare our semantics construction principles to the general model of Copestake et al (2001). The semantic interpretations are expressed using Minimal Recursion Semantics (MRS) (Copestake et al, 2001), which provides the means to represent interpretations with a flat, underspecified semantics using terms of the predicate calculus and generalized quantifiers. The grammar is couched in the theoretical framework of Head-Driven Phrase Structure Grammar (HPSG) (Pollard; Sag 1994), with semantic representations in Minimal Recursion Semantics (MRS) (Copestake et al 2001). Our method effectively performs automatic RMRS semantics construction from functional dependencies, following the semantic algebra of Copestake et al (2001). We show how to adapt the construction principles of the semantic algebra of Copestake et al (2001) to RMRS construction from dependencies in a rewrite scenario, and discuss the treatment of some special phenomena, such as verbal complementation, coordination and modification. For (R) MRS construction from dependencies we follow the algebra for semantics composition in Copestake et al (2001). Copestake et al (2001) mention a third feature to be included in the hook as an externally visible variable, which they instantiate with the index of the controlled subject in equi constructions and which is also used to implement the semantics of predicative modification. Otherwise, composition strictly follows the semantic operations of the algebra of Copestake et al (2001): the composition rules only refer to the hook and slots of functors and arguments, to achieve the binding of argument variables and the encoding of scope constraints. The algebra of Copestake et al (2001) defines modifiers to externalise the variable of the ARG1. In future research, we will investigate how the semantic algebra of Copestake et al (2001) compares to Glue Semantics (Dalrymple, 1999). The coincidence with Copestake et al.'s terminology (Copestake et al, 2001) is not casual; in fact, our formulation can be regarded as a decoupled fragment of theirs, since neither our holes involves syntactic labels nor are scopal issues ever touched. We use this parser to parse the defining sentences into a full meaning representation using minimal recursion semantics (MRS: Copestake et al (2001)). The output is written in Minimal Recursion Semantics (Copestake et al, 2001). An MRS consists of a bag of labeled elementary predicates and their arguments, a list of scoping constraints, and a pair of relations that provide a hook into the representation - a label, which must outscope all the handles, and an index (Copestake et al, 2001).
We are aware of the fact that other measures of lexical association have been proposed (Evert and Krenn, 2001, and MI values were computed using Adam Berger's trigger toolkit (Berger, 1997).  Following the methodology described by Evert and Krenn (2001), German PP-verb combinations were extracted from a chunk-parsed version of the Frankfurter Rundschau Corpus.  In particular, the precision/recall value comparison between the various AMs exhibits a rather inconclusive picture in Evert and Krenn (2001) and Krenn and Evert (2001) as to whether sophisticated statistical AMs are actually more viable than frequency counting. In particular Evert and Krenn (2001) use the chi-square test which assumes independent samples and is thus not really suitable for testing the significance of differences of two or more measures which are typically run on the same set of candidates (i.e., a dependent sample). As the standard statistical AM, we selected the t-test (see also Manning and Sch¨utze (1999) for a description on its use in CE and ATR) because it has been shown to be the best-performing statisticsonly measure for CE (cf. Evert and Krenn (2001) and Krenn and Evert (2001)) and also for ATR (see Wermter and Hahn (2005)). To overcome these limitations, we use the evaluation method described by Evert and Krenn (2001). Cf. also Evert and Krenn (2001) for empirical evidence justifying the exclusion of low-frequency data. The comparison to the t-test is especially interesting because it was found to achieve the best overall precision scores in other studies (see Evert and Krenn (2001)). We also define four features that represent known collocation measures (Evert and Krenn, 2001): Point-wise mutual information (PMI); T-Score; log-likelihood; and the raw frequency of N1 N2 in the corpus. To eliminate noisy low-frequency data (cf. also Evert and Krenn (2001)), we defined different frequency cut-off thresholds, c, for the bigram, trigram and quadgram candidate sets and only considered candidates above these thresholds. We compare our P-Mod algorithm against the t-test measure, which, of all standard measures, yields the best results in general-language collocation extraction studies (Evert and Krenn, 2001), and also against the widely used C-value, which aims at enhancing the common frequency of occurrence measure by making it sensitive to nested terms (Frantzi et al, 2000). Studies on collocation extraction (e.g., by Evert and Krenn (2001)) also point out the inadequacy of such evaluation methods. The evaluation procedure used here (first suggested by Evert and Krenn (2001) for evaluating measures of lexical association) involves producing and evaluating just such a ranking. This is consistent with results reportedby Evert and Krenn (2001). Our evaluation was partly inspired by Evert and Krenn (2001). In Krenn & Evert 2001, frequency outperformed mutual information though not the t test, while in Evert and Krenn 2001, log-likelihood and the t-test gave the best results, and mutual information again performed worse than frequency.
See (Brown et al, 1993) or (Germann et al, 2001) for a detailed discussion of this translation model and a description of its parameters. The decoding algorithm that we use is a greedy one - see (Germann et al, 2001) for details. As discussed by Germann et al (2001), the word-for-word gloss is constructed by aligning each French word fL with its most likely English translation efk. Similarly to the work by Germann et al (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al, 2001). For instance, Germann et al (2001) present an ILP formulation of the Machine Translation (MT) decoding task in order to conduct exact inference. This feature distinguishes our task from other decoding problems, such as decoding in machine translation (Germann et al, 2001), that are modeled using a standard TSP formulation. We also included in the figure the performance of an IBM Model 4 word based translation system (M4), which uses a greedy decoder [Germann et al, 2001]. (Germann et al, 2001) presents a greedy approach to search for the translation that is most likely according to previously learned statitistical models. The system uses models GIZA++ and ISI ReWrite decoder (Germann et al., 2001). As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al, 2001). Och et al (2001) and Germann et al (2001) both implemented optimal decoders and benchmarked approximative algorithms against them. Germann et al (2001) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem. While acceptably fast for the kind of evaluation used in Germann et al (2001), namely sentences of up to 20 words, its speed becomes an issue for more realistic applications. In this subsection we recapitulate the greedy hill climbing algorithm presented in Germann et al (2001). Thirdly, the results of our experiments with randomized searches show that greedy decoding does not perform as well on longer sentences as one might conclude from the findings in Germann et al (2001). In this paper, we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al (2001) and presented improvements that drastically reduce the decoder's complexity and speed to practically linear time. Germann et al (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.  The list of zero fertility words can be obtained from the viterbi alignment of training corpus (Germann et al, 2001).
CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al, 1998). Misra et al (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) using TMs.  (Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions. In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00.   Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). Two frequently-cited systems are LCSEG (Galley et al, 2003) and TEXTSEG (Utiyama and Isahara, 2001). A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001). It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). In this paper, we selected for comparison three systems based merely on the lexical reiteration feature: TextTiling (Hearst, 1997), C99 (Choi, 2000) and TextSeg (Utiyama and Isahara, 2001). The TextSeg algorithm (Utiyama and Isahara,2001) implements a probabilistic approach to determine the most likely segmentation, as briefly described below.
Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as input a parsed sentence rather than simple words. Zhu et al. (2010) constructed a parallel corpus (PWKP) of 108,016/114,924 complex/simple sentences by aligning sentences from EWKP and SWKP and used the resulting bitext to train a simplification model inspired by syntax-based machine translation (Yamada and Knight, 2001). We refer the reader to (Yamada and Knight, 2001) for more details. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig. This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to by). In applications such as the syntax based machine translation model of (Yamada and Knight, 2001), a low quality tree might lead to errorenous translation of the sentence. Additionally, we present novel on-the-fly variants of these algorithms, and compare their performance on a syntax machine translation cascade based on (Yamada and Knight, 2001). We adapt the Japanese-to-English translation model of Yamada and Knight (2001) by transforming it from an English-tree-to-Japanese-string model to an English-tree-to-Japanese-tree model. Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of 609 operations that transform a target parse tree into a source string. Therefore, we have introduced a variation of the Inside-Outside algorithm as seen in (Yamada and Knight, 2001) for E step computation. Yamada and Knight (2001) further extended the model to a syntax-to-string translation modeling.  Yamada and Knight (2001) present an algorithm for estimating probabilistic parameters for a similar model which represents translation as a sequence of re-ordering operations over children of nodes in a syntactic tree, using automatic parser output for the initial tree structures. We begin by summarizing the model of Yamada and Knight (2001), which can be thought of as representing translation as an Alexander Calder mobile. In part to deal with this problem, Yamada and Knight (2001) flatten the trees in a pre-processing step by collapsing nodes with the same lexical head-word. Based on an example from (Yamada and Knight, 2001), we provide a sample SCFG fragment translating from English to Japanese, specified by means of the following synchronous productions. Variant of this definition can be found where the input is a single parse tree for w (Yamada and Knight, 2001), or where the output is a single parse tree, chosen according to some specific criteria (Wu and Wong, 1998). Syntax-based Statistical Translation (Yamada and Knight, 2001): This model extends the above by allowing all possible permutations of the RHS of the English rules. Finally, there are three resulting parameter tables analogous to the r-table; as stated in (Yamada and Knight, 2001), consisting of POS and constituent based patterns allowing for reordering and monotone distortion (examples can be found in Table 5). As a result, there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase-based SMT (Koehn et al 2005), hierarchical phrase-based translation (Chiang, 2007), syntax-based translation (Yamada and Knight, 2001), or pre ordering (Xia and McCord, 2004).
The calculation of expected counts can be formulated using the expectation semiring frame work of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Li and Eisner, 2009). In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or epsilon-cycles in FSMs (Eisner, 2002). We use standard algorithms (Eisner, 2002) to compute the path sums as well as their gradients with respect to theta for optimization (section 4.1). We used the OpenFST library (Allauzen et al, 2007) to implement all finite-state computations, using the expectation semiring (Eisner, 2002) for training. Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hyper graphs). In this paper, we apply the expectation semiring (Eisner, 2002) to a hyper graph (or packed forest) rather than just a lattice. Eisner (2002) uses closed semirings that are also equipped with a Kleene closure operator.  However, Eisner (2002, section 5) observes that this is inefficient when n is large. This follows Eisner (2002), who similarly generalized the forward-backward algorithm. For example, Eisner (2002) uses finite-state operations such as composition, which do combine weights entirely within the expectation semiring before their result is passed to the forward-backward algorithm. We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings. This logic can be used with the expectation semiring (Eisner, 2002) to find the maximum likelihood estimates of the parameters of a word-to-word translation model. Eisner (2002) has claimed that parsing under an expectation semiring is equivalent to the Inside-Outside algorithm for PCFGs. To compute this, intersect the WFSA and the lattice, obtaining a new acyclic WFSA, and sum the u-scores of all its paths (Eisner, 2002) using a simple dynamic programming algorithm akin to the forward algorithm.  Eisner (2002) gives a general EM algorithm for parameter estimation in probabilistic finite-state transducers. Eisner (2002) describes the expectation semiring for parameter learning.
Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002).  Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts.  Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). Ravichandran and Hovy (2002) focus on scaling relation extraction to the Web. We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. RH02: The algorithm by Ravichandran and Hovy (2002) described in Section 2. These patterns could be manually generated, such as the ones described here, or learned from text, as described in Ravichandran and Hovy (2002). Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web.
See Ng and Cardie (2002) for a detailed description of the features. We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach. One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). In the testing phase, we used the best-first clustering as in Ng and Cardie (2002). We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported: 73.3%), the anaphora labelled portion of AQUAINT used in Cherry and Bergsma (2005) (1078 instances, highest accuracy: 71.4%), and the anaphoric pronoun subset of the MUC7 (1997) coreference evaluation formal test set (169 instances, highest precision of 62.1 reported on all pronouns in (Ng and Cardie, 2002)). Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases. Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002).
A number of studies are related to the work we presented, most specifically work on parallel-text based "information projection" for parsing (Hwa et al., 2002), but also grammar induction work based on constituent/distituent information (Klein and Manning, 2002) and (language-internal) alignment based learning (van Zaanen, 2000). Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. We primarily compare our method to the constituent-context model (CCM) of Klein and Manning (2002). CCM is used with the initializer proposed in Klein and Manning (2002). The EM algorithm with the CCM requires very careful initialization, which is described in Klein and Manning (2002). Empirically, our algorithm performs favorably to the CCM of Klein and Manning (2002) without the need for careful initialization. Finally, there are "unsupervised" strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ tree bank together with additional WSJ newswire (Klein and Manning,2002). An excellent recent result is by Klein and Manning (2002). We refer readers to Klein and Manning (2002) or Cover and Thomas (1991, p. 72) for details; computing expected counts for a sentence is a closed form operation. The third line corresponds to the setup reported by Klein and Manning (2002). We implement the baseline system, which Klein and Manning (2002) use for their grammar induction experiments with induced part-of-speech tags. We follow Klein and Manning (2002) in using K means to cluster the d dimensional word vectors into parts-of-speech. We chose the baseline system primarily to match previous evaluations of grammar induction using induced tags (Klein and Manning, 2002). Klein and Manning (2002) present a generative model for inducing constituent boundaries from part-of-speech tagged text. We evaluate induced constituency trees against those of the Penn Treebank using the versions of unlabeled precision, recall, and F-score used by Klein and Manning (2002). Evaluation of the algorithm is done according to PARSEVAL, except for a few changes that are also proposed by Klein and Manning (2002). Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. Finally, we intersect the feature-augmented PCFG with the CCM model of Klein and Manning (2002), a high quality bracketing model.
Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002).  In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002).  Comparing our results to Johnson (2002), we find that the NOINSERT model outperforms that of Johnson by 4.6% (see Table 7). Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head - dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. We compare our algorithm under a variety of conditions to the work of (Johnson, 2002) and (Gabbard et al, 2006). The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion. Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003). While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later. in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. For purposes of comparability with Johnson (2002) we used Charniak's 2000 parser as P. P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance.
Pronunciation modeling in (Toutanova and Moore, 2002) further improves spelling correction performance. (Toutanova and Moore, 2002) improved the string to string edits model by modeling pronunciation similarities between words. Choudhury et al. (2007) implemented the noisy channel through a Hidden-Markov Model (HMM) able to handle both graphemic variants and phonetic plays as proposed by (Toutanova and Moore, 2002), while Cook and Stevenson (2009) enhanced the model by adapting the channel's noise P (O|W, wf) according to a list of predefined observed word formations {wf}: stylistic variation, word clipping, phonetic abbreviations, etc. The key component here is the error model, which should not only capture orthographic similarities (Brill and Moore, 2000), but also phonetic similarities (Toutanova and Moore, 2002). (Brill and Moore, 2000) presented an improved error model over the one proposed by (Kernighan et al, 1990) by allowing generic string-to-string edit operations, which helps with modeling major cognitive errors such as the confusion between le and al. Via explicit modeling of phonetic information of English words, (Toutanova and Moore, 2002) further investigated this issue. This approach has also been extended to incorporate a pronunciation model (Toutanova and Moore, 2002). Neither do we require pairs of misspelled names and their correct spellings for learning the error model unlike (Brill and Moore, 2000) or large-coverage general purpose lexicon for unlike (Cucerzan and Brill, 2004) or pronunciation dictionaries unlike (Toutanova and Moore, 2002). For example, Brill and Moore (2000) developed a generative model including contextual substitution rules; and Toutanova and Moore (2002) further improved the model by adding pronunciation factors into the model. Toutanova and Moore (2002) improve the model by incorporating pronunciation information. Toutanova and Moore (2002) further explored this via explicit modeling of phonetic information of English words. Our model of pronunciation variation is used to extend a pronouncing dictionary for use in the spelling correction algorithm developed by Toutanova and Moore (2002), which includes models for both orthography and pronunciation. The pronunciation variation model is used to generate multiple pronunciations for each canonical pronunciation in a pronouncing dictionary and these variations are used in the spelling correction approach developed by Toutanova and Moore (2002), which uses statistical models of spelling errors that consider both orthography and pronunciation. Toutanova and Moore (2002) extend Brill and Moore (2000) to consider edits over both letter sequences and sequences of phones in the pronunciations of the word and misspelling. The spelling correction models from Brill and Moore (2000) and Toutanova and Moore (2002) use the noisy channel model approach to determine the types and weights of edit operations. Toutanova and Moore (2002) describe an extension to Brill and Moore (2000) where the same noisy channel error model is used to model phone sequences instead of letter sequences. Since a spelling correction model needs to rank candidate words rather than candidate pronunciations, Toutanova and Moore (2002) derive an error model that determines the probability that a word w was spelled as the non-word r based on their pronunciations. Like Toutanova and Moore (2002), we use the n-gram LTP model from Fisher (1999) to predict these pronunciations. The pronunciation based spelling correction approach developed in Toutanova and Moore (2002) requires a list of possible pronunciations in order to compare the pronunciation of the misspelling to the pronunciation of correct words. In order to evaluate the effect of pronunciation variation in Toutanova and Moore (2002)'s spelling correction approach, we compare the performance of the pronunciation model and the combined model. The noisy channel spelling correction approach developed by Brill and Moore (2000) and Toutanova and Moore (2002) appears well-suited for writers of English as a foreign language.
Perspective GATE (Cunningham et al, 2002a) is an architecture, a framework and a development environment for human language technology modules and applications. The GATE API (Application Programming Interface) is fully documented in Javadoc and also examples are given in the comprehensive User Guide (Cunningham et al, 2002b). JAPE is a version of CPSL (Common Pattern Specification Language) (Appelt, 1996) and is used to describe patterns to match and annotations to be created as a result (for further details see (Cunningham et al, 2002b)).  The automatic alignments have then been manually corrected through a graphical editing tool within the GATE framework (Cunningham et al, 2002). The GATE system (Cunningham et al., 2002) is used to tag named entities, which are categorized as <Person>, <Organization>, <Location> and <Date>. In order to produce the gold standard annotations in GerManC-GS we used the GATE platform, which facilitates automatic as well as manual annotation (Cunningham et al 2002). Linguistic analysis of textual input is carried out using the General Architecture for Text Engineering (GATE) - a framework for the development and deployment of language processing technology in large scale (Cunningham et al, 2002). The algorithm was implemented using the GATE NLP framework (Cunningham et al, 2002) and texts preprocessed using the tokeniser, sentence splitter, and part-of-speech (POS) tagger provided with GATE. These named entities are tagged with GATE (Cunningham et al, 2002). The linguistic component uses the infrastructure and the following resources from GATE (Cunningham et al, 2002): tokenizer, sentence splitter, part-of-speech tagger, morphological analyzer and VPchunker. We developed a set of algorithms along with existing NLP tools (GATE (Cunningham et al, 2002) etc.) for this task. For named-entity recognition, we use GATE (Cunningham et al, 2002), augmented with named entity lists for locations, food types, restaurant names, and food subtypes (e.g. pizza), scraped from the we8there web pages. To this end, the GATE Gazetteer (Cunningham et al., 2002) was used, and only entities recognized by it automatically were considered. Our system is based on the GATE natural language processing framework (Cunningham et al, 2002) and it uses the ANNIE IE system included in the standard GATE distribution for text tokenization, sentence splitting and part-of-speech tagging. Similarly, we define additional features using the gazetteers from GATE, (Cunningham et al., 2002) namely, countries, person first/last names, trigger words;. An INIT is defined as a dated and located subject-verb-object triple, relying mostly on syntactical analyses from the MINIPAR parser (Lin, 1998) and linguistic annotations from the GATE information extraction engine (Cunningham et al., 2002). These patterns are implemented as regular expressions using the JAPE language (Cunningham et al, 2002). Similar mechanisms have also been proposed in other architectures to help heterogeneous linguistic modules to communicate through a common XML interface (see Cunningham et al,2002, Blache and Gunot, 2003). Some other systems are frameworks for performing generic tasks in one area of focus such as NLTK (Bird and Loper, 2004) and GATE (Cunningham et al, 2002) for Natural Language Processing; Pajek (Batagelj and Mrvar, 2003) and Guess (Adar, 2006) for Network Analysis and Visualization; and Lemur for Language Modeling and Information Retrieval.
In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy. For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques.  The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with back off (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al, 2003), and a Support Vector Machine (SVM) (Pradhan et al, 2003).  Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J.  the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition.
The research most similar to ours is the work of Diab and Resnik (2002). TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). TransCont is based on the WSD system SALAAM (Diab and Resnik, 2002), henceforth (DR02). The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al, 2001), and Seneval 3 English AW (SV3AW) data set. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. This assumption of similar meaning when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of Diab and Resnik (2002) which posits different word senses when a single English word maps onto different words in the foreign language. As mentioned in Section 1, the way that we extract paraphrases is the converse of the methodology employed in word sense disambiguation work that uses parallel corpora (Diab and Resnik, 2002). Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. In order to accomplish these steps, Bhattacharya et al (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages. They have been employed in word sense disambiguation (Diab and Resnik, 2002), automatic construction of bilingual dictionaries (McEwan et al, 2002), and inducing statistical machine translation models (Koehn et al., 2003). Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). We show that this improves on the results of Diab and Resnik (2002).
(Collins and Duffy 2002) describe the voted perceptron applied to the named-entity data in this paper, but using kernel-based features rather than the explicit features described in this paper. The vector is trained using the perceptron algorithm in combination with the averaging method to avoid over fitting; see Freund and Schapire (1999) and Collins and Duffy (2002) for details. This is essentially the syntactic tree kernel (STK) proposed in (Collins and Duffy, 2002) in which syntactic fragments from constituency trees can be matched even if they only differ in the leaf nodes (i.e. they have different surface forms).  Instead, the method employed by many rerankers following Collins and Duffy (2002) directly learn a scoring function that is trained to maximize performance on the reranking task. A viable alternative has been proposed in (Collins and Duffy, 2002), where convolution kernels were used to implicitly define a tree substructure space. Thepastc uses the tree kernel function defined in (Collins and Duffy, 2002). The tree kernel used in this article was proposed in (Collins and Duffy, 2002) for syntactic parsing reranking. Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures (Collins and Duffy, 2002). delta can be efficiently computed with the algorithm proposed in (Collins and Duffy, 2002). Collins (2000) and Collins and Duffy (2002) also succeed in finding algorithms for training discriminative models which balance tractability with effectiveness, showing improvements over a generative model. Given the semantic objects defined in the previous section, we design a convolution kernel in a way similar to the parse-tree kernel proposed in (Collins and Duffy, 2002).  It is worth noting that even if the above equations define a kernel function similar to the one proposed in (Collins and Duffy, 2002), the substructures on which it operates are different from the parse-tree kernel. For this purpose, kernel methods, and in particular tree kernels allow for representing trees in terms of all possible subtrees (Collins and Duffy, 2002). In contrast, tree kernels (Collins and Duffy, 2002) can be used to efficiently generate the huge space of tree fragments but, to generate the space of pairs of tree fragments, a new kernel function has to be defined.  In this perspective, String Kernel (SK) proposed in (Shawe Taylor and Cristianini, 2004) and the Syntactic Tree Kernel (STK) (Collins and Duffy, 2002) allow for modeling structured data in high dimensional spaces. delta function counts the number of subtrees rooted in n1 and n2 and can be evaluated as follows (Collins and Duffy, 2002). the reranker learns directly from a scoring function that is trained to maximize the performance of the reranking task (Collins and Duffy, 2002).
Riezler et al (2002) report on our WSJ parsing experiments. The limitation of deterministic transfer rules has been recognized in prior work (Riezler et al, 2002).  Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al, 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Noun coordination, NN-compound, Adj-Mod. They are still reasonably popular today, as exemplified by major systems like PARC's XLE (Riezler et al, 2002). See e.g. Riezler et al (2002) and Zhang et al (2007) for chart based parsers which can produce fragmentary analyses. In this project, a broad-coverage LFG grammar and parser for English was employed (see Riezler et al (2002)). Alternatively, a single input parse could be selected by stochastic models such as the one described in Riezler et al (2002). Riezler et al (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure. For sentences out of coverage, it employs the robustness techniques (fragment parsing, 'skimming') implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. Our error reduction of 51.0% also compares favorably to the 36% error reduction on English LFG parses reported in Riezler et al (2002). For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al., 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al. (2002)). We follow Collins' (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). This was done to some extent in Riezler et al (2002) to automatically generate training data for the log-linear disambiguation component of XLE. XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model (Riezler et al, 2002) that works on the packed representations. XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear (a.k.a. maximum-entropy) probability model (Riezler et al, 2002). For a more detailed description of the optimization problem and the feature-functions we use for stochastic LFG parsing see Riezler et al (2002). We have access to the entire English-language text of Wikipedia (about 2M pages) that was parsed using the XLE parser (Riezler et al, 2002), as well as an architecture for distributed data mining within this corpus, called Oceanography (Waterman, 2009). The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al, 2005. Following Pereira and Schabes' (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005).
The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. An alternate way to optimize weights over translation features is described in Och and Ney (2002). MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002).  Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002).  To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002).
In comparison, in (Yamada and Knight, 2002), which was a phrasal structure based statistical MT system for Chinese to English translation, the Bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102. Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. The model is further extended to incorporate phrasal translations performed at each node of the input parse tree (Yamada and Knight, 2002). While (Yamada and Knight, 2002) represent syntactical information in the decoding process through a series of transformation operations, we operate directly at the phrase level. The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al (1993) for translation by Yamada and Knight (2002). (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). This model is then decoded as described in (Yamada and Knight, 2002). For reasons of speed, Yamada and Knight (2002) limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus. One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002).
As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation.  BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi.
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). Following Clark et al (2002), evaluation is by precision and recall over dependencies.  The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery.
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002).    In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. Hockenmaier and Steedman (2002) saw a similar effect. The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. First, we review the dependency model proposed by Hockenmaier and Steedman (2002b).
A bootstrapping approach using machine learning is a possible alternative that will be explored in the future (Abney 2002). Abney (2002) suggests that the disagreement rate of two independent hypotheses upper-bounds the error rate of either hypothesis. In recent work, (Abney, 2002) shows that the independence assumption can be relaxed, and co-training is still effective under a weaker independence assumption. However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. Dasgupta et al (2001) and Abney (2002) conducted theoretical analyses on the performance (generalization error) of co-training. Abney (2002) refined Dasgupta et als result by relaxing the view independence assumption with a new constraint. Rather than comparing the two learners on whether they categorically select the same preferred parse on a number of examples, we can view active learning as the inverse of agreement-based co-training (Abney, 2002). Further avenues to explore include the development of selection methods to efficiently approximate maximizing the objective function of parser agreement on unlabeled data, following the work of Dasgupta et al (2002) and Abney (2002). We have implemented the Greedy Agreement Algorithm (Abney, 2002) which, based on two independent views of the data, is able to learn two binary classifiers from a set of hand-typed seed rules. See (Abney, 2002) for a formal proof that this algorithm tends to gradually reduce the classification error given the adequate seed rules. In fact, results are reported to be competitive against more sophisticated methods (Co-DL, Co Boost, etc.) for this specific task in (Abney, 2002). Third, how the algorithm, presented in (Abney, 2002) for binary classification, can be extended to a multi class problem. These results are comparable to the ones presented in (Abney, 2002), taking into account, apart from the language change, that we have introduced a fourth class to be treated the same as the other three. Theorem 5 in (Abney, 2002) provides a theoretical explanation for these results: if certain independence conditions between the classifier rules are satisfied and the precision of each rule is larger than a threshold T, then the precision of the final classifier is larger than T. Abney (2002) presents an analysis to relax the (fairly strong) conditional independence assumption to weak rule dependence. Although this result was previously observed in a different context by Abney in (Abney, 2002), he does not use it to derive a semi-supervised learning algorithm. Semantic Classifiers Bootstrapping refers to a problem of inducing a classifier given a small set of labeled data and a large set of unlabeled data (Abney, 2002). We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier. Abney (2002) argues that the conditional independence assumption is remarkably strong and is rarely satisfied in real data sets, showing that a weaker independence assumption suffices. Co-training algorithms such as CoBoost (Collins and Singer, 1999) and Greedy Agreement (Abney, 2002) that explicitly trade classifier agreement on unlabeled data against error on labeled data may be more robust to the underlying assumptions of co-training and can conceivably perform better than the Blum and Mitchell algorithm for problems without a natural feature split.
A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. Inspired by Marcu and Echihabi (2002), to construct relatively low noise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations: Contrast is a union of Antithesis, Concession, Otherwise and Contrast from RST. Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002).  Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). (Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. There are other efforts that attempt to extend the work of (Marcu and Echihabi, 2002). (Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. (Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. (Marcu and Echihabi 2002) proposed a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus. When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. We draw on and extend the work of Marcu and Echihabi (2002).
Recent work (Hwa et al, 2002) suggests that translational corresponence of linguistic structures can indeed be useful in projecting parses across languages. The dependency projection method DPA (Hwa et al, 2005) based on Direct Correspondence Assumption (Hwa et al, 2002) can be described as: if there is a pair of source words with a dependency relationship, the corresponding aligned words in target sentence can be considered as having the same dependency relationship equivalently. This idea was followed by (Hwa et al, 2002) who investigated English to Chinese projections based on the direct correspondence assumption. Previous work has primarily focused on the projection of grammatical (Yarowsky and Ngai, 2001) and syntactic information (Hwa et al, 2002). Analogously to Hwa et al (2002), we investigate whether there are indeed semantic correspondences between two languages, since there is little hope for projecting meaningful annotations in nonparallel semantic structures. Similarly to previous work (Hwa et al, 2002), we find that some mileage can be gained by assuming direct correspondence between two languages. We take as the starting point of annotation projection the direct correspondence assumption as formulated in (Hwa et al, 2002): for two sentences in parallel translation, the syntactic relationships in one language directly map the syntactic relationships in the other, and extend it to POS tags as well. Hwa et al (2002) have noticed that applying elementary linguistic transformations considerably increases precision and recall when projecting syntactic relations, at least for the English/Chinese language pair. In a different approach, Hwa et al (2002) aligned the parallel sentences using phrase based statistical MT models and then projected the alignments back to the parse trees. Many MT models implicitly make the so-called direct correspondence assumption (DCA) as defined in (Hwa et al, 2002). Our DS projection algorithm is similar to the projection algorithms described in (Hwa et al, 2002) and (Quirk et al, 2005). Following Hwa et al (2002), we looked at dependency links in the true English parses from the KTB where both the dependent and the head were linked to words on the Korean side using the intersection alignment. Since unary productions do not translate well from language to language (Hwa et al, 2002), we collapse them to their lower nodes. Fox (2002) has considered English and French, and Hwa et al (2002) investigate Chinese and English. Our hand-aligned test data were those used in Hwa et al (2002), and consisted of 48 sentence pairs also with less than 25 words in either language, for a total of 788 English words and 580 Chinese words. Hwa et al (2002) found that human translations from Chinese to English preserved only 39-42% of the unlabeled Chinese dependencies. Moreover, as stressed in previous research, using syntactic dependencies seems to be particularly well suited to coping with the problem of linguistic variation across languages (Hwa et al, 2002). A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages. These sets were the data used by Hwa et al (2002). IGT's unique structure — effectively each instance consists of a bitext between English and some target language — can be easily enriched through alignment and projection (e.g., (Yarowsky and Ngai, 2001), (Hwa et al., 2002)).
Web counts are frequently used to automatically re-rank candidate lists for various NLP tasks (Al-Onaizan and Knight, 2002). Specifically, (Al-Onaizan and Knight, 2002) uses transliteration to generate candidates and then web corpora to identify translations. (Al-Onaizan and Knight 2002) showed that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy.  A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Yaser Al-Onaizan (Al-Onaizan and Knight, 2002) transliterated an NE in Arabic into several candidates in English and ranked the candidates by comparing their counts in several English corpora. Al-Onaizan and Knight (2002) used Web statistics information to validate the translation candidates generated by language model, and obtained the accuracy of 72.6% in Arabic-English OOV word translation. Al-Onaizan and Knight (2002) describe a system which combines a phonetic based model with a spelling model for transliteration. For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. Al-Onaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters. Similarly, Al-Onaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. For example, the work of (Al Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used only the pronunciation or spelling of w in translation. Previous work on defining subtasks within statistical machine translation has been performed on, e.g., noun-noun pair (Cao and Li, 2002) and named entity translation (Al-Onaizan and Knight, 2002). Al-Onaizan and Knight (2002), Huang (2003) and Ji and Grishman (2007) investigated the general name entity translation problem, especially in the context of machine translation.
We also note that Turney (2002) found movie reviews to be the most difficult of several domains for sentiment classification, reporting an accuracy of 65.83% on a 120-document set (random-choice performance: 50%). Most of the authors traditionally use a classification-based approach for sentiment extraction and sentiment polarity detection (for example, Pang et al (2002), Turney (2002), Kim and Hovy (2004) and others), however, the research described in this paper uses the information retrieval (IR) paradigm which has also been used by some researchers. Following (Turney, 2002), Yuen et al (2004) investigate the association between polarity words and some strongly-polarized morphemes in Chinese, and present a method for inferring sentiment orientations of Chinese words. At phase level, Turney (2002) presents a technique for inferring the orientation and intensity of a phrase according to its PMI-IR statistical association with a set of strongly-polarized seed words. Based on (Hatzivassiloglou and Wiebe, 2000) and (Turney, 2002), we consider four types of structures (as shown in Table 5) during sentiment phrase extraction. Different from (Turney, 2002), we consider phrases with negations as their initial words. Turney (2002) described a way to automatically build such a lexicon based on looking at co-occurrences of words with other words whose sentiment is known. The problem of sentiment extraction at the document level (sentiment classification) has been tackled as a text categorization task in which the goal is to assign to a document either positive ("thumbs up") or negative ("thumbs down") polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003), Pang and Lee (2004)). Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. In addition to the IE tasks in the biomedical domain, negation scope learning has attracted increasing attention in some natural language processing (NLP) tasks, such as sentiment classification (Turney, 2002). Thus, the method we investigate can be seen as a combination of methods for propagating sentiment across lexical graphs and methods for building sentiment lexicons based on distributional characteristics of phrases in raw data (Turney, 2002). Others, such as Turney (2002), Pang and Vaithyanathan (2002), have examined the positive or negative polarity, rather than presence or absence, of affective content in text. Much of the work in sentiment analysis in the computational linguistics domain has focused either on short segments, such as sentences (Wilson et al, 2005), or on longer documents with an explicit polarity orientation like movie or product reviews (Turney, 2002). These methods range from manual approaches of developing domain-dependent lexicons (Das and Chan, 2001) to semi-automated approaches (Hu and Liu, 2004) and fully automated approaches (Turney, 2002). Therefore, the overall sentiment of a document is not necessarily the sum of the content parts (Turney, 2002). Also, PMI-IR is useful for calculating semantic orientation and rating reviews (Turney, 2002). In previous work, statistical NLP computation over large corpora has been a slow, off line process, as in KNOWITALL (Etzioni et al, 2005) and also in PMI-IR applications such as sentiment classification (Turney, 2002). (Turney, 2002) worked on product reviews. Like our class-attribute associations, the common-sense knowledge that the word cool is positive while unethical is negative can be learned from associations in web-scale data (Turney, 2002). Search counts or search results have also been used for sentiment analysis (Turney, 2002), for transliteration (Grefenstette et al, 2004), candidate selection in machine translation (Lapata and Keller, 2005), text similarity measurements (Sahami and Heilman, 2006), in correct parse tree filtering (Yates et al, 2006), and paraphrase evaluation (Fujita and Sato, 2008).
 Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002).
We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). (Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels.   The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a). However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead. To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007). For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features.  This approach has been used earlier by (Collins, 2002). This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). The detailed algorithm can be found in (Collins, 2002). Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features.   Collins (2002) includes a number of interesting contextual predicates for NER. Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002). Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b).
(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set.  We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. This confirms the results of Fleischman et al (2003): shallow methods may benefit significantly from the post-processing. In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003).
 Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). The benefit of semantic roles has already been demonstrated for a number of tasks, among others for machine translation (Boas, 2002), information extraction (Surdeanu et al, 2003), and question answering (Narayanan and Harabagiu, 2004). We use a semantic parser (described in (Surdeanu et al, 2003)) that recognizes predicate-argument structures. We have found that the identification of the association between a candidate answer and a question depends on (a) the recognition of predicates and entities based on both the output of a named entity recognizer and a semantic parser (Surdeanu et al, 2003) and their structuring into predicate-argument frames. However, it is a daunting task for people to find out information they are interested in from such a huge number of news tweets, thus motivating us to conduct some kind of information extraction such as event mining, where SRL plays a crucial role (Surdeanu et al, 2003). Surdeanu et al (2003) applied semantic parsing to capture the predicate-argument sentence structure. In the same line, some systems also use features of the content words of the argument, using the heuristics of Surdeanu et al (2003). Concerning lexicalization of the argument, most of the techniques rely on head word rules based on Collins, or content word rules as in Surdeanu et al (2003). The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al, (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003). They are a combination of features introduced by Gildea and Jurafsky (2002), ones proposed in Pradhan et al (2004), Surdeanu et al (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Figure 1: A comparison of frames for buy.v defined in PropBank and FrameNet (Moschitti et al, 2007), and information extraction (Surdeanu et al, 2003) . Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al, 2003). Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al 2003), and Machine Translation (Boas 2002). SCFs can be useful for many NLP applications, such as parsing (John Carroll and Briscoe, 1998) or information extraction (Surdeanu et al, 2003). Surdeanu et al (2003) employ predicate-argument structures for information extraction. Semantic role analysis has the potential of benefiting a wide spectrum of applications ranging from information extraction (Surdeanu et al, 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al, 2005). For instance, information extraction (Surdeanu et al, 2003), question answering (Narayanan and Harabagiu, 2004) and machine translation (Boas, 2002) could stand to benefit from broad coverage semantic processing. Our method first converts the extracted answers into a series of open-domain templates, which are based on predicate-argument frames (Surdeanu et al 2003).
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data. We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers.
For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s | s ∈ S, fj ∈ s} from each feature fj to support vector s ∈ S to only consider support vector s relevant to given x such that s Tx 6= 0.  Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features. PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears. PKE - Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003). Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger. We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. Second, we replace the YAMCHA (Kudo and Matsumoto, 2003) implementation of Support Vector Machines (SVMs) with SVMTool (Gimenez and Marquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy. We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. This result conforms to the results reported in (Kudo and Matsumoto, 2003).  In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space.  We use the following tools for syntactic processing: OpenNLP4 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for constituent chunking, and the MALT parser (Nivre et al, 2007) for dependency parsing.
Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. Table 1: An excerpt of the gold-standard verb classes for several verbs from Korhonen et al (2003). We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003).  By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes.  There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. Korhonen et al (2003) observed the opposite with general language data. This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003).
We used a bilingual corpus (Utiyama and Isahara, 2003) to examine which semantic frames of BFN contained LUs relevant to the Japanese verb osou. The bilingual corpus used for our experiments was obtained from an automatically sentence aligned Japanese/English Yomiuri newspaper corpus consisting of 180K sentence pairs (refer to Table1) (Utiyama and Isahara, 2003). Utiyama and Isahara (2003) extract Japanese-English parallel sentences from a noisy-parallel corpus. We used 1000 sentence pairs extracted from pre-aligned data (Utiyama and Isahara, 2003) as a gold standard. For example, Munteanu and Marcu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. For example, Zhao and Vogel (2002), Utiyama and Isahara (2003), and Munteanu and Marcu (2005) all acquire their comparable corpora from a collection of news articles which are either downloaded from the Web or archived by LDC. Works aimed at discovering parallel sentences include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus. The translations published on MNH are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003). These values of the parameter are determined using English sentences from Reuters articles (Utiyama and Isahara, 2003). Unlike other language pairs, the availability of Japanese-English parallel corpora is quite limited: the NTCIR patent corpus (Fujii et al, 2010) of 3 million sentence pairs (the latest NTCIR-8 version) for the patent domain and JENAAD corpus (Utiyama and Isahara, 2003) of 150k sentence pairs for the news domain. The earliest efforts in this direction are those of Zhao and Vogel (2002) and Utiyama and Isahara (2003). This database was an aggregate of several Japanese-English corpora, notably the Yomiuri newspaper corpus (Utiyama and Isahara, 2003) and the JST paper abstract corpus created at NICT (www.nict.go.jp) through (Utiyama and Isahara, 2007).
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003].  However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). We began with the tree-to-tree alignment model presented by Gildea (2003). (Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. (Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. (Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003).
The details of this algorithm are described in (Cherry and Lin, 2003). (Cherry and Lin, 2003) exploited such cohesion between the dependency structures to improve the quality of word alignment of parallel sentences. (Cherry and Lin, 2003) recently proposed a direct alignment formulation and state that it would be straightforward to estimate the parameters given a supervised alignment corpus. As in (Cherryand Lin, 2003), the above functions simplify the conditioning portion, h by utilizing only the words and context involved in the link li. The basic intuition behind this feature is that words inside prepositional phrases tend to align, which is similar to the dependency structure feature of (Cherry and Lin, 2003). This parser has been used in a much different alignment model (Cherry and Lin, 2003). Recently, researchers like Cherry and Lin (2003) have begun to use syntactic analyses to guide and restrict the word alignment process. Finally, our work is similar to that of Cherry and Lin (2003) in our use of the conditional probability of a link given the co-occurrence of the linked words.  In (Cherry and Lin, 2003) a probability model Pr (aJ1 | fJ1, eI1) is used, which is symmetric per definition. These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). Our model extends to phrase alignment the concept of a sentence pair generating a word alignment developed by Cherry and Lin (2003). Finally, inspired by these intuitive notions of translational correspondence, Cherry and Lin (2003) include dependency features in a word alignment model to improve non-syntactic baseline systems.  Cherry and Lin (2003) developed a statistical model to find word alignments, which allow easy integration of context-specific features. Cherry and Lin (2003) use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner. More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003). Most other researchers take either the HMM alignments (Liang et al, 2006) or IBM Model 4 alignments (Cherry and Lin, 2003) as input and perform post-processing, whereas our model is a potential replacement for the HMM and IBM Model 4.
Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing.  Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German: (Dubey and Keller, 2003) have trained a PCFG on the Negra corpus (Skut et al, 1998), reporting labelled precision and recall between 70 and 75%. This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance.    The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004).  able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004).
It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows: a reordering violates the ITG constraints if and only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as a subsequence. A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). (Zens and Ney, 2003) did comparative study over different reordering constraints. 6 compares our results to related work, in particular Zens and Ney (2003). Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al (2006). xITGs (Zens and Ney, 2003) in part solves this problem.  Zens and Ney (2003) used GIZA++ to word-align the Verbmobil task (English and German) and the Canadian Hansards task (English and French) and tested the coverage of ITGs and xITGs, i.e. the ratio of the number of alignment configurations that could be induced by the theories and the sentences in the two tasks. ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bi text data (Zens and Ney, 2003). For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). Some of the properties of these alignments are studied in (Zens and Ney, 2003). A comparison of both methods can be found in Zens and Ney (2003). A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. 
Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003).  We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set.  Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU.  We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6.
Consider the following example (see Figure 1 for an illustration): In recent work dealing with pronoun resolution in spoken dialogue (Strube and Muller, 2003), different types of expressions (noun phrases, verb phrases, whole utterances and disfluencies) had to be annotated.   Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e.g. rewriting rule sets.    Strube and Mu?ller (2003) propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data.      This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)).      
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below.  Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates.  The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003).    However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task.  These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution.   On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively.
Sudo et al (2003) acquired subtrees derived from dependency trees as extraction rules for IE in general domains.  The idea of a self-customizing IE system emerged recently with the improvement of pattern acquisition techniques (Sudo et al, 2003b), where the IE system customizes itself across domains given by the user's query. (Sudo et al, 2003a) consists of three phases to learn extraction patterns from the source documents for a scenario specified by the user. In other closely related work, Sudo et al (2003) use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain. Following (Sudo et al, 2003) we are interested only in the lexemes which are near neighbors of the most frequent verbs. Sudo et al (2003) evaluated how well their IE patterns captured named entities of three predefined types. In addition, Sudo et al (2003) proposed representations for IE patterns which extends the SVO representation used here and, while they did not appear to significantly improve IE, it is expected that it will be straightforward to extend the vector space model to those pattern representations. For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al (2003) allow any subpart of the tree to act as an extraction pattern. Sudo et al (2003) compared three models in terms of their ability to identify event participants.  Subtrees: The final model to be considered is the subtree model (Sudo et al, 2003). Sudo et al (2003) extract dependency subtrees within relevant documents as IE patterns.  The subtree model considers all subtrees as pattern candidates (Sudo et al, 2003).  For example, Sudo et al (2003) used patterns consisting of a path from a verb to any of its descendents (direct or indirect) while Bunescuand Mooney (2005) suggest the shortest path between the items being related. An additional advantage of linked chain patterns is that they do not cause an unwieldy number of candidate patterns to be generated unlike some other approaches for representing extraction patterns, such as the one proposed by Sudo et al (2003) where any subtree of the dependency tree can act as a potential pattern. Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodes as variables (Sudo et al, 2003). Three classes of syntactic template learning approaches are presented in the literature: learning of predicate argument templates (Yangarber et al, 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al, 2003).
The superiority of the unified approach has been demonstrated empirically in Gao et al (2003), and will also be discussed in Section 5. All feature functions in Figure 1, except the NW function, are derived from models presented in (Gao et al, 2003).  The class mode score we used can be written as generate Score ()= P (|) P ()## w w c c The P (C) and P (?#? C) is similar to the one defined by Gao et al (2003). A Chinese resume C=c1',c2',...,ck' is first tokenized into C= w1,w2,...,wk with a Chinese word segmentation system LSP (Gao et al., 2003). Then, we use a back-off schema (Katz, 1987) to deal with the data sparseness problem when estimating the probability P (L) (Gao et al, 2003). We selected SVMlight (Joachims, 1999) as the SVM classifier toolkit and LSP (Gao et al, 2003) for Chinese word segmentation and named entity identification. In Gao et al (2003), an approach based on source-channel model for Chinese word segmentation was proposed. The word segmentation system is developed based on a source-channel model similar to that described in (Gao et al, 2003). That is if we collect all words seen in the training data and store them into a lexicon, then each word in a test set is either a lexicon word or an OOV (out of vocabulary) word (Gao et al., 2003). In our experiments we identify SL (Chinese) NEs implicitly found by the word segmentation algorithm stated in Gao et al (2003), and the dictionaries for translating NEs include the same one used for QSL-TFIDF, and the LDC Chinese/English NE dictionary. The Chinese side of all corpora are segmented into words by our implementation of (Gao et al, 2003). Gao et al (2003) uses class-based language for word segmentation where some word category information can be incorporated. To identify entities, we use a CRF-based named entity tagger (Finkel et al, 2005) and a Chinese word breaker (Gao et al, 2003) for English and Chinese corpora, respectively.
This problem is being addressed through automatic knowledge acquisition methods, such as unsupervised learning for domain-specific lexicons (Lin et al, 2003) and extraction patterns (Yangarber, 2003), which require the user to provide only a small set of lexical items of the target classes or extraction patterns for the target domain. This is termed constraint-driven learning in (Chang et al., 2007), coupled learning in (Carlson et al, 2010) and counter-training in (Yangarber, 2003). Once extraction relations were obtained for a particular set of documents, the resulting set of relations were ranked according to a method proposed in (Yangarber, 2003). We begin by outlining the general process of learning extraction patterns using a semi-supervised algorithm, similar to one presented by Yangarber (2003).    We begin by outlining the general process of learning extraction patterns, similar to one presented by (Yangarber, 2003).   For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al (2003) allow any subpart of the tree to act as an extraction pattern. Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb object tuples from the dependency parse as extraction patterns.   Yangarber (2003) and Etzioni et al (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time.  The predicate-argument (SVO) model allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates (Yangarber,2003). Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. Yangarber et al (2000) and Yangarber (2003) present an algorithm that can find patterns automatically, but it requires an initial seed of manually designed patterns for each semantic relation.
The segmentation model is similar to the one presented by Lee et al (2003), and obtains an accuracy of about 98%. For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMMsegmenter (Lee et al, 2003). The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al, 2003). Lee et al (2003) demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation.  As in (Lee et al, 2003), we used unsupervised training data which is automatically segmented to discover previously unseen stems. context sensitive Arabic stemmer (Lee et al 2003) to overcome the morphological complexity of Arabic. To separate the Arabic white-space delimited words into segments, we use a segmentation model similar to the one presented by (Lee et al, 2003). We propose in the following an extension to the aforementioned FST model, where we jointly determines not only diacritics but segmentation into affixes as described in (Lee et al, 2003). An Arabicsegmenter similar to (Lee et al, 2003) provides the segmentation features. This produces a segmentation view of the arabic source words (Lee et al., 2003). In (Lee et al, 2003) a statistical approach for Arabic word segmentation was presented.  The algorithm is inspired with the work on the segmentation of Arabic words (Lee et al, 2003). Lee et al (2003) use a corpus of manually segmented words, which appears to be a subset of the first release of the ATB (110,000 words), and thus comparable to our training corpus. Lee et al (2003) show that the unsupervised use of the large corpus for stem identification increases accuracy. Lee et al (2003) addressed supervised word segmentation in Arabic and have some aspects similar to our approach. As estimated by (Lee et al, 2003), we set the probability of ?u/k? to be 1E? 9. We found that the value proposed by (Lee et al, 2003) for Arabic gives good results also for Hebrew. Moving on to Arabic, Lee et al (2003) describe a word segmentation system for Arabic that uses an n gram language model over morphemes.
We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging. At present, the Stanford Parser (Klein and Manning, 2003) is used.  Duringthe NER extraction, we also employ phrase analysis based on our phrase utility extraction method using Standford dependency parser ((Klein and Manning, 2003)). The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. We follow a standard procedure to extract statements, as similarly adopted by Nakashole et al (2012), using Stanford CoreNLP (Klein and Manning, 2003) to lemmatize and parse sentences. We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. running), or whether the sentence is passive. We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material.    To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1 best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003).  Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003). In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford's (Klein and Manning, 2003) constituent parsers. 
Because of these characteristics, Chinese has a rather different set of salient ambiguities from the perspective of statistical parsing (Levy and Manning, 2003). We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter. To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the devset and test set. In this work, we use the Stanford Parser (Levy and Manning 2003). Levy and Manning (2003) used a factored model that combines an unlexicalized PCFG model with a dependency model. While it is uncommon to offer an error analysis for probabilistic parsing, Levy and Manning (2003) argue that a careful error classification can reveal possible improvements. Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. The closest previous work is the detailed manual analysis performed by Levy and Manning (2003).   See Levy and Manning (2003) for a similar discussion of Chinese and the Penn Chinese Treebank. We use the SVM-Light Toolkit (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser (Levy and Manning, 2003) as the parser and the constituent-to-dependency converter. Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology (Levy and Manning, 2003). It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). Levy and Manning (2003) established that properties of Chinese such as noun/verb ambiguity contribute to the difficulty of Chinese parsing. greatly affects parsing accuracy (Levy and Manning, 2003). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al, 2009). Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. As pointed out in (Levyand Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese. First of all, we adopt the head finding rules for Chinese used in (Levy and Manning, 2003), and this affects sieve 4, 6 and 7 which are all take advantage of the head words.
To tackle this problem, in one of our recent work (Ng et al, 2003), we had gathered training data from parallel texts and obtained encouraging results in our evaluation on the nouns of SENSEVAL-2 English lexical sample task (Kilgarriff, 2001). To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software. There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. For the translation of ambiguous English words Ng et al (2003) made use of the fact that the various senses are often translated differently. Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. Similarly, (Ng et al, 2003) report a research study which uses an English-Chinese parallel corpus in order to extract sense-tagged training data.  Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. For example, Ng et al (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al, 2003). For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. Ng et al (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic. To gather examples from these parallel corpora, we followed the approach in (Ng et al, 2003). For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003). As described in (Ng et al, 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory.
Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it.   The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003). It is typically applicable in the text generation field, both for concept-to-text generation and text-to text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005).   In contrast, the greedy algorithm of Lapata (2003) makes grave search errors. The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. Lapata (2003) has suggested a probabilistic model of text structuring and its application to the sentence ordering. Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003) directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies). Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences.
We use an automatic topic segmentation tool, LCSeg (Galley et al, 2003) setting parameters so that the derived segments are of the approximate desired length.  The gold standard for thematic segmentations has been kindly provided by (Galley et al., 2003) and has been chosen by considering the agreement between at least three human annotations. We evaluated WLM's performance on the ICSI meeting corpus (Janin et al 2003) by comparing our segmentation results to the results obtained by implementing LCSeg (Galley et al, 2003). For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al, 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. Previous work has shown that training a segmentation model with features that are extracted from knowledge sources other than words, such as speaker interaction (e.g., overlap rate, pause, and speaker change) (Galley et al, 2003), or participant behaviors, e.g., note taking cues (Banerjee and Rudnicky, 2006), can outperform LCSEG on similar tasks. Adapting the standard definition of topic (Galley et al, 2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or express their opinions.  Moving to the task of segmenting dialogs, (Galley et al, 2003) first proposed the lexical chain based unsupervised segmenter (LCSeg) and a supervised segmenter for segmenting meeting transcripts. For the topic level, they achieve similar results as (Galley et al, 2003), with the supervised approach outperforming LCSeg.  Our second model is the lexical chain based segmenter LCSeg, (Galley et al, 2003). However, Galley et al, (Galley et al, 2003) uses only repetition relation as previous research results (e.g., (Choi, 2000)) account only for repetition. The first dataset is a subset of the ICSI-MR corpus (Janin et al, 2004), where the gold standard for thematic segmentations has been provided by taking into account the agreement of at least three human annotators (Galley et al, 2003). The LCseg system (Galley et al, 2003), labeled here as G03, is to our knowledge the only word distribution based system evaluated on ICSI meeting data. Therefore, we replicate the results reported by (Galley et al, 2003) when evaluation of LCseg was done on ICSI data. The so-labeled G03* algorithm indicates the error rates obtained by (Galley et al, 2003) when extra (meeting specific) features have been adopted in a decision tree classifier. The work of (Galley et al, 2003) shows that the G03* algorithm is better than G03 by approximately 10%, which indicates that on meeting data the performance of our word-distribution based approach could possibly be increased by using other meeting-specific features. Our feature set incorporates information which has proven useful in meeting segmentation (Galley et al, 2003) and the task of detecting addressees of a specific utterance in a meeting (Jovanovic et al, 2006). 
Izumi et al (2003) and (2004) used error annotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions. For example, (Izumi et al, 2003) reported error rates for English prepositions that were as high as 10% in a Japanese learner corpus. (Izumi et al., 2003) and (Izumi et al, 2004) used an ME approach to classify different grammatical errors in transcripts of Japanese interviews. For example, in the Japanese Learners of English corpus (Izumi et al., 2003), errors related to verbs are among the most frequent categories. A maximum entropy model, using lexical and POS features, is trained in (Izumi et al, 2003) to recognize a variety of errors. Izumi et al (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. False starts and disfluencies were then cleaned up, and grammatical mistakes tagged (Izumi et al, 2003). The usage of articles has been found to be the most frequent error class in the JLE corpus (Izumi et al, 2003). In the future, we would like to search for more salient features through a careful study of non-native errors, using error-tagged corpora such as (Izumi et al., 2003).  We based our error annotation scheme on that used in the NICT JLE corpus (Izumi et al, 2003a), whose detailed description is readily available, for example, in Izumi et al (2005).  The method (Izumi et al, 2003) aims to detect omission-type and replacement-type errors and transformation-based leaning is employed in (Shi and Zhou, 2005) to learn rules to detect errors for speech recognition outputs. Izumi et al (2003) train a maximum entropy model on error-tagged data from the Japanese Learners of English corpus (JLE, (Izumi et al., 2004)) to detect 8 error types in the same corpus.
(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. (Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003).   A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003).  We use dynamic programming for parsing under this finite model (Eisner, 2003). Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining.
These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004].  These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001], where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004], where punctuation is ignored in both the training and testing data of the Switchboard corpus. When compared with the latest results from [Johnson and Charniak 2004], where no punctuations are used for either training or testing data, we also observe the same trend of the improved results. Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. Following Johnson and Charniak (2004), we use a noisy channel model to propose a 25-best list of possible speech disfluency analyses. Further details of the noisy channel model can be found in Johnson and Charniak (2004). To improve performance over the standard noisy channel model we use a re-ranker, as previously suggest by Johnson and Charniak (2004). As Johnson and Charniak (2004) noted, although this model performs well, a log linear re-ranker can be used to increase performance.  In this work, we use a total of 62 variables, which include 16 variables from Charniak and Johnson (2001) and Johnson and Charniak (2004), an additional 29 variables from Zhang and Weng (2005), 11 hierarchical POS tag variables, and 8 prosody variables (labels and their confidence scores). Because the edit region identification results on the original Switchboard are not directly comparable with the results on the newly segmented data, the state-of-art results reported by Charniak and Johnson (2001) and Johnson and Charniak (2004) are repeated on this new corpus by Kahn et al (2005). Speech is often disfluent, and speech repairs are known to repeat large portions of the preceding context (Johnson and Charniak, 2004). The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the filessw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak (2004). The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations. The prior probability distributions over alignment operations is estimated from data in the Switchboard in a similar manner to Johnson and Charniak (2004). Given that state-of-the-art edit detection performs at about 80% f-measure (Johnson and Charniak, 2004), much of the benefit derived here from oracle repair detection should be realizable in practice. The Johnson and Charniak (2004) approach, referred to in this document as JC04, combines the noisy channel paradigm with a tree-adjoining grammar (TAG) to capture approximately repeated elements. The output of the JC04 model (Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments. The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al, 1992) data as described in (Johnson and Charniak, 2004).
For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. There certainly exist competitive parsers that internally represent lexical items as real-valued vectors, such as the neural network-based parser of Henderson (2004), and even parsers which use pre-trained word embeddings to represent the lexicon, such as Socher et al. First, these parsers are among the best in the literature, with a test performance of 90.7 F1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. For example, the discriminative training techniques successfully applied in (Henderson, 2004) to the feed-forward neural network model can be directly applied to the mean field model proposed in this paper. Other techniques are also possible; Henderson (2004) uses neural networks to induce latent left-corner parser states.   Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al, 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. Also, as with any generative model, it may be easy to improve the parser's accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even with out introduction of any additional linguistic features. We would expect further improvement of ISBN results if we applied discriminative retraining (Henderson, 2004) or reranking with data-defined kernels (Henderson and Titov, 2005), even without introduction of any additional features. Henderson (2004) finds that discriminative training was too slow, and reports accuracy higher than generative models by discriminatively reranking the output of his generative model.  Also, as with any generative model, it should be easy to improve the parser's accuracy with discriminative reranking, such as discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without the introduction of any additional linguistic features. Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. The parser used in this paper is described in Clark and Curran (2004b).  In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery.  In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004).  Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. Clark and Curran (2004b) describes the CCG parser.   In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model.  We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. 
  We used an early update version of averaged perceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. It is possible to prove that, provided the training set (xi ,zi) is separable with margin > 0, the algorithm is assured to converge after a finite number of iterations to a model with zero training errors (Collins and Roark, 2004). Although we have not discussed it to this point, (Collins and Roark, 2004) present a perceptron algorithm for use with the Roark architecture. Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model. Shen et al (2007) have further shown that better results (97.3% accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. Early update was introduced by Collins and Roark (2004) for incremental parsing and adopted to forest re-ranking by Wang and Zong (2011). We also show, in Section 3.3, how perceptron training with early update (Collins and Roark, 2004) can be used in this setting. The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam.  Here, it might be useful to relax the strict linear control regime by exploring beam search strategies, e.g. along the lines of Collins and Roark (2004). We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the gold standard state item falls off the agenda.  Strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item. Collins and Roark (2004) proposed the early-update idea, and Huang et al (2012) later proved its convergence and formalized a general framework which includes it as a special case. This section gives a description of Collins and Roark's incremental parser (Collins and Roark, 2004) and discusses its problem.  The best results of Collins and Roark (2004) (LR=88.4%, LP=89.1% and F=88.8%) are achieved when the parser utilizes the information about the final punctuation and the look-ahead. The early-update strategy of Collins and Roark (2004) is used so as to improve accuracy and speed up the training.
Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. Details of this process can be found in Luo et al (2004).  Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together.    Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b).  Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). Distances have been used in e.g. Luo et al (2004).  Luo et al (2004) perform the clustering step within a Bell tree representation. They report considerable improvements over state-of the-art systems including Luo et al (2004).
(Li et al 2004) introduced the joint transliteration model whose variant augmented with adaptive re-ranking we used in our experiments.  Li et al (2004) presented a framework allowing direct orthographical mapping of transliteration units be tween English and Chinese, and an extended model is presented in Ekbal et al (2006). One Chinese Pinyin string can correspond to several Chinese characters (Li et al, 2004). For this reason, it has been reported that English-to-Chinese transliteration without Chinese phonemes outperforms that with Chinese phonemes (Li et al, 2004). We performed alignment between E G and E P and between E P and C P in a similar manner presented in Li et al (2004).  We used the same test set used in Li et al (2004) for our testing. (Xinhua News Agency, 1992), which includes names in English, French, German, and many other foreign languages (Li et al., 2004). Table 6 represents the overall performance of one system in a previous work (Li et al, 2004) and eighteen systems based on the transliteration models defined in this paper. To compare Li et al (2004) and transliteration models defined in this paper under the same condition, we also carried out experiments with the same training data in Li et al (2004). Since the training data used in Li et al (2004) is identical as the union of our training and development data, we denoted it as TRAIN+DEV in Table 6. The grapheme-based approach, also known as direct orthographical mapping (Li et al, 2004), which treats transliteration as a statistical machine translation problem under monotonic constraints, has also achieved promising results. Other transliteration systems focus on alignment for transliteration, for example the joint source channel model suggested by Li et al (2004). The grapheme-based approach, which treats transliteration as statistical machine translation problem under monotonic constraint, aims to obtain a direct orthographical mapping (DOM) to reduce possible errors introduced in multiple conversions. Phoneme-based approaches are usually not good enough, because name entities have various etymological origins and transliterations are not always decided by pronunciations (Li et al, 2004). Li et al (2004) propose a letter-to-letter n-gram transliteration model for Chinese-English transliteration in an attempt to allow for the encoding of more contextual information. Many transliterated words are proper names, whose pronunciation rules may vary depending on the language of origin (Li et al, 2004). So grapheme-based (Li et al, 2004) approach has gained lots of attention recently. Direct orthographic mapping (e.g. Li et al, 2004), making use of individual Chinese graphemes, tends to overcome the problem and model the character choice directly.
Moreover, Ng et al (2006) examine the FS of the weighted log-likelihood ratio (WLLR) on the movie review dataset and achieves an accuracy of 87.1%, which is higher than the result reported by Pang and Lee (2004) with the same dataset. In sentiment text classification, we also use two data sets: one is the widely used Cornell movie-review dataset (Pang and Lee, 2004) and one dataset from product reviews of domain DVD (Blitzer et al, 2007). For our experiments, we employ a large, recently introduced IMDB movie review dataset (Maas et al, 2011), in place of the smaller dataset introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. Pang and Lee (2004) use a graph-based technique to identify and analyze only subjective parts of texts. Graph based SSL learning has been successfully applied to opinion detection (Pang and Lee, 2004) but is not appropriate for dealing with large scale data sets. One of the standard data sets in opinion detection is the movie review data set created by Pang and Lee (2004). We also cannot use prior graph construction methods for the document level (such as physical proximity of sentences, used in Pang and Lee (2004)) at the word sense level. This is because certain parts-of-speech have been found to be better indicators of sentiment (Pang and Lee, 2004). Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents.  Sentence-level subjectivity detection, where training data is easier to obtain than for positive vs. negative classification, has been successfully performed using supervised statistical methods alone (Pang and Lee, 2004) or in combination with a knowledge based approach (Riloff et al, 2006). We build two classifiers based on the work of Pang and Lee (2004) to measure the polarity and objectivity of article edits. Sentiment analysis can be dependently or independently done from subjectivity detection, although Pang and Lee (2004) state that subjectivity detection performed prior to the sentiment analysis leads to better results in the latter.  In fact, it has already been established that sentence level classification can improve document level analysis (Pang and Lee, 2004). Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee (2004). For instance, in Pang and Lee (2004), yd would be the polarity of the document and ysi would indicate whether sentence si is subjective or objective. The local dependencies between sentiment labels on sentences is similar to the work of Pang and Lee (2004) where soft local consistency constraints were created between every sentence in a document and inference was solved using a min-cut algorithm. Alternatively, decisions from the sentence classifier can guide which input is seen by the document level classifier (Pang and Lee, 2004). On the other hand, we associate sentiment polarity to a document on the whole as opposed to Pang and Lee (2004) which deals with sentiment prediction of subjectivity content only.
The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al, 2004). Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available. The method is described in (McCarthy et al, 2004), which we summarise here.  McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD. Previous research in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction. McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus. Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn. In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense.  McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997). In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004). We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each senses' Sx, following the approach of McCarthy et al (2004). As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004).  This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004). More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009). In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004)). It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004). The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems.
 For this experiment, we choose the C&C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG.  The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual Lexical Functional Grammar (LFG) (Bresnan, 2001) resources from tree banks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al, 2004). The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al, 2004) uses configurational, categorial, function tag and trace information. A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II tree bank has been described by Cahill et al (2004). Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al (2004), which assigns grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head. Cahill et al (2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups: a group of predicate-only dependencies and non predicate dependencies. The translation and reference files are analyzed by a tree bank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al, 2004), which produces a set of dependency triples for each input. Cahill et al (2004) presents Penn-II Treebank based LFG parsing resources. In this paper, we use the parser developed by Cahill et al (2004), which automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates. We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al, 2004) automatically extracted from tree banks, maximising the probability of a tree given an f-structure. In this paper we present a novel PCFG-based architecture for probabilistic generation based onwide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from tree banks (Cahill et al, 2004). Cahill et al (2004) present two parsing architectures: the pipeline and the integrated parsing architecture. The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al (2004).   This conditioning effectively turns the f-structure annotated PCFGs of Cahill et al (2004) into probabilistic generation grammars. Our back off uses the built-in lexical macros of the automatic f-structure annotation algorithm of Cahill et al (2004) to identify potential part-of-speech categories corresponding to a particular set of features. The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahill et al (2004).
In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004).  We used support vector machines (Vapnik, 1995) with (a) polynomial kernels to learn the semantic role classification and (b) Tree Kernels (Moschitti, 2004) for learning both frame and ILC classification.  The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004).  As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure.  For learning, the SVM-Light software (Joachims, 1999) was employed with the convolution tree kernel implemented by Moschitti (2004). In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). A preliminary study on the benefit of such kernels was measured on the classification accuracy of semantic arguments in (Moschitti, 2004). The convolution kernel that we have experimented was devised in (Moschitti, 2004) and is characterized by two aspects: the semantic space of the subcategorization structures and the kernel function that measure their similarities. The evaluations were carried out with the SVMlight-TK software (Moschitti, 2004) available at http: //ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999). Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling. In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). Moschitti (2004) and Che et al (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar driven one.
Hasegawa, et al put forward an unsupervised approach for relation extraction from large text corpora (Hasegawa et al, 2004). For Hasegawa's method (Hasegawa et al, 2004), we set the cluster number to be identical with the number of ground truth classes.   One approach for Open IE is based on clustering of entity pairs to produce relations, as introduced by Hasegawa et al (Hasegawa et al, 2004). Fully unsupervised Open IE systems are mainly based on clustering of entity pair contexts to produce clusters of entity pairs that share the same relations, as introduced by Hasegawa et al (Hasegawa et al, 2004).   Hasegawa et al (2004) performs unsupervised hierarchical clustering over a simple set of features. Unfortunately, this number is often unavailable in in formation extraction tasks in general (Hasegawa et al, 2004), and attribute extraction in particular. (Hasegawa et al 2004) used large corpora and an Extended Named Entity tagger to find novel relations and their participants. Some existing studies use corpus-based statistics for relation extraction (Hasegawa et al, 2004). Hasegawa et al (2004) described a paraphrase discovery approach based on clustering concurrent name pairs. Compared with supervised and semi-supervised methods, Hasegawa et al (2004)'s unsupervised approach for relation extraction can overcome the difficulties on requirement of a large amount of labeled data and enumeration of all class labels. Hasegawa et al (2004)'s method is to use a hierarchical clustering method to cluster pairs of named entities according to the similarity of context words intervening between the named entities. It also does not need to pre-define the number of the context clusters or pre-specify the similarity threshold for the clusters as Hasegawa et al (2004)'s method.  In (Hasegawa et al, 2004), they preformed unsupervised relation extraction based on hierarchical clustering and they only used word features between entity mention pairs to construct context vectors. We reported the clustering results using the same clustering strategy as Hasegawa et al (2004) proposed. In Table 5, Hasegawa's Method1 means the test used the word feature as Hasegawa et al (2004) while Hasegawa's Method2 means the test used the same feature set as our method.
More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high precision but very low recall.  Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis.   Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction (Zelenko et al, 2003), named entity recognition (Culotta and Sorensen, 2004), semantic role labeling (Moschitti et al, 2008) and so on. For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. Table 5 summarizes the results of a comparison between the latent topic feature and the features used by (Culotta and Sorensen, 2004).  The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)).  Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts.  This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005).
We also provide the results from Bunescu and Mooney (2004) for comparison. In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task. The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes. Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005).  We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs.  Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al, 2003), and Relational Markov Networks (Bunescu and Mooney, 2004).
 However, previous computational models of grammar induction (Klein and Manning, 2004), including infant grammar induction (Kwiatkowski et al, 2012), have not addressed filler-gap comprehension. In the field of language acquisition computational linguists such as Klein and Manning (2004) have studied the unsupervised acquisition of syntactic structure, while linguists such as Boersma and Hayes (2001), Gold smith (2001), Pater (2008) and Albright and Hayes (2003) are developing probabilistic models of the acquisition of phonology and/or morphology, and Frank et al (2007) experimentally tests the predictions of a Bayesian model of lexical acquisition.  We use the standard generative Dependency Model with Valence (Klein and Manning, 2004). As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004). We follow an approach similar to the widely-referenced DMV model (Klein and Manning, 2004), which forms the basis of the current state-of-the-art unsupervised grammar induction model (Headden III et al, 2009). We encode more detailed valence information than Klein and Manning (2004) and condition child generation on parent valence. The resulting simplified model closely resembles DMV (Klein and Manning, 2004), except that it 1) explicitly generate words x rather than only part of-speech tags s, 2) encodes richer context and valence information, and 3) imposes a Dirichlet prior on the symbol distribution.    Finally, following (Klein and Manning, 2004) we strip out punctuation from the sentences. The maximum unsupervised accuracy it achieved on the Bulgarian data is 47.6% with initialization from Klein and Manning (2004) and this result is not stable. In the generative models of section 5, f has the form of a dependency model with valence (Klein and Manning, 2004). Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well. While these results are worse than those obtained previously for this model, the experiments in Klein and Manning (2004) only used sentences of 10 words or fewer, without punctuation, and with gold-standard tags. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). Fortunately, the state of the art in broad-coverage (Lin, 1993) and unsupervised (Klein and Manning, 2004) dependency parsing allows us to treat dependency parsing merely as a preprocessing step. In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al, 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated.
  Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence.  Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2.  (Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004).     Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance.  Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010).
It is only recently employed in NER (Shen et al, 2004). This issue was previously addressed in Shen et al (2004) in the context of named-entity recognition, where they used a two-step procedure to first select the most informative and representative samples, followed by a diversity filter. In a more recent study, Shen et al (2004) consider AL for entity recognition based on Support Vector Machines.  Given the variety of methods that are available for generating training data efficiently automatically using extant domain resources (Morgan et al, 2004) or semi-automatically (active learning approaches like Shen et al (2004) or systems using seed rules such as Mikheev et al. Active learning, which has been applied to the problem of NER in (Shen et al, 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive. Diversity measures as proposed by (Shen et al, 2004) might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER. It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al, 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. In order to circumvent this obstacle several approaches have been presented, among them active learning (Shen et al, 2004) and rule-based systems encoding domain specific knowledge (Gaizauskas et al, 2003). Shen et al (2004) combine multiple criteria to measure the informativeness, representativeness, and diversity of examples in active learning for named entity recognition. Therefore, in order to avoid recursion and over-complexity, we employ a diversity-motivated intra-stratum sampling scheme (Shen et al, 2004), called KDN (K-diverse neighbors), which aims to maximize the training utility of all seeds from a stratum. (Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al, 2005a) used self-training and co-training to detect and classify named entities in news domain, (Shen et al, 2004) conducted experiments with multi-criteria-based active learning for biomedical NER.  Shen et al (2004) proposed an approach to selecting examples based on informativeness, representativeness and diversity criteria.
 The other metric is ROUGE (Lin and Och, 2004), here named R. Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005).  Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation.      Stemming is enabled (Lin and Och, 2004a). The optimal set is: { METEOR wnsyn, ROUGE w 1.2} which includes variants of METEOR, and ROUGE (Lin and Och, 2004). Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a).   Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams.
  For example, requiring l not 0 and, if k not 0 then sk must be a child of sl in the source tree, we can implement a synchronous dependency grammar similar to (Melamed, 2004). Melamed (2004) establishes a theoretical framework for generalized synchronous parsing and translation. When a parser's grammar can have fewer dimensions than the parser's input, we call it a synchronizer (Melamed, 2004). Our synchronous parser is similar to the synchronous CKY parser presented at (Melamed, 2004). Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). Melamed (2004) formalizes machine translation problem as synchronous parsing based on multi text grammars.  To handle syntactic differences, Melamed (2004) proposes methods based on tree-to-tree mappings. A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and non isomorphic tree-to-tree mappings (Eisner, 2003). Melamed (2004) also used a similar way to integrate the language model. 
Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). Using them, Galley et al (2004) report an 8% increase in speaker identification. Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005).   The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). 
They use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004). Supervised learning method using syntactic and word-based features, the path of the pairs of NEs in the parse tree and the word n gram between pairs of NEs (Kambhatla, 2004). The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al, 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances. We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004). Kambhatla (2004) took a similar approach but used multivariate logistic regression (Kambhatla, 2004).  Kambhatla (2004) developed a method for extracting relations by applying Maximum Entropy models to combine lexical, syntactic and semantic features and report that they obtain improvement in results when they combine variety of features.  Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the smallest parse fragment connecting the two mentions, etc. For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods.  Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. (Culotta and Sorensen, 2004) extended this work to estimate kernel functions between augmented dependency trees, while (Kambhatla, 2004) combined lexical features, syntactic features, and semantic features in a maximum entropy model. However, the semantic features discussed in (Kambhatla, 2004) still focus on the word level instead of the conceptual level. Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction.  Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner.  Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks.  For a more complete description, see (Ando and Zhang, 2005a).  An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve.  Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:. This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems.     To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). We follow Ando and Zhang (2005a) and use the modified Huber loss. For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time.
Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours.  These scores are the same as the variational rule scores of Matsuzaki et al (2005).  Matsuzaki et al (2005) introduced a model for such learning: PCFG-LA.  Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],. Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005). Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data. Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find.  'Basic' models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank. With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing. We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc.   These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005). The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions. Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework. The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005).
Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy.  In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005).  By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005).  We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures.  The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005).   In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005).
The recent advances in parsing have achieved parsers with O(n3) time complexity without the grammar constant (McDonald et al, 2005).  A dependency-based system using MST Parser (McDonald et al, 2005). This may be partially compensated for by including features about the surrounding words (McDonald et al., 2005), but any feature templates which would be identical across the two contexts will be in tension.    Many of the features above were introduced in McDonald et al (2005a); specifically, the node type, inside, and edge features. In turn, those features were inspired by successful previous work in first order dependency parsing (McDonald et al, 2005). Although (McDonald et al, 2005) used the prefix of each word form instead of word form itself as features, character-level features here for Chinese is essentially different from that.  The latest state-of-the-art statistical dependency parsers are discriminative, meaning that they are based on classifiers trained to score trees, given a sentence, either via factored whole-structure scores (McDonald et al, 2005a) or local parsing decision scores (Hall et al, 2006). The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al (2005a).   We compare conditional training of a non projective edge-factored parsing model to the online MIRA training used by McDonald et al (2005b). Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000)). We used CoNLL 03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al, 1994) converted to dependency trees for DEPAR (McDonald et al, 2005). L2PA is also known as a loss augmented variant of one best MIRA, well-known in DEPAR (McDonald et al, 2005). 
Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005). Sagae and Tsujii (2007)'s dependency parser, based on a probabilistic shift-reduce algorithm extended by the pseudo-projective parsing technique (Nivre and Nilsson, 2005). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005). It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005). To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which non projective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the non projective links at parse time. Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective. The most popular strategy for capturing non projective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Novak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006). We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. However, just as it has been noted that most non-projective structures appearing in practice are only 'slightly' non projective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in tree banks can be viewed as being only 'slightly' ill-nested. In order to avoid losing the benefits of higher-order parsing, we considered applying pseudo-projective transformation (Nivre and Nilsson, 2005). Pseudo-projective parsing for recovering non projective structures (Nivre and Nilsson, 2005). Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4). Pseudo-projective parsing was proposed by Nivreand Nilsson (2005) as a way of dealing with non projective structures in a projective data-driven parser. We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label r-h, where r is the original label and h is the label of the original head in the non-projective dependency graph. For tree banks with non-projective trees we use the pseudo-projective parsing technique to transform the tree bank into projective structures (Nivre and Nilsson, 2005). Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases. It should be noted that the proportion of lost dependencies is about twice as high as the proportion of dependencies that are non-projective in themselves (Nivre and Nilsson, 2005). The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005).
This can help explain why, as Pang and Lee (2005) note, one person's four-star review is another's two-star.  We use a sentiment-annotated data set consisting of movie reviews by (Pang and Lee, 2005) and tweets from http: //help.sentiment140.com/for-students. There is a huge body of work on OM in movie reviews which was sparked by the dataset from Pang and Lee (2005). Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews.  In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al, 2005). The optimal POS bi-tags have been derived experimentally by using top 10% features on information gain based-pruning classifier on polarity dataset by (Pang and Lee, 2005). Cascaded models for sentiment classification were studied by (Pang and Lee, 2005). This indicates there is some difficulty distinguishing between the fine-grained categories we specified, but high agreement at a coarser level, which advocates using a ranking approach for evaluation (see also Pang and Lee 2005).  Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. Machine learning techniques have been proposed for sentiment classification (Pang et al., 2002; Mullen and Collier, 2004) based on annotated samples from experts, but they have limited performance especially when estimating ratings of a multi-point scale (Pang and Lee, 2005). We then adopt the machine learning method proposed in (Pangand Lee, 2005) and the Bayesian Network classifier (Russell and Norvig, 2002) for feature rating estimation. In 2005, Pang and Lee extended their earlier work in (Pang and Lee, 2004) to determine a reviewer's evaluation with respect to multi scales (Pang and Lee, 2005). We adopt the approach of Pangand Lee (Pang and Lee, 2005) described in Section 2 for feature rating estimation. RT-s: Short movie reviews dataset containing one sentence per review (Pang and Lee, 2005). We might attempt to exploit these dependencies in a manner similar to Pang and Lee (2005) to improve three-way classification. Data: The dataset consists of snippets from Rotten Tomatoes (Pang and Lee, 2005). This dataset was created and used by Pang and Lee (2005) to train a classifier for identifying positive sentences in a full length review.
Most recently, (Takamura et al, 2005) reports on the use of spin models to infer the semantic orientation of words. Hashimoto et al's method constructs a network of templates based on their co-occurrence in web sentences with a small number of polarity-assigned seed templates and infers the polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et al, 2005). Semantic Orientation Lexicon (Takamura et al, 2005): We used the words listed as having positive or negative polarity to produce +/- states, when they occur with the designated part-of-speech. The proposed method in (Takamura et al, 2005) extracts semantic orientations from a small number of seed words with high accuracy in the experiments on English as well as Japanese lexicons. Takamura et al (2005) extracted semantic orientations of words. In more recent work, Takamura et al (2005) used the spin model to extract word semantic orientation. Semantic Orientation Lexicon (Takamura et al., 2005): The positive/negative words in this list are assigned +/- tags, when they occur with the designated part-of-speech. The syntactic network is defined in a way similar to previous work such the Spin Model (Takamura et al, 2005) and Latent Semantic Analysis to compute the association strength with seed words (Turney and Litman, 2003). Takamura et al (2005) proposed using spin models for extracting semantic orientation of words.  Takamura et al built lexical network from not only such co-occurrence but other resources including thesaurus (Takamura et al, 2005). Takamura et al (2005) used the spin model to extract word semantic orientation. Takamura et al (2005) used the Ising model to extract semantic orientations of words (not phrases).   Takamura et al (2005) determine term orientation (for Japanese) according to a spin model, i.e. a physical model of a set of electrons each endowed with one between two possible spin directions, and where electrons propagate their spin direction to neighbouring electrons until the system reaches a stable configuration. Previous work on identifying the semantic orientation of words has addressed the problem as both a semi-supervised (Takamura et al, 2005) and an unsupervised (Turney and Littman, 2003) learning problem. Takamura et al (2005) proposed using spin models for extracting semantic orientation of words.  Under this setting, we compare our method to the spin model described in (Takamura et al, 2005).
The entity grid approach has already been applied to many applications relying on local coherence estimation: summary rating (Barzilay and Lapata, 2005), essay scoring (Burstein et al, 2010) or story generation (McIntyre and Lapata,2010). The experimental results demonstrate that our model is able to significantly outperform the state-of the-art coherence model by Barzilay and Lapata (2005), reducing the error rate of the previous approach by an average of 29% over three data sets against human upper bounds. To further refine the computation of the subsequence distribution, we follow (Barzilay and Lapata, 2005) and divide the matrix into a salient matrix and a non-salient matrix.   Barzilay and Lapata (2005) showed that their entity based model is able to distinguish a source text from its permutation accurately. The results on the Earthquakes and Accidents data are quite similar to those published in (Barzilay and Lapata, 2005) (they reported 83.4% on Earthquakes and 89.7% on Accidents), validating the correctness of our reimplementation of their method. This result supports the use of salience, in line with the conclusion drawn in (Barzilay and Lapata, 2005). The entity-based model of Barzilay and Lapata (2005) connects the local entity transition with textual coherence, while our model looks at the patterns of discourse relation transitions. This intuition has been formalized by (Barzilay and Lapata, 2005), who developed an entity-based statistical representation of local discourse and showed its usefulness for estimating coherence between sentences. Barzilay and Lapata (2005) exploited the use of the distributional and referential information of discourse entities to improve summary coherence. In order to evaluate the local coherence of the reports generated by the system, we employed an automatic coherence evaluation method introduced by Barzilay and Lapata (2005). Our model is inspired by Centering (Grosz et al, 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an entity is in focus through a sequence of sentences.  We expect that features such as entity grid (Barzilay and Lapata, 2005) will improve overall algorithm performance. We present a model for discourse coherence which combines the local entity based approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Moreover, an accurate model can reveal information about document structure, aiding in such tasks as supervised summarization (Barzilay and Lapata, 2005). Barzilay and Lapata (2005) uses the same grid representation, but treats the transition probabilities P (ri, j |~ri, j) for each document as features for input to an SVM classifier. 
(Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems. The main difference between this approach and ours is that (Ng, 2005)'s approach takes coreference resolution one step further, by comparing the results of multiple systems, while our system is a single resolver; furthermore, he emphasizes the global optimization of ranking clusters obtained locally, whereas our focus is on globally optimizing the clusterization method inside the resolver. There are many different training example generation algorithms, e.g., McCarthy and Lehnert's method, Soon et als method, Ng and Cardies method (Ng, 2005). Similar to many previous works on co-reference (Ng, 2005), we cast the problem as a classification task and solve it in two steps: (1) train a classifier to determine whether two mentions are co-referent or not, and (2) use a clustering algorithm to partition the mentions into clusters, based on the pairwise predictions. To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005). Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method. Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. This could be incorporated in a ranking scheme, as in Ng (2005). The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure of about 62% for the same data set.   MUC and B3 metrics (Ng, 2005a). Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. Other work on global models of coreference (as opposed to pairwise models) has included: Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al (2006) who use a probabilistic first-order logic model. Similarly, the method of (Ng, 2005) ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents. The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. According to Ng (2005), most learning based coreference systems can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions. This strategy has been described as best-first clustering by Ng (2005). In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning.
Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011).   We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories.  Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively.  Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser.  We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005). We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005). Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies.
Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU.  We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). For instance, Zollmann et al (2006) follow Chiang (2005) in disallowing adjacent non terminals. (Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side.  To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). (Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information.
 This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree.  Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). This is a syntactically-informed MT system, designed following (Quirk et al, 2005). For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). (Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs.  Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005).  Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones.     Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side.  Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005).
Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence.  We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005).  Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005).  System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005).
Finkel et al (2005) used simulated annealing with Gibbs sampling to find a solution in a similar situation. We also conduct experiments using simulated annealing in decoding, as conducted by Finkel et al (2005) for information extraction. Finkel et al (2005) proposed a method incorporating non-local structure for information extraction. To compute the features which we extract in the next section, all instances in our data sets were part-of-speech tagged by the MXPOST tagger (Ratnaparkhi, 1996), parsed with the MaltParser, and named entity tagged with the Stanford NE tagger (Finkel et al, 2005). One such technique is Markov chain Monte Carlo, and in particular Gibbs sampling (Finkel et al, 2005), another is (loopy) sum-product belief propagation (Smith and Eisner,2008). The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al, 2005). Following (Yao et al, 2011), we filter out noisy documents and use natural language packages to annotate the documents, including NER tagging (Finkel et al, 2005) and dependency parsing (Nivre et al, 2004). These include several off-the-shelf statisical NLP tools such as the Stanford POS tagger (Toutanova and Manning, 2000), the Stanford named-entity recognizer (NER) (Finkel et al, 2005) and the Stanford Parser (Klein and Manning, 2003). We run the Stanford Named Entity Recognizer (Finkel et al, 2005) and record the number of PERSONs, ORGANIZATIONs, and LOCATIONs. The Total column presents the number of extracted NEs and generated hypotheses and the Average column shows the average numbers per text respectively.2009), and we preprocess the data using the Stanford named-entity recognizer (Finkel et al, 2005). For the learning of patterns we used the top 64 documents retrieved by Google and to recognize the named entities in the pattern we apply several strategies, namely: 1) the Stanford's Conditional Random-Field-based named entity recognizer (Finkel et al, 2005) to detect entities of type HUMAN; 2) regular expressions to detect NUMERIC and DATE type entities; 3) gazetteers to detect entities of type LOCATION.   We used as candidates all strings labeled in the annotated data as well as all named entities found by the Stanford NER tagger for CoNLL (Finkel et al, 2005).  The named-entity features are generated by the freely available Stanford NER tagger (Finkel et al., 2005). We use the Stanford Named Entity Recognizer (Finkel et al, 2005) for this purpose. The Stanford CRF-based NER tagger was used as the monolingual component in our models (Finkel et al, 2005). For English, we use the default tagger setting from Finkel et al (2005). To determine entailment, BIUTEE performs the following main steps: Preprocessing First, all documents are parsed and processed with standard tools for named entity recognition (Finkel et al, 2005) and coreference resolution.
It has also been shown that these techniques prove useful for tasks such as word sense disambiguation (Patwardhan et al, 2003), real-word spelling correction (Budanitsky and Hirst, 2001) and information extraction (Stevenson and Greenwood, 2005), among others.  Stevenson and Greenwood (2005) evaluated their method through document and sentence filtering at the scenario level. Many of the adaptive IE systems rely on the existing part-of-speech (POS) taggers (Debnath and Giles, 2005) and/or syntactic parsers (Stevenson and Greenwood, 2005) for analysing and annotating text corpora. Stevenson and Greenwood (2005) propose a weakly supervised approach to sentence filtering that uses semantic similarity and bootstrapping to acquire IE patterns. Our method is modeled on the approach developed by Stevenson and Greenwood (2005) but uses a different technique for ranking candidate patterns. Stevenson and Greenwood (2005) use subject-verb-object triples for their features. This is a significant improvement over the 0.58 F-measure score reported by Stevenson and Greenwood (2005) for the same task.    Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns.   Stevenson and Greenwood (2005) suggested an alternative method for ranking the candidate patterns by lexical similarities. To our knowledge, the only previous study that embeds similarities into the acquisition of extraction patterns is (Stevenson and Greenwood, 2005). We adapted the method of matrix similarity given by Stevenson and Greenwood (2005).
Also using kernel methods and support vector machines, (Zhao and Grishman, 2005) combine clues from different levels of syntactic information and applies composite kernels to integrate the individual kernels. Although Zhao and Grishman (2005) defined a number of kernels for relation extraction, the method is essentially similar to feature-based methods.   Bigrams: A bigram feature (Zhao and Grishman, 2005) can be represented by a subgraph consisting of two connected nodes from the sequence representation, where each node is labeled with the token.  Zhao and Grishman (2005) defined several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus.  Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data.   This kernel is formally defined in (Zhao and Grishman, 2005). Zhao and Grishman (2005) reported that adding local information to deep syntactic information improved IE results. The main concerns here (see e.g. (Zhao and Grishman, 2005)) are the extraction of large quantities of facts, generally coupled with machine learning approaches. Bigram of the words between the two mentions: This was extracted by both Zhao and Grishman (2005) and Jiang and Zhai (2007), aiming to provide more order information of the tokens between the two mentions.  This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall on the ACE corpus (Zhao and Grishman, 2005). Zhao and Grishman (2005) define a feature based composite kernel to integrate diverse features. Zhao and Grishman (2005) also evaluated their algorithm on the ACE corpus and got good performance. Zhao and Grishman (2005) define several feature-based composite kernels to capture diverse linguistic knowledge and achieve the F-measure of 70.4 on the 7 relation types in the ACE RDC 2004 corpus.
 Zhou et al (2005) reported the best result as 63.1% / 49.5% / 55.5% in Precision / Recall / F-measure on the extraction of ACE relation subtypes using feature based method, which outperforms tree kernel based method by Culotta and Soresen (2004). However, detailed research (Zhou et al, 2005) shows that it is difficult to extract new effective features to further improve the extraction accuracy. For those interested in feature-based methods, please refer to Zhou et al (2005) for more details.  Based on his work, Zhou et al (2005) further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list.   Zhou et al (2005) argued that most information useful for IE derived from full parsing was shallow. This approach is also applicable to IE in other domains, where related entities are in a short distance like the work of Zhou et al (2005). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).  However, it is difficult for them to effectively capture structured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction. As the state-of-the-art, Zhang et al (2006) applied the convolution tree kernel (Collins and Duffy 2001) and achieved comparable performance with a state-of-the art linear kernel (Zhou et al　2005) on the 5 relation types in the ACE RDC 2003 corpus. Zhou et al (2005) further systematically explored diverse features through a linear kernel and Support Vector Machines, and achieved the F measures of 68.0 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively. Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods. Here, we use the same set of flat features (i.e. word, entity type, mention level, overlap, base phrase chunking, dependency tree, parse tree and semantic information) as Zhou et al (2005).   For the choice of features, we use the full set of features from Zhou et al (2005) since it is reported to have a state-of-the-art performance (Sun et al 2011).
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). (Liu et al., 2005) uses a log-linear model with a greedy search. We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). (Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. Liu et al (2005) used a conditional log-linear model with similar features to those we have employed.  Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005).  Liu et al (2005) also develop a log-linear model, based on IBM Model 3. A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). This is a key difference between our model and (Liu et al, 2005).
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied.  Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. We apply one of the pruning techniques used in Zhang and Gildea (2005). In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. Our pruning differs from Zhang and Gildea (2005) in two major ways. The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005).
For first language (L1) learners (i.e., children learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student's lexicon, grammatical surface features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005). Prior work on first language readability by Schwarm and Ostendorf (2005) incorporated grammatical surface features such as parse tree depth and average number of verb phrases.   Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. Support vector machines have already been shown to be useful for readability purposes (Schwarm and Ostendorf, 2005). A corpus of Weekly Reader articles was previously used in work by Schwarm and Ostendorf (2005). Schwarm and Ostendorf (2005) studied four parse tree features (average parse tree height, average number of SBARs, noun phrases, and verb phrases per sentences). For comparison, we replicated 6 out-of-vocabulary features described in Schwarm and Ostendorf (2005). We also replicated the 12 perplexity features implemented by Schwarm and Ostendorf (2005) (see Section 3.2). Table 8 compares a classifier trained on the four parse features of Schwarm and Ostendorf (2005) to a classifier trained on our expanded set of parse features. The most closely related previous study is the work of Schwarm and Ostendorf (2005). Also, relatedly, (Schwarm and Ostendorf, 2005) use a statistical language model to train SVM classifiers to classify text for grade levels 2-5. A measure by Schwarmand Ostendorf (2005) incorporates syntactic analyses, among a variety of other types of features.   Schwarm and Ostendorf (2005) implemented four parse tree features (average parse tree height, aver age number of SBARs, NPs per sentence and VPs per sentence) in their work. In order to verify the impact of our choice of features, we also did a replication of the parsed syntactic feature measures reported by (Schwarm and Ostendorf, 2005) on the WeeklyReader corpus and obtained essentially the same accuracy as the one published (50.7% vs. 50.91%), supporting the comparability of the WeeklyReader data used.  Syntactic complexity is an obvious factor: indeed (Heilman et al, 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.
  This result was found to be significant (p= 0.021) under the paired bootstrap resampling method of Koehn (2004), and is close to significant (p= 0.058) under the sign test of Collins et al (2005). Our best performing method used unsupervised morphology with L-match (see Section 2.2) and the improvement is significant: bootstrap resampling provides a confidence margin of 0.77 and a t-test (Collins et al, 2005) showed significance with p= 0.001.  We parse the input using the Collins parser (Collins, 1997) and apply a set of reordering rules to re-arrange the German sentence so that it corresponds more closely English word order (Collins et al, 2005). We are aware of two methods that have been proposed for significance testing with BLUE: bootstrap resampling (Koehn, 2004b; Zhang et al, 2004) and the sign test (Collins et al, 2005). But Collins et al (2005) note that it is not clear whether the conditions required by bootstrap resampling are met in the case of BLUE, and recommend the sign test instead. All results are statistically significant with p= 0.05 using the sign-test described in (Collins et al, 2005). Clause restructuring performed with hand-crafted reordering rules for German-to-English and Chinese-to-English tasks are presented in (Collins et al, 2005) and (Wang et al, 2007), respectively. At the same time, many reorderings can be performed more efficiently based on fixed (hand-crafted) rules (as it is done in (Collins et al, 2005)). We followed the approximation described in (Collins et al, 2005) to get around this problem. Collins et al (2005) approach the problem of properly translating negation in their general reordering setting. To make the word order of German input sentences more English-like a version of the rules of (Collins et al, 2005) were partially implemented using tagged output from the RFTagger. Bold numbers are not significantly different from the best result according to the sign test (p= 0.05) (Collins et al, 2005). The test data for the experiments consisted of 2,000 sentences, and was the same test set as that used by Collins et al (2005).  Collins et al (2005) address this problem by reordering German sentences to more closely parallel English word order, prior to translation by a PSMT system. Collins et al (2005) (German-to-English) use six hand-crafted reordering rules targeting the placement of verbs, subjects, particles and negation. Zwarts and Dras (2007) implement six rules for Dutch-to-English translation, analogous to those of Collins et al (2005), as part of an exploration of dependency distance in syntax-augmented PSMT.
Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al, 2002) and machine translation (Ding and Palmer, 2005). A syntax-based system might be able to check this sort of agreement if it produced a target-side dependency tree as in Ding and Palmer (2005). Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Ding and Palmer (2005) introduced a version of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive structure divergence. Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al, 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al, 2004).  Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005). Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). As mentioned in (Ding and Palmer, 2005), most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages.  It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). (Ding and Palmer, 2005) presents a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees.  
Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. Most available Arabic NLP tools and resources model morphology using surface inflectional features and do not mark rationality; this includes the PATB (Maamouri et al, 2004), the Buckwalter morphological analyzer (BAMA) (Buckwalter, 2004) and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) system (Habash and Rambow, 2005). MADA is an SVM based system that disambiguates among different morphological analyses produced by BAMA (Habash and Rambow, 2005). In a previous publication, we described the Morphological Analysis and Disambiguation of Arabic (MADA) system (Habash and Rambow, 2005). The algorithm we proposed in (Habash and Rambow, 2005) for choosing the best BAMA analysis simply counts the number of predicted values for the set of linguistic features in each candidate analysis. To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). We tokenize using the MADA morphological disambiguation system (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Sadat and Habash, 2006). MADA (Habash and Rambow, 2005) is used to pre-process the Arabic text for the translation model and 5-gram language model (LM). Our data is gold tokenized; however, all of the features we use are predicted using MADA (Habash and Rambow, 2005) following the work of Marton et al (2010). For predicting morphological features, we use the MADA system (Habash and Rambow, 2005). We compare our results with the form-based features from the state-of-the-art morphological analyzer MADA (Habash and Rambow, 2005). They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source. The Arabic side is segmented according to the Arabic Treebank tokenization scheme (Maamouri et al, 2004) using the MADA + TOKAN morphological analyzer and tokenizer (Habash and Rambow, 2005). Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). The Morphological Analysis and Disambiguation of Arabic (MADA) system is described in (Habash and Rambow, 2005). Habash and Rambow (2005) use SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. To make our results more comparable to those by Habash and Rambow (2005), we converted the test set with the POS tags from the whole word tagger to their tokenization and to a reduced tag set of 15 tags. Therefore, we repeated the experiments above with POS tags predicted by the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow, 2005).
In Pradhan et al (2005), we reported on a first attempt to overcome this problem by combining semantic role labels produced from different syntactic parses. Pradhan et al (2005) combined systems that are based on phrase-structure parsing, dependency parsing, and shallow parsing.   However, (Pradhan et al., 2005a) uses some additional information since it deals with incorrect parser output by using multiple parsers.    In (Pradhan et al, 2005b), some experiments were conducted on SRL systems trained using different syntactic views. More details of this system can be found in Pradhan et al, (2005). To solve these errors, we need to explore more, such as using n-best parses and the use of several syntactic views (Pradhan et al, 2005b). Some other work paid much attention to the robust SRL (Pradhan et al, 2005b) and post inference (Punyakanok et al, 2004). Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al, 2005a), as the baseline features.  Some other works paid much attention to the robust SRL (Pradhan et al, 2005b) and post inference (Punyakanok et al, 2004). Pradhan et al (2005) combine the outputs of multiple parsers to extract reliable syntactic information, which is translated into features for a machine learning experiment in assigning semantic roles. Most state-of-the-art methods for the latter two tasks use a cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasks as pattern matching (Johnson, 2002) or classification (Pradhan et al, 2005) problems.  In Table 4 we compare our system for semantic roles labeling with the output of Charniak's parser to the state-of-the-art system of (Pradhan et al, 2005). 
Reranking has previously been applied to semantic role labeling by Toutanova et al (2005), from which we use several features. Toutanova et al (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation.  Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al, 2005). To enforce this constraint, we employ the approach presented by Toutanova et al (2005).  We implemented a global reranker following Toutanova et al (2005). Statistical parsers are major components in NLP applications such as QA (Kwok et al, 2001), MT (Marcu et al, 2006) and SRL (Toutanova et al, 2005). We employ this decomposition mainly for efficiency in training: that is, the decomposition allows us to train the classification models on a subset of training examples consisting only of those phrases that have a case marker, following Toutanova et al (2005). Toutanova et al (2005) report a substantial improvement in performance on the semantic role labeling task by building a joint classifier, which takes the labels of other phrases into account when classifying a given phrase. We applied the joint classifiers in the framework of N-best re ranking (Collins, 2000), following Toutanova et al (2005). Our system, on the other hand, follows a joint approach in the spirit of Toutanova et al (2005) and performs the above steps collectively. In contrast to the work of Toutanova et al (2005) our system applies on line learning to train its parameters and exact inference to predict a collective role labelling. In addition, while the system described here is based on pipelined classification, recent research on semantic role labeling has shown that significant performance improvements can be gained by exploiting interdependencies between arguments (Toutanova et al., 2005). In a recent paper on the SRL on verbal predicates for English, (Toutanova et al, 2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label. The system, introduced in (Toutanova et al, 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. The ones denoted with asterisks (*) were not present in (Toutanova et al, 2005). We find the exact top N consistent most likely local model labelings using a simple dynamic program described in (Toutanova et al, 2005). Most of the features we use are described in more detail in (Toutanova et al, 2005). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as part of-speech (POS) tagging (Lafferty et al, 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al, 2005).
Bannard and Callison-Burch (2005) described a pivoting approach that can exploit bilingual parallel corpora in several languages. Bannard and Callison-Burch (2005) defined a paraphrasing probability between two phrases based on their translation probability through all possible pivot phrases as: Ppara (p1 ,p2)= sum piv Pt (piv|p1) Pt (p2|piv) where Pt denotes translation probabilies. One of the most popular methods leveraging bilingual parallel corpora is proposed by Bannard and Callison-Burch (2005). The Context-Sensitive Paraphrase Suggestion (CS-PS) model first finds a set of local paraphrases P of the input phrase K using the pivot-based method proposed by Bannard and Callison-Burch (2005).  This method was defined in (Bannard and Callison-Burch, 2005). ParaEval matches hypothesis and reference translations using paraphrases that are extracted from parallel corpora in an unsupervised fashion (Bannard and Callison-Burch, 2005). An alternative approach to paraphrase acquisition was proposed by Bannard and Callison-Burch (2005). We take advantage of transitivity of relevance to rank and filter the paraphrases generated by the pivot-based method (i.e., phrase are treated as paraphrases if they share the same translations) of Bannard and Callison-Burch (2005). For example, Bannard and Callison-Burch (2005) propose the pivot approach to generate phrasal paraphrases from an English-German parallel corpus. In this paper, we generate paraphrases adopting the pivot-based method proposed by Bannard and Callison-Burch (2005) in the first round. We first exploit the pivot-based method proposed by Bannard and Callison-Burch (2005) to populate our graph G using of candidate paraphrases cP={} from a bilingual parallel corpus B for a query phrase q. The algorithm in (Bannard and Callison-Burch, 2005) is used for this purpose by pivoting through phrases in the source and the target languages: for each source phrase, all occurrences of its target phrases are found, and all the corresponding source phrases of these target phrases are considered as the potential paraphrases of the original source phrase (Callison Burch et al, 2006). Following Bannard and Callison-Burch (2005), we identify Arabic phrases (a1) in the target corpus that are translated by at least one English phrase (e).  In this paper we examine the effectiveness of placing syntactic constraints on a commonly used paraphrasing technique that extracts paraphrases from parallel corpora (Bannard and Callison-Burch, 2005). Bannard and Callison-Burch (2005) extract paraphrases from bilingual parallel corpora. There are a number of strategies that might be adopted to alleviate this problem: Bannard and Callison-Burch (2005) rank their paraphrases with a language model when the paraphrases are substituted into a sentence. Bannard and Callison-Burch (2005) sum over multiple parallel corpora C to reduce the problems associated with systematic errors in the word alignments in one language pair. 
NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010).  Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005). Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005).  Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly. We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). This baseline system follows the design of previous work (Ravichandran et al, 2005). We followed the notation of the original paper (Ravichandran et al, 2005) here. Ravichandran et al (2005) applied LSH to the task of noun clustering. However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work. Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies.
In sentiment analysis research, Read (2005) used emoticons in newsgroup articles to extract instances relevant for training polarity classifiers. We therefore experiment with multiple such conventions with apparently similar meanings - here, emoticons (following (Read, 2005)) and Twitter hash tags - allowing us to examine the similarity of classifiers trained on independent labels but intended to detect the same underlying class. The regulating aspects of semantic orientation of a text are natural language context information (Pang et al, 2002) language properties (Wiebe and Mihalcea, 2006), domain pragmatic knowledge (Aue and Gamon, 2005) and lastly most challenging is the time dimension (Read, 2005). We have built a corpus of tweets written in English following the procedure described in (Read, 2005) and (Go et al, 2009). According to (Read, 2005), when authors of an electronic communication use an emotion, they are effectively marking up their own text with an emotional state. The regulating aspects which govern the lexical level semantic orientation are natural language context (Pang et al, 2002), language properties (Wiebe and Mihalcea, 2006), domain pragmatic knowledge (Aue and Gamon, 2005), time dimension (Read, 2005), colors and culture (Strapparava and Ozbal, 2010) and many more unrevealed hidden aspects. We seed the graph using the polarity values in the OpinionFinder lexicon (Wilson et al, 2005), the known polarity of emoticons, and a maximum entropy classifier trained on 1.8 million tweets with automatically assigned labels based on the presence of positive and negative emoticons, like Read (2005) and Go et al (2009). The regulating aspects which govern the lexical level semantic orientation are natural language context (Pang et al, 2002), language properties (Wiebe and Mihalcea, 2006), domain pragmatic knowledge (Aue and Gamon, 2005), time dimension (Read, 2005), colors and culture (Strapparava and Ozbal, 2010) and many more unrevealed hidden aspects. It is not a static sentiment lexicon set [polarity changes with time (Read, 2005)] as it is updated regularly. We use emoticons as indicators of an emotion (Read, 2005) to automatically classify texts into positive or negative sets. The approach is similar to the one in (Read, 2005).
Because of the fundamental nature of the semantic similarity problem, there are close connections with other areas of human language technologies such as information retrieval (Salton and Lesk, 1971), text alignment in machine translation (Jayaraman and Lavie, 2005), text summarization (Mani and Maybury, 1999), and textual coherence (Foltz et al, 1998).  (Jayaraman and Lavie, 2005) tried to overcome this problem by using confidence scores and language models in order to rank a collection of synthetic combinations of words extracted from the original translation hypotheses. Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000). Our system is an enhancement of our previous work (Jayaraman and Lavie, 2005). The algorithm is described fully by Jayaraman and Lavie (2005). Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005). Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&L. Jayaraman and Lavie (2005) proposed a heuristic-based matching algorithm which allows non monotonic alignments to align the words between the hypotheses. (Jayaraman and Lavie, 2005) proposed another black-box system combination strategy. Karakos, et al (2008) proposed an ITG based method for hypothesis alignment, Rosti et al (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005).
   Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). We use the evaluation source code provided by Malioutov and Barzilay (2006). Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisen stein and Barzilay, 2008) and identifying chapter breaks in novels. We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (EisensteinandBarzilay, 2008). In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006).  (Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). Segmentation may be particularly beneficial when working with documents without overt structure: speech transcripts (Malioutov and Barzilay, 2006), newswire (Misra et al, 2011) or novels (Kazantseva and Szpakowicz, 2011). Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity.
 We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by Bergsma and Lin (2006). Bergsma and Lin (2006) determine the likelihood of coreference along the syntactic path connecting a pronoun to a possible antecedent, by looking at the distribution of the path in text. Given an automatically parsed corpus, Bergsma and Lin (2006) extract from each parse tree a dependency path, which is represented as a sequence of nodes and dependency labels connecting a pronoun and a candidate antecedent, and collect statistical information from these paths to determine the likelihood that a pronoun and a candidate antecedent connected by a given path are coreferent.   Gender and Animacy processor: This modules collects gender information from the gender corpus (Bergsma and Lin, 2006) and checks a self-made corpus for profession (teacher, doctor, etc) and family relations (mother, father, etc), extracted from web searches. In the closed track, systems were limited to the provided data, plus the use of two pre-specified external resources: i) WordNet and ii) a pre-computed number and gender table by Bergsma and Lin (2006).  As noted above, systems were allowed to make use of gender and number predictions for NPs using the table from Bergsma and Lin (Bergsma and Lin, 2006). We assign number attributes based on: (a) a static list for pronouns; (b) NER labels: mentions marked as a named entity are considered singular with the exception of organizations, which can be both singular or plural; (c) part of speech tags: NN*S tags are plural and all other NN* tags are singular; and (d) a static dictionary from (Bergsma and Lin, 2006). All the knowledge required by the feature functions is obtained from the annotations of the corpora and no external resources have been used with the exception of WordNet (Miller, 1995), gender and number information (Bergsma and Lin, 2006) and sense inventories. Both Ge et al (1998) and Bergsma and Lin (2006) show that learned gender is the most important feature in their pronoun resolution systems. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online. We can regard the Bergsma and Lin (2006) approach and our discriminative system as two orthogonal views of gender, in a co-training sense (Blum and Mitchell, 1998). For English, number and gender for common nouns are computed via a comparison of head lemma to head and using the number and gender data of Bergsma and Lin (2006).  For non pronominal mentions, we used the number and gender data (Bergsma and Lin, 2006) provided by the task organizers and queried it for the head word of the mention. Bergsma and Lin (2006) built a statistical model from paths that include the lemma of the intermediate tokens, but replace the end nodes with noun, pronoun, or pronoun-self for nouns, pronouns, and reflexive pronouns, respectively. For the gender task that we study in our experiments, we acquire class instances by filtering the dataset of nouns and their genders created by Bergsma and Lin (2006).
These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). The one most similar to ours is the one presented by Blunsom and Cohn (2006). Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. (Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006).
In our work, we adopt the method proposed in (Tao and Zhai, 2005) and apply it to the problem of transliteration; note that (Tao and Zhai, 2005) compares several different metrics for time correlation, as we also note below and see (Sproat et al., 2006). However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al,2009b). These include studying the query logs (Brill et al, 2001), unrelated corpora (Rapp, 1999), and comparable corpora (Sproat et al 2006). An initial PSM is bootstrapped using limited prior knowledge such as a small amount of transliterations, which may be obtained by exploiting co-occurrence information (Sproat et al, 2006). In transliteration extraction, mining translations or transliterations from the ever-growing multilingual Web has become an active research topic, for example, by exploring query logs (Brill et al., 2001) and parallel (Nie et al, 1999) or comparable corpora (Sproat et al, 2006). (Sproat et al 2006) have compared names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time. For example, (Sproat et al, 2006) presents a supervised system that achieves a MRR score of 0.89, when evaluated over a dataset consisting of 400 English NE and 627 Chinese words. (Sproat et al 2006) report an oracle accuracy of 85%, but it depends on the source of the candidate transliterations.
 We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora. Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora.  Similarly, Munteanu and Marcu (2006) propose a method to extract sub sentential fragments from non-parallel corpora.  Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. Our first technique resembles the technique of Munteanu and Marcu (2006) who also perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction. The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanuand Marcu, 2006).  Munteanu and Marcu (2006) extract sub sentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a modified version of an approach by Munteanu and Marcu (2006) on translation fragment extraction. It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006). At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein).
However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006). Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarse grained sense distinctions of the Oxford Dictionary of English (ODE). The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavaglia`, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). In addition, we show in Table 7 the F-score results provided by Snow et al (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE. Navigli (2006) has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource. (Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. Finally, we use as a feature the mappings produced in (Navigli, 2006) of WordNet senses to Oxford English Dictionary senses. In order to evaluate the entire sense-clustered taxonomy, we have employed an evaluation method inspired by Word Sense Disambiguation (this is similar to an evaluation used in Navigli, 2006, however we do not remove monosemous clusters). To tackle the granularity issue, we produced a coarser-grained version of the WordNet sense inventory based on the procedure described by Navigli (2006). The data were annotated with coarse-grained senses which were obtained by clustering senses from the Word Net 2.1 sense inventory based on the procedure proposed by Navigli (2006).  This clustering was created automatically with the aid of a methodology described in (Navigli, 2006).  Automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (Navigli, 2006).
  Here we used the Tchai algorithm (Komachi and Suzuki, 2008), a modified version of Espresso (Pantel and Pennacchiotti, 2006) to collect such candidates. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. Short paths are more likely to be generic patterns such as 'of' and can be handled separately as in (Pantel and Pennacchiotti, 2006). We compare our results to two pattern based methods: CDP (the Stage 1 extractor) and Espresso (Pantel and Pennacchiotti, 2006a). In the pattern induction step (section 3.2 in (Pantel and Pennacchiotti, 2006a)), Espresso computes a reliability score for each candidate pattern based on the weighted PMI of the pattern with all instances extracted so far. Others have used classifications based on the requirements for a specific task, such as Information Extraction (Pantel and Pennacchiotti, 2006) or biomedical applications (Stephens et al, 2001). In our study, we use point wise mutual information (Cover and Thomas, 1991) to measure association strength, which has been proved effective in the task of semantic relation identification (Pantel and Pennacchiotti, 2006). Second, we extend Pantel and Pennacchiotti (2006)'s Espresso algorithm, which induces specific reliable LSPs in a bootstrapping manner for entity relation extraction, so that the extended algorithm can apply to event relations (Sections 4.2 to 4.4). This section overviews Pantel and Pennacchiotti (2006)'s Espresso algorithm. Espresso (Pantel and Pennacchiotti, 2006) is also concerned in finding patterns to represent relations. However, our initial experiments suggest that good pattern generalization would have a significant impact on recall, without negative impact on precision, which agrees with findings in the literature (Pantel and Pennacchiotti, 2006).  In this paper, we propose a graph-based approach to seed selection and stop list creation for the state-of-the-art bootstrapping algorithm Espresso (Panteland Pennacchiotti, 2006). To answer these questions, we bootstrapped a minimally-supervised relation extraction algorithm, based on Espresso (Pantel and Pennacchiotti, 2006), with different seed-sets for the various types of part-whole relations, and analyzed the harvested tuples and patterns. The Espresso algorithm (Pantel and Pennacchiotti, 2006) achieves a precision of 80% in learning part whole relations from the Acquaint (TREC-9) corpus of nearly 6M words. Similarly, the minimally-supervised Espresso algorithm (Pantel and Pennacchiotti, 2006) is initialized with a single set that mixes seeds of heterogeneous types, such as leader-panel and oxygen-water, which respectively correspond to the member-of and sub-quantity-of relations in the taxonomy of Keet and Artale (2008). Compared to traditional surface-pattern representations, used by Pantel and Pennacchiotti (2006), dependency paths abstract from surface texts to capture long range dependencies between terms. As IE algorithm for extracting part-whole relations from our texts, we relied on Espresso, a minimally-supervised algorithm, as described by Pantel and Pennacchiotti (2006).
The phrasal Statistical Machine Translation (SMT) technique is employed to identify and correct writing errors (Brockett et al, 2006). Recent work by Brockett et al (2006) utilized phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrated that this data-intensive SMT approach is very promising, but they also pointed out SMT approach relies on the availability of large amount of training data.  Brockett et al (2006) introduce errors involving mass/count noun confusions into English newswire text and then use the resulting parallel corpus to train a phrasal SMT system to perform error correction.  Brockett et al (2006) showed that phrase-based statistical MT can help to correct mistakes made on mass nouns. Similar to our approach, Brockettet al (2006) view error correction as a Machine Translation problem. Note that our approach is different from that of Brockett et al (2006), as we do make use of a truly multi-lingual translation model. Brockett et al (2006) uses phrasal SMT techniques to identify and correct mass noun errors of ESL students. Some researchers explicitly focus on individual classes of errors, e.g., mass vs count nouns in (Brockett et al, 2006) and (Nagata et al, 2006). Brockett et al (2006) employed phrasal Statistical Machine Translation (SMT) techniques to correct countability errors. Brockett et al (2006) propose the use of the phrasal statistical machine translation (SMT) technique to identify and correct ESL errors. To construct the training corpus, we followed the idea in Brockett et al (2006), and applied a similar strategy described in section 3.4 to the SRL system's training data to generate aligned pairs. Because of the great flexibility of the log-linear model, researchers have used the framework for other tasks outside SMT, including grammatical error correction (Brockett et al, 2006).
 In Section 2, we describe a corpus statistics approach, previously applied for web mining (Davidov and Rappoport, 2006), which we extend for relation discovery. To discover them, we use a slightly modified version of the method presented in (Davidov and Rappoport, 2006). It also significantly outperforms the single-language pattern-based method introduced by (Davidov and Rappoport, 2006), which achieves average precision of 79.3 on a similar set in English (in comparison to 86.7 in this study). Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. To specify patterns, following (Davidov and Rappoport, 2006) we classify words into high frequency words (HFWs) and content words (CWs). We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2. Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006).  Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. The Russian corpus (Davidov and Rappoport, 2006) was assembled from web-based Russian repositories, to yield 33GB and 4G words. All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). Unlike (Davidov and Rappoport, 2006), we consider all single punctuation characters or consecutive sequences of punctuation characters as HFWs.
      In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). (McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker.  Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b).    McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. 
The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005).  Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006).  However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)).  We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement.  We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006).  In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule.    For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder.   Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006).
 Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words.  Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006). The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model.  The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2.  In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006).
 (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds. This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006). These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length > 10. Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). These alignment classes are called configurations (Smith and Eisner, 2006a, and following). Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a).  Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training.
A very similar system for the reverse direction is described in (Liu et al, 2006). We perform derivation-level combination as described in (Liu et al, 2009b) for mixing different types of translation rules within one derivation. Liu et al (2006) propose a tree-to-string model. Liu et al (2006) treat all bilingual phrases as lexicalized tree-to-string rules, including those non-syntactic phrases in training corpus. Liu et al (2006) propose a tree-to-string translation model.   In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al, 2006) for a tree-to-string MT system. Liu et al (2006) also add non-syntactic PBSMT phrases into their tree-to-string translation system. Liu et al (2006) changed the translation unit from phrases to tree-to-string alignment templates (TATs) while we do not.   Liu et al (2006) experimented with tree-to-string translation models that utilize source side parse trees. When using the projected parser in a tree based translation model (Liu et al, 2006), we achieve translation performance comparable with using a state-of-the-art supervised parser trained on thousands of CTB trees. We first extract the tree-to-string translation rules from the training corpus by the algorithm of (Liu et al, 2006), and train a 4-gram language model on the Xinhua portion of GIGAWORD corpus with Kneser-Ney smoothing using the SRI Language Modeling Toolkit (Stolcke and Andreas, 2002). However, if one were to use rule Markov models with a conventional CKY-style bottom-up decoder (Liu et al, 2006), the complexity would increase to O (n Cm 1|V |4 (g 1)), where C is the maximum number of outgoing hyper edges for each node in the translation forest, and m is the order of the rule Markov model. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chinese to-English translation (Liu et al, 2006). For example, (Chiang, 2007) adopts a CKY style span-based decoding while (Liu et al, 2006) applies a linguistically syntax node based bottom-up decoding, which are difficult to integrate. In this paper, we incorporate the MERS model into a state of-the-art linguistically syntax-based SMT model, the tree-to-string alignment template (TAT) model (Liu et al, 2006). Our baseline system is Lynx (Liu et al, 2006), which is a linguistically syntax-based SMT system.
Our analyzer provides segmentation and PoS tags with 92.5% accuracy and full morphology with 88.5% accuracy (Adler and Elhadad, 2006). Recent PoS taggers and morphological analyzers for Hebrew (Adler and Elhadad, 2006) address this issue and provide for each word not only the PoS, but also full morphological features, such as Gender, Number, Person, Construct, Tense, and the affixes' properties. Since it cannot predict its own segmentation, automatic segments and tags are predicted using the system of Adler and Elhadad (2006). Recently, Adler and Elhadad (2006) presented an unsupervised, HMM-based model for Hebrew morphological disambiguation, using a morphological analyzer as the only resource. Our best result, 91.44% accuracy, reflects a reduction of 25% in error rate compared to the previous state of the art (Adler and Elhadad, 2006), and almost 40% compared to the baseline. Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%). A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be P (REL, VB|fmnh, context)= P (REL|f) P (VB|mnh, REL) P (REL, VB| context) and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al, 2005) present similar models. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of He brew, based on lattice parsing. Since these parsers cannot choose their own tags, automatically predicted segments and tags are provided by Adler and Elhadad (2006). (Adler and Elhadad,2006) perform Hebrew morphological disambiguation using an unsupervised morpheme-based HMM, but they report lower scores than those achieved by our model. In a preliminary experiment, the POS tagger (Adler and Elhadad, 2006) accuracy on the Responsa Corpus was less than 60%, while the accuracy of the same tagger on modern Hebrew corpora is ~90% (Bar Haim et al, 2007). Adler and Elhadad (2006) present a lattice-based modification of the BaumWelch algorithm to handle this segmentation ambiguity. Some are language independent (see e.g. Attia et al (2010), Adler et al (2008)) while others focus on specific languages (see e.g. Habash and Rambow (2005, 2007) and Marsi et al (2005) on Arabic and Adler and Elhadad (2006) on Hebrew, another Semitic language with similar morphological structure).
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities.  USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation. We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences).  Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006).  We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b).     We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b).  Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a).
 In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights.   Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance.  For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB).
  Our approach for labelling temporal relations (or TLINKs) is based on NLTK's maximum entropy classifier, using the feature sets initially proposed in Mani et al (2006). Thus, the features in Mani et al (2006) are augmented with those used to describe signals detailed in Derczynski and Gaizauskas (2010), with some slight changes. The performance of classifier based approaches to temporal link labelling seems to be levelling off - the 60% - 70% relation labelling accuracy of work such as Mani et al (2006) has not been greatly exceeded.  While machine learning approaches attempt to improve classification accuracy through feature engineering, Mani et al (2006) introduced a temporal reasoning component to greatly expand the training data. Recently, extensions of Mani et al (2006)'s research is briefly described in (Mani et al, 2007). This technical report addresses two problems found in (Mani et al, 2006): (1) feature vector duplication caused by the data normalization process (once fixed, the accuracy drops to 76.56% and 83.23%) and (2) a somewhat unrealistic evaluation scheme (we describe Mani et al (2007)'s results in Section 4.1).  Although Mani et al (2006) use the links introduced by closure to boost the amount of training data for a tlink classifier, this technique is not suitable for our learning task since the closure might easily propagate errors in the automatic annotations. Following (Mani et al, 2006), prior approaches exploit temporal inferences to enrich the set of training in stances used for learning.   These differences are likely to come from the fact that: (i) (Mani et al, 2006) perform a 6-way classification, and not a 13-way classification, and (ii) (Chambers and Jurafsky, 2008) use a relation set that is even more restrictive than TempEval's. As such, it emphasizes robustness at Web scale, without taking advantage of existing specification languages for representing events and temporal expressions occurring in text (Pustejovsky et al, 2003), and forgoing the potential benefits of more complex methods that extract temporal relations from relatively clean text collections (Mani et al, 2006). Taking a cue from Mani et al (2006), we also increased Timebank's size by applying transitivity rules to the hand labeled data.  Mani et al (2006) introduced a temporal reasoning component that greatly expands the available training data. In order to connect the event graph, we draw on work from (Mani et al, 2006) and apply transitive closure to our documents.
Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998).  EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements.  For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006).  If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. (Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. (Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models.
Recently, Snow, Jurafsky and Ng (2005) generated tens of thousands of hypernym patterns and combined these with noun clusters to generate high-precision suggestions for unknown noun insertion into WordNet (Snow et al, 2006). Following Snow et al (2006), we derive two types of evidence from these patterns: H is a hypernym of A, B and C, A, B and C are siblings of each other.  Obviously, all these semantic resources have been acquired using a very different set of processes (Snow et al, 2006), tools and corpora. We also compare ASIA on twelve additional benchmarks to the extended Wordnet 2.1 produced by Snow et al (Snow et al, 2006), and show that for these twelve sets, ASIA produces more than five times as many set instances with much higher precision (98% versus 70%). Snow et al (Snow et al, 2006) use known hypernym / hyponym pairs to generate training data for a machine-learning system, which then learns many lexico-syntactic patterns. Snow (Snow et al, 2006) has extended the Word Net 2.1 by adding thousands of entries (synsets) at a relatively high precision.  Snow et al (2006) add novel terms by greedily maximizing the conditional probability of a set of relational evidence given a taxonomy. Options for identifying interesting classes include manually created methods (WordNet (Miller et al, 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al, 2006). The work by Snow et al (2006) is the most similar to ours because they also took an incremental approach to construct taxonomies. We compare system performance between (Snow et al, 2006) and our framework in Section 5. To have a fair comparison, for PR, we estimate the conditional probability of a relation given the evidence P (Rij|Eij), as in (Snow et al 2006), by using the same set of features as in ME. An extension to WordNet was presented by (Snow et al, 2006). Snow et al (2006) use syntactic path patterns as features for supervised hyponymy and synonymy classifiers, whose training examples are derived automatically from WordNet. Following the spirit of the fine-grained human evaluation in (Snow et al, 2006), we randomly sampled 800 rules from our rule-base and presented them to an annotator who judged them for correctness, according to the lexical reference notion specified above. We observed that the likelihood of nouns mentioned in a definition to be referred by the concept title depends greatly on the syntactic path connecting them (which was exploited also in (Snow et al, 2006)). For example, (Snow et al 2006) proposed to estimate taxonomic structure via maximizing the overall likelihood of a taxonomy. More recently, Snow et al (2005) and Snow et al (2006) have described a method of hypernymy extraction using machine learning of 53 patterns. Due to the importance of WN for NLP tasks, substantial research was done on direct or indirect automated extension of the English WN (e.g., (Snow et al, 2006)) or WN in other languages (e.g., (Vintar and Fiser, 2008)).
Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora.  The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier.  Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC.  The iterative training algorithm described above is adopted from Klementiev and Roth (2006). (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006). Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information. Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles.  For Russian, we compare to the model presented in (Klementiev and Roth, 2006b), a weakly supervised algorithm that uses both phonetic information and temporal information. We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006). More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity.
To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). (Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% F measure. For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel.
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn's WSJ10 which contains 7422 sentences ? 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ? 10 words after removing punctuation. All trees in the test set were binarized beforehand, in the same way as in Bod (2006). Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models.
For more information on the TE system described in this section, please see (Hickl et al, 2006b) and (Harabagiu and Hickl, 2006). Following (Harabagiu and Hickl, 2006), we used TE information in order to filter answers identified by the Q/A system that were not entailed by the user's original question. While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al, 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. In order to improve QA systems' performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc. Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006). Instead of matching headline and first sentence of the document as in (Harabagiu and Hickl, 2006), we followed a different approach. In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al, 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al, 2009), when used for filtering and ranking answers. For the task of Question Answering, (Harabagiu and Hickl, 2006) applied a TE component to rerank candidate answers returned by a retrieval step. Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al, 2009), (Mirkin et al, 2009). This includes finding question answer pairs (Cong et al, 2008) from online forums, auto-answering queries on a technical forum (Feng et al, 2006), ranking answers (Harabagiu and Hickl, 2006) etc. Being a challenging task, it has been shown that it is helpful to applications like question answering (Harabagiu and Hickl, 2006). The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al, 2006), information extraction (Romano et al, 2006), and document summarization (Lloret et al, 2008). TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al, 2006) and question answering (Harabagiu and Hickl, 2006). Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009).
KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. For details please refer to (Kate and Mooney, 2006). Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). 
If we analyze these three models in terms of expressive power, the Galley et al (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang's model. The xRS formalism utilized by Galley et al (2006) allows for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases. The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et als (2006) and can easily exploit and expand on previous research in phrase-based machine translation.  Syntax-driven (Galley et al, 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. (Galley et al, 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al, 2006).  Six MT systems were combined: three (A, C, E) were phrase based similar to (Koehn, 2004), two (B, D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al, 2006). In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al, 2006). GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al, 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). Using these alignments, which we refer to as GIZA++ union + link deletion, we train a syntax-based translation system similar to that described in (Galley et al., 2006). For example, Galley et al (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al, 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. Galley et al (2006 )argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling. For example, Galley et al (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. Following Galley et al (2006)'s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. As shown in the following parts of this paper, it works very well with the existing techniques, such as rule composing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). In this work, the issue of translation rule extraction is studied in the string-to-tree model proposed by Galley et al (2006). Finally, the rule composing method (Galley et al, 2006) is used to compose two or more minimal GHKM or SPMT rules having shared states to form larger rules. Our baseline MT system is built based on the string-to-tree model proposed in (Galley et al, 2006).
Wellington et al (2006), in a more systematic study, find that, of sentences where the tree-to-tree constraint blocks rule extraction, the majority are due to parser errors. Wellington et al. (2006) reports that discontinuities are very useful for translational equivalence analysis using binary branching structures under word alignment and parse tree constraints while they are almost of no use if under word alignment constraints only. In this sense, our model behaves like a phrase based model, less sensitive to discontinuous phrases (Wellington et al, 2006). We propose SG-ITG that follows Wellington et al (2006)'s suggestion to model at most one gap. Grammar rules extracted from large parallel corpora by systems such as Galley et al (2004) can be quite large, and Wellington et al (2006) argue that complex rules are necessary by analyzing the coverage of gold-standard word alignments from different language pairs by various grammars. The methodology in Wellington et al (2006) measures the complexity of word alignment using the number of gaps that are necessary for their synchronous parser which allows discontinuous spans to succeed in parsing. Wellington et al (2006) indicate the necessity of introducing discontinuous spans for synchronous parsing to match up with human-annotated word alignment data. Wellington et al (2006) treat many-to-one word links disjunctively in their synchronous parser. We use the same alignment data for the five language pairs Chinese/English, Romanian/English, Hindi/English, Spanish/English, and French/English (Wellington et al, 2006). Wellington et al (2006) did a similar analysis on the English-English bitext. Translational equivalence is a mathematical relation that holds between linguistic expressions with the same meaning (Wellington et al, 2006). Wellington et al (2006) describes their study of the patterns of translational equivalence exhibited by a variety of bilingual/monolingual bitexts. Fox (2002), Galley et al (2004) and Wellington et al (2006) examine TEM only. Thus, it may not suffer from the issues of non-isomorphic structure alignment and non-syntactic phrase usage heavily (Wellington et al, 2006). (Wellington et al, 2006) argue that these restrictions reduce our ability to model translation equivalence effectively. In Wellington et al (2006), hand-aligned data are used to compare the standard ITG constraints to ITGs that allow gaps. In particular, Wellington et al (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. Wellington et al (2006) argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two. We use the same alignment data for the five language pairs Chinese-English, Romanian-English, Hindi-English, Spanish-English, and French-English as Wellington et al (2006). The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntax based machine translation systems such as Wu (1997), Zhang et al (2006) and Chiang (2007), in terms of what alignments they induce, has been discussed in Wu (1997) and Wellington et al (2006), but with a one-sided focus on so-called inside-out alignments.
Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form.  The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a). Growing discounts of this sort were previously suggested by the model of Teh (2006). Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006).    Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006). The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem. Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution. There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006).  Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings.
In recent years, Akkaya et al (2009) report a successful empirical result where WSD helps improving sentiment analysis, while Wiebe and Mihalcea (2006) study the distinction between objectivity and subjectivity in each different sense of a word, and their empirical effects in the context of sentiment analysis.  As an example, Wiebe and Mihalcea (2006) prove that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivity ambiguous words (such as positive). Wiebe and Mihalcea (2006) label word senses in WordNet as subjective or objective. We follow Wiebe and Mihalcea (2006) in that we see subjective expressions as private states that are not open to objective observation or verification. First, (Wiebe and Mihalcea, 2006) provide evidence that word sense labels, together with contextual subjectivity analysis, can be exploited to improve performance in word sense disambiguation. We adopt the definitions of subjective and objective from Wiebe and Mihalcea (2006) (hereafter WM).  We adopt the definitions from (Wiebe and Mihalcea, 2006), who describe the annotation scheme as follows. As noted in (Wiebe and Mihalcea, 2006), sentences containing objective senses may not be objective.  Both (Wiebe and Mihalcea, 2006) and (SuandMarkert, 2008) show that even reliable subjectivity clues have objective senses. They have shown that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses (Wiebe and Mihalcea, 2006). Wiebe and Mihalcea (2006) and Su and Markert (2008) both show that this is a well-defined concept via human annotation as well as automatic recognition.   Towards this, Wiebe and Mihalcea (2006) conduct a study on human annotation of 354 words senses with polarity and report a high inter-annotator agreement. (Wiebe and Mihalcea, 2006) define subjective expressions as words and phrases being used to express mental and emotional states, such as speculations, evaluations, sentiments, and beliefs. However, such clues often have both subjective and objective senses, as illustrated by (Wiebe and Mihalcea, 2006). 
This metaphor is, nonetheless, the one resorted to in (Aw et al, 2006), which uses a statistical phrase-based machine translation tool to convert English SMS texts into standardized English. Using this system, (Aw et al, 2006) reports a 0.81 BLEU (Papineni et al, 2001) score on a set of 5,000 English SMS. The corresponding BLEU score is close to 0,8, in line with the findings of (Aw et al, 2006) for English, and comparing favorably with the 0.68 score reported in (Guimierde Neef et al., 2007) (for French, using a different test bed). Following (Aw et al, 2006), we found that using off-the-shell statistical MT systems allows to achieve very satisfactory WER; combining this system with a system based on an analogy with the speech recognition problem yields an additional 1.5 absolute improvement in WER. (Aw et al, 2006) proposed an approach for normalizing Short Messaging Service (SMS) texts by translating it into normalized forms using Phrase-based SMT techniques on character level.  Parsing performance on noisy data can be improved by transforming the input data so that it resembles the parser's training data (Aw et al, 2006), transforming the training data so that it resembles the input data (van der Plas et al, 2009), applying semi supervised techniques such as the self-training protocol used by McClosky et al (2006), and changing the parser internals, e.g. adapting the parser's unknown word model to take into account variation in capitalisation and function word misspelling.  On this basis, Aw et al (2006) proposed a statistical machine translation model working at the phrase-level, by splitting sentences into their k most probable phrases.  In the case of text messages, text-to-speech synthesis may be particularly useful for the visually impaired; automatic translation has also been considered (e.g., Aw et al, 2006). Aw et al (2006) model text message normalization as translation from the texting language into the standard language. We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4 before, which tend to be considered beyond the remit of spell checking (Aw et al, 2006). For example, Aw et al (2006) propose a phrase-level SMT SMS normalisation method with bootstrapped phrase alignments. We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al (2006) as benchmark methods. We compared the results using both the baseline model and the model implemented using the same training data as in Aw et al (2006).   (Aw et al, 2006) adapted a phrase-based MT model for normalizing SMS and achieved satisfying performance. 
Briscoe and Carroll (2006) discuss issues raised by this re annotation. Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). The lemmatization was done by RASP (Briscoe and Carroll, 2006). We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006).  Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant.  C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006).
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. (Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used.
First, well-nestedness is interesting as a generalization of projectivity (Marcus, 1967) while more than 23% of the 73088 dependency structures in the Prague Dependency Treebank of Czech (Hajic et al., 2001) are non-projective, only 0.11% are not well-nested (Kuhlmann and Nivre, 2006). Mildly non-projective trees are of both theoretical and practical interest, as they correspond to derivations in Lexicalized Tree Adjoining Grammar (Bodirsky et al, 2005) and cover the overwhelming majority of sentences found in tree banks for Czech and Danish (Kuhlmann and Nivre, 2006). In this paper, we consider all constraints and measures evaluated by Kuhlmann and Nivre (2006) with some minor variations. None of the constraints and measures in Kuhlmann and Nivre (2006) take into account levels of nodes explicitly. They confirm the findings of Kuhlmann and Nivre (2006): planarity seems to be almost as restrictive as projectivity; well-nestedness, on the other hand, covers large proportions of trees in all languages. This supports our theoretical results and confirms that properties of non-projective edges provide a more accurate as well as expressive means for describing non projective structures in natural language than the constraints and measures considered by Kuhlmann and Nivre (2006). Kuhlmann and Nivre (2006) compare several constraints on dependency structures and among the considered ones find well-nestedness to be in good accord with empirical data. Kuhlmann and Nivre (2006) claim that the constraint of well-nestedness seems to approximate well dependency structures occurring in natural language. Unlike acyclicity and the single head constraints, which impose restrictions on the dependency relation as such, projectivity constrains the interaction between the dependency relations and the order of the nodes in the sentence (Kuhlmann and Nivre, 2006). Following (Kuhlmann and Nivre, 2006), we call this edge degree to avoid confusion.  This work further corroborates Kuhlmann's work on Czech (PDT) for Hindi (Kuhlmann and Nivre, 2006). Recent work identifies two properties that appear particularly relevant to the characterization of graph-based dependency models of syntactic structure: the absence of interleaving substructures (well-nestedness) and a bound on a type of discontinuity (gap-degree <= 1) successfully describe more than 99% of the structures in two dependency tree banks (Kuhlmann and Nivre 2006) . Relevant results from Kuhlmann and Nivre (2006). See Kuhlmann and Nivre (2006) for the definition of edge degree. If we suppose that the characterization of dependency structures as reported by Kuhlmann and Nivre (2006) for Czech and Danish extends cross-linguistically, i.e. the dependency structures for natural language falls within the class of well-nested and gap degree <= 1 dependency structures, then MC-TAG appears to correspond to the wrong class of model-theoretic dependency structures. This keeps the corresponding graph drawings within the class of structures identified by Bodirsky et al (2005) as a model of TAG derivations, and by Kuhlmann and Nivre (2006) as empirically relevant. While the number of highly non-projective dependency structures is negligible for practical applications (Kuhlmann and Nivre, 2006), the rank can not easily be bounded. It is interesting to compare our approach with techniques for well-nested dependency trees (Kuhlmann and Nivre, 2006). Alternative notions of mildly non-projective dependency structures are explored in Kuhlmann and Nivre (2006).
Our work is related to previous work on domain independent unsupervised relation extraction, in particular Sekine (2006), Shinyama and Sekine (2006) and Banko et al (2007). Sekine (2006) introduces On-demand information extraction, which aims at automatically identifying salient patterns and extracting relations based on these patterns. Shinyama and Sekine (2006) apply NER, coreference resolution and parsing to a corpus of newspaper articles to extract two-place relations between NEs. Like Sekine (2006) and Shinyama and Sekine (2006), we concentrate on relations involving NEs, the assumption being that these relations are the potentially interesting ones. Romano et al (2006) and Sekine (2006) used syntactic paraphrases to obtain patterns for extracting relations. This contrasts with Open Information Extraction (Banko and Etzioni, 2008) and On-Demand Information Extraction (Sekine, 2006), which aim to extract large databases of open-ended facts, and with supervised relation extraction, which requires additional supervised data to learn new relations. For instance, Sudo et al (2003) and Sekine (2006) proposed different methods for automatic IE pattern acquisition for a given domain based on frequent subtree discovery in dependency parse trees. To reduce the knowledge engineering burden on the user in constructing and porting an IE system, unsupervised learning has been utilized, e.g. Riloff (1996), Yangarber et al (2000), and Sekine (2006).
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006).  Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface.  In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems.  An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list.
 In Biemann (2006b), the tagger output was directly compared to supervised taggers for English, German and Finnish via information-theoretic measures. We train SVM Tool and an unsupervised tagger, Unsupos (Biemann, 2006), on our training sections and apply them to the development, test and unlabeled sections. Nevertheless, many of the practically useful spell checkers incorporate context information and the current analysis on SpellNet can be extended for such spell-checkers by conceptualizing a network of words that capture the word co-occurrence patterns (Biemann, 2006).  Previous graph-theoretic work (Biemann, 2006) uses order 1 representations. In order to test the argument above, and as an attempt to improve the results from the previous experiment, POS-tags were induced using Biemann's unsupervised POS-tagger (Biemann, 2006). Additionally, we used an unsupervised part-of-speech tagger (see (Biemann, 2006)) to tag the NEGRA corpus to be able to present a complete unsupervised parsing process relying on word strings only. Mintz (2003) only uses the most frequent 45 frames and Biemann (2006) clusters the most frequent 10,000 words using contexts formed from the most frequent 150-200 words.    Perhaps due to the overly simplistic methods employed to compute morphological information, morphology has only been used as what Biemann (2006) called add-on's in existing POS induction algorithms, which remain primarily distributional in nature. Biemann (2006) described a graph-based clustering methods for word classes.
Its main components are the Conditional Random Fields toolkit MALLET (McCallum, 2002) and the RASP syntactic parsing toolkit (Briscoe et al, 2006), which are both publicly available. In brief, the abstracts of 16,609 articles curated by FlyBase were retrieved and tokenized by RASP (Briscoe et al, 2006). The parameters of the model were estimated from the British National Corpus that was parsed using the RASP parser of Briscoe et al (2006). We parsed the corpus with Rasp (Briscoe et al, 2006) and with the Stanford PCFG parser (Klein & Manning, 2003). The RASP toolkit (Briscoe et al, 2006) is used for sentence boundary detection, tokenisation, PoStagging and finding grammatical relations (GR) between words in the text. The search space for metaphor identification was the British National Corpus (BNC) that was parsed using the RASP parser of Briscoe et al (2006). The Preiss system extracts a verb instance's GRs using the Rasp general-language unlexicalized parser (Briscoe et al, 2006) as input, and based on hand crafted rules, maps verb instances to a predefined inventory of 168 SCFs. Our input has been parsed into Rasp-style tGRs (Briscoe et al, 2006), which facilitates comparison with previous work based on the same data set. Since the parser that produced them is known to perform well on general language (Briscoe et al, 2006), the tGRs are of high quality: it makes sense that reverting to the pg Rs is unnecessary in this case.  All models were trained on the 90-million word written component of the British National Corpus, lemmatised, POS-tagged and parsed with the RASP toolkit (Briscoe et al., 2006). All features are automatically extracted from the Robust Minimal Recursion Semantics (RMRS, Copestake, 2004) representation of the sentence in which the noun phrase appears (obtained via a RASP parse, Briscoe et al, 2006). The corpus used for our distributional similarity baseline consists of a subset of Wikipedia to talling 500 MB in size, parsed first with RASP (Briscoe et al, 2006) and then into a Robust Minimal Recursion Semantics form (RMRS, Copes take, 2004) using a RASP-to-RMRS converter. We expect our reimplementation of the method to extract data more accurately, since we use a more robust parser (RASP (Briscoe et al, 2006)), take into account more syntactic structures (coordination, passive), and extract our data from a newer version of the BNC. The parameters of the model were estimated from the British National Corpus (BNC) (Burnard, 2007) that was parsed using the RASP parser of Briscoe et al (2006). In addition we use a version annotated with the Rasp system (Briscoe et al, 2006), that tokenizes, tags, lemmatizes and parses the input sentences, outputting syntactic trees and then adding grammatical relations (GR) as described by (Buttery and Korhonen, 2005). Conjunctions are identified in the BNC by first parsing the corpus with Rasp (Briscoe et al, 2006) and extracting in stances of the conj grammatical relation. Several methods exist to do this - e.g. producing RMRS output from RASP (Briscoe et al, 2006) is described in Frank (2004). The only major differences with (Furstenau and Lapata, 2009) are the dependency parser which was used (the MALT parser (Nivre et al, 2006) instead of the RASP parser (Briscoe et al, 2006)) and the corpus employed to learn semantic similarities (the Reuters corpus instead of the British National Corpus). We parsed the BNC corpus with the RASP parser (Briscoe et al, 2006) and used it for feature extraction.
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features.  This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments.  We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. 
Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). Such self-translation techniques have been introduced by Ueffing et al (2007). In follow up work, this approach was refined (Ueffing et al, 2007). Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007).
 A previous implementation of the IMS system, NUS-PT (Chan et al, 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and fine grained task, respectively. For example, (Chan et al, 2007) trained a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier.  Chan et al (2007) use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models.  A similar approach has been tried in the word-sense disambiguation (WSD) domain where local but also across-sentence unigram collocations of words are used to refine phrase pair selection dynamically by incorporating scores from the WSD classifier (Chan et al, 2007).  It has long been believed that being able to detect the correct sense of a word in a given context - performing word sense disambiguation (WSD) - will lead to improved performance of systems tackling high end applications such as machine translation (Chan et al, 2007) and summarization (Elhadad et al., 1997). To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6). We use a set of word context features motivated by word sense disambiguation (Chan et al, 2007) to test scalability. Independent of these lexical substitution tasks, the connection between word senses and word translation has been explored in Chan et al (2007) and Carpuat and Wu (2007), who predict the probabilities of a target word being translated as an item in a sense inventory, where the sense inventory is a list of possible translations. Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-artWSD system into a hierarchical phrase-based system (Chiang, 2005).  Chan et al (2007) incorporated a WSD system into the hierarchical SMT system, Hiero (Chiang, 2005), and reported statistically significant improvement.  This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al, 2007). See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT.  We note that the best performing system (Chan et al, 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity.
Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus.  In building training dataset by active learning, we use uncertainty sampling like (Chan and Ng, 2007) (Figure 1 line 30-31). Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where Inter Annotator Agreement (IAA) is low and thus the consistency of the human annotations decreases. Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al, 2007), parsing (Tang et al, 2002) and word sense disambiguation (Chan and Ng, 2007). Here, we take inspiration from the target-word specific results reported by Chan and Ng (2007) where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data.  Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al (2005) and Agirre et al (2009b). Our main inspiration comes from the target word specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning.
Since we approach decoding as xR transduction, the process is identical to that of constituency based algorithms (e.g. Huang and Chiang, 2007). Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Thus they cannot integrate LM scoring into their decoding, requiring them to rescore the decoder output with a variant of cube growing (Huang and Chiang, 2007). A hyper graph is analogous to a parse forest (Huang and Chiang, 2007). Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). Huang and Chiang (2007) searches with the full model, but makes assumptions about the the amount of reordering the language model can trigger in order to limit exploration. Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). In our experiments, we use the cube pruning algorithm (Huang and Chiang, 2007) to carry out the search. Tree-to-string decoding with STSG is usually treated as forest rescoring (Huang and Chiang, 2007) that involves two steps. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). In hierarchical phrase-based translation (Chiang, 2005) a weighted synchronous context-free grammar is induced from parallel text, the search is based on CYK+ parsing (Chappelier and Rajman, 1998) and typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007). Furthermore, language model integration becomes more expensive here since the decoder now has to maintain target-language boundary words at both ends of a sub translation (Huang and Chiang, 2007), whereas a phrase-based decoder only needs to do this at one end since the translation is always growing left-to-right. The complexity of this dynamic programming algorithm for g-gram decoding is O (2nn2|V|g-1) where n is the sentence length and |V| is the English vocabulary size (Huang and Chiang, 2007). An alternative approach to computing a synchronous parse forest is based on cube pruning (Huang and Chiang, 2007). We utilize the cube pruning algorithm (Huang and Chiang, 2007) for decoding and optimize the model weights with MERT. Per Non-Terminal Pruning The decoder uses a combination of beam and cube-pruning (Huang and Chiang, 2007). Huang and Chiang (2007) describe a variation of cube pruning called cube growing, and they apply it to a source-tree to target string translator.
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. Our model builds on the architecture of Erk (2007). Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it.  Selectional preferences are computed as in Erk (2007). In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). We implemented the current state-of-the-art smoothing model of Erk (2007). The Train size is approximately the same size used in Erk (2007), although on a different corpus. These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL.   The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007).
(Davidov et al, 2007) introduce the use of term frequency patterns for relationship discovery. For example, Pantel and Pennacchiotti (2008) linked instantiations of a set of semantic relations into existing semantic ontologies and Davidov et al (2007) employed seed concepts from a given semantic class to discover relations shared by concepts in that class. As a pre-requisite to extracting relations among pairs of classes, the method described in (Davidov et al., 2007) extracts class instances from unstructured Web documents, by submitting pairs of instances as queries and analyzing the contents of the top 1,000 documents returned by a Web search engine. The evaluation methodology is also quite different, as the instance sets acquired based on the input seed instances in (Davidov et al, 2007) are only evaluated for three hand-picked classes, with precision scores of 90% for names of countries, 87% for fish species and 68% for instances of constellations. (Davidov et al, 2007) proposed a method for unsupervised discovery of concept specific relations, requiring initial word seeds. Pattern driven search engine queries allow to access such information and gather the required data very efficiently (Davidov et al, 2007). For example, (Etzioni et al, 2004) discovered a set of countries and (Davidov et al, 2007) discovered diverse country relationships, including location relationships between a country and its capital and a country and its rivers. Following (Davidov et al, 2007) we seek symmetric patterns to retrieve concept terms. To use clusters for classification we define a HITS measure similar to that of (Davidov et al, 2007), reflecting the affinity of a given nominal pair to a given cluster. Davidov et al (2007) developed a web mining approach for discovering relations in which a specified concept participates based on clustering patterns in which the concept words and other words appear. The precision observed for this task is comparable to precision obtained for Country-Capital and Country-Language in a previous single-language acquisition study (Davidov et al, 2007). On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al, 2004), and the discovery of concept-specific relationships (Davidov et al, 2007). Our web mining part follows common pattern based retrieval practice (Davidov et al, 2007). Following (Davidov et al, 2007), we seek symmetric patterns to retrieve concept terms.
For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing.  The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB.  Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing.    We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing.
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C & C CCG parser (Clark and Curran, 2007). The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG super tagger; and three, a hierarchical level consisting of CCG derivations. The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a).
For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data.  We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account.  Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). (Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007).  Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation.  
We achieve competitive performance in comparison to alternate model families, in particular generative models such as MRFs trained with EM (Haghighi and Klein, 2006) and HMMs trained with soft constraints (Chang et al, 2007). Another recent method that has been proposed for training sequence models with constraints is Chang et al (2007).  Most constraints that prove useful for SRL (Chang et al., 2007) also require customization when applied to a new language, and some rely on language specific resources, such as a valency lexicon. Constraint driven learning (CoDL) was first introduced in Chang et al [2007], and has been used also in Chang et al [2008].  Likewise, Chang et al (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. Chang et al (2007) use a set of domain specific rules as automatic implicit feedback for training information extraction system. We compare our CRF model integrated with VE with two state-of-the-art models, i.e., constraint driven learning (Chang et al, 2007) and generalized expectation criteria (Mann and McCallum, 2008). Constraint-driven learning (Chang et al, 2007) expresses several kinds of constraints in a unified form.  (Chang et al 2007) incorporates domain specific constraints in semi-supervised learning. The learning algorithm in Figure 2 is an instance of augmented-loss training (Hall et al, 2011) which is closely related to the constraint driven learning algorithms of Chang et al (2007). Note that the objective function in Equation 5, if written in the additive form, leads to a cost function reminiscent of the one used in constraint-driven learning algorithm (CoDL) (Chang et al, 2007) (and similarly, posterior regularization (Ganchev et al, 2010), which we will discuss later at Section 6). Constraint-driven learning (CoDL) (Chang et al, 2007) and posterior regularization (PR) (Ganchev et al., 2010) are both primarily semi-supervised models. Most semi-supervised learning algorithms rely on marginals (GE, Mann and McCallum, 2008) or MAP assignments (CODL, Chang et al, 2007). corresponds to constraint satisfaction weights used in (Chang et al, 2007). Chang et al propose constraint-driven learning (CODL, Chang et al, 2007) which can be interpreted as a variation of self-training: Instances are selected for supervision based not only on the model's prediction, but also on their consistency with a set of user-defined constraints. We use the same token label constraints as Chang et al (2007). We also report supervised results from (Chang et al, 2007) and SampleRank.
This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al (2007). For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation. Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. Supertagging (Hassan et al, 2007b): incorporating lexical syntactic descriptions, in the form of supertags, to the language model and target side of the translation model in order to better inform decoding. We have previously shown this approach to be very effective for both case and punctuation restoration (Hassan et al, 2007a). (Huang and Knight, 2006) and (Hassan et al, 2007) introduce relabeling and supertagging on the target side, respectively. Two kinds of supertags, from Lexicalized Tree Adjoining Grammar and Combinatory Categorial Grammar (CCG), have been used as lexical syntactic descriptions (Hassan et al, 2007) for phrase based SMT (Koehn et al, 2007). Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side, translating a morphologically-poor language (English) to a morphologically-rich language (Greek). Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. Hassan et al (2007) improve the statistical phrase based MT model by injecting supertags, lexical information such as the POS tag of the word and its subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. Hassan et al (2007) and Birch et al (2007) use supertag n-gram LMs. This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al (2007) and Hassan et al (2007)). A similar approach based on supertagging was proposed by Hassan et al (2007). They used both CCG supertags and LTAG supertags in Arabic-to-English phrase-based translation and have reported about 6% relative improvement in BLEU scores. Hassan et al (2007) noticed that the target side POS sequences could be scored, much as we do in this work.
We build our confusion networks using the method of Rosti et al (2007), but, instead of forming alignments using the tercom script (Snover et al, 2006), we create alignments that minimize invWER (Leusch et al, 2003), a form of edit distance that permits properly nested block movements of substrings. For this, Rosti et al (2007) use the tercom script (Snover et al, 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another. ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al (2007). Note that the algorithm of Rosti et al (2007) used n-best lists in the combination. In our experience, this approach is advantageous in terms of translation quality, e.g. by 0.7% in BLEU compared to a minimum Bayes risk primary (Rosti et al., 2007). Similar to the features in Rosti et al (2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.  A multiple CN or super-network framework was firstly proposed in Rosti et al (2007) who used each of all individual system results as the backbone to build CNs based on the same alignment metric, TER (Snover et al, 2006). The subnetworks in the latter approach may be weighted by prior probabilities estimated from the alignment statistics (Rosti et al, 2007a). This can be seen as a simplified version of (Rosti et al, 2007b). This method can also be viewed to be a hypotheses reranking model since we only use the existing translations instead of performing decoding over a confusion network as done in the word-level combination method (Rosti et al, 2007). If the systems generated N best hypotheses, a fractional increment could be added to these vectors as in (Rosti et al, 2007). Powell's method (Press et al, 2007) on N -best lists was used in system combination weight tuning in Rosti et al (2007). We re-implemented a state-of-the-art system combi nation method (Rosti et al, 2007).  The current state-of-the-art is confusion-network based MT system combination as described by Rosti and colleagues (Rosti et al, 2007a, Rosti et al., 2007b).  Similar to (Rosti et al, 2007), each word in the confusion network is associated with a word posterior probability. The availability of the TER software has made it easy to build a high performance system combination baseline (Rosti et al, 2007). The hypothesis scores and tuning are identical to the setup used in (Rosti et al, 2007).
Seginer (2007) has an incremental parsing approach using a novel representation called common-cover-links, which can be converted to constituent brackets. In addition, we also analyze CCM's sensitivity to initialization, and compare our results to Seginer's algorithm (Seginer, 2007). We also compare our method to the algorithm of Seginer (2007). The parser of Seginer (2007) performs slightly better on CTB 5.0 sentences no more than 10 words, but obviously falls behind on sentences no more than 40 words.  Incremental refers to the results reported in Seginer (2007). We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) directly from words (Seginer, 2007). As is customary in unsupervised parsing work (e.g. (Seginer, 2007)), we bounded sentence length by 10 (excluding punctuation). As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sentence (Seginer, 2007). As is customary in unsupervised parsing (e.g. (Seginer, 2007)), we bounded the lengths of the sentences in the corpus to be at most 10 (excluding punctuation). We start by parsing the corpus using the Seginer parser (Seginer, 2007). For example, the Seginer (2007) parser achieves an F-score of 75.9% on the WSJ10 corpus and 59% on the NEGRA10 corpus, but the percentage of individual sentences with an F-score of 100% is 21.5% for WSJ10 and 11% for NEGRA10. The unsupervised parser we use is the Seginer (2007) incremental parser, which achieves state-of-the-art results without using manually created POS tags. The incremental parser of (Seginer, 2007) does not give any prediction of its output quality, and extracting such a prediction from its internal data structures is not straightforward.  The parser we use is the incremental parser of (Seginer, 2007), POS tags are induced using the unsupervised POS tagger of ((Clark, 2003), neyessen morph model). Seginer (2007)'s common cover links model (CCL) does not need any prior tagging and is applied on word strings directly. As most unsupervised parsing models (except (Seginer, 2007)), we apply the hand annotated data of the NEGRA corpus. An exception which learns from raw text and makes no use of POS tags is the common cover link sparser (CCL, Seginer 2007). Though punctuation is usually entirely ignored in unsupervised parsing research, Seginer (2007) departs from this in one key aspect: the use of phrasal punctuation - punctuation symbols that often mark phrasal boundaries within a sentence.
We used the best-performing model that fuses HCRF-Coarse and the supervised model (McDonald et al2007) by interpolation. McDonald et al (2007) also dealt with sentiment analysis, via the global joint-structural approach. More recently, McDonald et al (2007) have investigated a model for jointly performing sentence and document-level sentiment analysis, allowing the relationship between the two tasks to be captured and exploited. The data used in our initial English-only experiments were a set of 554 consumer reviews described in (McDonald et al, 2007). Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document and sentence-level classification accuracy (McDonald et al, 2007). Alternative approaches include explicitly ac counting for this structure by treating subjective sentence extraction as a sequence-labeling problem, such as in McDonald et al (2007). McDonald et al (2007) propose a model which jointly identifies global polarity as well as paragraph and sentence-level polarity, all of which are observed in training data. While our approach uses a similar hierarchy, McDonald et al (2007) is concerned with recovering the labels at all levels, whereas in this work we are interested in using latent document content structure as a means to benefit task predictions. Subsequently, many other studies make efforts to improve the performance of machine learning-based classifiers by various means, such as using subjectivity summarization (Pang and Lee, 2004), seeking new superior textual features (Riloff et al, 2006), and employing document subcomponent information (McDonald et al, 2007). NGram Back-off Features: Similar to McDonald et al (2007), we utilize backed-off versions of lexical bi grams and trigrams, where all possible combinations of the words in the ngram are replaced by their POS tags, creating features such as w j POS k, POS j w k, POS j POS k for each lexical bigram and similarly for trigrams. Most recently, McDonald et al (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.  Moreover, the assigned tag applies to the whole blog post while a finer grained sentiment extraction is needed (McDonald et al, 2007). McDonald et al (2007) later showed that jointly learning fine-grained (sentence) and coarse-grained (document) sentiment improves predictions at both levels. McDonald et al (2007) introduced a fully supervised model in which predictions of coarse-grained (document) and fine-grained (sentence) sentiment are learned and inferred jointly. But even in such approaches, McDonald et al (2007) note that information about the overall sentiment orientation of a document facilitates more accurate extraction of more specific information from the text. Quantitatively, subjective sentences in the product reviews amount to 78% (McDonald et al, 2007), while subjective sentences in the movie review dataset are only about 25% (Mao and Lebanon, 2006). McDonald (McDonald et al 2007) has reported some success mixing fine and course labeling in sentiment analysis. Most recently, McDonald et al (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. For example, with CRFs, Zhao et al (2008) and McDonald et al (2007) performed sentiment classification in sentence and document level.
For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. Our approach results in competitive domain adaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCLmethod (Blitzer et al, 2007). On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). We use the dataset from (Blitzer et al, 2007) for sentiment classification.
Recently (Riezler et al, 2007) used statistical machine translation for query expansion and took a step towards bridging the lexical gap between questions and answers. Riezler et al (2007) define the problem of answer retrieval from FAQ and social Q/A websites as a query expansion problem. With the exception of the query expansion approaches (Riezler et al, 2007), all works discussed here use some form of noisy-channel model (translation model and target language model) but do not perform the decoding part of the SMT process to generate translations, nor use the rich set of features of a full SMT. Furthermore, we plan to include the Level 1 translations into the candidate answer generation module in order to do query expansion in the style of Riezler et al (2007). Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al, 2007) or queries and clicked snippets from query logs (Riezler et al, 2008). (Riezler et al, 2007) adopted an SMT-based method to query expansion in answer retrieval. Our work is also related to that of Riezler et al (2007) where SMT-based query expansion methods are used on data from FAQ pages. Riezler et al (2007) demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web. Besides, Riezler et al (2007) and Zhou et al (2011) proposed the phrase-based translation models for question and answer retrieval.
Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated. There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error. We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). For language modeling, we use RandLM (Talbot and Osborne, 2007). The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately.
In this section, we show that many relationships are consistently expressed using a compact set of relation-independent lexico-syntactic patterns, and quantify their frequency based on a sample of 500 sentences selected at random from an IE training corpus developed by (Bunescu and Mooney, 2007). The first two datasets were collected from the Web, and made available by Bunescu and Mooney (2007). To resolve this problem, Bunescu and Mooney (2007), Riedel et al (2010) and Yao et al (2010) relaxed the DS assumption to the at least-one assumption and employed multi-instance learning techniques to identify wrongly labeled instances. Such data sets have been utilized successfully for relation extraction from the web (Bunescu and Mooney, 2007). Bunescu and Mooney (2007) follow a classification-based approach to RE. One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. We used the dataset by Bunescu and Mooney (2007), which we selected because it contains multiple realizations of an entity pair in a target semantic relation, unlike similar datasets such as the one by Roth and Yih (2002). One heuristic is to assume that each candidate mention tuple of a training fact is indeed expressing the corresponding relation (Bunescu and Mooney, 2007). Notable exceptions include Rosario and Hearst (2005) and Bunescu and Mooney (2007), who tackle relation classification and extraction tasks by considering the set of contexts in which the members of a candidate relation argument pair co-occur. The dataset was built following the approach of Bunescu and Mooney (Bunescu and Mooney, 2007). Bunescu and Mooney (2007) presented an approach to extract relations from the Web using minimal supervision. Bunescu and Mooney (2007) connect weak supervision with multi-instance learning and extend their relational extraction kernel to this context.  Bunescu and Mooney (2007) and Riedel et al (2010) model distant supervision for relation extraction as a multi-instance single-label problem, which allows multiple mentions for the same tuple but disallows more than one label per object. As pointed out by (Bunescu and Mooney, 2007), even though the same entities co-occur in multiple sentences, they are not necessarily linked by the same relationship in all of them. One means of combating this is suggested by (Bunescu and Mooney, 2007).
Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). To overcome the problem, Li et al (2007) proposed k-best approach. A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007).  Li et al (2007) used a parser to get the syntactic tree of the source language sentence.
A similar idea exists in machine translation where English is frequently used to pivot between other languages (Cohn and Lapata, 2007). In statistical machine translation (SMT), methods that incorporate translations from other languages (Cohn and Lapata, 2007) have proven effective in low-resource situations: when phrase translations are unavailable for a certain language, one can look at other languages where the translation is available and then translate from that language. Pivoting can finally be used to fix or improve the translation model: (Cohn and Lapata, 2007) augments the phrase table for a baseline bilingual system with supplementary phrases obtained by pivoting into a third language. One of them is often called triangulation and usually refers to the combination of phrase tables (Cohn and Lapata, 2007). For instance, (Cohn and Lapata, 2007) explore the use of triangulation for machine translation, where multiple translation models are learned using multilingual parallel corpora. This finding might also carry over to phrase-table triangulation (Cohn and Lapata, 2007), where multi-parallel data is used in training to augment a standard translation system. Cohn and Lapata (2007) explores how to utilize multilingual parallel data (rather than pivot data) to improve translation performance.
The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known: see Besag (2004) for a tutorial introduction and Goldwater and Griffiths (2007) for an application of Gibbs sampling to HMM inference for semi 300 supervised and unsupervised POS tagging. This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007). Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here. We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs.  We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing: raise the probabilities in the sampling distribution to the 1T power, where T is a temperature parameter, decrease T to wards zero, and take a single sample. We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model. In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model. In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models. Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers.
For our POS tagging experiments, we used the Wall Street Journal in PTB III (Marcus et al, 1994) with the same data split as used in (Shen et al., 2007). In POS tagging, the previous best performance was reported by (Shen et al, 2007) as summarized in Table 7. Feature templates are shown in Table 3, which are based on those of Ratnaparkhi (1996) and Shen et al (2007). For the experimental evaluations we use the Bidirectional Tagger with Guided Learning presented in Shen et al (2007). For the implementation, we used bpos (Shen et al., 2007) for the POS tagging. For comparison, our best model, the PLMRF, achieved a 96.8% in-domain accuracy on sections 22-24 of the Penn Treebank, about 0.5% shy of a state-of-the-art in-domain system (Shen et al, 2007) with more sophisticated supervised learning. The idea of bidirectional parsing is related to the bidirectional sequential classification method described in (Shen et al, 2007). Similar to bidirectional labelling in (Shen et al, 2007), there are two learning tasking in this model. The learning algorithm for level-0 dependency is similar to the guided learning algorithm for labelling as described in (Shen et al, 2007). The only preprocessing step needed is POS tagging of the data, for which we used the system of Shen et al (2007). It is competitive to CRF in tagging accuracy but requires much less training time (Shen et al, 2007). We propose a new category of dependency parsing algorithms, inspired by (Shen et al, 2007): non directional easy-first parsing. Indeed, one major influence on our work is Shen et.al's bi-directional POS-tagging algorithm (Shen et al, 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing.  Note that Shen et al (2007) employ contextual features up to 5-gram which go beyond our local trigram window. (Shen et al, 2007) developed new algorithms based on the easiest-first strategy (Tsuruoka and Tsujii, 2005) and the perceptron algorithm. Shen et al, (2007) report an accuracy of 97.33% on the same data set using a perceptron-based bidirectional tagging model. Shen et al (2007) have further shown that better results (97.3 % accuracy) can be obtained using guided learning, a framework for bidirectional sequence classification, which integrates token classification and inference order selection into a single learning task and uses a perceptron-like (Collins and Roark, 2004) passive-aggressive classifier to make the easiest decisions first. We used the feature set defined in (Shen et al 2007), which includes the following: 1. 
Shallow semantic representations, bearing a more compact information, can prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al, 2007). There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al, 2007). (Moschitti et al, 2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a ST.  Counting the number of matched dependencies is essentially a simplified tree kernel for QA (e.g., see (Moschitti et al, 2007)) matching only trees of depth 2. In particular, in (Moschitti et al, 2007) kernels for the processing of PASs (in PropBank1 format (Kingsbury and Palmer, 2002)) extracted from question/answer pairs were proposed. Then, we design a novel shallow semantic kernel which is far more efficient and also more accurate than the one proposed in (Moschitti et al, 2007). To overcome this problem, a Shallow Semantic Tree Kernel (SSTK) was designed in (Moschitti et al, 2007). Thus, ? can recursively be an SRK (and evaluate Nested PASs (Moschitti et al, 2007)) or any other potential kernel (over the arguments). The result is 89.05? 1.25 and 83.73? 1.61 for 6 and 50 classes, which outperforms the best result of 86.1? 1.1 for 6 classes as reported in (Moschitti et al, 2007). In (Moschitti et al, 2007) it was shown that the use of TK improves QC of 1.2 percent points, i.e. from 90.6 to 91.8: further analysis of these fragments may help us to device compact, less sparse syntactic features and design more accurate models for the task. In (Moschitti et al, 2007), we proposed the Shallow Semantic Tree Kernel (SSTK) designed to encode PASs1 in SVMs. Instead, the similar PAS-SSTK representation in (Moschittiet al, 2007) does not take argument order into account, thus it fails to capture the linguistic rationale expressed above. Instead, the similar PAS-SSTK representation in (Moschittiet al, 2007) does not take argument order into account, thus it fails to capture the linguistic rationale expressed above. In future work, we intend to expand our analysis of both the gold-standard answer and the student answers beyond the bag-of-words paradigm by considering basic logical features in the text (i.e., AND, OR, NOT) as well as the existence of shallow grammatical features such as predicate argument structure (Moschitti et al, 2007) as well as semantic classes for words.
On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007).  Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling.  (Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model).  More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset.  For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. This is different from the experiments reported in (Zhang and Clark, 2007).  For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).
More recently, Haghighi and Klein (2007) use the distinction between pronouns, nominals and proper nouns in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy.  Other recent unsupervised graphical model approaches using Gibbs sampling (Haghighi and Klein, 2007) may be able to incorporate partially annotated documents in semi-supervised training. Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. In terms of applying non-parametric Bayesian approaches to NLP, Haghighi and Klein (2007) evaluated the clustering properties of DPMMs by performing anaphora resolution with good results. In addition, their system does not classify non-anaphoric pronouns, A third paper that has significantly influenced our work is that of (Haghighi and Klein, 2007). The probabilities are ordered according to, at least my, intuition with pronoun being the most likely (0.094), followed by proper nouns (0.057), followed by common nouns (0.032), a fact also noted by (Haghighi and Klein,2007). Since Soon (Soon et al, 2001) started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (Haghighi and Klein, 2007). Since we can not afford to manually annotate our entire data set with coreference information, we follow Haghighi and Klein (2007)'s work on unsupervised co reference resolution, and develop a series of generative Bayesian models for our task.  In this work, we take a primarily unsupervised approach to co reference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue. As Blei and Frazier (2009) notes, when marginalizing out the Ai in this trivial case, the DD-CRP reduces to the traditional CRP (Pitman, 2002), so our discourse model roughly matches Haghighi and Klein (2007) for proper mentions. Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. Poon and Domingos (2008) outperformed Haghighi and Klein (2007). The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference. Our system improves over the latent named-entity tagging in Haghighiand Klein (2007), from 61% to 87%. Turning to unsupervised CoRe, Haghighi and Klein (2007) proposed a generative Bayesian model with good performance. P&D (Poon and Domingos, 2008) on ACE-2; and to Ng (Ng, 2008) and H&K5 (Haghighi and Klein, 2007) on ACE2003.
This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a).    It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b).    Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011).  Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text.  The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008).  Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). 
To date, the most closely related work is Mihalcea et al (2007), which explores cross-lingual projections to generate subjectivity analysis resources in Romanian by leveraging on the tools and resources available in English. In addition to the above methods for using English resources, the lexicon-based method investigated in Mihalcea et al (2007) can also use English resources by directly projecting English lexicons into Chinese lexicons. Mihalcea et al (2007) learn multilingual subjectivity via cross-lingual projections. Mihalcea et al (2007) propose a method to learn multilingual subjective language via cross-language projections.  Mihalcea et al (2007) propose two methods for translating sentiment lexicons. Some CLOA works used bilingual dictionaries (Mihalcea et al, 2007), or aligned corpus (Kim and Hovy, 2006) to align the expressions between source and target languages.  There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. For instance, (Mihalcea et al, 2007) use an English corpus annotated for subjectivity along with parallel text to build a subjectivity classifier for Romanian. Mihalcea et al (2007) and Banea et al (2008) proposed a number of approaches exploiting a bilingual dictionary, a parallel corpus, and an MT system to port the resources and systems available in English to languages with limited resources. For subjectivity lexicons translation, Mihalcea et al (2007) and Wan (2008) used the first sense in a bilingual dictionary. Our simple approach produces moderate-sized lexicons (3,808, 3,980, 3,027 for Korean, Chinese, and Japanese) compared to Mihalcea et al (2007)'s complicated translation approach (4,983 Romanian words). In (Mihalcea et al, 2007), different shortcomings of lexicon-based translation scheme was discussed for the more semantic-oriented task subjective analysis, instead the authors proposed to use a parallel-corpus, apply the classifier in the source language and use the corresponding sentences in the target language to train a new classifier. MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)). (Mihalcea et al, 2007) describes experiments with subjectivity classification for Romanian. The lexicon obtained after 5 iterations of the method was used for sentence level sentiment classification, indicating an 18% improvement over the lexicon of (Mihalcea et al, 2007). Mihalcea et al, (2007) and Banea et al, (2008) used machine translation technique to leverage English resources for analysis in Romanian and Spanish languages. Mihalcea et al (2007), for example, generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus. In (Mihalcea et al, 2007), a bilingual lexicon and a manually translated parallel corpus are used to generate a sentence classifier according to their level of subjectivity for Romanian.
Medlock and Briscoe (2007) proposed a weakly supervised setting for hedge classification in scientific texts where the aim is to minimise human supervision needed to obtain an adequate amount of training data. The authors are only aware of the following related corpora: the Hedge classification corpus (Medlock and Briscoe, 2007), which has been annotated for hedge cues (at the sentence level) and consists of five full biological research papers (1537 sentences). 5 articles from FlyBase (the same data were used by Medlock and Briscoe (2007) for evaluating sentence-level hedge classifiers) and 4 articles from the open access BMC Bioinformatics website were downloaded and annotated for negations, uncertainty and their scopes. Solving the sentence level task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or non speculative. A misinterpretation of the BioScope paper (Szarvas et al, 2008) led us to believe that five of the nine full articles in the training data were annotated using the guidelines of Medlock and Briscoe (2007). Medlock and Briscoe (2007) extended the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification. Szarvas (2008) extended the weakly supervised machine learning methodology of Medlock and Briscoe (2007) by applying feature selection to reduce the number of candidate keywords, by using limited manual supervision to filter the features, and by extending the feature representation with bigrams and trigrams. In addition, by following the annotation guidelines of Medlock and Briscoe (2007), Szarvas (2008) made available the BMC Bioinformatics data set, by annotating four full text papers from the open access BMC Bioinformatics website. In related work, Szarvas (2008) extended the methodology of Medlock and Briscoe (2007), and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords. For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. Medlock and Briscoe (2007) also used single words as input features in order to classify sentences from scientific articles in biomedical domain as speculative or non-speculative. Medlock and Briscoe (2007) use a similar baseline as the one adopted by Light et al (2004), i.e. a naive algorithm based on substring matching, but with a different list of terms to match against. However Medlock and Briscoe (2007) note that their model is unsuccessful in identifying assertive statements of knowledge paucity which are generally marked rather syntactically than lexically. Kilicoglu and Bergler (2008) did experiments on the same dataset as Medlock and Briscoe (2007) and their experimental results proved that the classification accuracy can be improved by approximately 9% (from an F-score of 76% to an F-score of 85%) if syntactic and semantic information are incorporated. The experiments run by Medlock (2008) on the same dataset as Medlock and Briscoe (2007) show that adding features based on part-of speech tags to a bag-of-words input representation can slightly improve the accuracy, but the improvements are marginal and not statistically significant. Other early work focused on semi supervised learning due to a lack of annotated datasets (Medlock and Briscoe, 2007). We can then use the large amounts of unannotated sentences that are available to extract n-gram features that have high uncertainty class conditional probability and add them to our training set with those features labeled as hedges as described in Medlock and Briscoe (2007). Medlock and Briscoe (2007) used single words as input feature sin order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. Early work on speculative language detection tried to classify a sentence either as speculative or non-speculative (see, for example, Medlock and Briscoe (2007)).
This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007). Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007). The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007). Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT. We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. (Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007). The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al (2002) Log probability of target segments on 5-gram MT-output-based LM: using MOSES (Koehn et al, 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-of-speech tag.
We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. The ability to identify entities like people, organizations and geographic locations (Tjong KimSang and De Meulder, 2003), extract their attributes (Pasca, 2008), and identify entity relations (Banko and Etzioni, 2008) is useful for several applications in natural language processing and knowledge acquisition tasks like populating structured knowledge bases (KB). The information extraction oeuvre has a gamut of relation extraction methods for entities like persons, organizations, and locations, which can be classified as open or closed-domain depending on the restrictions on extractable relations (Banko and Etzioni, 2008). Downey et al (2007) starts from the output of the unsupervised information extraction system TextRunner (Banko and Etzioni, 2008), and uses language modeling techniques to estimate the reliability of sparse extractions. The point of these work is that they selected relation expressions only from the words located between given entities in the text, because as far as Englishtexts are concerned, 86% of the relation expressions of named entity pairs appear between the pair (Banko and Etzioni, 2008). In (Banko and Etzioni, 2008) a Conditional Random Field (CRF) classifier is used to perform Open Relation Extraction which improves by more than 60% the F-score achieved by the Naive Bayes model in the TextRunner system. On four common relations (Acquisition, Birthplace, InvetorOf, WonAward), Open RE attained a recall of 18.4% in comparison to 58.4% achieved by Traditional RE (Banko and Etzioni 2008). Thus, as our input, we utilize tuples extracted by TEXTRUNNER (Banko and Etzioni, 2008) when run over a corpus of 500 million web pages. We use the Open IE corpus generated by running TEXTRUNNER on 500 million high quality Webpages (Banko and Etzioni, 2008) as the source of instance data for these relations. In contrast, the recently proposed Open Information Extraction paradigm aims to detect related pairs of entities without knowing in advance what kinds of relations exist between entities in the source data and without any seeding (Banko and Etzioni, 2008). In particular, they found that almost 40% of such relations are realized by the argument-verb-argument pattern (henceforth, AVA) (see Table 1 in Banko and Etzioni (2008)). The TextRunner system (Banko and Etzioni, 2008) is trained using a CRF classifier on S-V-O tuples from a parsed corpus as positive examples, and tuples that violate phrasal structure as negative ones. TextRunner achieves P=0.94, R=0.65, and F Score=0.77 on the AVA pattern (Banko and Etzioni,2008). We run LDA-SP to compute preferences on amassive dataset of binary relations r (a1 ,a2) extracted from the Web by TEXTRUNNER (Banko and Etzioni, 2008). For all experiments we make use of a corpus of r (a1 ,a2) tuples, which was automatically extracted by TEXTRUNNER (Banko and Etzioni, 2008) from 500 million Web pages. Banko and Etzioni (Banko and Etzioni, 2008) showed that a small set of POS-tag patterns cover a large fraction of relationships in English, but never incorporated the patterns into an extractor. The TEXTRUNNER Open IE system (Banko and Etzioni, 2008) employs only shallow syntactic features in the extraction process. Overall, we provide evidence that, contrary to belief in the Open IE literature (Banko and Etzioni, 2008), semantic approaches have a lot to offer for the task of Open IE and the vision of machine reading. Although OIE often has lower precision than traditional information extraction, it is able to extract a wider variety of relations at precision levels that are often useful (Banko and Etzioni, 2008). Banko and Etzioni (2008) cite a precision score of 88% for their system.
This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008). Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees. Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed.   We used a variant of the phrasal ITG described by Zhang et al (2008). Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008). Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters.
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008).  Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. Mi et al (2008) give a detailed description of the two-step decoding process. Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). The first direct use of packed forest is proposed by Mi et al (2008). Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. For more detail, we refer to the algorithms of Mi et al (2008).
Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable.  Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. (6) as max-derivation decoding, which are first termed by Blunsom et al (2008). Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). See Blunsom et al (2008) for more information. We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008).
Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge. In both experiments, we compare the SVS model against the state-of-the art model by Mitchell and Lapata 2008 (henceforth M&L; cf.  While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations. Note that Spearman's ? is often a little lower than Pear son? s (Mitchell and Lapata, 2008). Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression. Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting. And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors. Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor). As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now. Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition. In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995). Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001). The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset. We use other WSM settings following Mitchell and Lapata (2008). Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition. We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008).
Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. (Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus.  There are two common assumptions within a cluster of related documents (Ji and Grishman 2008): Trigger Consistency Per Cluster: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type. For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. Although this figure is very low, it is not surprising: the results on the English ACE 2005 corpus show that the inter-annotator agreement on trigger identification is only about 40% (Ji and Grishman, 2008). The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. Ji and Grishman (2008) enforce event role consistency across different documents. Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments.
 However, these two models either require post processing to calculate the positive/negative coverage in a document for polarity identification (Mei et al, 2007) or require some kind of supervised setting in which review text should contain ratings for aspects of interest (Titov and McDonald, 2008a). The multi-aspect sentiment (MAS) model (Titov and McDonald, 2008a), which is extended from the multi-grain latent Dirichlet allocation (MG-LDA) model (Titov and McDonald, 2008b), allows sentiment text aggregation for sentiment summary of each rating aspect extracted from MG-LDA. Unlike some of the previous work (e.g., (Titov and McDonald, 2008a)), we do not constrain aspect specific sentiment to be the same across the document. In contrast, MLSLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). Aspect-based sentiment analysis summarizes sentiments with diverse attributes, so that customers may have to look more closely into analyzed sentiments (Titov and McDonald, 2008). There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al, 2009), partially-labeled (Ramage et al, 2011), constrained (Andrzejewski et al, 2009) models, etc. Titov and McDonald (2008b) proposed a joint model of text and aspect ratings which utilizes a modified LDA topic model to build topics that are representative of ratable aspects, and builds a set of sentiment predictors.
Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew. Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. Adler and Elhadad (2006) presented an HMM-based approach for unsupervised joint morphological segmentation and tagging of Hebrew, and Goldberg and Tsarfaty (2008) developed a joint model of segmentation, tagging and parsing of Hebrew, based on lattice parsing. Goldberg and Tsarfaty (2008) propose a generative joint model. Goldberg and Tsarfaty (2008) concluded that an integrated model of morphological disambiguation and syntactic parsing in Hebrew Treebank parsing improves the results of a pipelined approach. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. Following (Goldberg and Tsarfaty, 2008) we deal with the ambiguous affixation patterns in Hebrew by encoding the input sentence as a segmentation lattice. The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008). A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew. Models that in addition incorporate morphological analysis and segmentation have been explored by Tsarfaty (2006), Cohen and Smith (2007), and Goldberg and Tsarfaty (2008) with special reference to Hebrew parsing. Parsing and segmentation are handled jointly by the parser (Goldberg and Tsarfaty, 2008). It is the same grammar as described in (Goldberg and Tsarfaty, 2008).  The most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the tree bank. The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token. Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units.
This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al, 2008a). Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al (2008) uses an algorithm to convert leaves in a parse tree to phrases be fore rule extraction. In particular, I will investigate settings that incorporate non syntactic phrases, using methods similar to Liu et al (2006) and Zhang et al (2008). For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008). Synchronous tree-sequence substitution grammar (STSSG) al lows either side of a rule to comprise a sequence of trees instead of a single tree (Zhang et al, 2008). Finally, STSSG, which have been derived from rational tree relations (Raoult, 1997), have been discussed by Zhang et al (2008a), Zhang et al (2008b), and Sun et al (2009). However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expressivity gained from non-contiguous rules greatly improves the translation quality. A model that is even more powerful than LMBOT is the non-contiguous version of STSSG (synchronous tree-sequence substitution grammar) of Zhang et al (2008a), Zhanget al (2008b), and Sun et al (2009), which allows sequences of trees on both sides of rules [see also (Raoult, 1997)]. Chiang (2005) and Graehl et al (2008) argue that STSG have sufficient expressive power for syntax based machine translation, but Zhang et al (2008a) show that the additional expressive power of tree sequences helps the translation process. Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008).  Similar to the definition of tree sequence used in a single parse tree defined in Liu et al (2007) and Zhang et al (2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping.  Among them, Zhang et al (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs, and find them useless via real syntax-based translation systems. In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al (2008a). tree-to-tree translation model based on tree sequence alignment (Zhang et al 2008a) without losing of generality to most syntactic tree based models. By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al (2008a). ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al (2008a).
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). (Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. (Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain. In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). Shen et al (2008) use only phrases that meet certain restrictions. Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data.
 All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). See Huang (2008) for more details. However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance.  With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy.  Huang (2008) proposed to use a parse forest to incorporate non-local features. Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005).  In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined.
Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008). Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing. Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure. We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al, 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency tree bank was assigned a cluster identifier. In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al, 2008) for the list of cluster-based feature templates. Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with an other semi-supervised approach, described in (Koo et al, 2008). In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). Our experiments investigate the effectiveness of: 1) the basics S-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al (2008)'s semi supervised approach (even in the case we used the same unlabeled data for both methods); 3) the two stage semi-supervised learning approach that in 551corporates a second-order parsing model. We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets.
Recent research indicates that using labeled and unlabeled data in semi-supervised learning (SSL) environment, with an emphasis on graph-based methods, can improve the performance of information extraction from data for tasks such as question classification (Tri et al, 2006), web classification (Liu et al., 2006), relation extraction (Chen et al, 2006), passage-retrieval (Otterbacher et al, 2009), various natural language processing tasks such as part of-speech tagging, and named-entity recognition (Suzuki and Isozaki, 2008), word-sense disambiguation (Niu et al, 2005), etc.  Suzuki and Isozaki (2008) provided evidence that the use of more unlabeled data in semi supervised learning could improve the performance of NLP tasks, such as POS tagging, syntactic chunking, and named entities recognition. We describe an extension of semi supervised structured conditional models (SS-SCMs) to the dependency parsing problem, whose framework is originally proposed in (Suzuki and Isozaki, 2008). Our approach basically follows a framework proposed in (Suzuki and Isozaki, 2008). Note that it is possible to iterate the method steps 2 and 3 can be repeated multiple times (Suzuki and Isozaki, 2008) but in our experiments we only performed these steps once. We follow a similar approach to that of (Suzuki and Isozaki, 2008) in partitioning f (x, y), where the k different feature vectors correspond to different feature types or feature templates. This paper has described an extension of the semi-supervised learning approach of (Suzuki and Isozaki, 2008) to the dependency parsing problem. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. 22-24 was 4.2%, which is comparable to related work in the literature, e.g. Suzuki and Isozaki (2008) (7%) and Spoustova et al (2009) (4-5%). In comparison, there are 79 templates in (Suzuki and Isozaki, 2008).  Wong and Ng (2007) and Suzuki and Isozaki (2008) are similar in that they run a baseline discriminative classifier on unlabeled data to generate pseudo examples, which are then used to train a different type of classifier for the same problem. Suzuki and Isozaki (2008), on the other hand, used the automatically labeled corpus to train HMMs. Although the method in (Suzuki and Isozaki 2008) is quite general, it is hard to see how it can be applied to the query classification problem. Suzuki and Isozaki (2008) also found a log linear relationship between unlabeled data (up to a billion words) and performance on three NLP tasks. Another approach (Suzuki and Isozaki, 2008) embeds a joint probability model.  Incorporating binary and real features yields a rough approximation of generative models in semi supervised CRFs (Suzuki and Isozaki, 2008). Suzuki and Isozaki (2008) is one such example.
Finally, there has been recent work on applying unsupervised multilingual learning to morphological segmentation (Snyder and Barzilay, 2008).  (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Snyder and Barzilay (2008) uses a set of aligned phrases across related languages to learn how to segment words with a Bayesian model and is otherwise fully unsupervised. The very interesting study by Snyder and Barzilay (2008) on multilingual approaches to morphological segmentation was difficult to classify. Snyder and Barzilay (2008a; 2008b) consider learning morphological segmentation with non parametric Bayesian model from multilingual data.  Snyder and Barzilay (2008) study the task of unsupervised morphological segmentation of multiple languages. For a majority of our testing we borrow the parallel phrases corpus used in previous work (Snyder and Barzilay, 2008), which we refer to as S&B.  Our work also has connections to multilingual tokenization (Snyder and Barzilay, 2008). Snyder and Barzilay (2008) use bilingual information but the segmentation is learned independently from translation modeling.
Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008). This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008).  All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. See Goldberg et al (2008) for details. Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. (Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs.  Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). Goldberg et al 2008 note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008). The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008). The importance is underscored succinctly by Goldberg et al (2008). EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008). Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008).
(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features.  The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.
Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008). (Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora. This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story. We utilize the definition of PMI described in (Chambers and Jurafsky, 2008). The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work. Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information. We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy. Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain: focus on the situational descriptions explicitly pertaining to a single protagonist, a series of references within a document that are automatically labeled as co referent. Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). Triples of verb tokens were sampled at random from the narrative cloze test set of Chambers and Jurafsky (2008). The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)). Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved.
 Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging.  Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding.  We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For POS tagging features, we follow the work of Zhang and Clark (2008a). Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets.
Following Jiang et al (2008), we describe segmentation and Joint S&T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2. As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the begin, middle and end of a word respectively. Templates called lexical-target in the column below are introduced by Jiang et al (2008). For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j. Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging. We use the feature templates the same as Jiang et al, (2008) to extract features form E model.  The first is the "character-based" approach, where basic processing units are characters which compose words (Jiang et al., 2008a).  We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results. We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm. However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper). Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0]). As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle. Inspired by (Jiang et al, 2008), we set the real-value of C0 to be 2.0, the value of C-1C0 and C0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model.  Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004).   
Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models.  For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008). A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008).  Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising.  Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser. Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008).  In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other.  Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser.
 Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008).  We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005).  The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008).
In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. Marton and Resnik (2008) and Cherry (2008) imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis. Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations. To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline. Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs.
The concept of compatible coverage vectors for the locations of translated words becomes the notion of reachability between frontier nodes in the lattice (Dyer et al, 2008). We also plan to jointly optimize MT and name tagging by propagating multiple word segmentation and name annotation hypotheses in lattice structure to statistical MT and conduct lattice based decoding (Dyer et al, 2008). In applications like the one described by Dyer et al (2008), where several different segmenters for Chinese are combined to create the lattice, this is not possible. Dyer et al (2008) use it to encode different Chinese word segmentations or Arabic morphological analyses. Our work differs from (Dyer et al, 2008) in that we explicitly distinguish the various preprocessing types in the lattice so that we can define specific path features and lexicalize the lattice path probabilities within the phrase model. It was noted by Dyer et al (2008) that the standard distance-based reordering model needs to be redefined for lattice input. Using the shortest path within the lattice is reported to have better performance in (Dyer et al, 2008), however we did not implement it due to time constraints. Our word lattices are similar to those used by Dyer et al (2008) for handling word segmentation in Chinese and Arabic. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). Lattice represent the system implemented as Dyer et al, (2008). Same as Dyer et al, (2008), we also extracted rules from a combined bilingual corpus which contains three copies from different segmenters. Du et al (2010), in this proceedings, explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation (Dyer et al, 2008) to efficiently represent and decode the resulting very large space of paraphrase alternatives. Finally, some researchers have advocated recently the use of shared structures such as parse forests (Mi and Huang, 2008) or word lattices (Dyer et al, 2008) in order to allow a compact representation of alternative inputs to an SMT system. All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them. Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Miet al, 2008) and replacing 1-best word segmentations with word lattices (Dyer et al, 2008). Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider: using packed forests instead of 1-best trees (Mi et al, 2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and n-best alignments instead of 1-best alignments (Venugopal et al, 2008). As lattice is a more general form of confusion network (Dyer et al, 2008), we expect that replacing confusion networks with lattices will further improve system combination. Our implementation's runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lattice (Dyer et al, 2008). Lattice parsing is not new to translation (Dyer et al, 2008), but to our knowledge it has not been used in this way. Dyer et al (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation.
(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to: (1) learn concepts with high precision compared to singly-anchored pat terns (Kozareva et al, 2008), (2) use only one seed instance for the discovery of new previously unknown terms, and (3) harvest knowledge with minimal supervision. With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009). In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al 2006), and others semi-supervised (Kozareva et al 2008). Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. (Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al, 2008). Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. The approach we describe here is most similar to that of Kozareva et al (2008). Kozareva et al (2008) test their approach on relatively simple and objective categories like states, countries (both closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007). Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10).
More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). (DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008).
 On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. One could use ILP-based decoding in the style of Finkel and Manning (2008) and Song et al (2012) to attempt to explicitly find the optimal C with choice of a marginalized out, but we did not explore this option. Our formulation is equivalent to the one suggested by Finkel and Manning (2008) in a coreference resolution task. As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output of the anaphoricity classifier and that of the coreference classifier. Second, we compare our cut-based approach with the five aforementioned approaches to anaphoricity determination (namely, Ng and Cardie (2002a), Ng (2004), Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008)) in terms of their effectiveness in improving a learning-based coreference system. It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys.  Duplicated Finkel and Manning (2008) baseline. Examples include (Finkel and Manning, 2008), using VI, Rand index and clustering F-score for evaluating coreference resolution. Recently Finkel and Manning (2008) show that the optimal ILP solution outperforms the first and best-link methods.   Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. Extending (Denis and Baldridge, 2007) and (Finkel and Manning,2008)'s work, we introduce a loose selection strategy for transitivity constraints, attempting to overcome huge computation complexity brought by transitivity closure constraints. Klenner (2007) and Finkel and Manning (2008)'s work extended the ILP framework to support transitivity constraints. Last, we note that transitive relations have been explored in adjacent fields such as Temporal Information Extraction (Ling and Weld, 2010), Ontology Induction (Poon and Domingos, 2010), and Coreference Resolution (Finkel and Manning, 2008). Transitivity was also used as an information source in other fields of NLP: Taxonomy Induction (Snow et al, 2006), Co-reference Resolution (Finkel and Manning, 2008), Temporal Information Extraction (Ling and Weld, 2010), and Unsupervised Ontology Induction (Poon and Domingos, 2010). see (Finkel and Manning, 2008) for an alternative but equivalent formalization. The model from (Finkel and Manning, 2008) utilizes transitivity, but not exclusivity.
We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser, implementing Collins' parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008). Concerning techniques for improving out-of domain parsing, a related approach has been to use self-training with auto-parsed out-of-domain data, as McClosky and Charniak (2008) do for English constituency parsing, though in that approach lexical generalization is not explicitly performed. Parse trees come from the Charniak-Johnson parser (Charniak and Johnson, 2005) with a self-trained biomedical parsing model (McClosky and Charniak, 2008), and are converted to dependency structures again using Stanford CoreNLP. The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. The re-ranking parser of Charniak & Johnson adapted to the biomedical domain (McClosky and Charniak, 2008). For syntactic parsing, we use the output of the BLLIP re-ranking parser adapted to the biomedical domain by McClosky and Charniak (2008), as provided by the shared task organizers in the Stanford collapsed dependency format with conjunct dependency propagation. Parsing is performed using the Charniak-Johnson parser (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of McClosky and Charniak (2008).
Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail). Previous work (Branavan et al, 2009) is only able to handle low-level instructions. This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments. In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009). Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09. Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009).  To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009].  We also compare against the policy gradient learning algorithm of Branavan et al (2009).
We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009). We used the dataset created by Liang et al (2009). By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one. In particular, a word is chosen from the parameters learned in the model of Liang et al (2009). We use the model of Liang et al (2009) to impute the decisions. We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors: (i) there are fewer training examples in SUMTIME and unsupervised learning typically works better with a large amount of data; and (ii) the alignment model does not exploit the temporal structure in the SUMTIME world state. Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states. On the other hand, Liang et al (2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method. Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics.  Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w|s), where w is an NL sentence and s is a world state containing a set of possible MR logical forms that can be matched to w. The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009). We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it.  In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation. Compared to Liang et al (2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation. In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption.
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). (Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009).
Following (Somasundaran and Wiebe, 2009), stance, as used in this work, refers to an overall position held by a person toward an object, idea or proposition. Our work is also different from related work in the domain of product debates (Somasundaran and Wiebe, 2009) in terms of the methodology. Some previous research looked at the related field of opinion mining, also on political discussion, as in (AbuJbara et al, 2012), (Anand et al., 2011) or (Somasundaran and Wiebe, 2009). As we aim at performing a finegrained analysis, approaches merely classifying pro or contra (like those of (Walker et al, 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an on line debate concerning the merits of competing products. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. There have been some related works that focus on discovering the general topics and ideological perspectives in online discussions (Ahmed and Xing, 2010), placing users in support/oppose camps (Agarwal et al, 2003), and classifying user stances (Somasundaran and Wiebe, 2009).   Sentence 14 is a negative pragmatic opinion (Somasundaran and Wiebe, 2009) which can only be detected with the help of external world knowledge. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates.
To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). For comparsion, we use the same data set in (Wan, 2009): Test Set (Labeled Chinese Reviews): The data set contains a total of 886 labeled product reviews in Chinese (451 positive reviews and 435 negative ones). The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). We compare our procedure with the co-training scheme reported in (Wan, 2009): CoTrain: The method with the best performance in (Wan, 2009). More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). Wan (2009) constructs a multilingual classifier using co-training. Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. SVM: We train a SVM classifier on the Chinese labeled data.MT-Cotrain: This is the co-training based approach described in (Wan, 2009). Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009).
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others.  Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head).   Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009). The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). Figure 3: Details of the factor graph underlying the parser of Martins et al (2009). We now turn to the concise integer LP formulation of Martins et al (2009). (20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop.  Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011).
For MaltParser we used the projective Stack algorithm (Nivre, 2009) with default settings and a slightly enriched feature model. To deal with crossing arcs, Titov et al (2009) and Nivre (2009) designed a SWAP transition that switches the position of the two topmost nodes on the stack. Recent works on dependency parsing speedup mainly focus on inference, such as expected linear time non-projective dependency parsing (Nivre, 2009), integer linear programming (ILP) for higher order non-projective parsing (Martinset al, 2009).        Despite this fact, it is possible to perform non-projective parsing in linear time in practice (Nivre, 2009). Note that Nivre (2009) has a similar idea of performing projective and non-projective parsing selectively. 'Nivre' is Nivre's swap algorithm (Nivre, 2009), of which we use the implementation from MaltParser (maltparser.org). In this section, we start by defining a transition system for joint tagging and parsing based on the non-projective transition system proposed in Nivre (2009). Figure 1: Transitions for joint tagging and dependency parsing extending the system of Nivre (2009). Except for the addition of a tag parameter p to the SHIFT transition, this is equivalent to the system described in Nivre (2009), which thanks to the SWAP transition can handle arbitrary non-projective trees. Non-projective parsing algorithms for supervised dependency parsing have, for example, been presented in McDonald et al. (2005) and Nivre (2009).  However, the goal of that transition is different from ours (selecting between projective and non-projective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic for example, the LEFT ARC transition cannot be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time.
Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish.  Table 4: Comparison of baseline, GNPPA and E GNPPA with baseline and discriminative model from (Ganchev et al, 2009) for Bulgarian and Spanish. Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours.  Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). An empirical comparison to Ganchev et al (2009) is given in Section 5. PR: The posterior regularization (PR) approach of Ganchev et al (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009).
Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first minimization procedure is seeded with a noisy gram mar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). (Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008).  Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model.
We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. Our learning and decoding algorithms are also different from Kruengkrai et al (2009).  K2009 is the result of Kruengkrai et al (2009).  Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)). "Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data.    We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data.
Second, recent work by Chambers and Jurafsky (2009) has induced narrative chains, i.e., likely sequences of events, by their use of similar head words. Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing co referring arguments, followed by temporal classification to induce partial order. Chambers and Jurafsky (2009, 2008) propose an unsupervised method for learning narrative schemas, chains of events whose arguments are filled with participant semantic roles defined over words. Inspired by Chambers and Jurafsky (2009) we acquire story plots automatically by recording events, their participants, and their precedence relationships as at tested in a training corpus. Our narrative schemas differ slightly from Chambers and Jurafsky (2009). The unsupervised setting has also been considering for the related problem of learning narrative schemas (Chambers and Jurafsky, 2009). Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al, 1998). Inducing narrative schemas (Chambers and Jurafsky, 2009) may be viewed as a possible next step in a narrative induction pipeline, subsequent to disentangling the text comprising individual narrative threads. Similar observations can be made with respect to Chambers and Jurafsky (2009) and Kasch and Oates (2010), who also study a single discourse relation (narration), and are thus more limited in scope than the approach described here. It thus represents the basis for further experiments, e.g., with respect to the enrichment the BKB with information provided by Riaz and Girju (2010), Chambers and Jurafsky (2009) and Kasch and Oates (2010). Chambers and Jurafsky (2009) described a system that can learn (without supervision) the sequence of events described in a narrative, and Elson and McKeown (2009) created a platform that can symbolically represent and reason over narratives. Narrative structure has also been studied by representing character interactions as networks. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). Chambers and Jurafsky (2009) present an unsupervised method for learning narrative schemas from news, i.e., coherent sets of events that involve specific entity types (semantic roles). McIntyre and Lapata (2010) create a story generation system that draws on earlier work on narrative schemas (Chambers and Jurafsky, 2009). Event structures in open domain texts are frequently highly complex and nested: a "crime" event can cause an "investigation" event, which can lead to an "arrest"event (Chambers and Jurafsky, 2009).
Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables.  A04CU: Train/dev/test split of the newswire portion of the ACE 2004 training set7 utilized in Culotta et al (2007), Bengston and Roth (2008) and Stoyanov et al (2009). A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al (2009).  We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009).  B3 here is the B3All version of Stoyanov et al (2009). The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold= 0.45, SIG for ACE04 and AP for ACE05, ACE05 ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used in their paper (p.c.).
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set.  Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). (Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task).  
Blunsom et al (2009) describe a blocked sampler following John son et al (2007) which uses the Metropolis-Hastings algorithm to correct proposal samples drawn from an approximating SCFG, however this is discounted as impractical due to the O (|f |3|e|3) complexity.  In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al, 2009), in terms of mixing and translation quality. We evaluate three initialisers: M4: the symmetrised output of GIZA++factorised into ITG form (as used in Blunsom et al (2009)). Following (Blunsom et al, 2009) we used a vague gamma prior (10? 4, 104), and sampled new values from a log-normal distribution whose mean was the value of the parameter, and variance was 0.3. Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). Other recent approaches use Gibbs sampler for learning the SCFG by exploring a fixed grammar having pre-defined rule templates (Blunsom et al, 2008) or by reasoning over the space of derivations (Blunsom et al, 2009). We explore a subset of the space of rules being considered by (Blunsom et al, 2009) i.e., only those rules satisfying the word alignments and heuristically grown phrase alignments. For this we adopt the approach of Blunsom et al (2009b), who present a method for maintaining table counts without needing to record the table assignments for each translation decision. We apply the technique from Blunsom et al (2009a) of using multiple processors to perform approximate Gibbs sampling which they showed achieved equivalent performance to the exact Gibbs sampler. Our ArEn training data comprises several LDCcorpora, using the same experimental setup as in Blunsom et al (2009a). We use this method motivated by Gibbs Sampler (Blunsom et al, 2009) which has been used for efficiently learning rules. Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). Our smoothing distribution of phrase pairs for all pre-terminals considers source-target phrase lengths drawn from a Poisson distribution with unit mean, drawing subsequently the words of each of the phrases uniformly from the vocabulary of each language, similar to (Blunsom et al, 2009). Recent work, e.g. by Blunsom et al (2009) and Haghihi et al. (2009) just to name a few, show that alignment models that bear closer resemblance to state-of-the art translation model consistently yields not only a better alignment quality but also an improved translation quality. Blunsom et al (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data. Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al, 2009).  Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method (Blunsom et al, 2009a). For AREN experiments the language model is trained on English data as (Blunsom et al, 2009a), and for FA-EN and UR EN the English data are the target sides of the bilingual training data.
Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality.  Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009).
This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. Haghighi et al (2009) give some restrictions on null-aligned word attachment. Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009).  This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al (2009). Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive.
Additionally, researchers have tried to automatically extract examples for supervised learning from resources such as Wikipedia (Weld et al,2008) and databases (Mintz et al, 2009), or attempted open information extraction (IE) (Banko et al, 2007) to extract all possible relations. It is a modification of the model proposed by Mintz et al (2009). However, Riedel et al's model (like that of previous systems (Mintz et al, 2009)) assumes that relations do not overlap there can not exist two facts r (e1, e2) and q (e1, e2) that are both true for any pair of entities, e1 and e2. We will make use of the Mintz et al (2009) sentence-level features in the expeiments, as described in Section 7. Additionally, their aggregate decisions make use of Mintzstyle aggregate features (Mintz et al, 2009), that collect evidence from multiple sentences, while we use 543 Inputs: (1)?, a set of sentences, (2) E, a set of entities mentioned in the sentences, (3) R, a set of relation names, and (4)?, a database of atomic facts of the form r (e1 ,e2) for r? R and ei? E. We use the set of sentence-level features described by Riedel et al (2010), which were originally developed by Mintz et al (2009). Mintz et al (2009) used Freebase facts to train 100 relational extractors on Wikipedia. Mintz et al (2009) use distant supervision to learn to extract relations that are represented in Freebase (Bollacker et al, 2008). This approach only has access to the text in a document and contains all the features mentioned in Wu and Weld (2010) and Mintz et al (2009). To address this limitation, a promising approach is distant supervision (DS), which can automatically gather labeled data by heuristically aligning entities in text with those in a knowledge base (Mintz et al, 2009). Craven and Kumlien (1999), Wu et al (2007) and Mintz et al (2009) were several pioneer work of distant supervision. This is a traditional DS assumption based model proposed by Mintz et al (2009). distant supervision: for each relation in the database D we assume that all the corresponding mentions are positive examples for the corresponding label (Mintz et al 2009). As discussed in Section 4.3, this model follows the "traditional" distant supervision heuristic, similarly to (Mintz et al., 2009). Contrasted to the alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently error prone heuristics (Mintz et al, 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al, 2010). MULTIR uses features which are based on Mintz et al (2009) and consist of conjunctions of named entity tags, syntactic dependency paths between arguments, and lexical information. Wieg and and Klakow (2011a) present an intermediate solution for opinion holder extraction inspired by distant supervision (Mintz et al 2009). A particularly attractive approach, called distant supervision (DS), creates labeled data by heuristically aligning entities in text with those in a knowledge base, such as Freebase (Mintz et al, 2009). We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al, 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multi instance learning system for relation extraction (see Section 7). Our work was inspired by Mintz et al (2009) who used Freebase as a knowledge base by making the DS assumption and trained relation ex tractors on Wikipedia.
We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances.  Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa.  We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. K-Means clustering algorithm described in Lin and Wu (2009). Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. 
The same assumption underlies methods for automatically identifying DRMs (Pitler and Nenkova, 2009). Such semantic information is based on external lists of lexical items and on the output of the add Discourse tagger (Pitler and Nenkova, 2009). The handling of explicit connectives can be split into three tasks (Pitler and Nenkova, 2009). The syntactic features (Syn) are inspired by (Pitler and Nenkova, 2009). Models M2-M4 do not rely on gold standard annotation or parsing (in contrast to the models for English in (Pitler and Nenkova, 2009)). Prior work (Pitler and Nenkova, 2009) showed that where explicit markers exist, the class of the relation can be disambiguated with f-scores higher than 90%. Predicting the class of implicit discourse relations, however, is much more difficult. We identify discourse connectives and their senses (TEMPORAL, COMPARISON, CONTINGENCY or EXPANSION) in each reference segment using the system in Pitler and Nenkova (2009). The state of-the-art for recognizing all types of explicit connectives in English is therefore already high, at 97% accuracy for disambiguating discourse vs. non discourse uses (Lin et al, 2010) and 94% for disambiguating the four main senses from the PDTB hierarchy (Pitler and Nenkova, 2009). The sense of the connective feature (F2) extracted from PDTB for the base system, though for the fully automatic one (Ghosh et al, 2011b) it needs the PTB (Penn TreeBank)-style syntactic parse trees as input (Pitler and Nenkova, 2009). The labeling of the four main senses from the PDTB sense hierarchy (temporal, contingency, comparison, expansion) reaches 94% ac curacy (Pitler and Nenkova, 2009) however, the baseline accuracy is already around 85% when using only the connective token as a feature. The state of the art for recognizing explicit connectives in English is therefore already high, at a level of 94% for disambiguating the four main senses on the first level of the PDTB sense hierarchy (Pitler and Nenkova, 2009). For instance, Pitler and Nenkova (2009) report an accuracy of 85.86% for correctly classified connectives (with the 4 main senses), when using the connective token as the only feature.
  Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009).   Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a).
Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)).  Kooand Collins (2010) propose a third-order graph based parser. In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). For further details about the parser, see Koo and Collins (2010). The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010).  We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees.  Koo and Collins (2010) further propose a third-order model that uses third-order features. Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers.
 Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008). In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. 256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters. Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. Further details and evaluations of these embeddings are discussed in Turian et al (2010). As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. We downloaded these embeddings from Turian et al (2010). Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a). For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature.
To support inference, (Ritter et al, 2010) learn the selectional restrictions of semantic relations, and (Pennacchiotti and Pantel, 2006) ontologize the learned arguments using WordNet. Alternatively, unsupervised approaches (Ritter et al., 2010) can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Dinu and Lapata (2010) used Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to model templates' latent senses, determining rule applicability based on the similarity between the two sides of the rule when instantiated by the context, while Ritter et al (2010) used LDA to model argument classes, considering a rule valid for a given argument instantiation if its instantiated templates are drawn from the same hidden topic. In other research on selectional preferences, Pantel et al (2007), Kozareva and Hovy (2010) and Ritter et al (2010) focus on generating admissible arguments for relations, and Erk (2007) and Bergsma et al (2008) investigate classifying a relation-instance pair as plausible or not. Important to this paper is the Wikipedia category network (Remy, 2002) and work on refining it. Se?aghdha (2010) and Ritter et al (2010) and the lexical substitution models of Dinu and Lapata (2010). We believe that a more data-driven type generalization that uses distributional similarity (e.g., (Ritter et al 2010)) may help much more. Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daume III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al, 2010) and cross-documeint co-reference resolution (Haghighi and Klein, 2010). Szpektor et al. (2008) introduce a model of contextual preferences, generalizing the notion of selectional preference (cf. Ritter et al, 2010) to arbitrary terms, allowing for context-sensitive inference. Ritter et al (2010) and O Se?aghdha (2010), e.g., model selectional restrictions of verb arguments by inducing topic distributions that characterize mixtures of topics observed in verb argument positions. O Se?aghdha (2010) applies topic models for the SP induction with three variations: LDA, RoothLDA, and Dual-LDA; Ritter et al (2010) focus on inferring latent topics and their distributions over multiple arguments and relations (e.g., the subject and direct object of a verb). This metric is firstly employed for the SP evaluation by Ritteret al (2010). It shows LDA SP performs good correlation with human ratings, where LDASP+Bayes refers to the Bayes prediction method of Ritter et al (2010). Specifically, Ritter et al (2010) utilized the dot product form for their similarity measure: (4) simDC (d, d?, w)=? t [p (t|d, w)? p (t|d?, w)] (the subscript DC stands for double-conditioning, as both distributions are conditioned on the argument word, unlike the measure below). Ritter et al (2010) investigated joint selectional preferences.
 Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al (2010) to regularize CRFs.     As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields (Lavergne et al, 2010) with the L-BFGS optimizer and elastic net regularization with default settings.   We report results for the following hybridizations and CRF-based system using Wapiti (Lavergne et al., 2010).          We therefore treat the prediction of IS labels as a sequence labeling task. We train a CRF using wapiti (Lavergne et al, 2010), with the features outlined in Table 1.
In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010). Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. H&S10 refers to the results of Huang and Sagae (2010). In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently.
As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model: Generation of (transitively) redundant pairs, as the formation of co reference sets (co reference clustering) is done after pairwise classification. (Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). Ng (2010) provides an excellent overview of the history and recent developments within the field. We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). Interested readers can refer to the literature review by Ng (2010). Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. 
Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions.  SAMT extension with source and target-side syntax described by Chiang (2010). In future work, the problem could be addressed by reconsidering our naming scheme for virtual nodes, by allowing fuzzy matching of labels at translation time (Chiang, 2010), or by other techniques aimed at reducing the size of the overall nonterminal set. Although MT systems that employ syntactic or hierarchical information have recently shown improvements over phrase-based approaches (Chiang, 2010), our initial investigation with syntactically driven approaches showed poorer performance on the text simplification task and were less robust to noise in the training data. The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). Chiang (2010) extended SAMT-style labels to both source and target-side parses, also introducing a mechanism by which SCFG rules may apply at runtime even if their labels do not match. However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. Inspired by Chiang (2010), we adopt a fuzzy way to label every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems be come too restrictive. Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical. Tellingly, in the entire proceedings of ACL 2010 (Hajic et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT.
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010).
For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010).  We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner.  In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit.
The SA component is implemented according to Jiang et al (2011), which incorporates target-dependent features and considers related tweets by utilizing a graph-based optimization. For target-dependent sentiment classification, the manual evaluation of Jiang et al (2011). Jiang et al (2011) combine the target-independent features (content and lexicon) and target-dependent features (rules based on the dependency parsing results) together in subjectivity classification and polarity classification for tweets. These features are all target-independent. SVMdep: We re-implement the method proposed by Jiang et al (2011). This is caused by mismatch of the rules (Jiang et al,2011) used to extract the target-dependent features. However, labeled data is expensive to create, and examples of Twitter classifiers trained on hand-labeled data are few (Jiang et al 2011). Subjectivity classification of small units of text, such as individual micro blog posts (Jiang et al, 2011) and sentences (Riloff et al, 2003), has been shown to benefit from additional context.
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system.  See Yannakoudakis et al (2011) for details. Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers.
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011). 3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). MSR video data (Chen and Dolan, 2011). To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): A man is sliding a cat on the floor. Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos.
We use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. Hanand Baldwin (2011) use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity. w2wN: The output of the word-to-word normalization of Han and Baldwin (2011). 5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011). We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011). Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. Recently, Han and Baldwin (2011) and Gouwsetal2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al 2011). In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al 2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available. To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011). Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011). In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. In addition, we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). 6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors. The present lexical normalisation used by our system is the dictionary lookup method of Hanand Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u). Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011). (Hanand Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%. (Han and Baldwin, 2011) developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity. 
Distant supervision is provided by the following constraint: every relation instance r (e 1, e 2) K must be expressed by at least one sentence in S (e 1, e 2), the set of sentences that mention both e 1 and e 2 (Hoffmann et al, 2011). We then extract relation instances from each parse and apply the greedy inference algorithm from Hoffmann et al, (2011) to identify the best set of parses that satisfy the distant supervision constraint. Our work is closest to Hoffmann et al 2011). They address the same problem we do (binary relation extraction) with a MIML model, but they make two approximations. However, our implementation has several advantages over the original model: (a) we model each relation mention independently, whereas Mintz et al collapsed all the mentions of the same entity tuple into a single datum; (b) we allow multi-label outputs for a given entity tuple at prediction time by OR-ing the predictions for the individual relation mentions corresponding to the tuple (similarly to (Hoffmann et al2011)). Hoffmann - This is the "MultiR" model, whic h performed the best in (Hoffmann et al2011). We applied our method to Wikipedia articles using Freebase as a knowledge base and found that (i) our model identified patterns expressing a given relation more accurately than baseline methods and (ii) our method led to better extraction performance than the original DS (Mintz et al, 2009) and MultiR (Hoffmann et al., 2011), which is a state-of-the-art multi instance learning system for relation extraction (see Section 7). We compared the following methods: logistic regression with the labeled data cleaned by the proposed method (PROP), logistic regression with the standard DS labeled data (LR), and MultiR propose din (Hoffmann et al, 2011) as a state-of-the-art multi instance learning system. For evaluating extraction accuracy, we follow the experimental setup of Hoffmann et al (2011), and use their implementation of MULTIR4 with 50 training iterations as our baseline. We use the same datasets as in Hoffmann et al (2011) and Riedel et al (2010), which include 3-years of New York Times articles aligned with Freebase. Figure 3 shows the precision/recall curves for MULTIR with and without pseudo-relevance feedback computed on the test dataset of 1000 sentence used by Hoffmann et al (2011). Note that the sentences are sampled from the union of Freebase matches and sentences from which some systems in Hoffmann et al (2011) extracted a relation. See Yao et al (2010) and Hoffmann et al (2011) for examples of such models. This is an at-least-one assumption based multi-instance learning method proposed by Hoffmann et al (2011).  This constraint is identical to the multiple deterministic-OR constraint used by Hoffmann et al (2011) to train a sentential relation extractor. The original formulation of the factors permitted tractable inference (Hoffmann et al 2011), but the Extracts function and the factors preclude efficient inference. Finally, we apply EXTRACTS to each parse, then use the greedy approximate inference procedure from Hoffmann et al 2011) for the factors. We compare our semantic parser to MULTIR (Hoffmann et al 2011), which is a state-of the-art weakly supervised relation extractor. Experimental results show that our trained semantic parser extracts binary relations as well asa state-of-the-art weakly supervised relation extractor (Hoffmann et al2011). The MultiR system allows entity tuples to have more than one relations, but still predicts each entity tuple locally (Hoffmann et al, 2011).
Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning. In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments. More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins. GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011).  Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world. It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011). One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liang et al 2011) or even a binary correct/incorrect signal (Clarke et al 2010). For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form. Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers. Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011). DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1). In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable. Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available. WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011). See Liang et al (2011) for work in representing lambda calculus expressions with trees.
Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. To this end, we use a variant of the quadratic cost criterion of Bengio et al (2006), also used by Subramanya et al (2010) and Das and Petrov (2011). Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011). Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics. Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011). Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters. MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)). For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. (Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages. Recent work by Das and Petrov (2011) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text. These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011). In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011). Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language.  We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM).
Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. Most recently, (Chambers and Jurafsky, 2011) acquire event words from an external resource, group the event words to form event scenarios, and group extraction patterns for different event roles. (Chambers and Jurafsky, 2011) (C+J) created an event extraction system by acquiring event words from WordNet (Miller, 1990), clustering the event words into different event scenarios, and grouping extraction patterns for different event roles. The use of case frames is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. We propose a method for inferring event templates based on word clustering according to their proximity in the corpus and syntactic function clustering. For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. Efficiently storing such structures is an important step in integrating document-level statistics into downstream tasks, such as characterizing complex scenarios (Chambers and Jurafsky, 2011), or story understanding (Gordon et al, 2011). Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. (Chambers and Jurafsky, 2011) (Poon and Domingos, 2010) (Chen et al 2011) focus on extracting frame-like structures (Baker et al 1998) by defining two types of clusters, event clusters and role clusters. (Chambers and Jurafsky, 2011) is similar to our approach in that it learns a semantic model, called template, from unlabelled news articles and then uses the template to extract information. There has been work that attempts to fill predefined templates using Bayesian nonparametrics (Haghighi and Klein, 2010) and automatically learns template structures using agglomerative clustering (Chambers and Jurafsky, 2011). Our first baseline is PROFINDER, a state of-the-art template inducer which Cheung et al (2013) showed to outperform the previous heuristic clustering method of Chambers and Jurafsky (2011).
Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Stro?tgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al, 2011). We tagged each sentence with Wikipedia terms using the Illinois Wikifier (Ratinov et al, 2011) and with UMLS (Bodenreider, 2004) terms using Health Term Finder (LipskyGorman and Elhadad, 2011). The need to identify named entities such as persons, locations, organizations and places, arises both in applications where the entities are first class objects of interest, such as in Wikification of documents (Ratinov et al, 2011), and in applications where knowledge of named entities is helpful in boosting performance, e.g., machine translation (Babych and Hartley, 2003) and question answering (Leidner et al, 2003). Semantic word vector spaces are at the core of many useful natural language applications such as search query expansions (Jones et al 2006), fact extraction for information retrieval (Pas?ca et al 2006) and automatic annotation of text with disambiguated Wikipedia links (Ratinov et al 2011), among many others (Turney and Pantel, 2010). In fact, (Ratinov et al, 2011) show that even though global approaches can be improved, local methods based on only similarity sim (d, e) of context d and entity e are hard to beat. Additionally, while (Rahman and Ng, 2011) uses the union of all possible meanings a mention may have in Wikipedia, we deploy GLOW (Ratinov et al., 2011), a context-sensitive system for disambiguation to Wikipedia. We uses a mixture of local and global features to train the coefficients of a linear ranking SVM to rank different NE candidates.
Finally, we use a POS tagger trained on tweets (Gimpel et al, 2011) to perform POS tagging on the tweets data and apart from the previously recognised named entities, only words tagged with nouns, verbs or adjectives are kept. However, POS taggers for Twitter are only available for a limited number of languages such as English (Gimpel et al, 2011). Table 3: Explicit requests for sharing (where only occurrences POS-tagged as verbs count, according to the Gimpel et al (2011) tagger). Our POS results, gathered using a Twitter-specific tagger (Gimpel et al, 2011), echo those of Ashok et al (2013) who looked at predict 14 Of course, simply inserting garbage isn't going to lead to more re-tweets, but adding more information generally involves longer text. The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al, 2011). This poses considerable problems for traditional NLP tools, which we redeveloped with other domains in mind, which of ten make strong assumptions about orthographic uniformity (i.e., there is just one way to spell you). One approach to cope with this problem is to annotate in-domain data (Gimpel et al, 2011). More specifically, we had lay annotators on the crowd sourcing platform Crowdflower re-annotate the training section of Gimpel et al (2011). We crowd source the training section of the data from Gimpel et al (2011) 2 with POS tags. Recognizing the limitations of existing systems, Gimpel et al (2011) develop a POS tagger specifically for Twitter, by creating a training corpus as well as devising a tag set that includes parts of speech that are uniquely found in on line language, such as emoticons (smilies). In this work, we focus on Twitter because the labeled corpus by Gimpel et al (2011) allows us to quantitatively evaluate our approach. We then present POS tagging results on the Twitter POS dataset (Gimpel et al, 2011). Our data is the recent Twitter POS dataset released at ACL 2011 by Gimpel et al (2011) consisting of approximately 26,000 words across 1,827 tweets. The Twitter dataset uses a domain-dependent tag set of 25 tags that are described in (Gimpel et al, 2011). For this experiment, we again make use of the Twitter POS dataset (Gimpel et al, 2011). We tokenize each tweet with Twitter NLP (Gimpel et al, 2011), remove the @ user and URLs of each tweet, and filter the tweets that are too short (< 7 words). A tweet-specific tokenizer (Gimpel et al, 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003).
MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011). In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script.
The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007). ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010).   Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of the-art transition-based system of Zhang and Nivre (2011) that employs beam search. We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. Second, at a similar toks/sec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model.
The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context. In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). Source embeddings: We employ three external embeddings (obtained from (Turian et al, 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models, 2) Collobert and Weston's (C&W) deep-learning architecture, and 3) Huang et al's polysemous neural language model (HUANG) (Huang et al, 2012).  Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1).
  Another application of word vectors is compositional vector grammar (Socher et al, 2013). To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels.  This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). 
  Grosz et al (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse. The proposed method employs contextual features based on centering theory (Grosz et al, 1983) as well as conventional syntactic and word-based features. To resolve referring expression, one of the well known methods is centering theory developed by Grosz, Joshi, and Weinstein (Grosz et al (1983)).
Fidditch is one such deterministic parser, designed to provide a syntactic analysis of text as a tool for locating examples of various linguistically interesting structures (Hindle 1983). Hindle (1983) addressed the problem of correcting self repairs by adding rules to a deterministic parser that would remove the necessary text.  Example: I think that you get - it is more strict in Catholic schools. (Hindle 1983). (Hindle, 1983) and (Bear et al., 1992) performed speech repair identification in their parsers, and removed the corrected material (reparandum) from consideration. (Hindle, 1983) states that repairs are available for semantic analysis but provides no details on the representation to be used. RIM builds upon Labov (1966) and Hindle (1983) by conceptually extending the EDIT SIGNAL HYPOTHESIS that repairs are acoustically or phonetically marked at the point of interruption of fluent speech. One proposal for repair processing that lends itself to both incremental processing and the integration of speech cues into repair detection is that of Hindle (1983), who defines a typology of repairs and associated correction strategies in terms of extensions to a deterministic parser. An hypothesized acoustic phonetic edit signal, "a markedly abrupt cut-off of the speech signal" (Hindle, 1983 ,p.123), is assumed to mark the interruption of fluent speech (cf. (Labov, 1966)). Importantly, Hindle's system allows for non surface-based corrections and sequential application of correction rules (Hindle, 1983, p. 123). RIM incorporates two main assumptions of Hindle (1983): (1) correction strategies are linguistically rule-governed, and (2) linguistic ues must be available to signal when a disfluency has occurred and to 'trigger' correction strategies. Other than syntactic knowledge includes grammar specific recovery rules such as recta-rules (Weishedel and Sondheimer, 1983), semantic or pragmatic knowledge which may depend on a particular domain (Carbonell and Hayes, 1983) or the characteristics of the ill-formed utterances observed in human discourse (Hindle, 1983). 
Description theory (henceforth, D-theory) (Marcus et al (1983)). This model is interesting in that it does not allow the parser to employ delay tactics, such as using a lookahead buffer (Marcus (1980), Marcus et al (1983)), or waiting for the head of a phrase to appear in the input before constructing that phrase (Abney (1987, 1989), Pritehett (1992)). The original D-theory model (Marcus et al (1983)) is also more powerful, because it allows the right-most daughter of a node to be lowered under a sibling node. More on Dominance Links Dominance links are quite common in tree description formalisms, where they were already in use in D-theory (Marcus et al, 1983) and in quasi-tree semantics for fb TAGs (Vijay-Shanker, 1992). S /npvp Mary/ V S th inks /npvp^ John M&C suggest various possibilities for packing the partial syntax trees, including using Tree Adjoining Grammar (Joshi 1987) or Description Theory (Marcus et al 1983).
such as Earley deduction, to coestruct a parser, a.s shown in Pereira and Warren (1983). For reasons of time, I won't go into details of Earley Deduction (see Pereira and Warren (1983) for details}. Deductive logic (Pereira and Warren, 1983), extended with semi rings (Goodman, 1999), is an established formal ism used in parsing. While chart parsing can famously be cast as deduction (Pereira and Warren, 1983), what chart parsing really is is an algebraic closure over the rules of a phrase structure grammar, which is most naturally expressed inside a constraint solver such as CHR (Morawietz, 2000). Basically, it is similar to Earley's algorithm (Earley, 1970), augmented with unification (Pereira and Warren, 1983) and probability (Paeseler, 1987), but with a delayed commitment approach to scoring (Aho and Peterson, 1972). The "parsing as deduction" framework (Pereira and Warren, 1983) is now over 20 years old. But if we use existing techniques for parsing DCGs, then we are also confronted with an undecidability problem: the recognition problem for DCGs is undecidable (Pereira and Warren, 1983). In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. It is exactly this mismatch between structure of the traversal and Pereira and Warren (1983) point out that Earley deduction is not restricted to a left-to-right expansion of goals, but this suggestion was not followed up with a specific a lgorithm addressing the problems discussed here. The parsing-as-deduction approach proposed in Pereira and Warren (1983) and exlended in Shieber et al (1995) and the parsing schemaladetincd in Sikkel (1997) are well established parsing paradigms in computalional linguistics. One of the first definitions was suggested by Pereira and Warren (1983).  Pereira and Warren (1983) and Shieber (1985) present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation.
Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. One example of the gap between description and computational implementation is disjunctive feature representation, which became popular in feature-based grammar formalisms in the 1980s (Karttunen, 1984). The feature structure (adopted from Karttunen, 1984, p. 30) represents disjunctions by enclosing the alternatives in curly brackets ({}).
Some work has touched on shared knowledge of lexical semantics [aacobs, 1985, Steinacker and Buchberger, 19831 and on grammatical framework suitable for bidirectional systems [Kay, 1984]. As mentioned, FCG organises the information about an utterance in feature structures, similar to other feature-structure based formalisms (as first introduced by Kay (Kay, 1984)) but with some important differences. The family of grammar models that are based on such formalisms include Generalized Phrase Structure Grammar (GPSG) [Gazdar et al 1985], Lexical Functional Grammar (LFG) [Bresnan 1982], Functional Unification Grammar (bUG) [Kay 1984], Head-Driven Phrase Structure Grammar (I-IPSG) [Pollard and Sag 1988], and Categorial Unification Grammar (CUG) [Karttunen 1986, Uszkoreit 1986, Zeevat et al 1987]. The text planner is implemented as a Functional Unification Grammar (Kay 1984) in FUF (Elhadad 1993).
Indeed, we have demonstrated the feasibility (Alshawi et al, 1985) of driving a parsing system directly from the information available in LDOCE by constructing dictionary entries for the PATR-H system (Shieber, 1984). We borrow the terminology and notation of PATR-II (Shieber, 1984), a minimal constraint-based formalism that extends context-free grammar. Nearly all existing unification grammars of this kind use either term unification (the kind of unification used in resolution theorem provers, and hence provided as a primitive in PROLOG) or some version of the graph unification proposed by Kay (1985) and Shieber (1984). It is not, unfortunately, possible to keep it close to both FUG and PATR (Shieber 1984), but it should be possible for readers familiar with PATR to see roughly what the relation between the two is.  Since formalisms (i) are used in the family of the PATR parsing systems (Shieber, 1984), hereafter they will be called PATR-like formalisms.
The discourse grammar used builds on [Polanyi and Scha 1984]. Such structures have received considerable attention and their models are often referred to as discourse/dialogue grammars (Polanyi and Scha, 1984) or conversational/dialogue games (Levin and Moore, 1988). It is also closer to some methods for incremental daptation of discourse structures, where additions are allowed to the right-frontier of a tree structure (e.g. Polanyi and Scha 1984).
Events are represented by event variables, as in [Hobbs, 1985], so that see' (e1, x1, x2) means e1 is a seeing event by x1 of x2. First, as originally advocated by Hobbs (1985), we adopt an ONTOLOGICALLY PROMISCUOUS representation that includes a wide variety of types of entities. Second, in keeping with ontological promiscuity (Hobbs, 1985), we represent the importance of attributes by the salience of events and states in the discourse model - these states and events now have the same status in the discourse model as any other entities. [Hobbs 1985] describes a similar approach by introducing what he calls 'nominalization'. Note that the predicate language representation utilized by Carmel-Tools is in the style of Davidsonian event based semantics (Hobbs, 1985).   See Hobbs (1985a) for explanation of this notation for events. For justification for this kind of logical form for sentences with quantifiers and intensional operators, see Hobbs (1983) and Hobbs (1985a). We adopt their idea of an utterance as a description, generated from a communicative goal, and also use an "ontologically promiscuous" formalism for representing meaning [Hobbs, 1985]. [Hobbs, 1985] framework for representing the propositional content (or meaning) of an expression as an ontologically richly sorted, relational structure. The underlying core theories are expressed as axioms in this notation (Hobbs, 1985). First, we adopt an ONTOLOGICALLY PROMISCUOUS representation (Hobbs, 1985) that includes a wide variety of types of entities. Doing inference with representations close to natural language has also been advocated by Jerry Hobbs, as in (Hobbs, 1985). Nevertheless, as (Hobbs, 1985) and others have argued, semantic representations for natural language need not be higher-order in that ontological promiscuity can solve the problem. Moreover, as stated in (Hobbs, 1985), we assume that the alleged predicate is existentially opaque in its second argument. All of the following discussion is based on a model of semantic analysis similar to that proposed in (Hobbs, 1985). We do not completely rule out the possibility that some more sophisticated, ontologically promiscuous, first-order analysis (perhaps along the lines of (Hobbs, 1985)) might account for these kinds of monotonicity inferences. An add-on to ESG converts the parse tree into an LF in the style of Hobbs (1985).
Schabes and Joshi (1988) and Vijay-Shanker and Joshi (1985) provide parsing algorithms for TAGs that could serve to parse the base formalism of a synchronous TAG. Vijay-Shanker and Joshi (1985) introduced the first TAG parser in a CYK-like algorithm. In particular, the algorithm presented in [Vijay-Shanker and Joshi, 1985] can be seen to corresponds to the approach involving the use of cfg to encode derivations, whereas, the algorithm of [Vijay-Shanker and Weir, in pressb] uses lig in this role.  As an example, we introduce a CYK-based algorithm (Vijay-Shanker and Joshi, 1985) for TAG. In this section, we make a comparison of several different TAG parsing algorithms - the CYK based algorithm described at (Vijay-Shanker and Joshi, 1985), Earley-based algorithms with (Alonso et al, 1999) and without (Schabes, 1994) the valid prefix property (VPP), and Nederhof's algorithm (Nederhof, 1999) - on the XTAG English grammar (release 2.24.2001), by using our system and the ideas we have explained.
An answer to these problems was presented by Shieber (1985) who proposed to do Earley prediction on the basis of some finite quotient of all constituent DAGs which can be specified by the grammar writer. Change in the citation purpose of Shieber (1985) paper. Similar analysis can be applied to the change in citation purpose of Shieber (1985) as illustrated in Figure 1. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley's algorithm by using. PATR-II (Shieber, 1985) into context-free grammars (CFG). The abstraction is specified by means of a restrict or (Shieber, 1985), the so-called lexicon restrict or. Pereira and Warren (1983) and Shieber (1985) present versions of Earley's algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation. After establishing a correspondence b tween attribute and unification grammar (UG), we may see that the technique of 'restrictions'; used by Shieber (1985) in his extended algorithm is related to finite partitioning on attribute domains, in fact a particular case which takes advantage of the more structured attribute domains of UG. We can use the technique of restriction (Shieber 1985) to remove these features from our feature structures. [Shieber, 1985] therefore proposes a modified version of the Earley-parser, using restricted top down prediction.  For UGs which lack a so-called context-free back-bone, such as CUG, the top-down prediction step can only be guaranteed to terminate if we make use of restriction, as defined in Shieber (1985).  The performance of the parsing algorithms discussed in the preceding sections (a bottom-up parser for UG (BU), a top-down parser for UG (of Shieber, 1985) (TD), a top-down parser operating on an instantiated grammar (TD/1), and a bottom-up parser with top down filtering operating on an instantiated grammar (BU/LC)) were tested on two experimental CUGs, one implementing the morphosyntactic features of German NPs, and one implementing the syntax of WH-questions in Dutch by means of a gap-threading mechanism. Comparing our results with those of Shieber (1985) and Haas (1989), we see that in all cases top-down filtering may reduce the size of the chart significantly. However, such an adaptation of CF algorithms involves their extension to possibly infinite nonterminal domains, which, as Shieber (1985) and Haas (1989) have shown, is nontrivial. Shieber (1985, 1992) follows established terminology in speaking of top-down filtering in connection with the prediction step of the Earley algorithm. We view the linking relation not simply as a filter to increase fficiency within the domain of syntactic analysis - this aspect is stressed by Shieber (1985) and other investigators such as Bouma (1991) - but rather as a device for the top-down predictive instantiation of information, as Shieber et al (1990) have shown for semantic-head-driven generation.  This amounts to the simplest case of the restriction technique of Shieber (1985).
In early work, Palmer et al (1986) discussed filling null complements from context by using knowledge about individual predicates. While the work of Palmer et al (1986) relied on special lexicons, one might instead want to learn information about the semantic content of different role fillers and then assess for each of the potential referents in the discourse context whether their semantic content is close enough to the expected content of the null instantiated role. The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al (1986). A further improvement may derive from the integration of an anaphora resolution step, as first proposed by Palmer et al (1986) and more recently by Gerber and Chai (2010). Palmer et al (1986) made one of the earliest attempts to automatically recover extra-sentential arguments. Palmer et al (1986) treated unfilled semantic roles as special cases of anaphora and co reference resolution (CR).
This is the same sort of preference that is addressed by property-sharing constraint (Kameyama, 1986). Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. In this paper, we use centering theory (Kameyama, 1986) to determine how easily a noun phrase can be referred to in the following context. Kameyama (1986) emphasized the importance of a property-sharing constraint.  There are two versions of the centering theory that have been applied to Japanese zero pronoun resolution: Kameyama's (Kameyama, 1986) and Walker's (Walker et al, 1994).
The approach that Vijay-Shanker et al (1987) and Weir (1988) take, elaborated on by Becker et al (1992), is to identify a very general class of formalisms, which they call linear context free rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Here we use the standard definition of LCFRS (Vijay-Shanker et al, 1987) and only fix our notation; for a more thorough discussion of this formalism, we refer to the literature. There are many (structural) mildly context sensitive grammar formalisms, e.g. mcfg, lcfrs, mg, and they have been shown to be equivalent (Vijay-Shanker et al., 1987). They are in particular more powerful than linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al, 1987). Following this line, (Vijay-Shanker et al, 1987) have introduced a formalism called linear context-free rewriting systems (LCFRSs) that has received much attention in later years by the community. We briefly summarize here the terminology and notation that we adopt for LCFRS; for detailed definitions, see (Vijay-Shanker et al, 1987). Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) (Vijay-Shanker et al, 1987), a class of formal systems that generalizes several mildly context-sensitive grammar formalisms. In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al, 1987). We briefly summarize the terminology and notation that we adopt for LCFRS; for detailed definitions, see Vijay-Shanker et al (1987). LCFRS (Vijay-Shanker et al, 1987) are a natural extension of CFG in which a single nonterminal node can dominate more than one continuous span of terminals. A LCFRS ( Vijay-Shanker et al, 1987) is a tuple G= (N, T, V, P, S) where N is a finite set of non-terminals with a function dim: N? N that determines the fan-out of each A? N. In particular, we cast new light on the relationship between CCG and other mildly context-sensitive formalisms such as Tree-Adjoining Grammar (TAG; Joshi and Schabes (1997)) and Linear Context-Free Rewrite Systems (LCFRS; Vijay-Shanker et al (1987)). By this result, CCG falls in line with context-free grammars, TAG, and LCFRS, whose sets of derivational structures are all regular (Vijay-Shanker et al., 1987). It is important to note that while CCG derivations themselves can be seen as trees as well, they do not always form regular tree languages (Vijay-Shanker et al, 1987). On this line of investigation, mildly context-sensitive grammar formalisms have been introduced (Joshi,1985), including, among several others, the tree ad joining grammars (TAGs) of Joshi et al (1975). Linear context-free rewriting system (LCFRS), introduced by Vijay-Shanker et al (1987), is a mildly context-sensitive formalism that allows the derivation of tuples of strings, i.e., discontinuous phrases. CFTG are weakly equivalent to the simple macro grammars of Fischer (1968), which are a notational variant of the well-nested linear context-free rewriting systems (LCFRS) of Vijay-Shanker et al (1987) and the well-nested multiple context-free grammars (MCFG) of Seki et al (1991).
  Centering models local coherence rather generally and has been applied to the generation of referring expressions (Kibble and Power, 2004), to resolve pronouns (Brennan et al, 1987, inter alia), to score essays (Miltsakaki and Kukich, 2004), to arrange sentences in the correct order (Karamanis et al, 2009), and to many other tasks. To our knowledge, there are only two focus-based pronoun resolution algorithms that are specified in enough detail to work on unrestricted naturally occurring text: Brennan et al (1987) using the definition of utterance according to Kameyama (1998), and Struhe (1998). In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al, 1987) with and without specifications for complex sentences (Kameyama, 1998). For their centering algorithm, Brennan et al (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift. To illustrate this algorithm, we consider example (1) (Brennan et al, 1987) which has two different final utterances (ld) and (ld~). Table 1 shows the most common classification into the four types CONTINUE, RETAIN, SMOOTH-SHIFT, and ROUGH-SHIFT, which are predicted to be less and less coherent in this order (Brennan et al, 1987). Finally, the measure M.BFP (Brennan et al, 1987) uses a lexicographic ordering on 4-tupleswhich indicate whether the transition is a CONTINUE, RETAIN, SMOOTH-SHIFT, or ROUGH SHIFT. Note that common centering algorithms (e.g., the one by Brennan et al (1987)) are specified only for the resolution of anaphors in Ui-1. The rules are applied locally, across adjacent sequences of utterances (Brennan et al., 1987). The ranking of the CFs other than the CP is defined according to the following preference on their gf (Brennan et al, 1987): obj>iobj>other. We also make note of the preference between these transitions, known as Centering Rule 2 (Brennan et al, 1987). The main assumptions of the theory as presented by (Gross et al 1995 (GJW), Brennan et al1987). For instance Passoneau (1998) refers to two variants of CT: Version A, based on Brennan et al (1987) and Version B, taken from Kameyama et al (1993). The first strategy is clearly appropriate for interpretation (cf. Brennan et al 1987) but for generation the issue is less clear-cut. Brennan et al [1987] propose an algorithm for pronoun resolution based on centering theory. The latter proposes the ranking subject, direct object, indirect object (Brennan et al 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects. Hard-core centering approaches only deal with the last sentence (Brennan et al, 1987). PF.BFP which is based on PF as well as the original formulation of CT in [Brennan et al, 1987].
It is well-known that disjunctive unification is NP complete (Kasper 1987). The unification of disjunctive feature structures is implemented according to Kasper's algorithm (Kasper, 1987). The general problem of unifying two disjunctive feature structures is non-polynomial in the number of disjunctions (Kasper, 1987). There is reason to hope that this will often be the case; while disjunction may be widespread in grammar ules and texical entries, Kasper (1987) observes that in his implementation "in the analysis of a particular sentence most features have a unique value, and some features are not present at, all". Kasper (1987) describes a technique which, for every set of n conjoined disjunctions, checks the consistency first of single disjuncts against the definite part of the description, then that of pairs and so on up ton-tuples for full consistency. This variation of the algorithm given in Kasper (1987) is closer to Propane's strategy, but the expansion to full DNF is itself in general an exponential process and will, when many disjunctions remain, be far more expensive than looking for a single realization. Unification time here is order n log n in the sizes n of the input structures (Kasper, 1987). Kasper (1987) and Eisele and DSrre (1988) have tackled this problem and proposed unification methods for disjunctive feature descriptions.
Two main extensions from that work that we are making use of are: 1) proofs falling below a user defined cost threshold halt the search 2) a simple variable typing system reduces the number of axioms written and the size of the search space (Hobbs et al, 1988, pg 102). The domain axioms will bind the body variables to their most likely referents during unification with facts, and previously assumed and proven propositions similarly to (Hobbs et al, 1988). A third approach, exemplified by Moldovan et al (2003) and Raina et al (2005), is to translate dependency parses into neo-Davidsonian-style quasi-logical forms, and to perform weighted abductive theorem proving in the tradition of (Hobbs et al, 1988). As shown in "Interpretation as Abduction" (Hobbs et al 1988), abductive inference is inference to the best explanation. While pursuing the path tracing enabling minimal explanation, now we are going to propose a connectability measure similar such as "weighted abduction" (Hobbs et al 1988).
Whittaker and Stenton (1988) devised rules for allocating dialogue control based on utterance types, and Walker and Whittaker (1990) utilized these rules for an analytical study on discourse segmentation. Suppose we adopt a model that maintains a single thread of control, such as that of (Whittaker and Stenton, 1988). Repetitions and prompts also suggest that the speaker has nothing more to say and indicate that the hearer should take over the initiative (Whittaker and Stenton, 1988). Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Initiative is held by the speaker who is driving the conversation at any given point in the conversation (Whittaker and Stenton, 1988). Whittaker and Stenton (1988) looked at the correlation of control boundaries to discourse markers, and Walker and Whittaker (1990) looked at anaphoric reference. In our study, the first author coded initiative using the annotation scheme of Whittaker and Stenton (1988). Whittaker and Stenton (1988) assigned initiative to the speaker of statements, except when it was the answer to a question, in which case it belonged to the speaker asking the question. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Following Whittaker and Stenton (1988), we use utterance tags to determine whether an utterance shows initiative: forward functions show initiative while others do not. After experimenting with several tagging methods, we concluded that the approach presented in Walker and Whittaker (1990) adopted from (Whittaker and Stenton, 1988) best captured the aspects of the dialogue we were interested in and, as with the DAs, could be tagged reliably on our data. Constructs such as Initiative and Control (Whittaker and Stenton, 1988), which attempt to operationalize the authority over a discourse's structure, fall under the umbrella of positioning. A speaker who begins a discourse segment is said to have initiative, while control accounts for which speaker is being addressed in a dialogue (Whittaker and Stenton, 1988).
  Some text planners take as their goal the delivery of a single fact, or a set of facts, but take into account that other information may need to be given first for each message to be understood, other information may need to be given after to counter misconceptions, and examples may be given to aide assimilation (for in stance, the RST text planners, such as Hovy 1988). This approach is adopted from text generation where plan-operators are responsible for choosing linguistic means in order to create coherent stretches of text (see, for instance, (Moore and Paris, 1989) and (Hovy, 1988)). As for any generated text, a good summary also requires a text plan (Hovy, 1988) (McKeown, 1985).
The infinite application of grammar rules is a common problem of the existing top-down unification-based generators (Shieber et al, 1989). One of the termination problems Shieber et al (1989) pointed out is in the left-recursive rules. Though the restriction can not be applied to languages like Dutch (Shieber et al, 1989), the limitation is irrelevant to our purpose (translation between Japanese and English). The actual realization of the component is based on a constraint-based inheritance algorithm that follows the example of PATR-II (Shieber et al, 1989). The lexicon match is not based on direct unification of the target phrase's semantics with that of its head, a fundamental requirement of the bottom-up head-driven algorithm of Shieber et al (1989) and Van Noord (1990).
 Full brevity, the strongest interpretation, is underlying Dale's algorithm (Dale, 1989), which produces a description entailing the minimal number of attributes possible, at the price of suffering NP-hard complexity. Two other interpretations, the Greedy heuristic interpretation (Dale, 1989) and the local brevity interpretation (Reiter, 1990a) lead to algorithms that have polynomial complexity in the same order of magnitude.  Presented in (Dale, 1989), (Dale and Haddock, 1991) and (Reiter and Dale, 1992), we explore how well these algorithms perform in the same context. The Full Brevity algorithm (Dale, 1989) attempts to build a minimal distinguishing description by always selecting the most discriminatory property available;. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribute set that uniquely refers to the intended referent (Dale, 1989). The first of these is Minimality, defined as the proportion of descriptions produced by a system that are maximally brief, as per the original definition in Dale (1989). Just one system adopts the FullBrevity approach of Dale (1989), while the majority (11 systems) adopt the Dale and Reiterconvention of always adding TYPE. Yet, brevity plays a part in all GRE algorithms, sometimes in a strict form (Dale, 1989), or by letting the algorithm approximate the shortest description (for example, in the Dale and Reiter's IA). Due to its hill climbing nature, the IA avoids combinatorial search, unlike some predecessors which searched exhaustively for the briefest possible description of a referent (Dale, 1989), based on a strict interpretation of the GriceanMaxim of Quantity (Grice, 1975). Several NLG systems adapt to the user's domain expertize at different levels of generation text planning (Paris, 1987), complexity of instructions (Dale, 1989), referring expressions (Reiter, 1991), and so on. These measures were included because they are commonly named as desiderata for attribute selection algorithms in the REG field (Dale, 1989). Many studies in natural language processing are concerned with how to generate definite descriptions that evoke a discourse entity already introduced in the context. A solution to this problem has been initially proposed by Dale (1989) in terms of distinguishing descriptions and distin guishable entities. Following Dale (1989), these definite descriptions are named distinguishing descriptions.  Since the system knows which nodes in the semantic graph have already been mentioned it would also be possible to configure an external call to a GRE system (Dale, 1989) an application which infers the content of a referring expression given the current semantic context.  Much of this work takes as its starting point the characterisation of the problem expressed in (Dale, 1989).
Church and Hanks (1989) discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions (content word/function word). We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective. Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). (Breidt, 1995) has evaluated the usefulness of the Point-wise Mutual Information measure (as suggested by (Church and Hanks, 1989)) for the extraction of V-N collocations from German text corpora. We encode the semantic compatibility between a noun and its parse tree parent (and grammatical relationship with the parent) using mutual information (MI) (Church and Hanks, 1989).  We weigh each context f using point wise mutual information (Church and Hanks 1989). Z (2) d:: l j= i (d -1 )dmax: max distance used wl: the i-th letter in the sentence w g (d): a certain weight for iV// concerning distance between letters The information between two remote words has less nmaning in a sentence when it comes to the semantic analysis (Church and Hanks, 1989). Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness be tween words. I also experimented with the co-occurence frequency c (s, w) and point-wise mutual information (Church and Hanks, 1989) as similarity functions. Beginning with (Church and Hanks, 1989), numerous authors have used the point wise mutual in formation between pairs of words to analyze word co-locations and associations. Point-wise mutual information (PMI, Church and Hanks (1989)) is used to capture the semantic relatedness of the candidate to the topic of the document. Early approaches to identifying MWEs concentrated on their collocational behavior (Church and Hanks, 1989). Specifically, the measure Snom (vt ,vh) is derived from point-wise mutual information (Church and Hanks, 1989): Snom (vt ,vh)= log p (vt, vh|nom) p (vt) p (vh|pers) (3) where nom is the event of having a nominalized textual entailment pattern and pers is the event of having an agentive nominalization of verbs. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003). Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989). Church and Hanks (1989) proposed a measure of association called Mutual Information. We have used POINTWISE MUTUAL INFORMATION (PMI, Church and Hanks (1989)) to account for the differences in information value between the several headwords and attributes. The explanation for such a behavior is: since we are not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989).
One is the case in which a pronoun x correctly says that it is coreferent with another pronoun y. However, the program misidentifies the antecedent of y. In this case (sometimes called error chaining (Walker, 1989)), both x and y are to be scored as wrong, as they both end up in the wrong coreferential chain. I use the following guidelines for the hand-simulated analysis (Walker, 1989). Following Walker (1989), a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal oun phrases matches its syntactic features. 
 An architecture which fulfills this requirement is Weighted Constraint Dependency Grammar, which was based on a model originally proposed by Maruyama (1990) and later extended with weights (Schroeder, 2002). Originally, the constraints were comprised of a set of hand-written rules specifying which role values (unary constraints) and pairs of role values (binary constraints) were grammatical (Maruyama, 1990). Using constraint satisfaction techniques for natural language parsing was introduced first in (Maruyama, 1990) by defining a constraint dependency grammar (CDG) that maps nicely on the notion of a CSP.
As initiative passes back and forth between discourse participants, control over the conversation similarly transfers from one speaker to another (Walker and Whittaker, 1990). Walker and Whittaker (1990) found a correlation between initiative switches and discourse segments. After experimenting with several tagging methods, we concluded that the approach presented in Walker and Whittaker (1990) adopted from (Whittaker and Stenton, 1988) best captured the aspects of the dialogue we were interested in and, as with the DAs, could be tagged reliably on our data. Before embarking on an exhaustive manual annotation of initiative, I chose to get a sense of whether initiative may indeed affect learning in this context by automatically tagging for initiative using an approximation of Walker and Whittaker's utterance based allocation of control rules (Walker and Whittaker, 1990). Walker and Whittaker claim that initiative encompasses both dialogue control and task control (Walker and Whittaker, 1990), however, several others disagree. For dialogue initiative annotation, I am using Walker and Whittaker's utterance based allocation of control rules (Walker and Whittaker, 1990), which are widely used to identify dialogue initiative. Whittaker and Stenton (1988) looked at the correlation of control boundaries to discourse markers, and Walker and Whittaker (1990) looked at anaphoric reference. Walker and Whittaker (1990) suggested that changes in initiative correspond to change sin discourse structure, but they did not determine the exact relationship between them. For dialogue initiative annotation, we used the well-known utterance-based rules for allocation of control from (Walker and Whittaker, 1990). Once the dialogue acts had been automatically annotated, two coders, one of the authors and an out side annotator, coded 24 dialogues (1449 utterances, approximately 45% of the corpus) for dialogue initiative, by using the four control rules from (Walker and Whittaker, 1990).
The next set of lookup levels described, in which the structural word is no longer the primary entity but just a handle for indexing graphs, can be built on the basis of knowledge assembled from large volumes of text; the methodology employed by Smadja and McKeown (1990) would be one of many techniques possible for obtaining and organizing the various levels of model graphs. Collocation acquisition is, of course, not a goal by itself, but rather aims at creating collocation lexicons for both language processing and generation (Smadja and McKeown, 1990).  Previous studies have proposed many promising ways for this purpose, for instance, Smadja and McKeown (1990), and Frantzi and Ananiadou (1996) tried to treat more general structures like collocations.
Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns. The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994).   Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs.  The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990). Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. The only difference is that we also work on partial parsing as a task in its own right: Hindle (1990) inter alia. The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval.  In (Hindle, 1990), a small set of sample results are presented. Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990).
 It is well known that two languages are more informative than one (Dagan et al, 1991). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al, 1991) and separation of senses (Dyvik, 1998). For example, Brown et al (1991) and Gale et al (1992a, 1993) used the parallel, aligned Hansard Corpus of Canadian Parhamentary debates for WSD, and Dagan et al (1991) and Dagan and Ital (1994) used monolingual corpora of Hebrew and German and a bilingual dictionary.
 Statistical alignment methods at sentence level have been thoroughly investigated Gale and Church, 1991a/ 1991b; Brown et al, 1991. Early work by Peter Brown et al (1991) and William Gale and Kenneth Church (1991) aligned sentences which had a proportional number of words and characters, respectively. There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale and Church 1991 and Brown et al 1991), the lexical approach (key and Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). Brown, Lai and Mercer (Brown et al, 1991) used word count as the sentence length, whereas Gale and Church (Gale and Church, 1991) used character count. Using this search method meant that no prior segmentation of the corpora was needed (Moore, 2002), either in terms of aligned paragraphs (Gale and Church, 1991), or some aligned sentences as anchors (Brown et al, 1991). This model is trained on approximately 5 million sentence pairs of Hansard (Canadian parliamentary) and UN proceedings which have been aligned on a sentence-by-sentence basis by the methods of (Brown et al, 1991), and then further aligned on a word-by-word basis by methods similar to (Brown et al, 1993). Brown et al (1991) and Gale and Church (1993) are amongst the most cited works in text alignment work. The method used by Brown et al (1991) measures sentence length in number of words. Given that lexical methods can be computationally expensive, our idea was to try a simple length-based approach similar to that of Brown et al (1991) for sentence alignment and then use lexical methods to align words within aligned sentences. Thus, dynamic programming based sentence alignment algorithms rely on paragraph anchors (Brown et al 1991) or lexical information, such as cognates (Simard 1992), to maintain a high accuracy rate. For instance, to produce sentence alignments, Brown et al (1991) and Gale and Church (1991) both proposed methods that completely ignored the lexical content of the texts and both reported accuracy levels exceeding 98%. The first length based algorithm was proposed in (Brown et al, 1991). Algorithm Length-based sentence alignment algorithm was first proposed in (Brown et al, 1991).  In (Gale and Church, 1991) and (Brown et al, 1991), the authors start from the fact that the length of a source text sentence is highly correlated with the length of its target text translation: short sentences tend to have short translations, and long sentences tend to have long translations. Among approaches that are unsupervised and language independent, (Brown et al, 1991) and (Gale and Church, 1993) use sentence-length statistics in order to model the relationship between groups of sentences that are translations of each other. Several automatic sentence alignment approaches have been proposed based on sentence length (Brown et al, 1991) and lexical information (Kay and Roscheisen, 1993). A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al, 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Inter net, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. The obtained accuracy is around 96% and was computed indirectly by checking disagreement with the Brown sentence aligner (Brown et al, 1991) on randomly selected 500 disagreement cases.
Gale and Church (1991) and Brown (1991) report the early works using length statistics of bilingual sentences. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. Brown, Lai and Mercer (Brown et al, 1991) used word count as the sentence length, whereas Gale and Church (Gale and Church, 1991) used character count. Using this search method meant that no prior segmentation of the corpora was needed (Moore, 2002), either in terms of aligned paragraphs (Gale and Church, 1991), or some aligned sentences as anchors (Brown et al, 1991). One reason for this is that there are more of characters than words (Gale and Church, 1991). Alignment is performed using dynamic programming (Gale and Church, 1991) with a weight function based on the number of common words in a sentence pair. In (Gale and Church, 1991) and (Brown et al, 1991), the authors start from the fact that the length of a source text sentence is highly correlated with the length of its target text translation: short sentences tend to have short translations, and long sentences tend to have long translations. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper noduns (organization, person, title), dates, and monetary information. We were motivated by statistical alignment models such as (Gale and Church, 1991) to investigate whether byte-length probabilities could improve or replace the lexical matching based method. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Automatic sentential alignment (Gale and Church, 1991) will be necessary if we have already had on line bilingual texts. Gale and Church (1991) noted that the byte length ratio of target sentence to source sentence is normally distributed. For bilingual texts, Gale and Church (1991) demonstrated the extraordinary effectiveness of a global alignment dynamic programming algorithm, where the basic similarity score was based on the difference in sentence lengths, measured in characters.  We used the precomputed alignments that are provided with the corpus, and which are based on the algorithm by Gale and Church (1991). A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al, 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Inter net, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. For instance, to produce sentence alignments, Brown et al (1991) and Gale and Church (1991) both proposed methods that completely ignored the lexical content of the texts and both reported accuracy levels exceeding 98%. Once the search space is reduced, the system aligns the sentences using the well-known sentence-length model described in (Gale and Church, 1991). The Bayesian prior can be estimated as per Gale and Church (1991) by assuming that it is equal to the frequency of distinct n-m matches in the training set.
The relatively high efficiency rate, as compared with the figures reported in (Brent, 1991), are due to the fact that Italian morphology is far more complex than English. Once a good morphologic analyzer is available (the one used in our work is very well tested, and has first described in (Russo, 1987)), problems such as verb detection, raised in (Brent, 1991), are negligible. Since (Brent 1991) there have been a considerable amount of researches focusing on verb lexicons with respective sub categorization information specified both in the field of traditional linguistics and that of computational linguistics. Finally, while statistical approaches like Brent (1991) can gather e.g. valence information from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.
(Hindle and Rooth, 1991) had shown the use of dependency in Prepositional Phrase disambiguation, and the experimental results reported in (Hock en maier, 2003) demonstrate that a language model which encodes a rich notion of predicate argument structure (e.g. including long-range relations arising through coordination) can significantly improve the parsing performances. Selectional preference has been shown to be useful for, e.g., resolving ambiguous attachments (Hindle and Rooth, 1991), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). But it is harder because it does not provide the predictor with all the information needed to solve many doubtful cases; (Hindle and Rooth, 1991) found that human arbiters consistently reach a higher agreement when they are given the entire sentence rather than just the four words concerned. (Hindle and Rooth, 1991) first proposed solving the prepositional attachment task with the help of statistical information, and also defined the prevalent formulation as a binary decision problem with three words involved.
As a result, we can find complementary cues in the two languages that help to disambiguate named entity mentions (Brown et al, 1991). It has been equally shown that collocations are useful in a range of other applications, such as word sense disambiguation (Brown et al, 1991) and parsing (Alshawi and Carter, 1994). Brown et al (1991) and Gale et al (1992) used the translations of the ambiguous word in a bilingual corpus as sense tags. Brown et al (1991) proposed a WSD algorithm to disambiguate English translations of French target words based on the single most informative context feature. The idea of obtaining linguistic information about a text in one language by exploiting parallel or comparable texts in another language has been explored in the field of Word Sense Disambiguation (WSD) since the early 90's, the most representative works being (Brown et al, 1991), (Gale et al, 1992), and (Dagan and Itai, 1994). TMs have been used for statistical machine translation (Bergeretal., 1996), word alignment of a translation corpus (Melamed, 2000), multilingual document retrieval (Franz et al, 1999), automatic dictionary construction (Resnik and Melamed, 1997), and data preparation for word sense disambiguation programs (Brown et al, 1991). Mutual information has been positively used in many NLP tasks such as collocation analysis (Church and Hanks, 1989), terminology extraction (Damerau, 1993), and word sense disambiguation (Brown et al, 1991). As a somewhat radical alternative to taxonomical relationships, other ways of measuring semantic similarity based on distributional evidence have been put forward in the literature (see, among others, Brown et al 1991, Gale et al 1992, Pereira and Tishby 1992), which emphasise the role played by context in this game.  Related Work Brown et al (1991) is the first to have explored statistical methods in word sense disambiguation in the context of machine translation. In contrast with Brown et al (1991), our approach incorporates the predictions of state-of-the art WSD models that use rich contextual features for any phrase in the input vocabulary. The reduction of input data requirements offers a significant advantage compared with methods such as those presented in Brown et al (1991), Gale et al (1992), Yarowsky (1995), and Karol and Edelman (1996) where strong reliance on statistical techniques for the calculation of word and context similarity commands large source corpora. Brown et al (1991) pioneered the use of statistical WSD for translation, building a translation model from one million sentences in English and French. The first statistical model of WSD was built by Brown et al (1991). A historical approach (Brown et al 1991) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.
Figure 1: Lexical entries and a sample derivation in LUD Logical Form, QLF which also is a monotonic representation language for compositional semantics as discussed in (Alshawi and Crouch, 1992). This led to the development of underspecified semantic representations (e.g. QLF, Alshawi and Crouch (1992) and MRS, Copestake et al (2005)) which are easier to produce from text without contextual inference but which can be further specified as necessary for the task being performed. In contrast to statements of a fully specified logic in which denotations are typically taken to be functions from possible worlds to truth values (Montague, 1973), denotations of a statement in an under specified logic are typically taken to be relations between possible worlds and truth values (Alshawi and Crouch (1992), Alshawi (1996)). Secondly, monotonicity guarantees that interpretation algorithms can proceed incrementally, combining information from various sources in a nondestructive way (Alshawi and Crouch, 1992). First, a quasi logical form allows the under-specification of several types of information, such as anaphoric references, ellipsis and semantic relations (Alshawi and Crouch, 1992).
Results of testing the first stage of this model, the lexical pattern matcher, are reported in (Bear et al, 1992): 309 of 406 utterance containing nontrivial  repairs in their 10,718 utterance corpus were correctly identified, while 191 fluent utterances were incorrectly identified as containing repairs. Bear et al (1992) also speculate that acoustic information might be used to filter out false positives for candidates matching two of their lexical patterns - repetitions of single words and cases of single inserted words - but do not report such experimentation. As noted in (Bear et al, 1992), knowledge about the location of word fragments would be an invaluable cue to both detection and correction of disfluencies. For annotating speech repairs, we have extended the scheme proposed by Bear et al (1992) so that it better deals with overlapping and ambiguous repairs. Bear et al (1992) explored pattern matching, parsing and acoustic cues and concluded that multiple sources of information would be needed to detect edit dis fluencies. The SRI group (Bear et al, 1992) employed simple pattern matching techniques for detecting and correcting modification repairs. (Hindle, 1983) and (Bear et al., 1992) performed speech repair identification in their parsers, and removed the corrected material (reparandum) from consideration. This approach is similar to an experiment in (Bear et al, 1992) except that Bear et al were more interested in reducing false alarms. 
The latter property is important as we would like to be able to use bracketing information in the input corpus as in (Pereira and Schabes, 1992). We use the data given by Pereira and Schabes (1992) on raw text and compare with an inferred SLTAG. As shown in Figure 2, the grannnar converges very rapidly to a lower value of the log probability than the stochastic ontext-free grammar eported by Pereira and Schabes (1992). Similarly to its context-free counterpart, the re-estimation algorithm can be extended to handle partially parsed corpora (Pereira and Schabes, 1992). This can be viewed as a variation of Pereira and Schabes (1992). The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992). The derivation of the estimation algorithm is largely omitted; see Pereira and Schabes (1992) for details. These improvements could make DBMs quick-and easy to bootstrap directly from any available partial bracketings (Pereira and Schabes, 1992), for example capitalized phrases (Spitkovsky et al2012b). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. These some what negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Following Pereira and Schabes (1992) given t=(s, U), a node's span in the parse forest is valid if it does not overlap with any span outlined in U, and hence, a derivation is correct if the span of every node in the derivation is valid in U. Experi mental results con rming this wisdom have been presented, e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. Hwa (1999) used a variant of the inside-outside algorithm presented in Pereira and Schabes (1992) to exploit a partially labeled out-of-domain tree bank, and found an advantage to adaptation over direct grammar induction. For this we use an approach similar to Pereira and Schabes' grammar induction from partially bracketed text (Pereira and Schabes, 1992). The role of supervision is to permit some constituents to be built but not others (Pereira and Schabes, 1992). Specifically, we use the Inside-Outside algorithm defined in (Pereira and Schabes, 1992) to learn transformed dependency grammars annotated with hidden symbols. CFGs extracted from such structures were then annotated with hidden variables encoding the constraints described in the previous section and trained until convergence by means of the Inside-Outside algorithm defined in (Pereira and Schabes, 1992) and applied in (Matsuzaki et al, 2005). The summation over target word sequences and alignments given fixed? t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992). Bracketed Corpora Training introduced by Pereira and Schabes (1992) employs a context free grammar and a training corpus, which is partially tagged with brackets. The language processing community actively works on the problem of automatically inducing grammatical structure from a corpus of text (Pereira and Schabes, 1992).
Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not.  The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. (Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation.
The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al, 1993), modified and extended to deal with robustness issues.  The work by Simard, Foster and Isabelle (1993) as well as Church (1993) demonstrated that cognate-matching strategies can be highly effective in aligning text. Church (1993) uses 4-grams at the level of character sequences. Using lexical information, Kenneth Church (1993) showed that cheap alignment of text segments was still possible exploiting orthographic cognates (Michel Simard et al, 1992), instead of sentence delimiters. We use the text Dotplotting representation by (Church, 1993) and plot the cosine similarity scores between every pair of sentences in the text. Simard and Plamondon (Simard and Plamondon, 1998) used a composite method in which the first pass does alignment at the level of characters asin (Church, 1993) (itself based on cognate matching) and the second pass uses IBM Model-1, following Chen (Chen, 1993).  In previous work (Church et al 1993), we have reported some preliminary success in aligning the English and Japanese versions of the AWK manual (Aho, Kernighan, Weinberger (1980)), using char align (Church, 1993), a method that looks for character sequences that are the same in both the source and target. Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992). This algorithm was applied to a fragment of the Canadian Hansards that has been used in a number of other studies: Church (1993) and Simard et al (1992). Currently ,word_align depends on char align (Church, 1993) to generate a starting point, which limits its applicability to European languages since char_align was designed for language pairs that share a common alphabet. Church (1993) observes that reliably distinguishing sentence boundaries for a noisy bi text obtained from an OCR device is quite difficult. The method uses length balance based alignment algorithm i.e. GaleChurch (Gale and Church, 1993), for the data collecting. Gale and Church (1993) describe a method for aligning sentences based on a simple statistical model of sentence lengths measured in number of characters. Levenshtein measure (Levenshtein, 1966) Church (1993) employs a method that induces sentence alignment by employing cognates (words that are spelled similarly across languages).
 As shown in Chen (1993) and Wu (1994), however, sentence length based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. Other translation-based alignments (Kay, 199l; Chen, 1993) show the difficulty in determining the word correspondence and are very complex. As Chen (1993) points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages. Simard and Plamondon (Simard and Plamondon, 1998) used a composite method in which the first pass does alignment at the level of characters asin (Church, 1993) (itself based on cognate matching) and the second pass uses IBM Model-1, following Chen (Chen, 1993). (The problem of aligning parallel corpora at the sentence level has been addressed by Meyers (1998b) Chen (1993) and others and is beyond the scope of this paper). By adopting the IBM model 1, (Chen 1993) used word translation probabilities, which he showed gives better accuracy than the sentence length based method. (Simard and Plamondon 1996) used a two-pass approach, where the first pass performs length-based alignment at the character level as in (Gale and Church 1993) and the second pass uses IBM Model 1, following (Chen 1993). Each type of the web page sentence aligner makes use of three conventional sentence alignment models, one is the length based model following (Brown 1991), one is the lexicon based model following (Chen 1993), and the other one is the hybrid model presented in (Zhao 2002).  Other examples of lexical methods are Warwick et al (1989), Mayers et al (1998), Chen (1993) and Haruno and Yamazaki (1996). Chen (1993) constructs a simple word-to-word translation model and then takes the alignment that maximizes the likelihood of generating the corpus given the translation model. Another case study of sentence alignment that we will present here is that of Chen (1993). EMACC finds only 1:1 textual units alignments in its present form but a document pair can be easily extended to a document bead following the example from (Chen, 1993). Asshown in (Chen, 1993) the accuracy of sentence length based methods decreases drastically when aligning texts containing small deletions or free translations. The approaches by (Chen, 1993), (Ceausu et al, 2006) or (Fattah et al, 2007) need manually aligned pairs of sentences in order to train the used alignment models. There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale and Church 1991 and Brown et al 1991), the lexical approach (Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). Chen (1993) combines the length-based approach and lexicon-based approach together.
 For instance, Kupiec (1993) uses statistical techniques and extracts bilingual noun phrases from parallel corpora tagged with terms. There is a long tradition of research into bilingual terminology extraction (Kupiec, 1993), (Gaussier, 1998). Kupiec (1993) applied finite state transducer in his noun phrases recogniser for both English and French. Kupiec (1993) attempted to find noun phrase correspondences in parallel corpora using part-of-speech tagging and noun phrase recognition methods. Kupiec (1993) also briefly mentions the use of finite state NP recognizers for both English and French to prepare the input for a program that identified the correspondences between NPs in bilingual corpora, but he does not directly discuss their performance. For example, Kupiec (1993) presented a method for finding translations of whole noun phrases. Bound-length N-gram correspondences include (Kupiec, 1993) where NP recognizers are used to extract translation units and (Smadja et al, 1996) which uses the Extract system to extract collocations. We informally evaluated the MWE extraction tool following Kupiec (1993) by manually inspecting the mapping of the 100 most frequent terms.  Kupiec proposes an Mgorithm for finding noun phrases in bilingual corpora (Kupiec, 1993). The plausible hypothesis that parallel sentences containing corresponding linguistic expressions is the major premise in Kupiec (1993). In Kupiec (1993) and Yamamoto (1993), term and phrase extraction is applied to both of parallel texts. Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences (Kupiec, 1993), fixed/flexiblecollocations (Smadja et al, 1996), n-gram word sequences of arbitrary length (Kitamura and Matsumoto, 1996), non-compositional compounds (Melamed, 2001), captoids (Moore, 2001), and named entities. Kupiec (1993) focuses on noun-phrase translations only, Smadja et al (1996) limits to find French translation of English collocation identified by his Xtract system, and Kitamura and Matsumoto (1996) can exhaustively enumerate only rigid word sequences. Most bilingual terminology extraction systems first identify candidate terms in the source language based on predefined source patterns, and then select translation candidates for these terms in the target language (Kupiec, 1993). Part-of-speech taggers are used in a few applications, such as speech synthesis (Sproat et al, 1992) and question answering (Kupiec, 1993b).  
They train a statistical parser on trees with only semantic labels on the nodes; however, they do not integrate syntactic and semantic parsing. History-based models of parsing were first introduced in (Black et al, 1993). In recent years, re-ranking techniques have been successfully used in statistical parsers to rerank the output of history-based models (Black et al, 1993). The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al, 1993). Instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, Black et al (1993) call this the history).
Recently, Dowding et al (1993) reported syntactic and semantic coverage of 86% for the DARPA Airline reservation corpus (Dowding et al, 1993). In a test set containing 26 repairs (Dowding et al, 1993), they obtained a detection recall rate of 42% and a precision of 84.6%; for correction, they obtained a recall rate of 30% and a recall rate of 62%. Dowding et al (1993) used a similar setup for their data. Typed Unification Grammars (TUG), like HPSG (Pollard and Sag 1994) and Gemini (Dowding et al. 1993) are a more expressive formalism in which to write formal grammars. Initial language processing is carried out Using the SRI Gemini system (Dowding et al, 1993), using a domain-independent unification. We use the Open Agent Architecture (Martin et al 1999) for communication between agents based on the Nuance speech recognizer, the Gemini natural language system (Dowding et al 1993), and Festival speech synthesis. Finally, we are developing an interface to a new large-vocabulary version of the Gemini parser (Dowding et al, 1993) which will allow us to use semantic parse information as features in the individual sub-class classifiers, and also to extract entity and event representations from the classified utterances for automatic addition of entries to calendars and to-do lists. Questions are posed to GeoLogica in a subset of English and translated into logic by a natural language parser, the system Gemini (Dowding et al, 1993). This is contrasted with the all-paths bottom-up strategy in GEMINI (Dowding et al 1993) that finds all admissable edges of the grammar. To handle the ASR results of disfluent utterances, we employ SRI's Gemini robust language parser (Dowding et al, 1993). The most likely hypothesis is input to SRI's Gemini natural language parser/generator (Dowding et al. 1993), which attempts to parse the speech recognition output. The dialogue system uses the Nuance 8.0 speech recognizer with language models compiled from a grammar (written using the Gemini system (Dowding et al, 1993)), which is also used for parsing and generation. Annotating sub-constituents with grammatical relations regularizes the syntactic structure with respect to particular grammatical rules, and allows a 'relation-to-relation' form of compositionality, as opposed to the more traditional 'rule-to-rule' version that is exemplified by such systems as Gemini (Dowding et al 1993) and the Core Language Engine (Alshawi, 1992).
This also distinguishes our approach from another major stream of object-oriented natural anguage parsing which is almost entirely concerned with implementational aspects of object-oriented programming, e.g., Habert (1991), Lin (1993) or Yonezawa& amp; Ohsawa (1994). Throughout this paper we used syntactic features generated by the Minipar depend ency parser (Lin, 1993). We obtain candidate properties by parsing a large textual corpus with the Minipar parser (Lin 1993). We used the Minipar parser (Lin 1993) to analyze each sentence and we collected the frequency counts of the grammatical contexts output by Minipar and used them to compute the probability and point wise mutual information values from Sections 4.1 and 4.2. We use Minipar (Lin, 1993), which produces functional relations for the components in a sentence, including subject and object relations with respect to a verb. The dataset is identical to the one used by TDP and has been constructed in the same wayas the dataset used by E&P: it contains those gold standard instances of verbs that have according to the analyses produced by the MiniPar parser (Lin, 1993) an overtly realized subject and object. We build the space from a Minipar-parsed version of the British National Corpus with dependency parses obtained from Minipar (Lin, 1993). For the current paper, we constructed a new subset of LexSub we call LEXSUB-PARA by parsing LexSub with Minipar (Lin, 1993) and extracting all 177 sentences with transitive verbs that had overtly realized subjects and objects, regardless of voice. English documents were parsed with the syntactic analyzer (Lin, 1993). This space was built from BNC dependency parses obtained from Minipar (Lin, 1993). Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar (Lin, 1993) and Mxpost (Ratnaparkhi, 1996) ,respectivelly. We used Minipar (Lin, 1993), a dependency-based parser, in order to identify verbs, nouns, verb dependencies and noun dependencies. We investigated four individual approaches for the syntax-features, a regular-expression-based quasi-parser, a system based on Dekang Lin's Mini Par (Lin, 1993), a system based on the Collins parser (Collins, 1999), and one based on the CMU Link Grammar Parser (Sleator and Temperley, 1993), as well as a family of voting-based combination schemes. The syntactic relations are generated by the Minipar dependency parser (Lin, 1993). The experiment was conducted using an 18 million tokens subset of the Reuters RCV1corpus, parsed by Lin's Minipar dependency parser (Lin, 1993). We used the dv package1 to compute type vectors from a Minipar (Lin, 1993) parse of the BNC. Specifically, we assume MINIPAR-style (Lin, 1993) dependency trees where nodes represent text expressions and edges represent the syntactic relations between them. Three parsers fulfilled all the requirements: Link Grammar (Sleator and Temperley, 1993), Minipar (Lin, 1993) and (Carroll and Briscoe, 2001). We used the Mini par parser (Lin 1993) to match DIRT patterns in the text. The syntactic relations were extracted using the Minipar parser (Lin, 1993).
To attempt to overcome this issue, both Passonneau and Litman (1993) and Hearst (1993) conflated multiple manual segmentations into one that contained only those boundaries which the majority of coders agreed upon. Additionally, the definition of what constitutes a majority is subjective (e.g., Passonneau and Litman (1993, p. 150), Litman and Passonneau (1995), Hearst (1993, p. 6) each used 4/7, 3/7, and > 50%, respectively). Similar to (Passonneau and Litman, 1993), we adopt a flat model of topic segmentation for our gold standard based on discourse segment purpose, where a shift in topic corresponds to a shift in purpose that is acknowledged and acted upon by both conversational agents. However, implementing the NM in a new domain re quires little expertise as previous work has shown that native users can reliably annotate the information needed for the NM (Passonneau and Litman, 1993). With these goals in mind, we adopted a definition of "topic" that builds upon Passonneau and Litman's seminal work on segmentation of monologue (Passonneau and Litman, 1993). Passonneau and Litman (1993) were the first to investigate the relationship between cue phrases and linear segmentation.
Dagan et al (1993) argue that using a relatively small number of classes to model the similarity between words may lead to substantial loss of information. As to the research on computing word sense relatedness, Dagan et al (1993) did some pilot work and Lee (1997) and Resnik (1999) contributed to the research on semantic similarity. In (Dagan et al., 1993) and (Pereira et al, 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
Early approaches to attribute learning include Hatzivassiloglou and McKeown (1993), who cluster adjectives that denote values of the same attribute. In this paper, we concentrate on adjectives, which have received less attention (see though Hatzivassiloglou and McKeown (1993) and Lapata (2001)). As evaluation measure, we used a pair-wise measure which calculates precision, recall and a harmonic fscore as follows: Each verb pair in the cluster analysis was compared to the verb pairs in the gold standard classes, and evaluated as true or false positive (Hatzivassiloglou and McKeown, 1993). We acknowledge previous work on the computational study of adjectival scales as in (Hatzivassiloglou and McKeown, 1993), where a system could group gradation scales using a clustering algorithm.
Class based methods (Pereira et al 1993) approximate the likelihood of unobserved words based on similar words.  Distributional clustering of words was first proposed by Pereira Tishby and Leein (Pereira et al, 1993): They cluster nouns according to their conditional verb distributions. The cluster output can then be used as classes for selectional preferences (Pereira et al, 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). This indicates that techniques for differentiating between different senses are needed e.g., using a soft clustering technique as in (Pereira et al 1993) instead of a hard clustering technique. Hatzivassiloglou and MeKeown (1993) clustered adjectives into semantic classes, and Pereira et al (1993) clustered nouns on their appearance in verb-object pairs. Of course, this is one of many different possible similarity measures which could have been used (Pereira et al (1993)), including ones that do not depend on additional labels. So far we have used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al (1993). Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. While previous cognitively-motivated computational frameworks required structured input (e.g. Falkenhainer et al, 1989), the CP method adapts distributional clustering (Pereira et al, 1993), a standard approach applicable to unstructured data. There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al, 1993). Our approach differs from standard word clustering in that the clustering criteria is directly linked to the re ranking objective, whereas previous word clustering approaches (e.g. Brown et al (1992) or Pereira et al (1993)) have typically leveraged distributional similarity. The importance of distributional features is well known for named entity recognition and part of speech tagging (Pereira et al, 1993). For example, Pereira et al (1993) begin with a co occurrence matrix and transform this matrix into a clustering. In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal. Pereira et al (1993) suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns.  Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns.
Following Manning (1993), we empirically determined the value of p. Our local language model approach also bears some resemblance to statistical approaches to modeling sub categorization frames (Manning, 1993). Our work differs from corpus-based work such as Manning (1993) or Kawahara and Kurohashi (2001) in that we are using existing lexical resources rather than a corpus.  Techniques for the automatic acquisition of subcategofization dictionaries have been developed by Manning (1993), Bfiscoe and Carroll (1997) and Carroll and Rooth (1998). This can be done automatically with unparsed corpora (Briscoe and Carroll 1997, Manning 1993, Ushioda et al 1993), from parsed corpora such as Marcus et al's (1993) Treebank (Merlo 1994, Framis 1994) or manually as was done for COMLEX (Macleod and Grishman 1994). Our eventual goal is to develop a set of regular expressions that work on fiat tagged corpora instead of TreeBank parsed structures to allow us to gather information from larger corpora than have been done by the TreeBank project (see Manning 1993 and Gahl 1998). This shows to which extent the range of arguments is fine grained, in contrast to other works where the range is at the categorial level, such as NP or PP (M. Brent 1993, C. Manning 1993, P. Merlo & M. Leybold 2001). C. Manning (1993) presents the acquisition of sub categorization frames from unlabelled text corpora.  The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus,), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. Increasingly, tools are also becoming available for acquiring sub categorization information from corpora, i.e. for inferring the sub categorization frames of a given lemma (e.g. Manning 1993). Both Brcnt (1993) and Manning (1993), who attempt to induce a lexicon of sub categorization features do so by completely discarding all preexisting knowledge; both systems are stand-ahmc, without a parsing engine to test or use the "learned" information.  (Manning, 1993) observes that Brent's recognition technique is a rather simplistic and inadequate approach to verb detection, with a very high error rate. (Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative sub cat egorization frequency for individual predicates.
Second, there is a class of techniques for learning rules from text, a recent example being Brill 1993. Other works describe systems that induce structures from corpora, but they use tagged corpora (Brill, 1993), or grammatical informations (Brent, 1993), or work with artificial samples (Elman, 1990). Transformation-based tagging as introduced by Brill (1993) also requires a handtagged text for training. This idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and (Ribarov, 1996), but not in models based on probability distributions. 
Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991), Kozima (1993), and Galley, et al. As in Kozima's work (Kozima, 1993), this computation operates on words belonging to a focus window that is moved all over the text. Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. But work such as (Kozima, 1993), (Ferret, 1998) or (Kaufmann, 1999) showed that using a domain independent source of knowledge for text segmentation doesn't necessarily lead to get better results than work that is only based on word distribution in texts. This significance is defined as in (Kozima, 1993) as its normalized information in a reference corpus. He identified topic boundaries where the LCP score was low (Kozima, 1993). For example, the lexical cohesion profile (Kozima, 1993) should be perfectly usable with our fragmentation method. Ever since Morris and Hirst (1991)'s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993). Therefore, many approaches have concentrated on different ways of estimating lexical coherence of text segments, such as semantic similarity between words (Kozima, 1993), similarity between blocks of text (Hearst, 1994), and adaptive language models (Beeferman et al, 1999). As in Kozima (1993), the second method exploits lexical cohesion to segment exts, but in a different way. Kozima (1993), for example, used cohesion based on the spreading activation on a semantic network.
Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation word repetition and computes similarities between textual units based on the similarities of word space vectors. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). Messages are partitioned into multi-paragraph segments using TextTiling, which reportedly has an overall precision of 83% and recall of 78% (Hearst, 1994). This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. However, Hearst (1994, 1997) and Foltz et al (1998) differ on how text units are defined and on how to interpret the results of a comparison. The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results. Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. Hearst (1994, 1997) in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware.
In byte-length ratio approaches, the presence of long stretches of blocks that have roughly similar lengths can be problematic, and some improvement can be achieved by augmenting the byte-length measure by scores derived from lexical feature matching (Wu, 1994). For instance, the statistical correlation of sentence length between English and Chinese is not as high as that between two Indo European languages (Wu, 1994). As shown in Chen (1993) and Wu (1994), however, sentence length based methods suffer when the texts to be aligned contain small passages, or the languages involved share few cognates. There are basically three kinds of approaches on sentence alignment: the length-based approach (Gale and Church 1991 and Brown et al 1991), the lexical approach (Key and Roscheisen 1993), and the combination of them (Chen 1993, Wu 1994 and Langlais 1998, etc.). The first sentence alignment model used to align English-Chinese bilingual texts is proposed by Wu (1994). We use parts of the HKUST English-Chinese Bilingual Corpora for our experiments (Wu 1994), consisting of transcriptions of the Hong Kong Legislative Council debates in both English and Chinese. Work on sentence alignment of English and Chinese texts (Wu 1994), indicates that the lengths of English and Chinese texts are not as highly correlated as in French-English task, leading to lower success rate (85-94%) for length-based aligners. (DeKai Wu, 1994) offered that (1:1) sentence beads occupied 89% in English-Chinese as well. The approaches by (Wu, 1994), (Haruno and Yamazaki, 1996), (Ma, 2006) and (Gautam and Sinha, 2007) require an externally supplied bilingual lexicon. Wu (Wu, 1994) also used lexical cues from corpus-specific bilingual lexicon for better alignment. Existing parallel corpora have illustrated their particular value in empirical NLP research, e.g., Canadian Hansard Corpus (Gale and Church, 1991b), HK Hansard (Wu, 1994), INTERSECT (Salkie, 1995), ENPC (Ebeling, 1998), the Bible parallel corpus (Resnik et al, 1999) and many others. All experiments reported in this paper were performed on sentence-pairs from the HKUST English-Chinese Parallel Bilingual Corpus, which consists of governmental transcripts (Wu 1994).  Previous studies such as (Fung and Wu, 1994) have commented 268 that methods developed for Indo-European language pairs using alphabetic haracters have not addressed important issues which occur with European-Asian language pairs. It has also been suggested by (Wu, 1994) that sentence length ratio correlations may arise partly out of historic cognate-based relationships between Indo-European languages. (Wu, 1994), about whether the byte length model by itself can perform well.
 Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). In cases like (Yarowsky, 1995), unsupervised methods offer accuracy results than rival supervised methods (Yarowsky,1994) while requiring only a fraction of the data preparation effort. Decision lists (Rivest, 1987) have been used for a variety of natural language tasks, including accent restoration (Yarowsky, 1994), word sense disambiguation (Yarowsky, 2000), finding the past tense of English verbs (Mooney and Califf, 1995), and several other problems. The standard algorithm for learning decision lists (Yarowsky, 1994) is very simple. Yarowsky (1994) suggests two improvements to the standard algorithm. Accent restoration (Yarowsky, 1994), word sense disambiguation (Yarowsky, 2000), and other problems all fall into this framework, and typically use similar feature types. They include those using Naive Bayes (Gale et al 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. Lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together, an effect already commented on by Yarowsky (1994). As has already been noted by Yarowsky (1994), using lemmas helps to produce more concise and generic evidence than inflected forms. Yarowsky (1994) defined a basic set of features that has been widely used (with some variations) by other WSD systems. Despite their simplicity, Decision Lists (Dlist for short) as defined in Yarowsky (1994) have been shown to be very effective for WSD (Kilgarriff & Palmer, 2000). In general, the strategy adopted to model syntagmatic relations in WSD is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994).  Yarowsky (1994 and 1995), Mihalcea and Moldovan (2000), and Mihalcea (2002) have made further research to obtain large corpus of higher quality from an initial seed corpus. In general, the strategy adopted to model syntagmatic relations is to provide bigrams and trigrams of collocated words as features to describe local contexts (Yarowsky, 1994), and each word is regarded as a different instance to classify. As baseline, we report the result of a standard approach consisting on explicit bigrams and trigrams of words and POS tags around the words to be disambiguated (Yarowsky, 1994). The more recent set of techniques includes multiplicative weight update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al, 1993, Golding, 1995, Golding and Schabes, 1996). Yarowsky (1994) argued the optimal value is sensitive to the type of ambiguity.
  Note that Wu and Palmer (1994) designed their measure such that shallow nodes are less similar than nodes that are deeper in the WordNet hierarchy. This included similarity measures introduced by Wu and Palmer (1994) (wup), Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. We here experiment with the Hirst-St.Onge and the Wu and Palmer (Wu and Palmer, 1994) measures, as they are pure taxonomic measures, i.e. they do not require any corpus statistics. The Wu and Palmer (Wu and Palmer, 1994) similarity metric measures the depth of the two concepts in the WordNet taxonomy, and the depth of the least common subsumer (LCS), and combines these figures into a similarity score. Figure 2: A bipartite graph representing patterns and tuples 504 measure described in (Wu and Palmer, 1994) which finds the path length to the root node from the least common subsumer (LCS) of the two word senses which is the most specific word sense they share as an ancestor. (4) reported by (Wu and Palmer, 1994). (Wu and Palmer, 1994) uses the path length from the root to the lowest common subsumer (LCS) of two concepts scaled by the distance from the LCS to each concept.  The synset similarity metric defined by Wu and Palmer (1994) combines the path length and synset depth intuitions into a single numeric score that is defined as follows: 2? depth (lca (synset1, synset2)) depth (synset1)+ depth (synset2) (12) In Equation 12 ,lca returns the lowest common ancestor of the two synsets within the WordNet is-a hierarchy. (Wu and Palmer, 1994) combines the depth of the LCS of two concepts into a similarity score. The semantic similarity between words is computed based on Wu and Palmer's measure (Wu and Palmer,1994) using WordNet (Fellbaum, 1998).  Wu and Palmer (1994) proposed to scale the depth of the two synset nodes (depthc1 and depthc2) by the depth of their LCS (depth(lcsc1,c2)). We used the Wu and Palmermea sure (Wu and Palmer, 1994) applied to Dutch EWN for computing the semantic similarity between two words. ATEC is equipped with these two modules as well, and furthermore, with two measures for word similarity, including a WordNet-based (Wu and Palmer, 1994) and a corpus-based measure (Landauer et al, 1998) for matching word pairs of similar meanings. Wu and Palmer (1994) proposed a concept similarity measure between two concepts c1 and c2 as :sim (c1 ,c2)= 2 ?dep (c )len (c1, c) +len (c2, c) +2 ?dep (c) (1) where c is the lowest common subsumer (LCS) of c1 and c2, and len (?,?) is the number of edges between two nodes. The two similarity measures we experiment with are that of Wu and Palmer (1994) and Jiang and Conrath (1997). 
[Interest] is a binary version of the word sense disambiguation data from (Bruce and Wiebe, 1994). One set was extracted from a hand-tagged corpus (Bruce and Wiebe, 1994) and the other by our algorithm.  We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994). An example of the type of feature used is the part-of-speech of the word to the right; see (Bruce and Wiebe, 1994) for the other ones we use. The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with sense tags from the Longman Dictionary of Contemporary English (LDOCE).   The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996).
A concise review of this research area can be found in, for instance, Lauer (1995), which dates back to Finin (1980). As Lauer (1995) pointed out, using (partial) parsing of the text is too costly. Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better. This is an extension of left branch preference in Lauer (1995).  Lauer (1995): adjacency 68.90 Lauer (1995): dependency 77.50 Best Altavista 78.68 Lauer (1995): tuned 80.70 Upper bound 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993). Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words. Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauers set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). (Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. (Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. (Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts.
In this section we consider two existing grammars: the XTAG grammar, a wide coverage LTAG, and the LEXSYS grammar, a wide coverage D-Tree Substitution Grammar (Rambow et al, 1995). Sister-adjunctionis not an operation found in standard de ni tions of TAG, but is borrowed from D-TreeGrammar (Rambow et al, 1995). It also has much in common with d tree substitution grammar (Rambow et al, 1995).  These spines can be combined using a sister-ad junction operation (Rambow et al, 1995), to form larger pieces of structure. Chiang's model adds a third composition operation called sister-ad junction (see Figure 3), borrowed from D-tree substitution grammar (Rambow et al, 1995). Variants of TAG like TIG (Schabes and Waters, 1995) or D-Tree grammars (Rambow et al, 1995) are motivated by linguistic or formal considerations rather than pedagogical or computational ones. Other formalisms aiming to model dependency correctly similarly expand weak generative capacity, notably D-tree Substitution Grammar (Rambow et al, 1995), and consequently end up with much greater parsing complexity. Consider the case of sentences which contain both bridge and raising verbs, noted by Rambow et al (1995). A more recent instance is D-Tree Substitution Grammars (DSG) (Rambow et al, 1995), where the derivations are also interpreted as dependency relations. D-Tree Grammar (DTG) is proposed in (Rambow et al, 1995) to remedy some empirical and theoretical shortcomings of TAG; Tree Description Grammar (TDG) is introduce din (Kallmeyer, 1999) to support syntactic and semantic underspecification and Interaction Grammar is presented in (Perrier, 2000) as an alternative way of formulating linear logic grammars.
In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. This heuristic mimes the one-sense-per collocation heuristic presented in (Yarowsky, 1995). These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation (Tran et al, 1996), sense disambiguation in speech synthesis (Yarowsky, 1995), and relation tagging in information retrieval (Marsh and Perzanowski, 1999). Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). See Yarowsky (1995) for details. The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result. The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). Self-training (Yarowsky, 1995) is a form of semi-supervised learning.
Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. Over the years, several proposals of generic NLG system shave been made: Penman (Matthiessen and Bate man, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al, 2002), etc. The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. (Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructions are generated and then filtered based on a statistical model.
The conversion uses head propagation rules to find the head on the right-hand side of the CFG rules, first proposed for English in (Magerman, 1995). Not to mention earlier non-PCFG lexicalized statistical parsers, notably Magerman (1995) for the Penn Treebank, also modified the treebank to contain different labels for standard and for base noun phrases. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) dier in their details but are based on similar features. FTB-UC-DEP is a dependency tree bank derived from FTB-UC using the classic technique of head propagation rules, first proposed for English by Magerman (1995). Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. This is a similar, but more limited, strategy to the one used by Magerman (1995). In both cases, we report PARSEVAL labeled bracket scores (Magerman, 1995), with the brackets labeled by syntactic categories but not grammatical functions. We apply canonical lexical head projection rules (Magerman, 1995) in order to lexicalize syntactic trees. In addition to antecedent recovery, we also report parsing accuracy, using the bracketing F-Score, the combined measure of PARSEVAL-style labeled bracketing precision and recall (Magerman, 1995). These features are also computed for the head of the phrase, determined using a set of head finding rules in the style of Magerman (1995) adapted to TiGer. The head word is identified by using the head-percolation table (Magerman, 1995). For example, statistical parsers from Magerman (1995) on use features based on head-dependent relationships. The head word is identified by using the head percolation table (Magerman, 1995). For each possible constituent in a parse tree, rules first described in (Magerman, 1995) and (Jelinek et al, 1994) identify the head-child and propagate the head-word to its parent. Lexical heads have been calculated using the projection rules of Magerman (1995), and annotated between brackets. The conventional wisdom since Magerman (1995) has been that lexicalization substantially improves performance compared to an unlexicalized baseline model (e.g., a probabilistic context-free grammar, PCFG). The input to our sentence realiser are bag of words with dependency constraints which are automatically extracted from the Penn tree bank using head percolation rules used in (Magerman, 1995), which do not contain any order information.  
A similar idea is later applied by (Rapp 1995) to show the plausibility of correlations between words in non-parallel text. Theoretically, it is possible to use these methods to build a translation lexicon from scratch [Rapp, 1995].     Rapp (1995), Grefenstette (1998), Fung and Lo (1998), and Kaji (2003) derived bilingual lexicons or word senses from such corpora. Rapp (1995) proposed a computationally demanding matrix permutation method which maximizes a similarity between co-occurrence matrices in two languages. In order to expand the dictionaries using a set of monolingual comparable corpora, the basic approach pioneered by Fung and McKeown (1997) and Rapp (1995, 1999) is to be further developed and refined in the second phase of the project as to obtain a practical tool that can be used in an industrial context. In particular, we extend the long line of work on inducing translation lexicons (beginning with Rapp (1995)) and propose to use multiple independent cues present in monolingual texts to estimate lexical and phrasal translation probabilities for large, MT-scale phrase-tables. Rapp (1995) was the first to propose using non-parallel texts to learn the translations of words. Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words nouin Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). Building a dictionary from scratch is not possible this way or at least computationally un-feasible (see Rapp, 1995). Approaches for lexicon extraction from comparable corpora have been proposed that use the bag of-words model to find words that occur in similar lexical contexts (Rapp, 1995). One of the first works in the area of comparable corpora mining was based on word co-occurrence based approach (Rapp, 1995). The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Rapp (1995) suggested his idea about the usage of context vectors in order to find the words that are the translation of each other in comparable corpora. The underlying assumption is that translations of words that are related in one language are also related in the other language (Rapp 1995). In this situation, Rapp (1995) proposed using a clue different from the three mentioned above: His co-occurrence clue is based on the assumption that there is a correlation between co occurrence patterns in different languages. By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995).
We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of co-occurrence features the accuracy only drops to 80%.   Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. Our approach to memory-based all-words WSD follows the memory based approach of (Ng and Lee, 1996), and the work by (Veenstra et al, 2000) on a memory based approach to the English lexical sample task of SENSEVAL-1. The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations. The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). The set of features needed for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al (2000).
The OVIS annotations are in contrast with other corpora and systems (e.g. Miller et al. 1996), in that our annotation convention exploits the Principle of Compositionality of Meaning. This use of model-theoretic interpretation represents an important extension to thesemantic grammars used in existing statistical spoken language interfaces, which rely on co-occurrences among lexically-determined semantic classes and slot fillers (Miller et al, 1996), in that the probability of an analysis is now also conditioned on the existence of denoted entities and relations in the world model. Examples include an early statistical method for learning to fill slot-value representations (Miller et al, 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006). An influential precursor to this integration is the system described in (Miller et al, 1996). We are only aware of one system that learns to construct context-dependent interpretations (Miller et al, 1996). The Miller et al (1996) approach is fully supervised and produces a final meaning representation in SQL. Evaluation Metrics Miller et al (1996) report accuracy rates for recovering correct SQL annotations on the test set. In the domain of the Air Traveler Information System (Miller et al, 1996) the authors apply statistical methods to compute the probability that a constituent can fill in a semantic slot within a semantic frame. For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al (1996) computed the probability that a constituent such as Atlanta filled a semantic slot such as Destination in a semantic frame for air travel.
 Eisner (1996, p.81) in fact suggested that the labeling system can be implemented in the grammar by templates, or in the processor by labeling the chart entries. But, as pointed out by Eisner (1996, p.85), this is not spurious ambiguity in the technical sense, just multiple derivations due to alternative lexical category assignments. The C&C parser employs the normal-form constraints of Eisner (1996) to address spurious ambiguity in 1-best parsing. The second strategy is to use Eisner's normal form constraints (Eisner, 1996).  The parser only used a subset of CCG, pureCCG (Eisner, 1996), consisting of the Application and Composition rules. We propose and implement a modification of the Eisner (1996) normal form to account for generalized composition of bounded degree, and an extension to deal with grammatical type-raising. Eisner (1996, section 5) also provides a safe and complete parsing algorithm which can return non-NF derivations when necessary to preseve an interpretation if composition is bounded or the grammar is restricted in other (arbitrary) ways. We have proposed a modification and extension of Eisner (1996)'s normal form that is more appropriate for commonly used variants of CCG with grammatical type-raising and generalized composition of bounded degree, as well as some non-combinatory extensions to CCG. Hockenmaier (2003a) uses a model which favours only one of the derivations leading to aderived structure, namely the normal-form derivation (Eisner, 1996). For the normal-form model we were able to reduce the size of the charts considerably by applying two types of restriction to the parser: first, categories can only combine if they appear together in a rule instantiation in sections 2 21 of CCGbank; and second, we apply the normal-form restrictions described in Eisner (1996). Our transformation is also technically related to the normal form construction for CCG parsing presented by Eisner (1996). Consider the derivations in Figures 1 and 2, which show a normal form derivation (Eisner, 1996) and fully incremental derivation, respectively. CCG parsers often limit the use of the combinatory rules (in particular: type-raising) to obtain a single right-branching normal form derivation (Eisner, 1996) for each possible semantic interpretation. Both perspectives on D ensure that the new rules are compatible with normal form constraints (Eisner, 1996) for controlling spurious ambiguity. In this section, we show that the D rules fit naturally within standard normal form constraints for CCG parsing (Eisner, 1996), by providing both combinatory and logical bases for D. Furthermore, CCG augmented with D is compatible with Eisner NF (Eisner, 1996), a standard technique for controlling derivational ambiguity in CCG-parsers, and also with the modalized version of CCG (Baldridge and Kruijff, 2003). One set are the normal form constraints, as described by Eisner (1996). We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can 12 only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints.
As with the pure statistical translation model described by Wu (1996) (in which a bracketing transduction grammar models the channel), alternative hypotheses compete probabilistically, exhaustive search of the translation hypothesis pace can be performed in polynomial time, and robustness heuristics arise naturally from a language-independent inversion transduction model. A step was taken by Wu (Wu, 1996) who introduced a polynomial-time algorithm for the runtime search for an optimal translation. Subsequently, a method was developed to use a special case of the ITGR the aforementioned BTGR for the translation task itself (Wu, 1996). Wu (Wu, 1996) experimented with Chinese-English translation, while this paper experiments with English-Chinese translation. To tackle the problem of glue rules, He (2010) extended the HPB model by using bracketing transduction grammar (Wu, 1996) instead of the monotone glue rules, and trained an extra classifier for glue rules to predict reorderings of neighboring phrases. Wu (1996) presented a polynomial-time algorithm for decoding ITG combined with an m-gram language model. The of ITG decoding algorithm of Wu (1996) can be viewed as a variant of the Viterbi parsing algorithm for alignment selection. To do bigram-integrated decoding, we need to augment each chart item (X,i, j) with two target-language boundary words u and v to produce a bigram-item like u ··· v Xi j, following the dynamic programming algorithm of Wu (1996). NP (1) VPP-VP (2), NP (1) VPP-VP (2) VPP-VP? VP (1) PP (2), PP (2) VP (1) In this case m-gram integrated decoding can bedone in O (|w|3+4 (m? 1)) time which is much lower order polynomial and no longer depends on rule size (Wu, 1996), allowing the search to be much faster and more accurate facing pruning, as is evidenced in the Hiero system of Chiang (2005) where he restricts the hierarchical phrases to be a binary SCFG. In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation. 3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically. See Wu (1996) or Melamed (2004) for a detailed exposition. Reordering restrictions for word-based SMT decoders were introduced by (Berger et al, 1996) and (Wu, 1996). (Wu,1996) propose using contiguity restrictions on the reordering. It is also relevant since it can form the basis of a decoder for inversion transduction grammar (Wu, 1996). It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework ,including information extraction (Miller et al, 2000) and syntax-based machine translation (Wu, 1996). To integrate with a bigram language model, we can use the dynamic-programming algorithms of Och and Ney (2004) and Wu (1996) for phrase-based and SCFG-based systems, respectively, which we may think of as doing a finer-grained version of the deductions above.
Two measures are used to evaluate the parses: lexical accuracy, which is the percentage of correctly tagged words compared to the extracted gold standard corpus (Watkinson and Manandhar, 2001) and average crossing bracket rate (CBR) (Goodman, 1996). The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004). A probability model permits alternative decoding procedures (Goodman, 1996). These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005). Their algorithm is therefore the labelled recall algorithm of Goodman (1996) but applied to rules. Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al (2005). The coarse PCFG has an extremely beneficial interaction with the fine all-fragments SDP grammar, wherein the accuracy of the combined grammars is significantly higher than either individually (This is similar to the maximum recall objective for approximate inference (Goodman, 1996b)). Goodman (1996) observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse. The performance of web-page structuring algorithms can be evaluated via the nested-list form of tree by bracketed recall and bracketed precision (Goodman, 1996). This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). We then compute outside scores for bi spans under a max-sum (Goodman, 1996).   Goodman (1996b), Petrov and Klein (2007), and Matsuzaki et al (2005) describe the details of constituent, rule-sum and variational objectives respectively. In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002). And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996). This is equivalent to minimum Bayes risk decoding (Goodman, 1996), which is used by Cohen and Smith (2007) and Smith and Eisner (2008). A closely related method, applied by Goodman (1996) is called minimum-risk decoding. While the most probable parse problem is NP-complete (Simaan, 1992), several approximate methods exist, including n-best re-ranking by parse likelihood, the labeled bracket alorithm of Goodman (1996), and a variational approximation introduced in Matsuzakiet al (2005).
Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Maximum likelihood estimation is used to estimate P(wROOT|S) on training data set with the smoothing method used in (Collins, 1996). To overcome the data sparseness problem, we not only apply the smoothing method used in (Collins, 1996) for a lexicalized head to back off it to its part-of-speech, but also assign a very small value to P (cpi|wi) when there is no cpi modifying wi in the constructed case patterns. (9) The smoothing method used in (Collins, 1996) is applied during estimation. Extending on the notion of a Base NP, introduced by Collins (1996), we mark any nonterminal that dominates only preterminals as Base. For the discourse structure analysis, we suggest a statistical model with discourse segment boundaries (DSBs) similar to the idea of gaps suggested for a statistical parsing (Collins (1996)). for the Penn Treebank (Marcus et al, 1994), and Collins (1996), whose constituent parser is internally based on probabilities of bi lexical dependencies, i.e. dependencies between two words. In general, the likelihood of a head-dependent relation decreases as distance increases (Collins, 1996). model (Collins, 1996) to Japanese dependency analysis. It is therefore necessary to either discard infrequent rules, do manual editing, use a different rule format such as individual dependencies (Collins, 1996) or gain full linguistic control and insight by using a hand written grammar, each of which sacrifices total completeness. To generate the training examples forthe classifier, we generate a parse tree for every sentence in the SENSEVAL-3 training data, using the Collins (1996) statistical parser. This is a modified version of the backed-off smoothing used by Collins (1996) to alleviate sparse data problems. We generate our training data from the Wall Street Journal Section of the Penn Tree Bank (PTB), by transforming it to projective dependency structures, following (Collins, 1996), and extracting rules from the result. We first transform the PTB into projective dependencies structures following (Collins, 1996). This lexicalization procedure is commonly used in statistical parsing (Collins, 1996) and produces a dependency tree. While the model of Collins (1996) is technically unsound (Collins, 1999), our aim at this stage is to demonstrate that accurate, efficient wide-coverage parsing is possible with CCG, even with an over-simplified statistical model. The estimation method is based on Collins (1996). Relationship: in this paper, we not only use the label of the constituent label as Collins (1996), but also use some well-known context in parsing to define the head-modifier relationship r (.), including the POS of the modifier m, the POS of the head h, the dependency direction d, the parent label of the dependency label l, the grandfather label of the dependency relation p, the POS of adjacent siblings of the modifier s. However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of (Collins, 1996), named Collins distance for convenience. Capturing question or answer dependencies can be cast as a straightforward process of mapping syntactic trees to sets of binary head modifier relationships, as first noted in (Collins, 1996).
The method is an extension of the chart based generation algorithm described in Kay (1996). Kay (1996) provides a more general view of the chart structure which is designed to provide for generation advantages comparable to those it provides for parsing. We will concentrate on a detailed description of these annotations as they are a crucial component of our method and they are the major difference between the current proposal and the one described in Kay (1996). (Wang, 1980) uses handwritten rules to generate sentences from an extended predicate logic formalism; (Shieber et al., 1990) introduces a head-driven algorithm for generating from logical forms; (Kay, 1996) defines a chart based algorithm which enhances efficiency by minimising the number of semantically incomplete phrases being built; and (Shemtov, 1996) presents an extension of the chart based generation algorithm presented in (Kay, 1996) which supports the generation of multiple paraphrases from underspecified semantic input. The PCFG-based generation algorithms are implemented in terms of a chart generator (Kay, 1996). Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. In the chart realization tradition (Kay, 1996), the OpenCCG realizer takes logical forms as input and produces strings by combining signs for lexical items. We followed two partial solutions to this problem by Kay (1996). The baseline generation algorithm, following Kay (1996)'s work on chart generation, already contains the hard constraint that when combining two chart edges they must cover disjoint sets of words. It is a bottom up, tabular algorithm [Kay, 1996] optimised for TAGs. Following Stone and Doran (1997) and Kay (1996), we enhance this TAG grammar with a syntax-semantics interface in which nonterminal nodes of the elementary trees are equipped with index variables, which can be bound to individuals in the semantic input. We follow a bottom-up chart generation approach (Kay, 1996) for production systems similar to (Varges, 2005). For instance, (Kay, 1996) proposes to reduce the number of constituents built during realisation by only considering for combination constituents with non overlapping semantics and compatible indices. The tree combining algorithm used after filtering has taken place, is a bottom-up tabular algorithm (Kay, 1996) optimised for TAGs. The standard solution to this problem (cf. (Kay, 1996)) is to index edges with semantic indices (for instance, the edge with category N/x: dog (x) will be indexed with x) and to restrict edge combination to these edges which have compatible indices. More recently, Carroll and Oepen (2005) present a perfor 1As first noted by Brew (1992) and Kay (1996), given a set of n modifiers all modifying the same structure, all possible intermediate structures will be constructed, i.e., 2n+1. The work in Kay (1996) and the extension to ambiguous input in Shemtov (1996) and Shemtov (1997) describes a chart based generation process which takes packed representations as input and generates all paraphrases without expanding first into disjunctive normal form. The basic surface realisation algorithm used is a bot tom up, tabular realisation algorithm (Kay, 1996) optimised for TAGs. The generation algorithm is based on chart generation as first introduced by Kay (1996) with Viterbi-pruning. This results in efficiently treating the well known problem originally described in Kay (1996), where one unnecessarily retains sub-optimal strings.
We use AuToBI's models, which were trained on the spontaneous speech Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), to identify prosodic events in our corpus. Note that, in order to transcribe the test data, we have trained a 20 Gaussian per state 39 MFCC Hidden Markov Model speech recognizer with HTK, using the training and development sets together with TIMIT (Fisher et al, 1986), the Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), and the Columbia Game Corpus (Hirschberg et al, 2005). Expert discourse structure analyses are used to derive CONSENSUS SEGMENTATIONS, consisting of discourse boundaries whose coding all three labelers agreed upon (Hirschberg and Nakatani, 1996). Speech was found to improve inter-annotator agreement in discourse segmentation of monologues (Hirschberg and Nakatani 1996). Most previous work that has combined multiple annotations has used linear segmentations, i.e. discourse segmentations without hierarchies (Hirschberg and Nakatani, 1996). A conservative way to combine segmentationsinto a gold standard is the Consensus (CNS) (or raw agreement) approach (Hir chberg and Nakatani, 1996). Discourse structure has been successfully used in non-interactive settings (e.g. understanding specific lexical and prosodic phenomena (Hirschberg and Nakatani, 1996), natural language generation (Hovy, 1993), essay scoring (Higgins et al, 2004) as well as in interactive settings (e.g. predictive/generative models of postural shifts (Cassell et al., 2001), generation/interpretation of anaphoric expressions (Allen et al, 2001), performance modeling (Rotaru and Litman, 2006)). We report results on the Boston University (BU) Radio Speech Corpus (Ostendorf et al, 1995) and Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), two publicly available speech corpora with manual ToBI annotations intended for experiments in automatic prosody labeling. In spontaneous monologue, Butterworth (1972) found that the beginning of a discourse segment exhibited slower speaking rate; Swerts (1995), and Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries; Hirschberg and Nakatani (1996) found that the beginning of a discourse segment correlates with higher pitch.
Smoothing typically adds considerable computational complexity to the system since multiple models need to be estimated and applied together, and it is often considered a black art (Chen and Goodman, 1996). Constant restoring is similar to the additive smoothing, which isused to solve the zero-frequency problem of language models (Chen and Goodman, 1996). We refer the reader to (Chen and Goodman, 1996) for a survey of the discounting methods for n-gram models. I used a smoothing method loosely based on the one-count method given in (Chen and Goodman, 1996). The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. This was done using the SRI Language Modelling toolkit (Stolcke, 2002) employing linear interpolation and modified Kneser Ney discounting (Chen and Goodman, 1996). Alternatively, smoothing techniques (Chen and Goodman, 1996) redistribute the probabilities, taking into account previously unseen word sequences. In particular, instead of back-off, smoothing techniques could be investigated to reduce the impact of zero probability problems (Chen and Goodman, 1996). This is a general issue in statistical Natural Language Processing (NLP) and many possible remedies have been proposed in the literature, such as, for instance, using smoothing techniques (Chen and Goodman, 1996), or working with linguistically enriched, or more abstract, representations. Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity. A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Giga word corpus (Parker et al, 2009). We use one-count smoothing (Chen and Goodman, 1996), where (ti) is based on the number of words that occur with ti once: (ti)= |wi: C (ti ,wi)= 1|. Modified Knesser-Ney smoothed (Chen and Goodman, 1996) n-gram language models are built from the monolingual English data using the SRI language modeling toolkit (Stolke, 2002). A conventional ngram language model of the target language provides the third component of the system. In all our experiments, we used 4-gram reordering models and bilingual tuple models built using Kneser-Ney back off (Chen and Goodman, 1996). Even with limited context, the parameter space can be quite sparse and requires sophisticated techniques for reliable probability estimation (Chen and Goodman, 1996). We used one-count smoothing (Chen and Goodman, 1996). When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used. We constructed a 5-gram language model from the provided English News monolingual training data as well as the English side of the parallel corpus using the SRI language modeling toolkit with modified Kneser Ney smoothing (Chen and Goodman, 1996).
 Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. 
The last two counts (CAUS and ANIM) were performed on a 29-million word parsed corpus (gall Street Journal 1988, provided by Michael Collins (Collins, 1997)). These sentences were parsed with the Collins' parser (Collins, 1997). Substantial improvements have been made to parse western language such as English, and many powerful models have been proposed (Brill 1993, Collins 1997). This model is very similar to the markovized rule models in Collins (1997). Collins (1997)'s parser and its reimplementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al, 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikel's web page, Arabic. In particular, empty nodes (represented as -NONE in the tree bank) were turned into rules that generated the empty string (), and there was no collapsing of categories (such as PRT and ADVP) as is of ten done in parsing work (Collins, 1997, etc.). Answer Extraction: We select the top 5 ranked sentences and return them as Collins, 1997, can be used to capture the binary dependencies between the head of each phrase. For example, the lexicalized grammars of Collins (1997) and Charniak (1997) and the state split grammars of Petrov et al (2006) are all too large to construct unpruned charts in memory. Recently, it has gained renewed attention as empirical methods in parsing have emphasized the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly, but context-free phrase-structure grammars do not. The supervised component is Collins' parser (Collins, 1997), trained on the Wall Street Journal.  We use a mechanism similar to (Collins, 1997) but adapted to Chinese data to find lexical heads in the tree bank data. Judge et al (2006) produced a corpus of 4,000 questions annotated with syntactic trees, and obtained an improvement in parsing accuracy for Bikel's reimplementation of the Collins parser (Collins, 1997) by training a new parser model with a combination of newspaper and question data. In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. In order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using a maximum entropy tagger (Ratnaparkhi, 1998) and parsed using the Collins parser (Collins, 1997). Collins (1997)'s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Our model is thus a simplification of more sophisticated models which integrate PCFGs with features, such as those in Magerman (1995), Collins (1997) and Goodman (1997). This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported in (Collins, 1997). At last, the dependency parser presented in (Collins, 1997) is used to generate the full parse. For getting the syntax trees, the latest version of Collins' parser (Collins, 1997) was used.
We use a different kind of model here, logistic regression, which is especially well suited for categorical data analysis (cf. eg. Agresti (1990) or Kessler et al (1997)). Kessler et al (1997) mention that parsing and word-sense disambiguation can also benefit from genre classification. Like Kessler et al (1997) and Argamon et al (1998) after them, they exploit (partly) hand-crafted sets of features, which are specific to texts in English. However, additional aspects of a document in uence its relevance, including, e.g., the evidential status of the material presented, and the attitudes expressed about the topic (Kessler et al, 1997). Apart from two notable exceptions, namely Kessler et al (1997) and Rehm (2006) whose implementations require extensive manual annotation (Kessler et al, 1997) or analysis (Rehm, 2006), genres are usually classified as single-label discrete entities, relying on the simplified assumption that a document can be assigned to only one genre. We plan to investigate more types of document collections pairs, e.g., the document collections from different text genres (Kessler et al, 1997). (Kessler et al, 1997) combine these views by saying that a genre should not be so broad that the texts belonging to it don't share any distinguishing properties. ... we would probably not use the term "genre" to describe merely the class of texts that have the objective of persuading someone to do something, since that class which would include editorials, sermons, prayers, advertisements, and so forth has no distinguishing formal properties (Kessler et al, 1997, p. 33). This is where the caveat from (Kessler et al, 1997) be comes relevant: A particular genre shouldn't betaken so broadly as to have no distinguishing features, nor so narrowly as to have no general applicability. Kessler et al (1997) avoid structural markers since they require tagged or parsed text and replace them with character-level markers (e.g., punctuation mark counts) and derivative markers, i.e., ratios and variation measures derived from measures of lexical and character-level markers. Another straightforward method is the assumption that Latinate prefixes and suffixes are indicators of formality in English (Kessler et al, 1997), i.e. informal words will not have Latinate affixes such as -ation and intra-.
Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. The similarity measure simwN is based on the proposal in (Lin, 1997). We determine closeness using two similarity measures Jiang and Conrath (1997) and Lin (1997) and two relatedness measures Lesk (Banerjee and Pedersen, 2003) and gloss vector overlap (Pedersen et al, 2004) from the Word Net Similarity package. On the other hand, (Lin, 1997) proposes a disambiguation algorithm that relies on the basic intuition that if two occurrences of the same word have identical meanings, then they should have similar local context. Dependencies and a Conceptual Network Similar to (Lin, 1997), we consider the syntactic dependency of words, but we also consider the conceptual hierarchy of a word obtained through the WordNet semantic network as a means for generalization, capable to handle unseen words. These methods acquire contextual information directly from unannotated raw text, and senses can be induced from text using some similarity measure (Lin, 1997). The same compound (or its variant) would be difficult to detect in a document talking about traveling to Java: the two words may appear at some distance or not in some specific syntactic structure as required in (Lin, 1997). (Lin, 1997) also tries to solve word ambiguity by adding syntactic dependency as context. In particular, this method has been used for word sense disambiguation (Lin, 1997) and thesaurus construction (Lin, 1998). We then compute the semantic similarity measure as the Jensen-Shannon (Lin, 1997) divergence JS (L (hi) ||L (h (i)))= 1 2 [D (L (hi) ||avg) +D (L (h (i)) ||avg)] where avg= (L (hi)+ L (h (i))) /2 is the average between the two distributions and D (L (hi) ||avg) is the Kullback Leiber divergence (Cover and Thomas, 2006). To do this, we use one information-content based measure (Lin, 1997), which is provided in Wordnet Similarity package (Pedersen et al, 2004) to evaluate the similarity between two concepts in Wordnet. The term selector comes from (Lin, 1997), and refers to a word which can take the place of another given word within the same local context. Differing from previous systems, the language model in ARE is based on dependency relations obtained from Minipar by Lin (1997). A consideration for future work to enhance para phrasal meaning preservation would be to explore other contextual representations, such as syntactic dependency parsing (Lin, 1997), mutual information between co-occurences of phrases Church and Hanks (1991), or increasing the number of neighboring words used in n-gram based repesentations.  Lin (Lin 1997) described a distributional hypothesis that if two words have similar set of collocations, they are probably similar. Selectors are words which take the place of a given target word within its local context (Lin, 1997). (Lin, 1997) takes a supervised approach that is unique as it did not create a classifier for every target word. We identify a third approach through the use of selectors, first introduced by (Lin, 1997), which help to disambiguate a word by comparing it to other words that may replace it within the same local context. We adopted the term selector from (Lin, 1997) to refer to a word which takes the place of another in the same local context.
Percent agreement with the majonty opinion for each text We took then the RS-trees built by the analysts and used our formalizaUon of RST (Marcu, 1996, Marcu, 1997b) to assocmte with each. The mathematical foundations of the rhetorical parsing algorithm rely on a first order formatioon of valid text structures (Marcu, 1997b). Document structure (WD) is another important clue in determining which elements of a text are important enough to include in a summary (Marcu, 1997). Most recently, Marcu (1997) has described a method for text summarization based on nuclearity and selective retention of hierarchical fragments.  Since discourse markers, such as because and and, have been shown to play a major role in rhetorical parsing (Marcu, 1997), we also consider a list of features that specify whether a lexeme found within the local contextual window is a potential discourse marker. Our task is similar to the concept of discourse parsing (Marcu (1997)), where discourse structures are extracted from the text.  Argumentation belongs to discourse analysis, with fairly complex computational models such as the implementation of the rhetorical structure theory proposed by (Marcu, 1997), which proposes dozens of rhetorical classes. Marcu (1997) describes a rhetorical parsing approach which takes unrestricted text as input and derives the rhetorical structure tree. One exception is Marcu's work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Considering the problem from another angle, discourse approaches have focused on shorter units than multi-paragraph segments, but Rhetorical Structure Theory (Marcu 1997 and others) may be able to scale up to associate rhetorical functions with segments. In the approach proposed in (Marcu, 1997), for example, the presence of discourse markers is used to hypothesize individual textual units and relations holding between them. Marcu (1997) parsed documents as rhetorical trees and identified important sentences based on the trees.
Extending this notion, (Knight and Graehl, 1997) built five probability distributions:. Fortunately, the first two models of (Knight and Graehl, 1997) deal with English only, so we can reuse them directly for Arabic/English transliteration. We applied the EM learning algorithm described in (Knight and Graehl, 1997) on this data, with one variation. Transliterations can be generated for tokens in a source phrase (Knightand Graehl, 1997), with o (f, e) calculating phonetic similarity rather than orthographic. Transliteration of a name, for the purpose of this work, is defined as its transcription in a different language, preserving the phonetics, perhaps in a different orthography [Knight and Graehl, 1997]. In this work we treat the process of transliteration as a process of direct transduction from sequences of tokens in the source language to sequences of tokens in the target language with no modeling of the phonetics of either source or target language (Knight and Graehl, 1997). Knight and Graehl (1997) build a generative model for back ward transliteration from Japanese to English. (Knight and Graehl, 1997) proposed a generative transliteration model to transliterate foreign names in Japanese back to English using finite state transducers. For example, in order to translate names and technical terms, (Knight and Graehl, 1997) introduced a probabilistic model that replaces Japanese katakana words with phonetically equivalent English words. Previous works usually take a generative approach, (Knight and Graehl, 1997). However, back transliteration is known to be a very difficult task (Knight and Graehl, 1997). There are several studies on transliteration (e.g., (Knight and Graehl, 1997)), and they tell us that machine transliteration of language pairs that employ very different alphabets and sound systems is extremely difficult, and that the technology is still to immature for use in practical processing. (Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English. (Knight and Graehl, 1997) and extended Markov window (Jung et al, 2000) treat transliteration as a phonetic process rather than an orthographic process. Back-transliteration is a difficult problem as exemplified in (Knight and Graehl 1997, Chen, et.al. 1998). Finite state transducers that implement transformation rules for back-transliteration from Japanese to English have been described by Knight and Graehl (1997), and extended to Arabic by Glover-Stalls and Knight (1998). Using machine transliteration can resolve part of UNK translation (Knight and Graehl, 1997). Other proposed methods include paraphrasing (Callison-Burch et al, 2006) and transliteration (Knight and Graehl, 1997) that uses the feature of phonetic similarity. One usually distinguishes between two types of transliteration (Knight and Graehl, 1997): Forward transliteration, where an originally Hebrewterm is to be transliterated to English; and Backward transliteration, in which a foreign term that has already been transliterated into Hebrew is to be recovered. Knight and Graehl (1997) have proposed to compose a set of weighted finite state transducers to solve the much more complicated problem of back-transliteration from Japanese Katakana to English.
Hatzivassiloglou and McKeown (1997) presented a method for automatically assigning a + or - orientation label to adjectives known to have some semantic orientation. In this paper, we use the model labels assigned by hand by Hatzivassiloglou and McKeown, and tile labels automatically obtained by their method and reported in (Hatzivassiloglou and McKeown, 1997) with the following extension: An adjective that appears in k conjunctions will receive (possibly different) labels when analyzed together with all adjectives appearing in at least 2, 3 ..., k conjunctions. The features developed in lhis paper are not only good clues of subjectivity, they can be Mentilied automatically from corpora (see (Hatzivassiloglou and McKeown, 1997), and Section 3 in the present paper). We are using the observations about conjunctive constructions from (Hatzivassiloglou and McKeown, 1997) in our approach. As explained by Hatzivassiloglou and McKeown (1997), two opinion terms appearing in a conjunctive constructions tend to have semantic orientations with the same or opposite directions, depending on the conjunction employed. (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). Hatzivassiloglou and McKeown (1997) determine the polarity of adjectives by mining pairs of conjoined adjectives from text, and observing that conjunctions such as and tend to conjoin adjectives of the same polarity while conjunctions such as but tend to conjoin adjectives of opposite polarity. Hatzivassiloglou and McKeown (1997) clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering. One of the earliest work on learning polarity of terms was by Hatzivassiloglou and McKeown (1997) who deduce polarity by exploiting constraints on conjoined adjectives in the Wall Street Journal corpus.  The work of Hatzivassiloglou and McKeown (1997) is among the earliest efforts that addressed this problem. We used the list of labeled seeds from (Hatzivassiloglouand McKeown, 1997) and (Stone et al, 1966).  In previous work, Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them in a large corpus. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. For sentiment, the authors use a seed set of positive and negative adjectives, and iteratively propagate sentiment polarity through conjunction relations (like those used by Hatzivassiloglou and McKeown 1997, above). Our method for determining sentiment polarity is based on an adaptation of Hatzivassiloglou and McKeown (1997). In each case, OPINE makes use of local constraints on label assignments (e.g., conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997)). Hatzivassiloglou and McKeown (1997) proposed a method for identifying word polarity of adjectives. Following the method presented in (Hatzivassiloglou and McKeown, 1997), we can connect words if they appear in a conjunctive form in the corpus.
In the remainder of this paper, we discuss the PARADISE framework (PARAdigm for Dialogue System Evaluation) (Walker et al, 1997), and that it addresses these limitations, as well as others. Instead, predictions about user satisfaction can be made on the basis of the predictor variables, which is illustrated in the application of PARADISE to sub dialogues in (Walker et al., 1997). While we discussed the representation of an information-seeking dialogue here, AVM representations for negotiation and diagnostic dialogue tasks are also easily constructed (Walker et al, 1997). The first approach to predict user judgments on the basis of interaction metrics is the well-known PARADISE model (Walker et al, 1997). There are also well-known evaluation efforts such as EAGLES (Sparck Jones and Galliers, 1996) and the Paradise evaluation framework (Walker et al, 1997). Walker et al (1997) identified three factors which carry an influence on the performance of SDSs, and which therefore are thought to contribute to its quality perceived by the user: agent factors (mainly related to the dialogue and the system itself), task factors (related to how the SDS captures the task it has been developed for) and environmental factors (e.g. factors related to the acoustic environment and the transmission channel). The PARADISE framework (Walker et al, 1997) produces such a relationship for a specific scenario, using multivariate linear regression. As stated above, the separation of environmental, agent and task factors was motivated by Walker et al (1997). In the PARADISE framework, user satisfaction is composed of maximal task success and minimal dialogue costs (Walker et al, 1997), thus a type of efficiency in the way it was defined here. User satisfaction is a function of task success and the number of user turns based on the PARADISE framework (Walker et al, 1997) and CAS refers to the proportion of repetition and variation in surface forms. Previous studies (E.g., Walker et al, 1997) use a corpus level semantic accuracy measure (semantic Accuracy) to capture the system's understanding ability. Once they had completed all tasks in sequence using one system, they filled out a questionnaire to assess user satisfaction by rating 8-9 statements, similar to those in (Walker et al, 1997), on a scale of 1-5, where 5 indicated highest satisfaction. Following the PARADISE evaluation scheme (Walker et al, 1997), we divided performance features into four groups. Some studies (e.g., (Walker et al, 1997)) build regression models to predict user satisfaction scores from the system log as well as the user survey. Future work focuses on usability tests of the prototype system, e.g. using the PARADISE evaluation framework to evaluate the general usability of the system (Walker et al, 1997). They then derived dialogue act metrics from the DATE tags and showed that when these metrics were used in the PARADISE evaluation framework (Walker et al, 1997) that they improved models of user satisfaction by an absolute 5%, and that the new metrics could be used to understand which system's dialogue strategies were most effective. Such metrics have been introduced in other fields, including PARADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation, and ROUGE (Lin, 2004) for summarisation. In doing so, we are essentially exploring system behaviour in a glass box approach: this does not constitute an evaluation method for dialogue performance [Walker et al, 1997]. In particular, unlike the PARADISE framework (Walker et al, 1997), which aims to evaluate dialogue agent strategies by relating overall user satisfaction to various other metrics (task success, efficiency measures, and qualitative measures) our approach takes the agent's dialogue strategy for granted. Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al, 1997).
Palmer (1997) conducted a Chinese segmenter which merely made use of a manually segmented corpus (without referring to any lexicon). In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al, 1994).  This approach was used by Palmer (1997) for word segmentation. Our work adds an existing system to improve the rules learned, while Palmer (1997) adds rules to improve an existing system's performance. One may note that the error reductions here are smaller than Palmer (1997)'s error reductions. In Palmer (1997), the baseline is how well an existing system performs before the rules are run. If we were to use the same baseline as Palmer (1997), our baseline would be an F of 37.5% for IaB and 52.6% for IaC. For example, (Palmer, 1997) developed a Chinese word segmenter using a manually segmented corpus. Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation. In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data. The use of TBL for Chinese word segmentation was first suggested in Palmer (1997).
All translation models were induced using the method of Melamed (1997). SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). In its basic form the Competitive Linking algorithm (Melamed, 1997) allows for only up to one link per word. The selection order is similar to that in the competitive linking algorithm (Melamed, 1997). The first algorithm is similar to Competitive Linking (Melamed, 1997). In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm. (Melamed,1997) has proposed the Competitive Linking Algorithm for linking the word pairs and a method which calculates the optimized correspondence level of the word pairs by hill climbing. Every English word has either 0 or 1 alignments (Melamed, 1997). The problem, which was described in (Melamed, 1997) in a word-to-word alignment context, is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation. EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997).  This can be seen as a generalization of the one-to-one assumption for word-to-word translation used by Melamed (1997a) and is exploited for the same purpose, i.e. to exclude large numbers of candidate alignments, when good initial alignments have been found. The basic algorithm combines the K-vec approach, described by Fung and Church (1993), with the greedy word-to-word algorithm of Melamed (1997a). Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates.
Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al, 1998) and others. SV phrases, following the definition suggested in (Argamon et al, 1998), are word phrases starting with the subject of the sentence and ending with the first verb, excluding modal verbs. As reported in (Argamon et al, 1998), most base NPs present in the data are less or equal than 4 words long. Argamon et al (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. (Argamon et al, 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences.
Biographic Data Past work on this task (e.g. Bagga and Baldwin, 1998) has primarily approached personal name disambiguation using document context profiles or vectors, which recognize and distinguish identical name instances based on partially indicative words in context such as computer or car in the Clark case. Some refer to the task as cross-document co reference resolution (Bagga and Baldwin, 1998), name discrimination (Pedersen et al, 2005) or Web People Search (WebPS) (Artiles et al, 2007). In our experiments, we use the training texts to acquire co reference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used co reference scoring programs: the MUC scorer (Vilain et al, 1995) and the B-CUBED scorer (Bagga and Baldwin, 1998). Four-fold cross validation is employed and B-CUBED metric (Bagga and Baldwin, 1998) is adopted to evaluate the clustering results. We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. Bagga and Baldwin (1998) proposed entity based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. Similar approach was developed by (Bagga and Baldwin, 1998), who created first order context vectors that represent the instance in which the ambiguous name occurs. In this paper, we present a new text semantic similarity approach for fine-grained person name categorization and discrimination which is similar to those of (Pedersen et al, 2005) and (Bagga and Baldwin, 1998), but instead of simple word co-occurrences, we consider the whole text segment and relate the deduced semantic information of Latent Semantic Analysis (LSA) to trace the text cohesion between thousands of sentences containing named entities which belong to different fine-grained categories or individuals. The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. We base our work partly on previous work done by Bagga and Baldwin (Bagga and Baldwin, 1998), which has also been used in later work (Chen and Martin, 2007). To score the output of a coreference model, we employ three scoring programs: MUC (Vilain et al, 1995), B3 (Bagga and Baldwin, 1998), and 3 -CEAF (Luo, 2005). The aforementioned complication does not arise from the construction of the mapping, but from the fact that Bagga and Baldwin (1998) and Luo (2005) do not specify how to apply B3 and CEAF to score partitions generated from system mentions. We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions. Early work in the field of name disambiguation is that of (Bagga and Baldwin, 1998) who proposed cross-document coreference resolution algorithm which uses vector space model to resolve the ambiguities between people sharing the same name. On this dataset, our proposed model yields a B3 (Bagga and Baldwin, 1998) F1 score of 73.7%, improving over the baseline by 16% absolute (corresponding to 38% error reduction). One of the first approaches to cross-document coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks: Word Sense Disambiguation (WSD) (Agirre and Edmonds, 2006) and Cross-document Coreference (CDC) (Bagga and Baldwin, 1998). Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities across documents according to their real referents.
Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al, 1998). Recently, the integration of NLP systems with manually-built resources at the predicate argument-level, such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al, 2005) has received growing interest. Building on this frame-semantic model, the Berkeley FrameNet project (Baker et al, 1998) has been developing a frame-semantic lexicon for the core vocabulary of English since 1997. Several lexical resources have been built manually, most notably WordNet (Fellbaum, 1998), FrameNet (Baker et al, 1998) and VerbNet (Baker et 122al., 1998). Therefore, many large human-annotated corpora have been constructed to support related research, such as FrameNet (Baker et al, 1998), PropBank (Kings bury and Palmer, 2002), NomBank (Meyers et al,2004), and so on. UBY currently contains nine resources in two languages: English WordNet (WN, Fellbaum (1998), Wiktionary2 (WKT-en), Wikipedia3 (WP en), FrameNet (FN, Baker et al (1998)), and VerbNet (VN, Kipper et al (2008)); German Wiktionary (WKT-de), Wikipedia (WP-de), and Ger ma Net (GN, Kunze and Lemnitzer (2002)), and the English and German entries of OmegaWiki4 (OW), referred to as OW-en and OW-de. In formalisms such as Frame Semantics (Baker et al, 1998), semantic roles generalize across semantically similar predicates belonging to the same frame. Other groups however have shown that this can be done, e.g., in Framenet (Baker et al, 1998) and more recently in PropBank (Kingsbury and Palmer, 2002). This type of approach has led to two highly valued semantic resources: the Prince ton WordNet (Fellbaum, 1998) and the Berkeley Framenet (Baker et al, 1998). FrameNet (FN; Baker et al (1998), Ruppenhofer et al (2010)) is a lexical resource based on FS. (6) To deal with these errors, we may use rich knowledge about verbs such as VerbNet (Kipper et al, 2000) and FrameNet (Baker et al, 1998) in order to judge whether a verb is transitive or intransitive. The current computational scene has witnessed an increased interest in the creation and use of semantically annotated computational lexica and their associated annotated corpora, like PropBank (Palmeretal., 2005), FrameNet (Baker et al, 1998) and NomBank (Meyers, 2007), where the proposed annotation scheme has been applied in real contexts. Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al, 1998), then the identification of locally uninstantiated roles (NIs). In Rule 14, we use FrameNet (Baker et al 1998) to determine whether med/situation should be assigned to an NP, NPi. The modeling of Process as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data (Baker et al, 1998). For predicate structure annotation, we followed the FrameNet model (Baker et al., 1998) (see Section 2.2). We carried out predicate argument structure annotation applying the FrameNet paradigm as described in (Baker et al, 1998). Driven by annotation resources such as FrameNet (Baker et al, 1998) and PropBank (Palmer et al, 2005), many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and Marquez (2005). FrameNet (Baker et al, 1998) and OpenCyc are other valuable resources for English, also hand-created, that contain a rich set of relations between words and concepts. In our experiments we used two datasets: FN extracted from FrameNet II 1.1 (Baker et al., 1998) TB2 extracted from Penn Treebank-II.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. Committee-based approaches to POS tagging have been in focus the last decade: Brill and Wu (1998) combined four different taggers for English using unweighted voting and by exploring contextual cues (essentially a variant of stacking). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy. The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998). The investigated MMVC model proves to be a very effective participant in classifier combination, with substantially different output to Naive Bayes (9.6% averaged complementary rate, as defined in Brill and Wu (1998)). We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. One opossibility is the example-based combiner in Brill and Wu (1998, Sec. 3.2). Related work includes learning ensembles of POS taggers, as in the work of Brill and Wu (1998), where an ensemble consisting of a unigram model, an N-gram model, a transformation-based model, and an MEMM for POS tagging achieves substantial results beyond the individual taggers. (Van Halteren et al, 1998) and (Brill and Wu, 1998) describe a series of successful experiments for improving the performance of part-of-speech taggers. This suggests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform Brill and Wu (1998), the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them). Brill and Wu (1998) call this complementary disagreement complementarity. Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. Comparison of different taggers on the WSJ corpus TBL and ME (Brill and Wu, 1998). Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000).
While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. The approaches tested were Error Driven Pruning (EDP) (Cardie and Pierce, 1998) and Transformational Based Learning of IOB tagging (TBL) (Ramshaw and Marcus, 1995). Conjunctions are a major source of errors for English chunking as well (Ramshaw and Marcus, 1995, Cardie and Pierce, 1998), and we plan to address them in future work. Our implementation of the NP-based QA system uses the Empire noun phrase finder, which is described in detail in Cardie and Pierce (1998). (Cardie and Pierce, 1998, 1999) applied a scoring method to select new rules and a naive heuristic for matching rules to evaluate the results and accuracy. (Cardie and Pierce, 1998) present an approach to chunking based on a mixture of finite state and context-free techniques. (Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data. We tested this hypothesis by training the Error-Driven Pruning (EDP) method of (Cardie and Pierce, 1998) with an extended set of features.
But (Chelba and Jelinek, 1998) chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model. This marraige of models has been tested in other fields such as speech recognition (Chelba and Jelinek, 1998) with success. (Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications. Aware that the n-gram obscures many linguistically-signi cant distinctions (Chomsky, 1956, section 2.3), many speech researchers (Jelinek and Laerty, 1991) sought to incorporate hierarchical phrase structure into language modeling (see (Stolcke, 1997)) although it was not until the late 1990s that such models were able to signi cantly improve on 3-grams (Chelba and Jelinek, 1998). The language model described by Chelba and Jelinek (1998) similarly conditions on linguistically relevant words by assigning partial phrase structure to the history and percolating headwords. In this situation, a structured language model (SLM) was proposed by Chelba and Jelinek (1998).  Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alternative technique in language modeling. As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) aretested when translating to morphologically rich languages. In the first type, probability of a word is decided based on a parse-tree information like grammatical headwords in a sentence (Charniak, 2001) (Chelba and Jelinek, 1998), or based on part of-speech (POS) tag information (Galescu and Ringger, 1999).
Complicating the issue is the phenomenon of regular sense extensions (Dang et al, 1998), where what once may have been coercion has become entrenched and is now seen as a different sense of the verb. Dang et al (1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intersective Levin classes, which are a more fine-grained, syntactically and semantically coherent refinement of basic Levin classes. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al, 1998), aimed at achieving more coherent classes both semantically and syntactically. Dang et al (1998) have supplemented the taxonomy with intersective classes: special classes for verbs which share membership of more than one Levin class because of regular polysemy. One can grasp this easily by looking at intersective Levin classes (Dang et al., 1998), created by grouping together subsets of existing classes with overlapping members. Several clusters were produced which represent linguistically plausible inter sective classes (e.g. A3) (Dang et al, 1998) rather than single classes. A very good candidate seems to be the Intersective Levin class (ILC) (Dang et al, 1998) that can be found as well in other predicate resources like PB and VerbNet (VN) (Kipper et al, 2000). This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levin's classification, called Intersective Levin classes (ILCs) (Dang et al, 1998). A very good candidate seems to be the Intersective Levin classes (Dang et al, 1998) that can be found as well in other predicate resources like PropBank and VerbNet (Kipper et al., 2000). This constraint of having the same semantic roles is further ensured inside the VerbNetlexi con that is constructed based on a more refined version of the Levin classification called Intersective Levin classes (Dang et al, 1998). Palmer (1999) and Dang et al (1998) argue that the use of syntactic frames and verb classes can simplify the definition of different verb senses. We also plan to experiment with different classification schemes for verb semantics uch as WordNet (Miller et al, 1990) and intersective Levin classes (Dang et al, 1998). This reorganization, which was facilitated by the use of inter sective Levin classes (Dang et al, 1998), refined the classes to account for semantic and syntactic divergences within a class. We first tried using the verb classes in VerbNet (Dang et al, 1998).
Fung and Yee (1998) demonstrated that the associations between a word and its context seed words are preserved in comparable texts of different languages. The material for the present experiments consists of comparable medical corpora in French and English and a French-English medical lexicon (Fung and Yee (1998) call its words seed words?).  An additional difference with Fung and Yee (1998) is that they look for translational equivalents only among words that are unknown in both corpora. Comparable corpora have primarily been used to build bilingual lexical resources (Fung and Yee, 1998). One approach that can, in principle, better exploit both alignments from bitextsand make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). Beside simple co occurrence counts within sliding windows, other SoA measures include functions based on TF/IDF (Fung and Yee, 1998), mutual information (PMI) (Lin, 1998), conditional probabilities (Schuetzeand Pedersen, 1997), chi-square test, and the log likelihood ratio (Dunning, 1993). A variety of subsequent work has extended the original idea either by exploring different measures of vector similarity (Fung and Yee, 1998) or by proposing other ways of measuring similarity beyond co-occurence within a context window.  Here, a standard technique of estimating bilingual term correspondences from comparable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. Fung and Yee (1998) also use a vector space approach, but use TF/IDF values in the vector components and experiment with different vector similarity measures for ranking the translation candidates. Paststatistical methods for non-parallel corpora (Fung and Yee, 1998) are not valid for finding translations of words or expressions with low frequency. Following standard practice in bilingual lexicon extraction from comparable corpora, we rely on the approach proposed by Fung and Yee (1998).  Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999). Fung and Yee (1998), for example, proposed to represent the contexts of a word or phrase with a real-valued vector (e.g., a TF-IDF vector), in which one element corresponds to one word in the contexts. Fung and Yee (1998) point out that not only the number of common words in context gives some similarity clue to a word and its translation, but the actual ranking of the context word frequencies also provides important clue to the similarity between a bilingual word pair. This fact has motivated Fung and Yee (1998) to use tfidf weighting to compute the vectors.
We consider three voting strategies suggested by van Halteren et al (1998): equal vote, where each classifier's vote is weighted equally, overall accuracy, where the weight depends on the overall accuracy of a classifier, and pair-wise voting. In the weighted voting method, we can assign different weights to the results of the individual system (van Halteren et al, 1998). In that paper, pairwise voting (van Halteren et al, 1998) has been used to combine the results of two super taggers that scan the input in the opposite directions. In the experiments presented in van Halteren et al (1998), this method was the best performer among the presented methods. And finally, TAGPAIR uses classification pair weights based on the probability of a classification for some predicted classification pair (van Halteren et al, 1998). Parallel to (van Halteren et al, 1998), we ran experiments with two stacked classifiers, Memory-Based, and Decision-Tree-Based. In all experiments, the TotPrecision voting scheme of (van Halteren et al, 1998) has been used.
It is possible to obtain a polynomial parser provided that we limit the number of nodes simultaneously involved in non-projective configurations (see Kahane et al 1998 for similar techniques). This formalism is based on previous work presented in (Kahane et al, 1998), which has been substantially reformulated in order to simplify it. We will extend our basic approach in the spirit of (Kahane et al, 1998) in future work. Kahane et al (1998) present three different types of rules, for sub categorization, modification, and linear precedence. It is also related to the lifting rules of (Kahane et al, 1998), but where they choose to stipulate rules that license liftings, we opt instead for placing constraints on otherwise unrestricted climbing. The pseudo-projective grammar proposed by Kahane et al (1998) can be parsed in polynomial time and captures non-local dependencies through a form of gap-threading, but the structures generated by the grammar are strictly projective. With this conversion technique, output dependency trees are necessarily projective, and extracted dependencies are necessarily local to a phrase, which means that the automatically converted trees can be regarded as pseudo-projective approximations to the correct dependency trees (Kahane et al, 1998). This concept was introduced as lifting in (Kahane et al, 1998). based: for example, those described by Lombardoand Lesmo (1996), Barbero et al (1998) and Kahane et al (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, predictive grammar-based algorithms such as those of Lombardo and Lesmo (1996) and Kahane et al (1998) have operations which postulate rules and can not be defined in terms of dependency graphs, since they do not do any modifications to the graph. In addition, the work of Kahane et al (1998) provides a polynomial parsing algorithm for a constrained class of non projective structures. The definition of non-projectivity can be found in Kahane et al (1998).  First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al, 1998) and encoding information about these lifts in arc labels. We call this pseudo projective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al, 1998). The dependency graph in Figure 1 satisfies all the defining conditions above, but it fails to satisfy the condition of projectivity (Kahane et al, 1998). Using the terminology of Kahane et al (1998), we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation. Unlike Kahane et al (1998), we do not regard a projectivized representation as the final target of the parsing process.
Verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play a major role in the organization and use of this knowledge: Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dorr, 1997), and document classification (Klavans and Kan, 1998). Levin-inspired classes have been used in several NLP tasks, such as Machine Translation (Dorr, 1997) and Document Classification (Klavans and Kan, 1998). The problems of non-nominal terms (Klavans and Kan, 1998), term variation (Jacquemin et al, 1997), and relevant contexts (Maynard and Ananiadou, 1998), can be considered for improving the performance. Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and sub categorization acquisition (Korhonen, 2002). Knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (Dolt, 1997) and document classification (Klavans and Kan, 1998), yet manual classification of large numbers of verbs is a difficult and resource intensive task (Levm, 1993 Miller et al , 1990, Dang et al, 1998). Out of the large syntactic constituents of a sentence, e.g. noun phrases, verb phrases, and prepositional phrases, we assume that noun phrases (NPs) carry the most contentful information about the document, even if sometimes the verbs are important too, as reported in the work by [Klavans and Kan (1998)]. In document classification, Klavans and Kan (1998) demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class (Levin 1993).
 Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping. We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998). Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002).   Lin (1998) created a thesaurus using syntactic relationships with other words. Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998). The thesaurus was acquired using the method described by Lin (1998). For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998).  As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. This seems to be a reasonable compromise between the approach of (Freitag et al, 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec. = 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta.ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. One of the most important approaches is Lin (1998). 
The algorithm with the next-to-highest results in (Charniak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit. They provide restrictions on co-indexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).  ParalStuct marks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998). Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors. We have already voiced concern (Mitkov, 1998a), (Mitkov, 2000b) that the evaluation of anaphora resolution algorithms and systems is bereft of any common ground for comparison due not only to the difference of the evaluation data, but also due to the diversity of pre-processing tools employed by each anaphora resolution system. RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b). Mitkov's approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent. As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000). Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified. The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre specific antecedent indicators to the remaining candidates (Mitkov, 1998). Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).  Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov,1998). Mitkov's knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates. How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished. Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be over come by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora. The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998). For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).
Paper will be implemented in ICONOCLAST, an authoring tool which enables domain experts to create a knowledge base through a sequence of interactive choice and generates hiexarchitally structured text according to various tylistic constraints (See Power and Scott 1998). One important source of inspiration for GF was the WYSIWYM system (Power and Scott 1998), which used domain-specific interlinguas and produced excellent quality in multilingual generation. Figure 1: Architecture of a MDA system creation to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the WYSIWYM (What You See Is What You Meant) approach (Power and Scott, 1998). The MDA (Multilingual Document Authoring) system [Brun et al2000] is an instance (descended from Ranta's Grammatical Framework [Ranta 2002]) of a text-mediated interactive natural language generation system, a notion introduced by [Power and Scott 1998] under the name of WYSIWYM. querying Conceptual authoring through WYSIWYM editing alleviates the need for expensive syntactic and semantic processing of the queries by providing the users with an interface for editing the conceptual meaning of a query instead of the surface text (Power and Scott, 1998). The "WYSIWYM" approach was proposed ([Power and Scott, 1998], [Paris and Vander Linden, 1996]) as a system design methodology where users author and manipulate an underlying logical form through a user interface that provides feedback in natural language text. In an influential series of papers [Power and Scott, 1998], WYSIWYM (What You See Is What You Mean) was proposed as a method for the authoring of semantic information through direct manipulation of structures rendered in natural language text. The system belongs to the family of WYSIWYM (What You See Is What You Mean) (Power and Scott, 1998) text generation systems: logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages. The method was invented to meet the needs of applications using WYSIWYM editing (Power and Scott, 1998), which allow an author to control the content of an automatically generated text without prior training in knowledge engineering. Wysiwym (What You See Is What You Meant) is a user-interface technology through which a domain expert can formally encode knowledge by structured editing of an automatically generated feedback text (Power and Scott, 1998).
We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. (Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs.  The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous.    More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. (Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations.
Instead, we parse them into a default analysis, which can then be expanded and disambiguatcd at later stages of processing using a large semantic knowledge base (Richardson 1997, Richardson et al 1998). Others have automatically extracted attribute relations from dictionary definitions (Richardson et al, 1998), structured online sources such as Wikipedia info boxes, (Wu and Weld, 2007) and large-scale collections of high-quality tabular web data (Cafarella et al, 2008). The Mindnet is a general-purpose database of semantic information (Richardson et al 1998) that has been repurposed as the primary repository of translation information for MT applications. MindNet (Richardson et al, 1998) is both an extraction methodology and a lexical ontology different from a word net since it was created automatically from a dictionary and its structure is based on such resources.  Such an approach was taken by the MindNet project (Richardson et al, 1998). One early example was MindNet (Richardson et al, 1998), which was based on collecting 24 semantic role relations from MRDs such as the American Heritage Dictionary. Richardson et al (1998) describes how MindNet began as a lexical knowledge base containing LF-like structures that were produced automatically from the definitions and example sentences in machine-readable dictionaries. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al, 1998) contain them. Graphs also can be 69 generated from dictionaries, and used to produce knowledge bases (Richardson et al., 1998) or proximity information (Gaume et al, 2006).
We chose three clusters produced by a program similar to Roark and Charniak (1998) except that it is based on a generative probability model and tries to classify all nouns rather than just those in pre-selected clusters. This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002). Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used co occurrence statistics in local context to discover sibling relations. Rilo and Shepherd (Rilo and Shepherd, 1997) developed a bootstrap ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) re ned this algorithm to focus more explicitly on certain syntactic structures. The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). Roark and Charniak describe a "generic algorithm" for extracting such lists of similar words using the notion of semantic similarity, as follows (Roark and Charniak, 1998). Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that parsing is invaluable. Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns. The goal of extracting semantic information from text is well-established, and has encouraged work on lexical acquisition (Roark and Charniak, 1998), information extraction (Cardie, 1997), and ontology engineering (Hahn and Schnattinger, 1998). This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5). To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category.  For example, Hearst (Hearst, 1992) learned hyponymy relationships by collecting words in lexico-syntactic expressions, such as NP, NP, and other NPs, and Roarkand Charniak (Roark and Charniak, 1998) generated semantically related words by applying statistical measures to syntactic contexts involving appositives, lists, and conjunctions. In previous research on semantic lexicon induction, Roark and Charniak (Roark and Charniak, 1998) showed that 3 of every 5 words learned by their system were not present in WordNet. Roark and Charniak (Roark and Charniak, 1998) followed up on this work by using a parser to explicitly capture these structures. Roark and Charniak (1998) applied this idea to extraction of words which belong to the same categories, utilizing syntactic relations such as conjunctions and appositives.
 According to Nissim et al, their definitions are built upon Prince? s (1981), and the categorization into old, new, and mediated entities resemble those of Strube (1998) and Eckert and Strube (2001). Our model of prominence is a simple local one similar to (Strube, 1998).  We have compared the obtained results with those obtained by testing bfp (Brennan et al, 1987) and str98 (Strube, 1998). Strube (1998)'s centering approach (whose sentence ordering is designated as SR2 in Table 2) also deals with and even prefers intra sentential anaphora, which raises the upper limit to a more acceptable 80.2%. Strube (1998) and Strube and Hahn (1999) argue that the information status of an antecedent is more important than the grammatical role in which it occurs. Like (Ge et al, 1998), Strube (1998) evaluates on ideal hand annotated data. Strube (1998)'s S-list algorithm is also restricted to the current and last sentence.   The POS properties could indicate whether a candidate refers to a hearer old entity that would have a higher preference to be selected as the antecedent (Strube, 1998). We now turn to our method of anaphora resolution, which extends the algorithm presented in Strube (1998), in order to be able to account for discourse deictic anaphora as well as individual anaphora. In addition to the S-List (Strube, 1998), which contains the referents of NPs available for anaphoric reference, our model includes an A-List for abstract objects. We have compared the obtained results with those obtained by testing bfp (Brennan et al, 1987) and str98 (Strube, 1998). Centering theory has also guided the development of pronoun resolution algorithms, such as the BFP algorithm (Brenan, Friedman and Pollard, 1987) and the S-list algorithm developed by Strube (Strube, 1998).
Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990). In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand. For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness.  We are interested in some distributional similarities (Lee, 1999) given certain context. Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. The second approach is from Lee (1999). This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999). Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions.  We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos).
Berland and Charniak (1999) use Hearst style techniques to learn meronym relation ships (part-whole) from corpora. (Berland and Charniak, 1999) proposed similar lexico-syntactic patterns to extract part-whole relationships. The OtherY-Model performs particularly poorly on smaller data sizes, where coverage of the Hearst-style patterns maybe limited, as also observed by Berland and Charniak (1999). Berland and Charniak (1999) report what they believed to be the first work finding part-whole relations from unlabelled corpora. In 1999, Berland and Charniak (Berland and Charniak, 1999) applied statistical methods on a very large corpus to find PART-WHOLE relations. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. Berland and Charniak (1999) used a similar method for extracting instances of meronymy relation. Lexical patterns have been successfully used to represent various semantic relations between words such as hypernymy (Hearst, 1992), and meronymy (Berland and Charniak, 1999). A similar approach was pursued in parallel by Berland and Charniak (1999). Berland and Charniak (1999) suggest their work may be useful for building a lexicon or ontology, like WordNet. Berland and Charniak (1999) use Hearst's manual procedure. In attribute extraction, typically one must choose between the precise results of rich patterns (involving punctuation and parts-of-speech) applied to small corpora (Berland and Charniak, 1999) and the high-coverage results of superficial patterns applied to web-scale data, e.g. via the Google API (Almuhareb and Poesio, 2004). Prior work has mostly focused on finding "relevant" attributes (Alfonseca et al., 2010) or "correct" parts (Berland and Charniak, 1999). Indeed, Berland and Charniak (1999) attempted to filter out attributes that were regarded as qualities (like drive ability) rather than parts (like steering wheels) by removing words ending with the suffixes -ness, -ing, and -ity. Also, previous relation extraction work, http: //projects.ldc.upenn.edu/ace/ such as Berland and Charniak (1999) and Girju et al. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations. Berland and Charniak (1999) proposed a system for part-of relation extraction, based on the (Hearst 1992) approach. (Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relation ships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs. OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004). relation (Berland and Charniak 1999), causal relation (Girju 2003), and entailment relation (Geffet and Dagan 2005).
Two-dimensional EM-based clustering has been applied to tasks in syntax (Rooth et al, 1999), but so far this approach has not been used to derive models of higher dimensionality and, to the best of our knowledge, this is the first time that it is being applied to speech. EM-based clustering has been derived and applied to syntax (Rooth et al, 1999). We evaluated our 3-dimensional clustering models on a pseudo-disambiguation task similar to the one described by Rooth et al (1999), but specied to onset, nucleus, and coda ambiguity. e. g. Rooth et al (1999) and Erk (2007). Similar models have been used to learn sub categorization information (Rooth et al, 1999) or properties of verb argument slots (Yao et al,2011). Rooth et al (1999) referring to a direct object noun for describing verbs, or used any syntactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003). Rooth et al (1999) generalize over seen head words using EM-based clustering rather than WordNet. Experimental design Like Rooth et al (1999) we evaluate selectional preference induction approaches in a pseudo disambiguation task. Rooth et al (1999) tested 3000 random (v, n) pairs, but required the verbs and nouns to appear between 30 and 3000 times in training. EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al, 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm. Usually, the classes are from WordNet (Miller et al, 1990), although they can also be inferred from clustering (Rooth et al., 1999).  Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al (1993) and Rooth et al (1999). Rooth et al (1999) introduced a model of selectional preference induction that casts the problem in a probabilistic latent-variable framework. Semantic classes assigned to predicate arguments in sub categorization frames are either derived automatically through statistical clustering techniques (Rooth et al (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. Rooth et al (1999) present a fundamentally different approach to selectional preference induction which uses soft clustering to form classes for generalisation and does not take recourse to any hand-crafted resource. The strategy of our model to derive generalisations directly from corpus data, without recourse to re sources, is similar to another family of corpus-driven selectional preference models, namely EM-based clustering models (Rooth et al, 1999).  We explore a variant of the pseudo-disambiguation task of Rooth et al (1999) which has been applied to SP acquisition by a number of recent papers. Three and five-dimensional EM-based clustering has been applied to monolingual phonological data (Muller et al, 2000) and two-dimensional clustering to syntax (Rooth et al, 1999).
See Caraballo (1999) for a detailed description of a method to construct such a hierarchy. In Caraballo (1999), we construct a hierarchy of nouns, including hypernym relations. Caraballo (1999) proposed the first attempt, which used conjunction and apposition features to build noun clusters. Caraballo (1999) uses a hierarchical clustering technique to build a hyponymy hierarchy of nouns. The built-in ambiguity in the hyponymy hierarchy presented in (Caraballo, 1999) is primarily an effect of the fact that all information is composed into one tree. As was also reported by Caraballo (1999), the judges sometimes found proper nouns (as hyponyms) hard to evaluate. It is difficult to compare these results with results from other studies such as that of Caraballo (1999), as the data used is not the same. Caraballo (1999) let three judges evaluate ten internal nodes in the hyponymy hierarchy that had at least twenty descendants. In Section 4, we show how correctly extracted relationships can be used as "seed-cases" to extract several more relationships, thus improving recall; this work shares some similarities with that of Caraballo (1999). Another definition is given by Caraballo (1999). First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in (Caraballo, 1999), perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node. As stated in (Caraballo, 1999), WordNet has been an important lexical knowledge base, but it is insufficient for domain specific texts. So, many attempts have been made to automatically produce taxonomies (Grefenstette, 1994), but (Caraballo, 1999) is certainly the first work which proposes a complete overview of the problem by (1) automatically building a hierarchical structure of nouns based on bottom-up clustering methods and (2) labeling the internal nodes of the resulting tree with hypernyms from the nouns clustered underneath by using patterns such as B is a kind of A. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. (Caraballo, 1999B) also used contextual information to determine the specificity of nouns. Contrary, domain specific terms don't tend to be modified by other words, because they have sufficient information in themselves (Caraballo, 1999B).  Caraballo (1999) uses conjunction and appositive annotations in the vector representation. Other approaches use natural language data, sometimes just by analyzing the corpus (Sanderson and Croft 1999), (Caraballo 1999) or by learning to expand WordNet with clusters of terms from a corpus, e.g., (Girju et al 2003). For example, based on (Caraballo 1999), each parent of a leaf node could be viewed as a cluster label for its children, with the weight of a parent-child link being determined based on how strongly the child is associated with the cluster.
Specifically, each sentence was assigned a subjective or objective classitication, according to concensus lags derived by a stalistical analysis of the chisses assigned by three human judges (see (Wiebe et al, 1999) for further information). Other approaches to annotator quality control include using EM-based algorithms for estimating annotator bias (Wiebe et al 1999, Ipeirotis et al 2010). Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. Wiebe et al (1999) use statistical methods to automatically correct the biases in an notations of speaker subjectivity. Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al, 1999). Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al, 1999). In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al (1999). Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Following (Wiebe et al, 1999), if the primary goal of a sentence is judged as the objective reporting of information, it was labeled as OBJ. Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. Again, our feature set is richer than Wiebe et al (1999). Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity.  According to the coding manual (Wiebe et al, 1999), subjective sentences are those expressing evaluations, opinions, emotions, and speculations. One judge annotated all articles in four datasets of the Wall Street Journal Treebank corpus (Marcus et al, 1993) (W9-4, W9-10, W9-22, and W9 33, each approximately 160K words) as well as the corpus of Wall Street Journal articles used in (Wiebe et al, 1999) (called WSJ-SE below). Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999). During the past few years, the problem of polarity recognition has been usually faced as a step beyond the identification of the subjectivity or objectivity of texts (Wiebe et al, 1999).
Minipar outputs dependency trees (Lin, 1999) from the input sentences. Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSA to distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. Dekang Lin proposes a way to automatically identify the noncompositionality of MWEs (Lin, 1999). Least mutual information difference with similar collocations: this feature is based on Lin's work (Lin, 1999). Lin (1999) gave a corpus-based method for finding various types of non-compositional phrases, including the sort discussed in this paper. Recent work which attempts to discriminate between compositional and non-compositional MWEs include Lin (1999), who used mutual information measures identify such phrases, Bald win et al (2003), who compare the distribution of the head of the MWE with the distribution of the entire MWE, and Vallada Moiro and Tiedemann (2006), who use a word-alignment strategy to identify non-compositional MWEs making use of parallel texts. Melamed (1997) and Lin (1999) have done some research on non compositional phrases discovery. Inspired by Lin (1999), we examine the strength of association between the verb and noun constituents of the target combination and its variants, as an indirect cue to their idiomaticity. Lin (1999) assumes that a target expression is non-compositional if and only if its (I) J+ value is significantly different from that of any of the variants. Lin (1999) and Wermter and Hahn (2005) go one step further and look into a linguistic property of non-compositional compounds their lexical fixedness to identify them.  Later work such as by Lin (1999) continued this tradition. This is also the place where linguistic constraints can be applied, say to avoid non compositional phrases (Lin, 1999). Second, some n-grams themselves carry no linguistic meaning; their phrase translations can be misleading, for example non-compositional phrases (Lin, 1999). To protect against that problem, we compute the 99.99999% confidence intervals around the PMI (Lin, 1999), and use the lower bound as a measure of association. These properties can be used to identify potential idioms, for instance, by employing measures of association strength between the elements of an expression (Lin, 1999). Lin (1999) argues that non-compositional expressions need to be treated differently than other phrases in many statistical or corpus based NLP methods.  Lin (1999) defines a decision criterion for non compositional phrases based on the change in the mutual information of a phrase when substituting one word for a similar one based on an automatically constructed thesaurus. We propose a new method that compares phrases with their alternative phrases, in the spirit of Lin (1999)'s substitution approach (see Section 4.3).
The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). (Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. The Deep Read reading comprehension system (Hirschman et al, 1999) uses a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story. Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. We call this set the MITRE corpus (Hirschman et al, 1999). We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments.
More recently, Bean and Riloff (1999) proposed methods for autolnatically extracting from a corpus such special predicates, i.e., heads that correlate well with discourse novelty. Bean and Riloff (1999) developed a system for identifying discourse-new DDs that incorporates, in addition to syntax-based heuristics aimed at recognizing predicative and established DDs using post modification heuristics similar to those used by Vieira and Poesio, additional techniques for mining from corpora unfamiliar DDs including proper names, larger situation, and semantically functional. More recently, the problem has been tackled using unsupervised (e.g., Bean and Riloff (1999)) and supervised (e.g., Evans (2001), Ng and Cardie (2002a)) approaches. Bean and Riloff (1999) extracts rules from non-anaphoric noun phrases and noun phrases patterns, which are then applied to test data to identify existential noun phrases. In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood. Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another. First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process. Bean and Riloff (1999) and Uryupina (2003) construct quite accurate classifiers to detect unique NPs. The system described in (Bean and Riloff, 1999) also makes use of syntactic heuristics. For the statistics-based approaches, Bean and Riloff (1999) developed a statistics-based method for automatically identifying existential definite NPs which are non-anaphoric. Bean and Riloff (1999) and Uryupina (2003) have already employed a definite probability measure in a similar way, although the way the ratio is computed is slightly different. A more fine-grained distinction is made by Bean and Riloff (1999) and Vieira and Poesio (2000) to distinguish restrictive from non-restrictive post modification by ommitting those modifiers that occur between commas, which should not be classified as chain starting. We borrow the idea of classifying definites occurring in the first sentence as chain starting from Bean and Riloff (1999).  An anaphoricity feature indicates whether an NP to be resolved is anaphoric, and is typically computed using an anaphoricity classifier (Ng, 2004), hand-crafted patterns (Daum´e III and Marcu, 2005), and automatically acquired patterns (Bean and Riloff, 1999). Non-anaphoric definite descriptions have been detected using heuristics (e.g., Vieira and Poesio (2000)) and unsupervised methods (e.g., Bean and Riloff (1999)).  
For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999). The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m). While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity. In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). On this point, see (Eisner and Satta, 1999, and footnote 6). First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies. The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states. The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999).
  Collins et al (1999) used transformed constituency tree bank from Prague Dependency Treebank for constituent parsing on Czech. This is consistent with the findings of Collins et al (1999) for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where Charniak (2000) reports an increase in F-score of approximately 0.3%.   To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999). In particular, we used the method of Collins et al (1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features. The Czech parser of Collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using English. Lattice parsing (Chappelier et al, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (Hajiˇc et al., 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al (1999).   We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999). This F-measure is based on the recall and precision figures reported in Figure 7.15 in Collins (1999). As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al, 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). The mechanism we employ for incorporating morphology into the PCFG model (the Model 1 parser in (Collins, 1999)) is the modification of its part-of speech (POS) tag set; in this paper, we explain how this mechanism allows the parser to better capture morphological constraints. The authors in (Collins et al, 1999) describe an approach that gives 80% accuracy in recovering unlabeled dependencies in Czech. Ourbaseline model, which we used to evaluate the effects of using morphology, was Model 1 (Collins, 1999) with a simple POS tag set containing almost no morphological information. It is also true of the adaptation of the Collins parser for Czech (Collins et al, 1999) and the finite-state dependency parser for Turkish by Oflazer (2003).
On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. (Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus.  To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages.  One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window).
This is implemented by Resnik and described in detail in [Resnik, 1999], calculates the similarity between all the words' senses of words in a set. Finally, I show that this algorithm performs competitively with the approach of Resnik (1999), in which only Association for Computational Linguistics. The STRAND system (Resnik, 1999), for example, uses structural markup information from the pages, without looking at their content, to attempt to align them. These were the same pairs for which human evaluations were carried out by Resnik (1999). The STRAND scores are similar to those published by Resnik (1999). It is capable of pulling parallel texts out of a large multilingual collection, and it rivals the performance of structure-based approaches to pair classification (Resnik, 1999), having better agreement with human judges. Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. The lack of large-scale parallel corpora no doubt has impeded progress in this direction, although attempts have been made to mine parallel corpora from the Web (Resnik, 1999). First, parallel corpora, especially accurately aligned parallel corpora are rare, although attempts have been made to mine them from the Web (Resnik, 1999). Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for mining the web for bilingual texts.  Resnik (1999) mined comparable corpora on the assumption that the pages which are comparable of each other share a similar structure (headers, paragraphs, etc.) when text is presented in many languages in the Web. Grefenstette (1999) used the Web for example-based ma chine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data. For example, Table 5 shows the abstract of Resnik (1999) and 5 selected sentences that cite it in AAN. Many research ideas have exploited the Web in unsupervised or weakly supervised algorithms for natural language processing (e.g., Resnik (1999)). In Resnik (1999), the Web is harvested in search of pages that are available in two languages, with the aim of building parallel corpora for any pair of target languages. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data. The STRAND system of (Resnik, 1999), uses structural markup information from the pages, without looking at their content, to attempt to align them. Mining the Web for bilingual text (Resnik, 1999) is not likely to provide sufficient quantities of high quality data.
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999).  The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)).  The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism).
As new applications appear, that cannot start generation from a semantic input because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (Barzilay et al, 1999) or generation of complex NPs in a hybrid template system for business letters (Gedalia, 1996)), this motivation has lost some of its strength. Barzilay et al (1999) were one of the first to use time for multi-document summarization. Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al, 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al, 1999). Then one can apply the method inspired by (Barzilay et al, 1999) to identify common phrases across sentences and use language generation to form a more coherent summary. Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al, 1999). Barzilay et al (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. In this work, we use the clusters of event related sentences from the Information Fusion work by Barzilay et al [1999]. For our evaluation cases, we use the Information Fusion data collected by [Barzilay et al, 1999]. ). Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al, 2005), multi document summarization (Barzilay et al, 1999) and machine translation (Quantz and Schmitz, 1994. As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation.  Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multi Gen (Barzilay et al, 1999), and MEAD (Radev et al, 2004). A famous effort in this direction is the information fusion approach proposed in Barzilay et al (1999). Barzilay et al (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentence rewrite techniques, e.g., (Barzilay et al, 1999). Since paraphrases capture the variations of linguistic expressions while preserving the meaning, they are very useful in many applications, such as machine translation (Marton et al, 2009), document summarization (Barzilay et al, 1999), and recognizing textual entailment (RTE) (Dagan et al, 2005). Lexical chains have been used in text summarization (Barzilay et al, 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. It has been observed that in the context of multi-document summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al, 1999). The merging task is a more logic-based approach than similar techniques like information fusion used in multi-document summarization (Barzilay et al, 1999).
Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3). We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical.
Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010).
Recently, a new dataset including "Unknown" pairs has been used in the "Cross-Lingual Textual Entailment for Content Synchronization" task at SemEval-2012 (Negri et al, 2012).  For a comprehensive description of the task see (Negri et al, 2012). Readers can refer to M. Negri et al 2012.s., for more detailed introduction. Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al, 2012). The SemEval-2012 CLTE task (Negri et al, 2012) asks participants to judge entailment pairs in four language combinations, defining four target entailment relations, for ward, backward, bidirectional and no entailment. Cross-Lingual Text Entailment (CLTE), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization (Negri et al, 2012). In this paper we have presented the DirRelCond3 systems that participated at the CLTE task (Negri et al., 2012) from SemEval-2012.
Obviously, intrinsic evaluation is more reliable, but it remains an extremely laborious process, where inter-judge disagreement is still an issue, see (Radev et al, 2000). The first of these, relative utility (RU) (Radev et al,2000) allows model summaries to consist of sentences with variable ranking. For details see (Radev et al, 2000). Three judges have assessed the sentences in each cluster and have provided a score on a scale from 0 to 10 (i.e. utility judgement), expressing how important the sentence is for the topic of the cluster (Radev et al, 2000). However, this experiment differs from prior work in that we use judges to determine the relevance of sentences to sub-events rather than to evaluate summaries (Radev et al, 2000).  Following (Radev et al, 2000), we used relative utility as our metric. MEAD (Radev et al, 2000): MEAD is a centroid-based extractive summarizer that scores sentences based on sentence-level and inter-sentence features which indicate the quality of the sentence as a summary sentence. As in (Radev et al, 2000), in order to create an extract of a certain length, we simply extract the top scoring sentences that add up to that length. MEAD (Radev et al, 2000): MEAD is a centroid-based extractive summarizer that scores sentences based on sentence-level andinter-sentence features which indicate the quality of the sentence as a summary sentence. 3.1.3 Relative Utility Relative Utility (RU) (Radev et al, 2000) is tested on a large corpus for the first time in this project. This is in contrast with a method proposed by Radev (Radev et al, 2000), where the centroid of a cluster is selected as the representative one. A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al, 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al, 2000). We use the Relative Utility (RU) method (Radev et al, 2000) to compare our various summaries. These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al, 2000) .Both summarizers rely on information extraction from the corpus. The extractive approach is represented by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000). First, our method focuses on subject shift of the documents from the target event rather than the sets of documents from different events (Radev et al, 2000). Finally, MEAD is a widely used MDS and evaluation platform (Radev et al, 2000). We have implemented an extractive summarizer for educational science content, COGENT, based on MEAD version 3.11 (Radev et al, 2000). Radev et al (2000) use it in their MDS system MEAD.
Our algorithm extends earlier approaches to morphology induction by combining various induced information sources: the semantic relatedness of the affixed forms usinga Latent Semantic Analysis approach to corpus based semantics (Schone and Jurafsky, 2000), affix frequency, syntactic context, and transitive closure. Most of the existing algorithms described focus on approach falls into this category (expanding upon suffixing in inflectional languages (though our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and Jean describe work on prefixes). In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes. We had noted previously (Schone and Jurafsky, 2000), however, that errors can arise from strictly orthographic systems. As in our earlier approach (Schone and Jurafsky, 2000), we begin by generating, from an untagged corpus, a list of word pairs that might be morphological variants. Using this final lexicon, we can now seek for suffixes in a manner equivalent to what we had done before (Schone and Jurafsky, 2000). In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)). To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated before (Schone and Jurafsky (2000)). We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000)) as well as to Goldsmith's Linguistica (2000). They are either based solely on corpus statistics (Djean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al, 2012). Using latent semantic analysis, Schone and Jurafsky (2000) have previously demonstrated the success of using semantic information in morphological analysis. Many researchers, including Schone and Jurafsky (2000), Harris (1958), and Djean (1998), suggest looking for nodes with high branching (out-degree) or a large number of continuations. In a different approach, Schone and Jurafsky (2000) utilize the context of each term to obtain a semantic representation for it using LSA. The idea thata stem and stem+affix should be semantically similar has been exploited previously for morphological analysis (Schone and Jurafsky, 2000). Next along the spectrum of orthographic similarity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMVs) using an or tho graphic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. To reduce the running time of the model we limit the space of considered morpheme boundaries as follows: Given the target side of the corpus, we derive a list of K most frequent prefixes and suffixes using a simple trie-based method proposed by (Schone and Jurafsky, 2000). Following Schone and Jurafsky (2000), clusters are evaluated for whether they capture inflectional paradigms using CELEX (Baayen et al, 1993). Schone and Jurafsky (2000) attempts to cluster morphologically related words starting with an unrefined trie search (but with a parameter of minimum possible stem length and an upper bound on potential affix candidates) that is constrained by semantic similarity in a word context vector space. Schone and Jurafsky (2000) give definitions for correct (C), inserted (I), and deleted (D) words in model-derived conflation sets in relation to a gold standard. 
Preliminary experiments with tags derived automatically using distributional clustering (Clark, 2000), have shown essentially the same results. It has often been proposed that children might make use of information about the contextual distribution of usage of words to induce the parts-of-speech of their native language (e.g. Maratsos and Chalkley, 1980), and work by, e.g., Redington, Chater and Finch (1998) and Clark (2000), showed that parts-of-speech can indeed be induced by clustering together words that are used in similar contexts in a corpus. Schutze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. Our work builds on two older part-of-speech inducers word clustering algorithms of Clark (2000) and Brown et al (1992) that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al, 2010).  Our main purely unsupervised results are with a flat clustering (Clark, 2000) that groups words having similar context distributions, according to Kullback Leibler divergence.  We found that Brown et al's (1992) older information-theoretic approach, which does not explicitly address the problems of rare and ambiguous words (Clark, 2000) and was designed to induce large numbers of plausible syntactic and semantic clusters, can perform just as well.  Unsupervised word clustering techniques of Brown et al (1992) and Clark (2000) are well-suited to dependency parsing with the DMV. We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus. Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. Clark (2000) reports results on a corpus containing 12 million terms, Schutze (1993) on one containing 25 million terms, and Brown, et al (1992) on one containing 365 million terms. Clark (2000) presentsa framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so. We trained a variant of our system without gold part-of-speech tags, using the unsupervised word clusters (Clark, 2000) computed by Finkel and Manning (2009). This approach is taken by Clark (2000), where the perplexity of a finite-state model is used to compare different category sets. To overcome this problem, Clark (2000) proposes a bootstrapping approach, in which he (1) clusters the most distributionally reliable words, and then (2) incrementally augments each cluster with words that are distributionally similar to those already in the cluster. It is perhaps not immediately clear why morphological information would play a crucial role in the induction process, especially since the distributional approach has achieved considerable success for English POS induction (see Lamb (1961), Schutze (1995) and Clark (2000)). In fact, Jardino and Adda (1994), Schutze (1997) and Clark (2000) have attempted to address the ambiguity problem to a certain extent.
This data set was used for CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). Text chunking consists of dividing text into syntactically related non overlapping groups of words (Tjong Kim Sang and Buchholz, 2000). Unlike the shallow phrases defined for the CoNLL-2000 Shared Task (Tjong Kim Sang and Buchholz, 2000), base phrases correspond directly to constituents that appear in full parses, and hence can provide a straightforward constraint on edges within a chart parser. For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000 style (Tjong Kim Sang and Buchholz, 2000) chunk converted versions of the full Brown and WSJ corpora. The data set, extracted from the WSJ Penn Tree bank, and first used in the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000), contains 211,727 training examples and 47,377 test instances. We annotate the same set of 800 tweets mentioned previously with tags from the CoNLL shared task (Tjong Kim Sang and Buchholz,2000). Our chunks were derived from the Tree bank trees using the conversion described by Tjong Kim Sang and Buchholz (2000). The shallow parse tags define non hierarchical base constituents (chunks), as defined for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). For the chunker, we ran fnTBL over the lemmatised tagged data, training over CoNLL 2000 style (Tjong Kim Sang and Buchholz, 2000) chunk converted versions of the full Brown and WSJ corpora. (Tjong Kim Sang and Buchholz, 2000) Unlike a parse tree, a set of syntactic chunks has no hierarchical information on how sequences of words relate to each other. We used the same data set as the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). The project provided a data set for this task at the CoNLL-2000 workshop (Tjong Kim Sang and Buchholz, 2000). Chunking was the shared task of CoNLL-2000, the workshop on Computational Natural Language Learning, held in Lisbon, Portugal in 2000 (Tjong Kim Sang and Buchholz, 2000). The task was extended to additional phrase types for the CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. NP chunking results have been reported on two slightly different data sets: the original RM data set of Ramshawand Marcus (1995), and the modified CoNLL-2000 version of Tjong Kim Sang and Buchholz (2000). Headwords are obtained from a parse tree with the script used for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). In the chunk inventory devised for the CoNLL-2000 test chunking shared task (Tjong Kim Sang and Buchholz, 2000), a dedicated particle chunk type once again exists. Indeed, all the best systems in the CoNLL shared task competitions (e.g. Chunking (Tjong Kim Sang and Buchholz, 2000)) make extensive use of lexical information. The conversion program is the same as used for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000). Tjong Kim Sang and Buchholz (2000) give an overview of the CoNLL shared task of chunking.
the number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto,2000a). In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b) 5. For SVM, we used the YAMCHA (Kudo and Matsumoto, 2000) sequence labeling system,1 which uses the TinySVM package for classification. For our experiments, we used TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and test software. Kudo and Matsumoto (2000) used an SVM based algorithm, and achieved NP chunking results of 93.72% precision, 94.02% recall and 93.87 F-measure for the same shared task data, using only the words and their PoS tags. They used the same features as Kudo and Matsumoto (2000), and achieved over-all chunking performance of 92.06% precision, 92.09% recall and 92.08 F-measure (The results for NP chunks alone were not reported). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto (2000, 2001)) as the SVM training and test software. We used the TinySVM toolkit (Kudo and Matsumoto, 2000), with a degree 2 polynomial kernel. Following Pradhan et al (2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software. This type of sequential tagging method regard as a chunking procedure (Kudo and Matsumoto, 2000) at sentence level. Kudo and Matsumoto (Kudo and Matsumoto, 2000) applied SVMs to English chunking and achieved the best performance in the CoNLL00 shared task (Sang and Buchholz, 2000). Thissetting is shown to produce good results for sequence labeling tasks in previous work (Kudo and Matsumoto, 2000), and is what most end-users of SVM classifiers are likely to use. In (Goldberg et al, 2006), we established that the task is not trivially transferable to Hebrew, but reported that SVM based chunking (Kudo and Matsumoto, 2000) performs well. Kudo and Matsumoto (2000) used SVM as a classification engine and achieved an F Score of 93.79 on the shared task NPs. We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. Even with a more feasible method, pairwise (Kreel, 1998), which is employed in (Kudo and Matsumoto, 2000), we can not train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation. Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al, 2001). We use YamCha (Kudo and Matsumoto, 2000) to perform phrase chunking.
A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study.  This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages.  Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank.  The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). 
The number of votes for the class obtained through the pairwise voting is used as the certain score for beam search with width 5 (Kudo and Matsumoto, 2000a). In the CoNLL-2000 shared task, we achieved the accuracy of 93.48 using IOB2-F representation (Kudo and Matsumoto, 2000b). Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000).  Kudo and Matsumoto (2000) used the sigmoid function to obtain pseudo probabilities in SVMs. To cope with this problem, Kudo and Matsumoto (2000) introduced a new type of feature called dynamic features, which are created dynamically during the parsing process. We used a third degree polynomial kernel function, which is exactly the same setting in (Kudo and Matsumoto, 2000). The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs (Kudo and Matsumoto, 2000) are summarized in Table 2. For example, Haruno et al (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Therefore, our methods analyze a sentence backwards as in Sekine (2000) and Kudo and Matsumoto (2000).  Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). Kudo and Matsumoto (2000) describe a dependency parser for Japanese and Yamada and Matsumoto (2003) an extension for English. Therefore, SVMs have shown good performance for text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2001), and dependency structure analysis (Kudo and Matsumoto, 2000). Kudo and Matsumoto (2000) also used the same backward beam search together with SVMs rather than ME. 
The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000).  In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags.  So we have also used the Stanford POS tagger (Toutanova and Manning, 2000) to tag these transcripts before calculating the Discriminative TFIDF score. Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000).
Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. More on these and other metrics can be found in [Bangalore et al, 2000]. Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]); Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]); Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al 2000]). The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. Simple String Accuracy (SSA, Bangalore et al 2000) has been proposed as a baseline evaluation metric for natural language generation. Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output.  We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length.  (Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006).
The relation tuple is then converted to root form using the Sussex morphological analyser (Minnen et al, 2000) and the POS tags are removed. Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). Each word was reduced to its morphological root using the morphological analyser described in (Minnen et al, 2000). The relation tuple is then converted to root form using the Sussex morphological analyser (Minnen et al., 2000) and the POS tags are stripped. Information on lemmatisation, as well as abbreviations and their long forms, is added using the morpha lemmatiser (Minnen et al, 2000) and the ExtractAbbrev script of Schwartz and Hearst (2003), respectively. Heads are extracted from the chunks and lemmatized (Minnen et al, 2000). Further linguistic markup is added using the morpha lemmatiser (Minnen et al, 2000) and the C&C named entity tagger (Curran and Clark, 2003) trained on the data from MUC-7. We next lemmatised the data using morpha (Minnen et al, 2000), and chunk parsed the WSJ with TiMBL 4.1 (Daelemans et al, 2001) using the Brown corpus as training data. Tokenisation, species word identification and chunking were implemented in-house using the LTXML2 tools (Grover and Tobin, 2006), whilst abbreviation extraction used the Schwartz and Hearst abbreviation extractor (Schwartz and Hearst, 2003) and lemmatisation used morpha (Minnen et al, 2000). We used the morpha lemmatizer (Minnen et al, 2000), which is built into the C&C tools, to match tokens across T and H; and we converted all tokens to lowercase. Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). The Grefenstette (1994) relation extractor produces context relations that are then lemmatised using the Minnen et al (2000) morphological analyser. B5: Lemmatize the tokens using morpha, (Minnen et al, 2000). The tokenisation, sentence boundary detection, head word identification and chunking components were implemented with the lt-xml2tools (Grover and Tobin, 2006), and the lemmatisation used morpha (Minnen et al, 2000). Since MINIPAR performs morphological analysis on the context relations we have added an existing morphological analyser (Minnen et al, 2000) to the other extractors. For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001), and further lemmatise the output of the tagger using morph (Minnen et al, 2000). We use a morphological tool (Minnen et al, 2000) to obtain the base form from the original verb or noun, so that YAG can generate grammatical sentences.
Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).  The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001). A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach. The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001).  In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001).
In previous work (Rosario and Hearst, 2001), we demonstrated the utility of using a lexical hierarchy for assigning relations to two-word noun compounds. Finally, Rosario and Hearst (2001) make use of a domain-specific lexical resource to classify according to neural networks and decision trees. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. This includes the seminal paper of (Gildea and Jurafsky, 2002), Senseval and Conll competitions on automatic labeling of semantic roles detection of noun compound semantics (Lapata, 2000), (Rosario and Hearst, 2001) and many others. Rosario and Hearst (2001) constructed feature vectors for each noun modifier pair using MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) as lexical resources. In other methods, lexical resources are specifically tailored to meet the requirements of the domain (Rosario and Hearst, 2001) or the system (Gomez, 1998). This approach was used by Rosario and Hearst (2001) within a specific domain - medical texts.   (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques.  Rosario and Hearst (2001) used a discriminative classifier to assign 18 relations for noun compounds from biomedical text, achieving 60% accuracy. For example, Rosario and Hearst (2001) propose 18 abstract relations for interpreting NCs in biomedical text, e.g., DEFECT, MATERIAL, PERSON AFFILIATED, ATTRIBUTE OF CLINICAL STUDY. While most of the noun compound research to date is not domain specific, Rosario and Hearst (2001) create and experiment with a taxonomy tailored to biomedical text. Rosario and Hearst (2001) utilize neural networks to classify compounds according to their domain-specific relation taxonomy. Rosario and Hearst (2001) used MeSH, a lexical hierarchy of medical terms. Domain noun compound semantics, including static relations, have been considered in studies by (Rosario and Hearst, 2001) and (Nakov et al, 2005), but in IE settings static relations tend to appear only implicitly, as in the RelEx causal RE system of (Fundel et al, 2007), or through the causal relations they imply: for example, in the AIMed corpus (Bunescu et al, 2005) statements such as NE1/NE2 complex are annotated as a binding relation between the two NEs, not Part Whole relations with the broader entity. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier.
Schone and Jurafsky (2001) applied LSA to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from a corpus. Indeed, Schone and Jurafsky (2001) provide evidence that suggests that WordNet is as effective an evaluation resource as the web for MWE detection methods, despite its inherent size limitations and static nature. Schone and Jurafsky (Schone and Jurafsky, 2001) applied Latent-Semantic Analysis (LSA) to the analysis of MWEs in the task of MWE discovery, by way of rescoring MWEs extracted from the corpus. Regarding (i), Schone and Jurafsky (2001) compare the semantic vector of a phrase p and the vectors of its component words in two ways: one includes the contexts of p in the construction of the semantic vectors of the parts and one does not. Statistical association measures are frequently used for MWU detection and collocation extraction (e.g. Schone and Jurafsky (2001), Evert and Krenn (2001), Pecina (2010)). We use all measures used by Schone and Jurafsky (2001) that can be derived from a phrase's contingency table. As our baseline, we use two methods of comparing semantic vectors: sj1 and sj2, both introduced by Schone and Jurafsky (2001). Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Schutze, 1998) and disambiguation (McCarthy et al, 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al, 2001), and notably information retrieval (Salton et al, 1975). By contrast, Schone and Jurafsky (2001) evaluate the identification of phrasal terms without grammatical filtering on a 6.7 million word extract from the TREC databases, applying both WordNet and on line dictionaries as gold standards. Since Schone and Jurafsky (2001) demonstrated similar results whether WordNet or on line dictionaries were used as a gold standard, WordNet was selected. There have also been a number of papers focusing on the detection of semantic non-compositional items in recent years beginning with the work of Schone and Jurafsky (2001). Furthermore, lists of topical unigrams are often made only marginally interpretable by virtue of their non-compositionality, the principle that a collocation's meaning typically is not derivable from its constituent words (Schone and Jurafsky, 2001).
 PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al, 2001), and offers a more solid statistical foundation. Choi at al. used LSA for segmentation (Choi et al., 2001). (Choi et al, 2001) used all vocabulary words to compute low-dimensional document vectors. (Choi et al, 2001) used clustering to predict boundaries whereas we used the average similarity scores. It is a very powerful technique already used for NLP applications such as information retrieval (Berry et al, 1995) and text segmentation (Choi et al, 2001) and, more recently, multi and single-document summarization. In CWM (Choi et al, 2001), a variant of C99, each word of a sentence is replaced by its representation in a Latent Semantic Analysis (LSA) space.  While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al, 2001)) and synchronous dialogs (e.g., (Galley et al, 2003), (Hsueh et al, 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). The CWM algorithm (Choi et al, 2001) applies the same procedure to a similarity matrix of LSI vectors. As a preliminary test of the error measure, I evaluated two algorithms from Choi et al (2001) on the standard segmentation data set that Choi (2000) compiled. The C99 (Choi, 2000) and CWM (Choi et al, 2001) algorithms were evaluated. Because of these differences, the implementation of HCWM reported here differs somewhat from the implementation of CWM reported by Choi et al (2001). (12.5%) matches what Choi et al (2001) reported (12%), while the error for HCWM (12.1%) is higher than that reported for the version with a paragraph-based 500-dimension LSI space (9%) but appears comparable to their sentence-based 400-dimension LSI space. And the result for NONE (46.1%) agrees with Choi et al (2001)'s results for their NONE (46%) base line. In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al, 2001).
Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). See Gildea (2001) for the exact setup. For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests.
These numbers compare favorably with the previous literature: (Filatova and Hovy 2001) obtained 82% accuracy on anchoring for a single type of event/topic on 172 clauses, while (Mani and Wilson 2000) obtained accuracy of 59.4% on anchoring over 663 verb contexts. Filatova and Hovy (2001) also explore temporal linking with events, but do not assume that events and time stamps have been provided by an external process. Filatova and Hovy (2001) assign time stamps to clauses in which an event is mentioned. Filatova and Hovy (2001) also process explicit temporal expressions within a text and apply this information throughout the whole article, assigning activity times to all clauses. Filatova and Hovy (2001) infer time values based on the most recently assigned date or the date of the article.  
The additional two systems were: PD-EDU: Same as EDU except using the perfect discourse trees, available from the RST corpus (Carlson et al, 2001). We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RSTDTB) (Carlsonetal., 2001) distributed by the Linguistic Data Consortium (LDC). Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus. Fortunately, RST Discourse Treebank (RSTDT) (Carlson et al, 2001) is an available resource to help with. In the Cause versus Contrast case, their reported performance exceeds ours significantly; however, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, they report only 63% accuracy over a 56% baseline (the baseline is > 50% because the number of input examples is unbalanced).  For the first, the labelled/unlabelled relations f scores are 50.3% /73.0% and for the latter, they are 75.3% /84.0%: this is similar to the performance on other discourse annotation projects, e.g., Carlson et al (2001). The generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora: the RST Discourse Treebank (Carlson et al., 2001) and the Penn Discourse Treebank. We evaluate DPLP on the RST Discourse Tree bank (Carlson et al, 2001), comparing against state-of-the-art results. To compare with previous works on RSTDT, we use the 18 coarse-grained relations defined in (Carlson et al, 2001). (Carlson et al 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. To demonstrate the functionality of our system without relying on still imperfect discourse parsing, we use the RST parsed Wall Street Journal corpus as input (Carlson et al, 2001). In Discourse Tree Bank (Carlson et al, 2001) only 26% of Contrast relations were indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases. They use the RST corpus (Carlson et al,2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). (Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al, 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), is a corpus annotated in the framework of RST. In the corpus of Rhetorical Structure trees built by Carlson et al (2001), for example, we have observed that only 61 of 238 CONTRAST relation sand 79 out of 307 EXPLANATION-EVIDENCE relations that hold between two adjacent clauses were marked by a cue phrase. However, empirical work of Marcu (2000) and Carlson et al (2001) suggests that the majority of occurrences of but, for example, do signal CONTRAST relations. To test this, we used the corpus of discourse trees built in the style of RST by Carlson et al (2001). If no cue phrases are used to signal the relation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most frequently used relations (Carlson et al, 2001).
If evaluated against the requirements for teaching environments discussed in (Loper and Bird, 2002), GATE covers them all quite well. However, other such modules, e.g., those from NLTK (Loper and Bird, 2002), can be used for such assignments. We use the Punkt sentence splitter from NLTK (Loper and Bird, 2002) to perform both sentence and word segmentation on each text chunk. NLTK, the Natural Language Toolkit, is a suite of Python modules providing many NLP data types, processing tasks, corpus samples and readers, together with animated algorithms, tutorials, and problem sets (Loper and Bird, 2002). For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). We have not yet used the Natural Language Toolkit (Loper and Bird, 2002) (see Section 3.1) in this course. Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowballstemmers for other languages using an implementation provided by the NLTK (Loper and Bird, 2002). We strip unnecessary HTML tags and Wiki templates with mwlib5 and split sentences with NLTK (Loper and Bird, 2002). Some popular options include the NLTK (Loper and Bird, 2002), CSLU (Cole, 1999), Trindi (Larsson and Traum, 2000) and Regulus (Rayner et al, 2003) toolkits. We use a simple path distance similarity measure, as implemented in NLTK (Loper and Bird, 2002). Our word pairs are lemmatized using the Wordnet based lemmatizer of NLTK (Loper and Bird, 2002). The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). Tokenization ,lemmatization, and stop word removal was performed using the Natural Language Toolkit (Loper and Bird, 2002). Systems like NLTK (Loper and Bird, 2002) and Gate (Cunningham, 2002) do not offer functionality for Lexical Resource Management. To identify content words, we used the NLTK-Lite tagger to assign a part of speech to each word (Loper and Bird, 2002). For the NL processing, the Natural Language Toolkit (NL Toolkit or NLTK), developed at the University of Pennsylvania by Loper and Bird (2002), and available for download from Source Forge at http: //nltk.sourceforge.net/ was used.  In the end I decided to require the students to learn python because I wanted to use NLTK, the Natural Language Toolkit (Loper and Bird, 2002).  The Natural Language Toolkit (NLTK) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 (Loper and Bird, 2002).
 In addition, we split the GENIA 1.1 subset into the test dataset of 80 abstracts used in Kazama et al (2002) and the training dataset of the remaining 590 abstracts. Results of protein name recognition in Kazama et al (2002) using GENIA 1.1 are 0.492, 0.664 and 0.565 for precision, recall, f-score respectively. In the case of term classification, Kazama et al (2002) used a more exhaustive feature set containing lexical information, POS tags, affixes and their combinations in order to recognise and classify terms into a set of general biological classes used within the GENIA project (GENIA, 2003). Experiments with augmented tag sets in the biomedical domain also show performance loss with respect to smaller tagsets; e.g., Kazama et al (2002) report an F score of 56.2% on a tag set of 25 Genia classes, compared to the 75.9% achieved on the simplest binary case. Kazama et al (2002) reported an F-measure of 56.5% on the GENIA corpus (Version 1.1) using Support Vector Machines. (Kazama et al, 2002) proposed a machine learning approach to BNE tagging based on support vector machines (SVM), which was trained on the GENIA corpus and reported an F-measure score of 0.73 for different mixtures of models tested on 20 abstracts.  In previous research, (Kazama et al 2002) make use of POS information and conclude that it only slightly improves performance.  On V1.1, we use the same training and testing data and capture the same NE classes as (Kazama et al 2002). Kazama et al (2002) addressed the data imbalance problem and sped up the training process by splitting the type O instances into sub classes using part-of-speech information. In general, machine learning based methods to relation extraction perform very well for any task where sufficient, representative and high quality training data is available (Kazama et al., 2002).
The spelling-based model we propose (described in detail in (Al-Onaizan and Knight, 2002)) directly maps English letter sequences into Arabic letter sequences, which are trained on a small English/Arabic name list without the need for English pronunciations. The adapted spelling-based generative model is similar to (Al-Onaizan and Knight, 2002). Transliteration from English to Arabic and Chinese is complicated (Al-Onaizan and Knight, 2002). Section 3 presents the letter-based transducer approach to Arabic-English transliteration proposed in (Al-Onaizan and Knight,2002), which we use as the main point of comparison for our substring-based models. Al-Onaizan and Knight (2002) find that a model mapping directly from English to Arabic letters outperforms the phoneme-to-letter model. The main point of comparison for the evaluation of our substring-based models of transliteration is the letter-based transducer proposed by (Al-Onaizan and Knight, 2002). Al-Onaizan and Knight (Al-Onaizan and Knight, 2002) proposed a spelling-based model which directly maps English letter sequences into Arabic letter sequences. of these entities is in most cases actually a (more or less phonetic) transliteration, see for example (Al-Onaizan and Knight, 2002). Al-Onaizan and Knight (2002) transliterated named entities in Arabic text to English by combining phonetic-based and spelling-based models, and re-ranking candidates with full-name web counts, named entities co-reference, and contextual web counts. These results are also comparable to other state-of-the-art statistical Arabic name trans literation systems such as (Al-Onaizan and Knight, 2002). A spelling-based model that directly maps English letter sequences into Arabic letters was developed by Al-Onaizan and Knight (2002). (Al-Onaizan and Knight 2002) showed that use of outside linguistic resources such as WWW counts of transliteration candidates can greatly boost transliteration accuracy. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. To improve this performance, we plan to enrich the Arabic lexicon with more proper names, using either name recognition (Maloney and Niv, 1998) or a back translation approach after name recognition in English texts (Al-Onaizan and Knight, 2002). Subsequently, the performance of this system was greatly improved by combining different spelling and phonetic models (Al-Onaizan and Knight,2002). For example, the work of (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002b; Knight and Graehl, 1998) used the pronunciation of w in translation. Al-Onaizan and Knight (2002b) suggested that pronunciation can be skipped and the target language letters can be mapped directly to source language letters. Similarly, Al-Onaizan and Knight (2002a; 2002b) only made use of transliteration information alone and so was not directly comparable. 
We previously presented two segmentation algorithms suitable for agglutinative languages (Creutz and Lagus, 2002). A Poisson distribution can be justified and has been used in order to model the length distribution of word and morph tokens [e.g., (Creutz and Lagus, 2002)], but for morph types we have chosen the gamma distribution, which has a thicker tail. The search for the optimal model given our input data corresponds closely to the recursive segmentation algorithm presented in (Creutz and Lagus, 2002). We utilize an evaluation method for segmentation of words presented in (Creutz and Lagus, 2002). For highly inflecting languages more generally, morphological analysis is often treated as a segment and-normalise problem, amenable to analysis by weighted finite state transducer (wFST), for example, Creutz and Lagus (2002) for Finnish. Creutz and Lagus (2002) proposed two unsupervised methods for word segmentation, one based on maximum description length, and one based on maximum likelihood. Finnish is also the language for which the algorithm for the unsupervised morpheme discovery (Creutz and Lagus, 2002) was originally developed. Morfessor Baseline (Creutz and Lagus, 2002): This is a public baseline algorithm based on jointly minimizing the size of the morph codebook and the encoded size of all the word forms using the minimum description length MDL cost function. Similarly, Creutz and Lagus (2002) use an MDL formulation for word segmentation.   
The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002). Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus. Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. Lately, many such corpora have been developed in different languages, including SemCor (Miller et al, 1993), LDC-DSO (Ng and Lee, 1996), Hinoki (Kasahara et al, 2004), and the sense annotated corpora with the help of Web users (Chklovski and Mihalcea, 2002).
Koehn and Knight (2002) combine a vector-space approach with other clues such as orthographic similarity and frequency. Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. The second method is to heuristically induce, where applicable, a seed lexicon using edit distance, as is done in Koehn and Knight (2002). In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002): we ran EDITDIST on EN-ES-D and took the top 100 most confident translation pairs. However, we attempted to run an experiment as similar as possible in setup to Koehn and Knight (2002), using English Gigaword and German Europarl. In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon.  Koehn and Knight (2002) derived such a seed lexicon from German-English cognates which were selected by using string similarity criteria. Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage.
Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons.
We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). (Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels. (Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks. We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. We split the Penn Treebank corpus (Marcus et al, 1994) into training, development and test sets as in (Collins, 2002).  A perceptron algorithm gives 97.11% (Collins, 2002).   Theparameters? of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). We trained this model using the averaged perceptron algorithm (Collins, 2002). We adopt the perceptron algorithm (Collins, 2002) to train the re-ranker.  Collins (2002)'s perceptron training algorithm were adopted again, to learn a discriminative classifier, mapping from inputs x X to outputs y Y. The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model. Collins (2002) reported and we confirmed that this averaging reduces over fitting considerably. Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al, 2000) with the same efficient feature encoding.
In this paper, we used the WSD program reported in (Lee and Ng, 2002). More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: n-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stop words) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech n-grams of nearby words; (4) Syntactic relations: syntactic information obtained from parser output. Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation.  For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002).  We adopt the same syntactic relations as (Lee and Ng, 2002). In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense).
For example, (Pang et al, 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g., number of stars) given by the reviewer. It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al, 2002). Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al, 2002), e.g., favorable or unfavorable, positive or negative. For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). Automatic identification of subjective content of ten relies on word indicators, such as unigrams (Pang et al, 2002) or predetermined sentiment lexica (Wilson et al, 2005).  For the purpose of this work, we follow the definition of Pang et al (2002) and Turney (2002) and consider a binary classification task for output labels as positive and negative. The low recall for adjective POS based synsets can be detrimental to classification since adjectives are known to express direct sentiment (Pang et al, 2002). We choose to use SVM since it performs the best for sentiment classification (Pang et al, 2002). Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al (2002), Turney (2002), Dave et al (2003), Pang and Lee (2004)). Each has proven to be effective in previous sentiment analysis studies (Pang et al, 2002), so as this experiment is rooted in sentiment classification, these methods were also assumed to perform well in this cross-discourse setting. These results support experiments carried out for topic based classification using Bayesianclassifiers by McCallum and Nigam (1998), but differs from sentiment classification results from Pang et al (2002) that suggest that term-based models perform better than the frequency-based alternative. Pang et al (2002) applied these classifiers to the movie review domain, which produced good results.  Previouswork has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Pang et al (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Although the discriminative model (e.g., SVM) is proven to be more effective on unigrams (Pang et al, 2002) for its ability of capturing the complexity of more relevant features, WR features are more inclined to work better in the generative model (e.g., NB) since the feature independence assumption holds well in this case. We do this because we already have enough training data, so there is no need to resort to cross-validation (Pang et al, 2002). Applications of text categorization, such as sentiment classification (Pang et al, 2002), are now required to run on multiple languages.
posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. In (Marcu and Wong, 2002), a joint probability phrase model is presented. The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler.  Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units. A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one. Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem: because the MT system learns directly from existing translations, it can be automatically customized to new domains and tasks. For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. Marcu and Wong (2002) propose a joint probability model. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist.  For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). Marcu and Wong (2002) describes an approximation to O. Other phrase-based models model the joint distribution P (e, f) (Marcu and Wong, 2002) or made P (e) and P (f| e) into features of a log-linear model (Och and Ney, 2002). The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT).
The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). The machine translation system is a graph based decoder (Ueffing et al, 2002). The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). The only publication, we are aware of, is (Ueffing et al, 2002).  The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). (Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea: use a modified A* algorithm with an optimal rest cost estimation. The algorithm in (Ueffing et al, 2002) has two disadvantages: it does not care about duplicates and the rest cost computation is suboptimal as the described algorithm has an exponential worst-case complexity. We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). For a description on how to generate lattices, see (Ueffing et al, 2002).
However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). (Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. We use the following algorithms as baseline: EM, co training, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously.
The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Melcuk, 1987) of the English sentence to restrict possible link combinations. Fox (2002) shows that translation between English and French satisfies cohesion in the majority cases. Similar alternations are rife in bilingual data, e.g., ne pas in French (Fox, 2002) and separable prefixes in German (Collins et al 2005). In addition to lexical translation, our system models structural changes and changes to feature values, for although dependency structures are fairly well preserved across languages (Fox, 2002), there are certainly many instances where the structure must be modified. For this, we need a formalism that is expressive enough to deal with cases of syntactic divergence between source and target languages (Fox, 2002): for any given (pi, f, a) triple, it is useful to produce a derivation that minimally explains the transformation be tween pi and f, while remaining consistent with a. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. Compare the systematic study for English-French alignments by (Fox, 2002), who compared (i) tree-bank parser style analyses, (ii) a variant with flattened VPs, and (iii) dependency structures. In real corpora, cases such as node N2 are frequent enough to be noticeable (see Fox (2002) or section 4.1 in this paper). Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones. It encodes semantic relations directly, and has the best inter-lingual phrasal cohesion properties (Fox,2002). One of assumptions of phrase-based SMT is that phrase cohere across two languages (Fox, 2002), which means phrases in one language tend to be moved together during translation. (Fox, 2002) is characterized by its simplicity, which has attracted researchers for years. Dependencies were found to be more consistent than constituent structure between French and English by Fox (2002), though this study used a tree representation on the English side only. This confirms the results of Fox (2002) and Galley et al (2004) that many translation operations must span more than one parse tree node. The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints (Fox, 2002), and (ii) we can model broad syntactic reordering phenomena, such as subject-verb-object constructions translating into subject-object-verb ones, as is generally the case for English and Japanese. In a very interesting study of syntax in statistical machine translation, Fox (2002) looks at how well proposed translation models fit actual translation data. Previous to Fox (2002), it had been observed that this model would prohibit certainre-orderings in certain language pairs (such as subject VP (verb-object) into verb-subject-object), but Fox carried out the first careful empirical study, showing that many other common translation patterns fall outside the scope of the child-reordering model. For this reason, we think it is important to learn from the model/data explain ability studies of Fox (2002) and to extend her results.
 By using a Japanese grammar (JACY: Siegel and Bender (2002)) based on a monostratal theory of grammar (HPSG: Pollard and Sag (1994)) we could simultaneously annotate syntactic and semantic structure without overburdening the annotator. JACY (Siegel and Bender, 2002) is a hand-crafted Japanese HPSG grammar that provides semantic information as well as linguistically motivated analysis of complex constructions. The grammars used for the experiments reported here are the LinGO English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002), precision grammars of English and Japanese, respectively. In this paper, we explore the utility of different evaluation metrics at predicting parse performance through a series of experiments over two broad coverage grammars: the English Resource Grammar (ERG; Flickinger (2000)) and JACY (Siegel and Bender, 2002). JACY (Siegel and Bender, 2002) is a broad coverage linguistically precise HPSG-based grammar of Japanese. Of these, the most thorough work on Japanese honorification is seen in JACY, a Japanese HPSG grammar (Siegel 2000, Siegel and Bender 2002).  The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations.  We take our examples from JACY (Siegel and Bender, 2002), a large grammar of Japanese built in the HPSG framework. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. The Grammar Matrix was developed initially on the basis of broad coverage grammars for English (Flickinger, 2000) and Japanese (Siegel and Bender, 2002), and has since been extended and refined as it has been used in the development of broad-coverage grammars for Norwegian (Hellan and Haugereid, 2003), ModernGreek (Kordoni and Neu, 2005), and Spanish (Marimon et al, 2007), as well as being applied to 42 other languages from a variety of language families in a classroom context (Bender, 2007). There are also several grammars: e.g. ERG; the English Resource Grammar (Flickinger, 2000), Jacy; a Japanese Grammar (Siegel and Bender, 2002), the Spanish grammar, and so forth. In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Fujita et al (2007) add sense information to improve parse ranking with JaCy (Siegel and Bender,2002), an HPSG-based grammar which uses similar machinery to the ERG. In order to examine the cross-lingual applicability of our methods, we also use Jacy, an HPSG-based grammar of Japanese (Siegel and Bender, 2002). Language specific analyses have been implemented in deep, broad-coverage grammars for languages such as Japanese (Masuichi et al (2003), Siegel and Bender (2002)) and Portuguese (Branco and Costa (2008)). In order to produce semantic representations we are using an open source HPSG grammar of Japanese: JACY (Siegel and Bender, 2002), which we have extended to cover the dictionary definition sentences (Bond et al, 2004). We used the Japanese grammar Jacy (Siegel and Bender, 2002), a deep parsing HPSG grammar that produces RMRSs for our primary input source.
 The SRG is implemented within the Linguistic Knowledge Building (LKB) system (Copestake, 2002), based on the basic components of the grammar Matrix, an open source starter-kit for the development of HPSG grammars developed as part of the LinGO consortium's multilingual grammar engineering (Bender et al, 2002). What I am reporting is thus a perspective on work done primarily by Flickinger within the English Resource Grammar (ERG: Flickinger (2000)) and by Bender in the context of the Grammar Matrix (Bender et al, 2002), though I've been involved in many of the discussions. In this paper I shall present a treatment of lexical and grammatical tone and vowel length in Hausa, as implemented in an emerging bidirectional HPSG of the language based on the Lingo Grammar Matrix (Bender et al, 2002). To better understand the needs of grammar developers we carefully explored two existing grammars: the LINGO grammar matrix (Bender et al,2002), which is a basis grammar for the rapid development of cross-linguistically consistent grammars; and a grammar of a fragment of Modern Hebrew, focusing on inverted constructions (Melnik, 2006). The LinGO Grammar Matrix project (Bender et al., 2002) is situated within the DELPH-IN2 collaboration and is both a repository of reusable linguistic knowledge and a method of delivering this knowledge to a user in the form of an extensible precision implemented grammar. We share grammar libraries with the Grammar Matrix in the grammar (Bender et al, 2002) as the foundation of KRG2.  To this end, the Grammar Matrix project (Bender et al, 2002) has been developed which, through a set of questionnaires, allows grammar engineers to quickly produce a core grammar for a language of their choice. In this research, we focus particularly on the Grammar Matrix-based DELPH-IN family of grammars (Bender et al, 2002), which includes grammars of English, Japanese, Norwegian, Modern Greek, Portuguese and Korean.  However, developing such grammars has been made much more efficient with the emergence of the Grammar Matrix (Bender et al,2002).
Over the past few years, there has been considerable progress in the ability of manually created large-scale grammars, such as the English Resource Grammar (ERG, Copestake and Flickinger (2000)) or the ParGram grammars (Butt et al, 2002), to parse wide-coverage text and assign it deep semantic representations. These tools have been developed to serve an Arabic Lexical Functional Grammar parser using XLE (Xerox Linguistics Environment) platform as part of the ParGram Project (Butt et al 2002). The experiments reported in this paper use the English LFG grammar constructed as part of the ParGram project (Butt et al, 2002). We parse a tree bank with the XLE platform (Crouch et al, 2008) and the English grammar developed within the ParGram project (Butt et al, 2002). Aspects of this research have often had their own separate fora, such as the ACL 2005 workshop on deep lexical acquisition (Baldwin et al, 2005), as well as the TAG+ (Kallmeyer and Becker, 2006), Alpino (van der Beek et al, 2005), ParGram (Butt et al, 2002) and DELPH-IN (Oepen et al, 2002) projects and meetings. A full system using the Lexical-Functional Grammar (LFG) parsing system XLE and the grammars from the Parallel Grammar development project (ParGram; (Butt et al., 2002)) has been implemented, and we present preliminary results on English-to-German translation with a tree-labeling system trained on a small subsection of the Europarl corpus. In the work described in this paper, we employ the XLE platform using the grammars available for English and German from the ParGram project (Butt et al, 2002).  The experiments reported in this paper use the German LFG grammar constructed as part of the ParGram project (Butt et al, 2002). This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al, 2002). We compared the output of the XLE system, a deep-grammar-based parsing system using the English Lexical-Functional Grammar previously constructed as part of the Pargram project (Butt et al, 2002), to the same gold standard. The grammar used for this experiment was developed in the ParGram project (Butt et al, 2002). Within the ParGram project (Butt et al, 2002), Kim et al (2003) were able to directly port the argument optionality related rules from a Japanese grammar to Korean. The only rule-based approach to German LFG-parsing we are aware of is the hand-crafted German grammar in the ParGram Project (Buttet al, 2002). In order for our approach to work, the coverage of the precision grammars must be broad enough to parse a large corpus of grammatical sentences, and for this reason, we choose the XLE (Maxwell and Kaplan, 1996), an efficient and robust parsing system for Lexical Functional Grammar (LFG) (Kaplanand Bresnan, 1982) and the ParGram English grammar (Butt et al, 2002) for our experiments. The ParGram English LFG is a hand-crafted broad-coverage grammar developed over several years with the XLE platform (Butt et al, 2002). The tree bank is based on the output of individual deep LFG (LexicalFunctional Grammar) grammars that were developed independently at different sites but within the overall framework of ParGram (the Parallel Grammar project) (Butt et al, 1999a; Butt et al, 2002).
The dependency parser we apply is an implementation of a shift-reduce dependency parser which uses a bunsetsu-chunk as a basic unit for parsing (Kudo and Matsumoto, 2002). We converted the POS system used in the Kyoto Text Corpus into ChaSen's POS system because we used ChaSen, a Japanese morphological analyzer, and CaboCha3 (Kudo and Matsumoto, 2002), a dependency analyzer incorporating SVMs, as a state-of the art corpus-based Japanese dependency structure analyzer that prefers ChaSen's POS system to that of JUMAN. Next, against the results of identifying compound functional expressions, we apply the method of dependency analysis based on the cascaded chunking model (Kudo and Matsumoto, 2002), which is simple and efficient because it parses a sentence deterministically only deciding whether the current bunsetsu segment modifies the one on its immediate right hand side. Next, we further show that the dependency analysis model of (Kudo and Matsumoto, 2002) applied to the results of identifying compound functional expressions significantly outperforms the one applied to the results without identifying compound functional expressions. Unlike probabilistic dependency analysis models of Japanese, the cascaded chunking model of Kudoand Matsumoto (2002) does not require the probabilities of dependencies and parses a sentence deterministically. As a Japanese dependency analyzer based on the cascaded chunking model, we use the publicly available version of CaboCha (Kudo and Matsumoto, 2002), which is trained with the manually parsed sentences of Kyoto text corpus (Kurohashi and Nagao, 1998), that are 38,400 sentences selected from the 1995 Mainichi newspaper text. Dynamic features include bunsetsu segments modifying the current candidate modifiee (see Kudo and Matsumoto (2002) for the details). In our experiments, we used the same settings as (Kudo and Matsumoto, 2002). In previous research, we presented a state-of-the-art SVMs-based Japanese dependency parser (Kudo and Matsumoto, 2002). (Since we used Isozaki's methods (Isozaki and Kazawa, 2002), the run-time complexity is not a problem.) Kudo and Matsumoto (2002) proposed an SVM based Dependency Analyzer for Japanese sentences. Japanese dependency parsers such as Cabocha (Kudo and Matsumoto, 2002) can extract Bps and their dependencies with about 90% accuracy. The recent availability of more corpora has enabled much information about dependency relations to be obtained by using a Japanese dependency analyzer such as KNP (Kurohashi and Nagao, 1994) or CaboCha (Kudo and Matsumoto,2002). The articles were morphologically analyzed by Mecab (Kudo et al, 2003) and syntactically parsed by Cabocha (Kudo and Matsumoto, 2002).  2.4.3 Medium Parser (CaboCha-RMRS) For Japanese, we produce RMRS from the dependency parser Cabocha (Kudo and Matsumoto, 2002). The sentences were dependency parsed with Cabocha (Kudo and Matsumoto, 2002), and co occurrence samples of event mentions were extracted. When parsing with Jacy failed, comparisons could still be made with RMRS produced from shallow tools such as ChaSen (Matsumo to et al, 2000), a morphological analyser or CaboCha (Kudo and Matsumoto, 2002), a Japanese dependency parser. One of the most related models is the cascaded chunking model by (Kudo and Matsumoto, 2002). Kudo and Matsumoto (2002) give more comprehensive comparison with the probabilistic models as used in (Uchimoto et al, 1999). Note that this use of local contexts is similar to the dynamic features in (Kudo and Matsumoto, 2002) 4.
Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002). We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights.  Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage.   In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). because this method seems substantially faster than comparable methods (Malouf, 2002). We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). For model estimation we use the TADM3 software (Malouf, 2002). There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). For parameter estimation, we use the open source TADM system (Malouf, 2002). Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM:2 Malouf, 2002) for training, using its limited-memory variable metric as the optimization method and determining best-performing convergence thresholds and prior sizes experimentally. To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed.
The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002). We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al, 2005). Empirical evidence for this argument can be seen from the result of the CoNLL shared tasks (Tjong Kim Sang, 2002) (Tjong Kim Sang and Meulder, 2003), where the ranking of the participating systems changes with the test corpora. Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002). In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002). Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost (Carreras et al., 2002), Hidden Markov Models (Bikel et al,), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used. This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. Similarly to classical NLP tasks, such as Base Phrase Chunking (Ramshaw and Marcus, 1999) (BPC) or NER (Tjong Kim Sang, 2002), we formulate the MD task as a sequence classification problem, i.e. the classifier assigns to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL 2002 shared task (Tjong Kim Sang, 2002). We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002). The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types.
This idea is similar to translation lexicon extraction via a bridge language (Schafer and Yarowsky, 2002). Nevertheless, Schafer and Yarowsky (2002) have shown that these two techniques can be combined efficiently. Schafer and Yarowsky, (2002) independently proposed using frequency, orthographic similarity and also showed improvements using temporal and wordburstiness similarity measures, in addition to context. Our setup we used cosine similarity, Rapp (1999) used l1-norm metric after normalizing the vectors to unit length, Koehn and Knight (2002) used Spearman rank order correlation, and Schafer and Yarowsky (2002) use cosine similarity. Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. Schafer and Yarowsky (2002) induced translation lexicons for languages without common parallel corpora using a bridge language that is related to the target languages. For example, researchers have automatically developed translation lexicons by seeing if words from each language have similar frequencies, contexts (Koehn and Knight, 2002), burstiness, inverse document frequencies, and date distributions (Schafer and Yarowsky, 2002). Lexicon induction methods for closely related languages using phonetic similarity have been proposed by Mann and Yarowsky (2001) and Schafer and Yarowsky (2002), and applied to Swiss German data by Scherrer (2007). For instance, Schafer and Yarowsky (2002) demonstrated that word translations tend to co occur in time across languages. Various other similarity scores can be computed depending on the available monolingual data and its associated meta data (see, e.g. Schafer and Yarowsky (2002)). This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. Schafer and Yarowsky (2002) created lexicons between English and a target local language (e.g. Gujarati) using a related language (e.g. Hindi) as pivot. Pivot-based methods have also been used in other related areas, such as translation lexicon induction (Schafer and Yarowsky, 2002), word alignment (Wang et al, 2006), and cross language information retrieval (Gollins and Sanderson, 2001).
 We use the data provided forthe French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). The evaluation is done using the performance measures described in (Mihalcea and Pedersen, 2003): precision, recall and F-score on the probable and sure alignments, as well as the Alignment Error Rate (AER), which in our case is a weighted average of the recall on the sure alignments and the precision on the probable. HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3 (Mihalcea and Pedersen, 2003). HLT-03 best is Ralign.EF.1 (Mihalcea and Pedersen, 2003). hlt-03 best is xrce.nolem (mihalcea and pedersen, 2003). We evaluated the alignment performance of the proposed models with two tasks: Japanese English word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al, 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). (Mihalcea and Pedersen, 2003). This is the same training data set as used in the 2003 word alignment evaluation (Mihalcea and Pedersen, 2003). The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the French English Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003). In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). We applied this matching algorithm to word level alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). We used the same training and test data as in our previous work, a subset of the Canadian Hansards bilingual corpus supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). The French/English data were those used by Mihalcea and Pedersen (2003). The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the word alignment task, and established standard performance measures and some benchmark tasks.
Others were derived from corpora, including subjective nouns learned from unannotated data using bootstrapping (Riloff et al, 2003). Subjectivity classification of small units of text, such as individual micro blog posts (Jiang et al, 2011) and sentences (Riloff et al, 2003), has been shown to benefit from additional context. Riloff et al (2003) learned lists of subjective nouns in English, seeding their method with 20 high-frequency, strongly subjective words. There are some Natural Language Processing (NLP) researches that demonstrate the benefit of hedge detection experimentally in several subjects, such as the ICD-9-CM coding of radiology reports and gene named Entity Extraction (Szarvas, 2008), question answering systems (Riloff et al, 2003), information extraction from biomedical texts (Medlock and Briscoe, 2007). However, as demonstrated by Pang et al (2002), Pang and Lee (2004), Hu and Liu (2004), and Riloff et al (2003), there are some nouns and verbs that are useful sentiment indicators as well. Riloff et al. proposed a bootstrapping approach for learning subjective nouns (Riloff et al, 2003). In a different work, Riloff et al (2003) use manually derived pattern templates to extract subjective nouns by bootstrapping. Riloff et al (2003) develop a method to determine whether a term has a Subjective or an Objective connotation, based on bootstrapping algorithms. To build our subjective spoken corpus (more than 2,000 texts), we used a parallel corpus of English Portuguese speeches and a tool to automatically classify sentences in English as objective or subjective (OpinionFinder (Riloff et al, 2003)). Extracting syntactic patterns contribute towards the affective orientation of a sentence (Riloff et al, 2003). Riloff et al, (2003) have conducted experiments that use Bag Of-Words (BoW) as features to generate a Naive Bayes subjectivity classifier for the MPQA corpus in English. Riloff et al (2003) extracted nouns and Riloff and Wiebe (2003) extracted patterns for subjective expressions using a bootstrapping process. More details are provided in (Riloff et al, 2003). Basilisk was originally designed for semantic class induction using lexico-syntactic patterns, but has also been used to learn subjective and objective nouns (Riloff et al, 2003). Riloff et al (2003) explore bootstrapping techniques to identify subjective nouns and subsequently classify subjective vs. objective sentences in newswire text. Riloff et al (2003) mined subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds. Riloff et al (Riloff et al, 2003) applied bootstrapping to recognise subjective noun keywords and classify sentences as subjective or objective in newswire texts. Riloff et al (2003) focused on the collection of subjective nouns. Most previous works used seeds selected based on a user's or domain expert's intuition (Curran et al, 2007), which may then have to meet a frequency criterion (Riloff et al, 2003). Patterns are extracted using AutoSlog (Riloff et al, 2003).
This approach is practical for geographic names, for which broad-coverage gazetteers exist, though less so for personal names (Mann and Yarowsky, 2003). Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). The training set contains the top 100 web search results of 49 names from the Web03 corpus (Mann and Yarowsky, 2003), Wikipedia and European Conference on Digital Library (ECDL) participants; the test data are comprised of the top 100 documents of 30 names from Wikipedia, US Census and ACL participants. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for co reference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. (Mann and Yarowsky, 2003) first extract biographical information, such as birthdates, birthplaces ,occupations, and so on. (Wan et al, 2005) employ an approach similar to that of (Mann and Yarowsky,2003), and have developed a system called Web Hawk. Clustering approaches (e.g. hierarchical agglomerative clustering (Mann and Yarowsky, 2003)) have been commonly used for CDC due to the variety of data distributions of different names. Our work goes beyond the simple co-occurrence features (Bagga and Baldwin, 1998) and the limited extracted information (e.g. biographical information in (Mann and Yarowsky, 2003) that is relatively scarce in web data) using the broad range of relational information with the support of information extraction tools. This final similarity function will then be embedded into a normal HAC algorithm to group the web pages into different namesakes where we compute the centroid-based distance between clusters (Mann and Yarowsky, 2003). This is in line with Mann and Yarowsky (2003)'s modification, consisting in replacing all numbers in the patterns with the symbol ####. Bagga and Baldwin (1998) used a Bag of Words (BOW) model to resolve ambiguities among people. Mann and Yarowsky (2003) improved the performance of personal names disambiguation by adding biographic features. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. Mann and Yarowsky (2003) and Niu et al (2004) extended the vector representation with extracted biographic facts. Similarly, approaches in unstructured data (e.g., text) have involved using clustering techniques over biographical facts (Mann and Yarowsky, 2003), within-document resolution (Blume, 2005), and discriminative unsupervised generative models (Lietal., 2005). 277 Motivated by ambiguity in personal name search, Mann and Yarowsky (2003) disambiguate person names using biographic facts, like birth year, occupation and affiliation. An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information. To model these characteristics, Bunescu and Pasca (2006) and Cucerzan (2007) incorporate information from Wikipedia articles, Artiles et al (2007) use Webpage content, Mann and Yarowsky (2003) extract biographic facts. Mann and Yarowsky (2003) extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people. In the future, we would like to model the biographical fact extraction approach of (Mann and Yarowsky, 2003) in our LDA model. On the other hand, Mann and Yarowsky (2003) proposes a richer document representation involving automatically extracted features.
We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. (Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner.
For comparison, we also computed two baselines: one in which each character is labeled with its most frequent label (Baseline1 in Table 2), and one in which each entity that was seen in training data is labeled with its most frequent classification (Baseline2 in Table 2 this baseline is computed using the software provided with the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)).  In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). The annotation is distributed in the standard column based BIO format applied for e.g. CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004) data, among other established datasets. The IOB2 strategy, which is very popular, having been used in public challenges such as those of CoNLL (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004), has been found to be indeed the best of all established tagging strategies. Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. We benchmark the performance of our baseline MaxEnt classifier using the feature set from Section 5.1 (MaxEnt-A henceforth) on the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), the de-facto standard for evaluating coarse-grained NERC systems. This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003). We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Finaly, combining models has been a successful way of achieving good results, such as those of Florian et al (2003) who had the top performance in the named entity recognition shared task of CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003). language newspaper domain (English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)).  The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types.
We use the C&C tools (Curran and Clark, 2003) for POS and NE tagging and the and the Berkeley Parser (Petrov and Klein, 2007), trained with default parameters. Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). As the vanilla C&C tagger (Curran and Clark, 2003) is optimised for performance on newswire text, various modifications were applied to improve its performance for biomedical NER. The named entity recognizer of Curran and Clark (2003) is also used to recognize the standard set of muc entities, including person, location and organisation. These are based on those found in (Curran and Clark, 2003). The part-of-speech tagging uses the Curran and Clark POS tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al, 2004), whilst the other preprocessing stages are all rule based. The NER module uses the Curran and Clark NER tagger (Curran and Clark, 2003), augmented with extra features tailored to the biomedical domain. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document. A number of NER systems have made effective use of how the same token was tagged in different parts of the same document (see (Curran and Clark, 2003) and (Mikheev et al, 1999)).  Further linguistic markup is added using the morpha lemmatiser (Minnen et al, 2000) and the C&C named entity tagger (Curran and Clark, 2003) trained on the data from MUC-7. Part-of-speech (POS) tagging is done using the C&C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C&C maximum entropy NER tagger (Curran and Clark, 2003b). We use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted LT TTT rules, while for the non domain-specific ones we use the C&C named entity tagger (Curran and Clark, 2003) trained on the MUC-7 data set. The part-of-speech tagging uses the Curran&Clark maximum entropy Markov model tagger (Curran and Clark, 2003) trained on MedPost data (Smith et al., 2004), whilst the other preprocessing stages are all rule-based. We use different strategies for the identification of the two classes of entities: for the domain-specific ones we use hand-crafted LT TTT rules, while for the non-domain-specific ones we use the C& amp; C named entity tagger (Curran and Clark, 2003) trained on the MUC7 data set. For this we have used the C&C named entity recogniser (Curran and Clark, 2003), which is run on pos-tagged and chunked documents in the corpus to identify and extract named entities as potential topics. Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous in stance of that same token in a previous sentence of the same document. By training the C&C tagger (Curran and Clark, 2003) on the gold-standard corpora an dour new Wikipedia-derived training data, we evaluate the usefulness of the latter and explore the nature of the training corpus as a variable in NER. We trained the C&C NER tagger (Curran and Clark,2003) to build separate models for each gold standard corpus.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b).    Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc.  Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003).  Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003). Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). Florian et al (2003) employed the same technique in a combination of learners. Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. Florian et al (2003) have also obtained the highest F1 rate for the German data.
Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers. Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models. Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003). The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.   Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information. We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005). Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003). Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003). Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information. Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003). In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al 2003). The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%.
Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. CRFs have been used successfully for Named Entity recognition (e.g., McCallum and Li (2003), Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in several domains (Riloff, 1996a). NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al, 2005) and semi-supervised (Pasca et al, 2006b) learning methods. CRFs have been shown to perform well in a number of natural language processing applications, such as POS tagging (Lafferty et al, 2001), shallow parsing or NP chunking (Sha and Pereira, 2003), and named entity recognition (McCallum and Li, 2003).  In recent years, conditional random fields (CRFs) (Lafferty et al, 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). Standard statistical techniques for named entity recognition (NER) can be used for Step (2) (McCallum and Li, 2003). A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. We experimented with popular feature sets previously used for named entity (McCallum and Li, 2003) and gene (McDonald and Pereira, 2005) recognition including orthographic, part-of-speech (POS), shallow parsing and gazetteers. Part of the features we used for our CRF classifier are common features that are widely used in NER (McCallum and Li, 2003), as shown below. Conditional random fields (Lafferty et al, 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and named entity extraction (McCallum and Li, 2003). In recent years discriminative probabilistic model shave been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004). CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al, 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al, 2003), among other tasks. Named entities are identified by a CRF-based NER system, similar to that described in (McCallum and Li, 2003). It works quite well on NE tagging tasks (McCallum and Li, 2003). The modeling power of CRFs has been of great benefit in several applications, such as shallow parsing (Sha and Pereira, 2003) and information extraction (McCallum and Li, 2003). CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003), table extraction (Pinto et al, 2003) and shallow parsing (Sha and Pereira, 2003). CRFs have been shown to perform well on a number of NLP problems such as shallow parsing (Sha and Pereira, 2003), table extraction (Pinto et al, 2003), and named entity recognition (McCallum and Li, 2003).
A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al, 2003) and annotated with additional information (Zajic et al, 2004). HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al, 2003), and Topiary is our implementation of the Topiary system (Zajicet al, 2004). For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). Hedge Trimmer (Dorr et al, 2003) is a system that creates a headline for an English newspaper story using linguistically-motivated heuristics to choose a potential headline. The algorithm used by Dorr et al (2003) removes subordinate clauses, to name one example.  
Similarly, Chen and Rambow (2003) argue that the kind of deep linguistic features we harvest from FrameNet is beneficial for the successful assignment of PropBank roles to constituents, in this case using TAGs generated from PropBank to generate the relevant features. However, in most cases they can only provide a local dependency between predicate and argument for 87% of the argument constituents (Chen and Rambow, 2003), which is too low to provide high SRL accuracy. (Chenand Rambow, 2003) use LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) for SRL. Path feature from the derived tree, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. Prop was extracted using the PropBank annotations for ar gument/modifier distinction by a method similar to Chen and Rambow (2003). Although the results can not directly be compared with another work using LTAG (Chen and Rambow, 2003) because their target annotations were limited to those localized in an elementary tree, considering that their target annotations were 87% of core arguments, our results are competitive with their results (82.57/71.41). Another possibility is to directly extract PropBank-style semantic representations by reforming the grammar extraction algorithm (Chen and Rambow, 2003), and to estimate a disambiguation model using the PropBank. As a result some of the features that undo long distance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. (Chen and Rambow, 2003) discuss amodel for SRL that uses LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing). Instead of using the typical parse tree features used in typical SRL models, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. As a result, if we do not compare the machine learning methods involved in the two approaches, but rather the features used in learning, our features are a natural generalization of (Chen and Rambow, 2003). The LTAG-spinal Treebank can be used to overcome some of the limitations of the previous work on SRL using LTAG: (Liu and Sarkar, 2007) uses LTAG-based features extracted from phrase-structure trees as an additional source of features and combined them with features from a phrase-structure based SRL framework; (Chenand Rambow, 2003) only considers those complement/adjunct semantic roles that can be localized in LTAG elementary trees, which leads to a loss of over 17% instances of semantic roles even from gold-standard trees.
The Gildea and Hockenmaier (2003) system uses features extracted from Combinatory Categorial Grammar (CCG) corresponding to the features that were used by G&J and G&P systems. This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003).  Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank. In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al, 2005b). For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. As with a previous approach in CCG semantic role labeling (Gildea and Hockenmaier, 2003), this feature shows the exact nature of the syntactic dependency between the predicate and the word we are considering, if any such dependency exists. We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003) , which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Tree bank. As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier, 2003) oracle results for the numbered arguments only (A0,..., A5). In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). This feature has been shown (Gildea and Hockenmaier, 2003) to be an effective substitute for tree path-based features.  We follow a previous CCG based approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists. Using a version of Brutus incorporating only the CCG-based features described above, we achieve better results than a previous CCG based system (Gildea and Hockenmaier, 2003, henceforth G&H).
 (Weeds and Weir, 2003)) measure of Lin (1998) as a representative case, and utilized it for our analysis and as a starting point for improvement.  However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003).  Weeds and Weir (2003) proposed a general framework for distributional similarity that mainly consists of the notions of what they call Precision and Recall. title=Textual_Entailment_Resource_Pool 69 To date, most distributional similarity research concentrated on symmetric measures, such as the widely cited and competitive (as shown in (Weeds and Weir, 2003)) LIN measure (Lin, 1998): LIN (u, v)=? f? FV u? FV v [w u (f)+ w v (f)]? f? FV u w u (f)+? f? FV v w v (f) where FV x is the feature vector of a word x and w x (f) is the weight of the feature f in that word? s vector, set to their point wise mutual information.  For this reason, a new approach could be envisaged for this task, in the direction of the work by (Weeds and Weir, 2003), by building rankings of similarity for each verb. As a case study, we used our evaluation methodology to compare four methods for learning entailment rules between predicates: DIRT (Lin and Pantel,2001), Cover (Weeds and Weir, 2003), BInc (Szpek tor and Dagan, 2008) and Berant et al (2010).
 Riloff and wiebe (2003) constructed a high precision classifier for contiguous sentences using the number of strong and weak subjective words in current and nearby sentences.  Although this work aims at learning only nouns, in the subsequent work, they also proposed a bootstrapping method that can deal with phrases (Riloff and Wiebe, 2003). Subjective sentences are sentences used to express opinions, evaluations, and speculations (Riloff and Wiebe, 2003).  Starting with the Romanian lexicon, we developed a lexical classifier similar to the one introduced by (Riloff and Wiebe, 2003). The major source of this list is from (Riloff and Wiebe, 2003) with additional words from other sources. Most of the previous work on sentiment lexicon construction relies on existing natural language processing tools, e.g., syntactic parsers (Wiebe, 2000), information extraction (IE) tools (Riloff and Wiebe, 2003) or rich lexical resources such as WordNet (Esuli and Sebastiani, 2006). These terms have been previously shown to be high precision for recognizing subjective sentences (Riloff and Wiebe, 2003).  We decided to expand our subjective term lists by using automatic term extraction, inspired by (Riloff and Wiebe, 2003). All trigrams are then scored according to their prevalence in relevant versus irrelevant documents (e.g. subjective vs. non-subjective sentences), following the scoring methodology of Riloff and Wiebe (2003).  Believing that the current approach may offer benefits over state-of-the-art pattern-based subjectivity detection, we also implement the AutoSlogTS method of Riloff and Wiebe (2003) for extracting subjective extraction patterns. Riloff and Wiebe (2003) performed pattern learning through bootstrapping while extracting subjective expressions. For example, Riloff and Wiebe (2003) proposed a bootstrapping process to automatically identify subjective patterns. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Riloff and Wiebe (2003) describe a bootstrapping method to learn subjective extraction patterns that match specific syntactic templates, using a high-precision sentence-level subjectivity classifier and a large unannotated corpus. As shown in previous work, a high-precision classifier can be used to automatically generate subjectivity annotated data (Riloff and Wiebe, 2003).
 (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details).  Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences.    The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). (Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). More linguistic knowledge has been explored by Hulth (2003). Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. As shown in (Hulth, 2003), most key phrases are noun phrases. Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall.
Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese character. Virga and Khudanpur (2003) and Kuo et al (2005) adopted the noisy channel modeling framework. Technologies developed for SMTare borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). The proposed transliteration framework obtained significant improvements over a strong baseline transliteration approach similar to AbdulJaleel and Larkey (2003) and Virga and Khudanpur (2003). This result was comparable to other state-of-the-art statistical name transliteration systems (Virga and Khudanpur, 2003).  Virga and Khudanpur (2003) model this scoring function using a separate translation and language model, that is, s (e, f)= Pr (f |e) Pr (e). Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003). Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese characters, due to homophone confusion. In CLIR or multilingual corpus alignment (Virga and Khudanpur, 2003), N-best results will be very helpful to increase chances of correct hits. The reference data are extracted from Table 1 and 3 of (Virga and Khudanpur 2003).
For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation. In the last SIGHAN bakeoff, there is no single system consistently outperforms the others on different test standards of Chinese WS and NER standards (Sproat and Emerson, 2003). The official scorer program is publicly available and described in (Sproat and Emerson, 2003). Measuring homogeneity by counting word/ lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the Chinese (Sproat and Emerson 2003) or Japanese language (Matsumoto et al, 2002), for instance, where word segmentation is not trivial. The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier.  This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). No other material was allowed (Sproat and Emerson, 2003). In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). For instance, 'vice president' is considered to be one word in the Penn Chinese Treebank (Xue et al, 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). 
The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003). The default feature, boundary tag feature of the previous character, and boundary tag feature of the character two before the current character used in (Xue and Shen, 2003) were dropped from our word segmenter, as they did not improve word segmentation accuracy in our experiments. We observed that character features were successfully used to build our word segmenter and that of (Xue and Shen, 2003). Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the post processing step gave improved word segmentation accuracy. Word segmentation can be formalized as a character classification problem (Xue and Shen, 2003), where each character in the sentence is given a boundary tag representing its position in a word. This kind of strategy has been widely used in the applications of machine learning to named entity recognition and has also been used in Chinese word segmentation (Xue and Shen, 2003). Table 3 compares the three types of kernel for Perceptron, where for the semi quadratic kernel we used the co-occurrences of characters in context window as those used in (Xue and Shen, 2003), namely{ c? 2c? 1, c? 1c0 ,c0c1 ,c1c2, c? 1c1}. Character n-gram features have proven their effectiveness in ML-based CWS (Xue and Shen,2003). It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al, 2004). The conditional maximum entropy model in our implementation is based on the one described in Section 2.5 in (Ratnaparkhi, 1998), and features are the same as those described in (Xue and Shen, 2003). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ngand Low, 2004). By casting the problem as a character labeling task, sequence labeling models such as Conditional Random Fields can be applied on the problem (Xue and Shen, 2003). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (Max Ent) (Xue and Shen, 2003), support vector machine Now the second author is affiliated with NTT. In the third step, we used the maximum entropy (MaxEnt) approach (the results of CRF are given in Section 3.4) to train the IOB tagger (Xue and Shen, 2003). It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Xue and Shen (2003) describe for the first time the character classification approach for Chinese word segmentation, where each character is given a boundary tag denoting its relative position in a word. In this paper, we propose a statistical approach based on the works of (Xue and Shen, 2003), in which the Chinese word segmentation problem is first transformed into a tagging problem, then the Maximum Entropy classifier is applied to solve the problem. we briefly discuss the scheme proposed by (Xue and Shen, 2003), followed by our additional works to improve the performance. One of the difficulties in Chinese word segmentation is that, Chinese characters can appear in different positions within a word (Xue and Shen, 2003), and LMR Tagging was proposed to solve the problem.
(Note that the top participant of CTBc (Zhang et al, 2003) used additional named entity knowledge/data in their word segmenter). ICTCLAS Segmenter: this model, trained by Zhang et al (2003), is a hierarchicalHMM segmenter that incorporates parts-of speech (POS) information into the probability models and generates multiple HMM mod els for solving segmentation ambiguities. ICTCLAS (Zhang et al., 2003), a tool developed by the Institute of Computing Technology of Chinese Academy of Sciences (ICT), is used for word segmentation and part-of-speech tagging. Then we apply a hierarchical Hidden Markov Model (HMM) based Chinese lexical analyzer ICTCLAS (Zhang et al, 2003) to extract named entities, noun phrases and events. HMMsegmenter (Zhang et al, 2003) that uses the specifications of PKU. Most word-based segmenters in Chinese IR are either rule-based models, which rely on a lexicon, or statistical-based models, which are trained on manually segmented corpora (Zhang et al,2003). Its segmentation model is a 3The query set and relevance judgements are available at http: //www.cs.ualberta.ca/ ?yx2/research.html 59 class-based hidden Markov model (HMM) model (Zhang et al, 2003). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (Max Ent) (Xue and Shen, 2003), support vector machine. Decrease in H (XjX n) for Chinese characters when n is increased software such as (Zhang et al, 2003) whose performance is also high. Hence the need for automatic word segmentation systems (Zhang et al, 2003). Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al, 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al, 2004). Then for every path of the N+1paths4 (N best paths and the atom path), we perform a process of Roles Tagging with HMM model (Zhang et al 2003). The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al, 2003). Both ICTCLAS and Stanford segmenters utilise machine learning techniques, with Hidden Markov Models for ICT (Zhang et al, 2003) and conditional random fields for the Stanford segmenter (Tseng et al, 2005). In this work, we resort to ICTCLAS (Zhang et al, 2003), a widely used tool in the literature. The posts were then part-of speech tagged using a Chinese word segmentation tool named ICTCLAS (Zhang et al, 2003). Their system only does IWR, using the CWS and POS tagging output of the ICTCLAS segmenter (Zhang et al, 2003) as in put.  ICTCLAS is developed by Chinese Academy of Science, the precision of which is 97.58% on tagging general words (Huaping Zhang et al, 2003). The Chinese word segmentation tool is ICTCLAS (Zhang et al 2003) and Google Translator is the MT for the source language.
Bannard et al (2003) extended this research in looking explicitly at the task of classifying verb-particles as being compositional or not. Semantically, the compositionality of MWEs is gradual, ranging from fully compositional to idiomatic (Bannard et al, 2003). They are also extremely diverse: for example ,onthe semantic dimension alone, MWEs cover an en tire spectrum, ranging from frozen, fixed idioms to free combinations of words (Bannard et al, 2003). Note that although Lin characterizes his work as detecting non-compositionality, we agree with Bannard et al (2003) that it is better thought of as tapping into productivity.plore whether the light verbs themselves show different patterns in terms of how they are used semi productively in these constructions. Bannard (2005), extending work by Bannard et al (2003), instead considers the extent to which the verb and particle each contribute semantically to the VPC. Semantically, the compositionality of MWEs is gradual, ranging from fully com positional to idiomatic (Bannard et al, 2003). Bannard et al (2003), on the other hand, look at the separate contribution of the verb and particle, but assume that a binary decision on the compositionality of each is sufficient.  They do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (Bannard et al, 2003). Possible alternatives for dealing with this issue are discussed by both Bannard et al (2003) and McCarthy et al (2003). Bannard et al (2003) tested techniques using statistical models to infer the meaning of verb-particle constructions (VPCs), focus 2 In this lexicon, many MWEs are encoded as templates,. 
McCarthy et al (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verb particle and head verb ,e.g., to determine compositionality. Building on Lin (1998), McCarthy et al (2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb). Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. A similar comparison between the ranks according to Latent-SemanticAnalysis (LSA) based features and the ranks of human judges has been made by McCarthy, KellerandCaroll (McCarthy et al, 2003) for verb-particle con st ructions. McCarthy, Keller and Caroll (McCarthy et al,2003) judge compositionality according to the degree of overlap in the set of most similar words to the verb-particle and head verb. (1993) and Rooth et al (1999) referring to a direct object noun for describing verbs), or used any syn tactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003)).  Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate several tests for compositionality including one (simplex score) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. Table 5 shows the results of using different similarity measures with the simplex score test and data of McCarthy et al (2003). McCarthy et al (2003) determine a continuum of compositionality of VPCs, but do not distinguish the contribution of the individual components. McCarthy et al (2003) evaluate the precision of Rasp at identifying VPCs to be 87.6% and the recall to be 49.4%. In particular, other than the re ported results of McCarthy et al (2003) targeting VPCs vs. all other analyses, we had no a priori sense of RASP? s ability to distinguish VPCs and verb-PPs. Also, we ignore the ambiguity between particles and adverbs, which is the principal reason for our evaluation being much higher than that reported by McCarthy et al (2003). In order to get a clearer sense of the impact of selectional preferences on the results, we investigated the relative performance over VPCs of varying semantic compositionality, based on 117 VPCs (f? 1) attested in the data set of McCarthy et al (2003). McCarthy et al (2003) provides compositionality judgements from three human judges, which we take the average of and bin into 11 categories (with 0= non-compositional and 10= fully compositional). The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et als (2003) manually ranked phrasal verbs. McCarthy et al (2003) suggested that compositional phrasal verbs should have similar neighbours as for their simplex verbs. In order to evaluate our algorithm in comparison with previous work, we also tested it on the manual ranking list created by McCarthy et al (2003) . This result is comparable with or better than most measures reported by McCarthy et al (2003). 
Baldwin et al (2003) use LSA as a technique foranalysing the compositionality (or decomposability) of a given MWE. Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003). Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). For example, Baldwin et al (2003) studied vector extraction for phrases because they were interested in the decomposability of multi word expressions. According to Baldwin et al (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSAto distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows (Baldwin et al, 2003). They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Word net based decomposability theory (Baldwin et al, 2003). The LSA model we built is similar to that described in (Schutze, 1998) and (Baldwin et al, 2003).  Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. (Baldwin et al, 2003) use WordNet: :Similarity to provide an evaluation tool for multi word expressions that are identified via Latent Semantic Analysis. Baldwin et al (2003) proposed a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability. Baldwin et al (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al (2003). Baldwin et al, (2003) focus more narrowly on distinguishing English noun-noun compound sand verb-particle constructions which are compositional from those which are not compositional. To compare our method with that proposed by Baldwin et al (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to theMWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE. There is some evidence (Baldwin et al, 2003) that part of speech tagging might improve results in this kind of task. Other approaches use Latent Semantic Analysis (LSA) to determine the similarity between a potential idiom and its components (Baldwin et al, 2003).
Our bottom-up deterministic parser adopts Nivre's algorithm (Nivre, 2004) with a preprocessor. The parser is a bottom-up deterministic dependency parser based on the algorithm proposed by (Nivre, 2004). The main part of our dependency parser is based on Nivre' s algorithm (Nivre, 2004), in which the dependency relations are constructed by a bottom up deterministic schema. If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). Nivre (2004) investigated the issue of (strict ) incrementality for this type of parsers ;i.e., if at any point of the analysis the processed input forms one connected structure. Incrementality is not strict here in the sense of (Nivre, 2004), because sometimes more than one word is needed before parts of the frame are constructed and out put: into the right, for instance, needs to wait for a word like leg that completes the chunk. The semantics of this transition system is described in (Nivre,2004). When restricted to these three transitions, the system is equivalent to the so-called stack-based arc-standard model of Nivre (2004). This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the arc-standard version of Nivre (2004). The MaltParser is a dependency parser generator, with three parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Covington's (Covington, 2001). The parsing algorithm is the arc-standard method (Nivre, 2004), which is briefly described in Section 2. These features are found to have high overall accuracy in the Nivre parser (Nivre, 2004) and in human sentence processing modeling (Boston et al, 2008) . In this paper, we propose a model based on Arc-Standard Transition System of Nivre (2004), which is known as an incremental greedy projective parsing model that parses sentences in linear time. Actions in Arc-Standard Transition System (Nivre, 2004) clues to unsupervised parsing. We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). In this respect such a model is very restrictive and suffers from the pitfalls of the incremental processing (Nivre, 2004). This is in line with the Arc-Standard parsing strategy of shift-reduce dependency parsers (Nivre, 2004). There are other transition-based dependency parsing algorithms that take a similar approach; Nivre (2009) integrated a SWAP transition into Nivre's arc-standard algorithm (Nivre, 2004) and Fernandez Gonzalez and Gomez-Rodriguez (2012) integrated a buffer transition into Nivre's arc-eager algorithm to handle non-projectivity.
To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Marquez, 2004) and SENSEVAL-3 (Litkowski, 2004). Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see Litkowski, 2004 for more details). Although the strict measures are more interesting, we include these figures for comparison with the systems participating in the Senseval-3 Restricted task (Litkowski, 2004). This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004). Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). The same problem was again highlighted by the results obtained with and without the frame information in the Senseval-3 competition (Litkowski,2004) of FrameNet (Johnson et al, 2003) role labeling task. Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see (Litkowski, 2004) for more details). This task has been the subject of a previous Senseval task (Automatic Semantic Role Labeling, Litkowski (2004)) and two shared tasks on semantic role labeling in the Conference on Natural Language Learning (Carreras Marquez (2004) and Carreras Marquez (2005)).  In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Mrquez, 2005).
This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)).  S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007). In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004).
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. Senseval 3 shared task data (Snyder and Palmer, 2004). We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall).
ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). We used ROUGE (Lin, 2004) as an evaluation criterion. Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004). with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries;. Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries. We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004).   They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). However, original measures based on lexical matching, such as BLEU (Papineni et al, 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively. In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004). We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures. We used ROUGE (Lin, 2004) for evaluating the content of summaries.
Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al, 2004). However, one participant (Settles, 2004) reported that their at tempt to utilize gazetteers (together with other resources) had failed in gaining better overall performance. Settles (2004)'s CRF system deserves special note in the sense that it achieved comparable performance to top ranked systems with a rather simple feature set. While most of earlier approaches rely on handcrafted rules or dictionaries, many recent works adopt machine learning approaches ,e.g, SVM (Lee, 2003), HMM (Zhou, 2004), Maximum Entropy (Lin, 2004) and CRF (Settles,2004), especially with the availability of annotated corpora such as GENIA, achieving state-of-the-art performance. Genes/Proteins are not the Same Many of the existing BNER systems, which are mainly tuned for gene/protein identification, use features such as token shape (also known as word class and brief word class (Settles, 2004)), Greek alphabet matching, Roman number matching and so forth. (Settles, 2004) reported that a system using a subset of features out performed one using a full set of features.  The features used in our experiments mainly follow the work of (Settles, 2004) and (Collins, 2001). Our performance of the single-phase CRF with maximum likelihood training is 69.44%, which agrees with (Settles, 2004) who also uses similar settings.   The named entity tagger used throughout in this section is based on Conditional Random Fields and similar to the one presented by (Settles, 2004). Hence, the use of a named entity tagger supports the evaluation results when comparing the various biomedical entity recognition (Settles, 2004).
Following the work of Shriberg et al (2004), we use the 5 general tags in our experiments: Disruption indicates the current Dialogue Act is interrupted. Like the Switchboard corpus, the ICSI Meeting Room DA (MRDA) corpus (Shriberg et al, 2004) was annotated using a variant of the DAMSL tag set, similar but not identical to the Switchboard DAMSL annotation. The differences (and a translation between the two sets) can be seen in Shriberg et al (2004). To facilitate cross-corpus classification, we will cluster these labels as described in Shriberg et al (2004). We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). The following experiments used manual meeting transcripts and relied on manual dialogue act segmentation (Shriberg et al, 2004). All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al, 2005). While there are related tags for dialogue act tagging schema? like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al, 2004) which includes a committag these classes are too general to allow identification of action items specifically. All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). First, it is rare to have sub DAs labeled in training data, and indeed this is true of the corpus (Shriberg et al, 2004) that we use. We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al, 2004) corpus, and find that our novel hidden back off model can significantly improve dialog tagging accuracy. In all our models, to simplify we assume that the sentence change information is known (as is common with this corpus (Shriberg et al, 2004)). We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRDA) corpus (Shriberg et al, 2004). All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). In our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the ICSI corpus (Shriberg et al, 2004). This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding (Shriberg et al, 2004). We use only the forced alignments of these meetings, available in the accompanying MRDA Corpus (Shriberg et al 2004). Experiments are performed using all train/test pairs among three conversational speech corpora: the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), Switch board DAMSL (Swbd) (Jurafsky et al, 1997), and the Spanish Callhome dialog act corpus (SpCH) (Levin et al, 1998). Each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information (Shriberg et al, 2004).
A similar method was used for entity/relation recognition (Roth and Yih, 2004). Supervised methods such as (Culotta and Sorensen, 2004) and (Roth and Yih, 2004) provide only a partial solution, as there are many possible relations and entities of interest for a given domain, and such approaches require new annotated data each time a new relation or entity type is needed. Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks inNLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). Roth and Yih (2004) use log probabilities as weights. Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006). Also, we minimize rather than maximize due to the fact we transform the model probabilities with? log (like Roth and Yih (2004)). Roth and Yih (2004) use ILP to deal with the joint inference problem of named entity and relation identification. We phrase the inference task as an integer linear program (ILP) following the approach developed in Roth and Yih (2004). The approach we develop in this paper follows the one proposed by Roth and Yih (2004) of training individual models and combining them at inference time. Roth and Yih (2004) formulated the problem of extracting entities and relations as an integer linear program, allowing them to use global structural constraints at inference time even though the component classifiers were trained independently. Roth and Yih (2004) combined information from named entities and semantic relation tagging, adopting a similar overall goal but using a quite different approach based on linear programming. 804 task, for which Integer Linear Programming (ILP) introduced to NLP by Roth and Yih (2004) and successfully applied by Denis and Baldridge (2007 ) to the task of jointly inferring anaphoricity and determining the antecedent would be appropriate. Roth and Yih (2004) advocated ILP as a general solution for a number of NLP tasks that re quire combining multiple classifiers and which the traditional pipeline architecture is not appropriate, such as entity disambiguation and relation extraction. Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). We borrow the data and the setting from (Roth and Yih, 2004). Refer to (Roth and Yih, 2004) for more statistics on this data and a list of all the type constraints used. Integer linear programs have already been successfully used in related fields including semantic role labelling (Punyakanok et al, 2004), relation and entity classification (Roth and Yih, 2004), sentence compression (Clarke and Lapata, 2008) and dependency parsing (Martins et al, 2009). Recently, [Roth and Yih, 2004] applied an ILP model to the task of the simultaneous assignment of semantic roles to the entities mentioned in a sentence and recognition of the relations holding between them. Roth and Yih (2004) also described a classification-based framework in which they jointly learn to identify named entities and relations. (Roth and Yih, 2004) suggests a model in which global constraints are taken into account in a later stage to fix mistakes due to the pipeline.
 The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). (Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. A detailed description can be found in Purandare and Pedersen (2004). The method is described in Purandare and Pedersen (2004). Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts).  Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. (Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007).
Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). These settings are the result of extensive experiments partially reported in Nivre et al (2004). The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004). It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English).  Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004).  To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004).
The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. The list of 35 semantic relations was presented in (Moldovan et al 2004). We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). Moldovan et al (2004) also use WordNet. The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). Moldovan et al (2004) proposed a different scheme with 35 classes. Moldovan et al (2004) also use WordNet. Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004).
The Chinese Nombank extends the general annotation framework of the English Proposition Bank (Palmer et al, 2005) and the English Nombank (Meyers et al, 2004) to the annotation of nominalized predicates in Chinese. NomBank (Meyers et al, 2004) is a similar resource for nominal predicates, but we do not consider it in our experiments. We then describe a novel CCG analysis of NP predicate argument structure, which we implement usingNomBank (Meyers et al, 2004). We currently do not have an analysis that allows support verbs to supply noun arguments, so we do not recover any of the long-range dependency structures described by Meyers et al (2004). Our analysis requires semantic role labels for each argument of the nominal predicates in the Penn Treebank precisely what NomBank (Meyers et al, 2004) provides. We have a list of approximately 4000 deverbal noun/ verb pairs, constructed from a combination of WordNet? s derivational links (Fellbaum,1998), NomLex (Macleod et al, 1998), NomLexPlus (Meyers et al, 2004b) and some independent curation. In recent years, NomBank (Meyers et al,2004a) has provided a set of about 200,000 manually annotated instances of nominalizations with arguments, giving rise to supervised machine learned approaches such as (Pradhan et al, 2004) and (Liu and Ng, 2007), which perform fairly wellin the overall task of classifying deverbal arguments. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al, 2005) and NomBank (Meyers et al, 2004) frameworks. Both parsing is formulated as a single-stage word-pair classification problem, and the latter is carried out by a search through the NomBank (Meyers et al, 2004) or the PropBank (Palmer et al, 2005). Within the context of NomBank, a project dedicated to annotation of argument structure, Meyers et al (2004a) describe the linguistics of nominalizations ,emphasizing semantic roles. This is a purely syntactic resource, but we can also include this tree bank in the category of multistratal resources since the PropBank (Palmer et al, 2005 ) and NomBank (Meyers et al, 2004) projects have an notated shallow semantic structures on top of it. In English predicate argument structure analysis, large corpora such as FrameNet (Fillmore et al, 2001), PropBank (Palmer et al, 2005) and NomBank (Meyers et al, 2004) have been created and utilized. As a complement to PropBank, NomBank (Meyers et al,2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank. One of the most popular, semantic role labels (annotation and transducers based on the annotation) characterize relations anchored by select predicate types like verbs (Palmer et al, 2005), nouns (Meyers et al., 2004a), discourse connectives (Miltsakaki et al, 2004) or those predicates that are part of particular semantic frames (Baker et al, 1998). These features are marked in the NOMLEX-PLUS dictionary (Meyers et al, 2004b). NomBank annotation (Meyers et al., 2004) uses essentially the same framework as PropBank to annotate arguments of nouns. The PASbio (Wattarujeekrit et al, 2004) proposes Predicate Argument Structures (PASs), a type of linguistically-oriented semantic structures, for domain-specific lexical items, based on PASs defined in PropBank (Wattarujeekrit et al, 2004 ) and NomBank (Meyers et al, 2004). For predicate argument structure analysis, we have the following representative large corpora: FrameNet (Fillmore et al,2001), PropBank (Palmer et al, 2005), and NomBank (Meyers et al, 2004) in English, the Chinese PropBank (Xue, 2008) in Chinese, the GDA Corpus (Hashida, 2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al, 2002), and the NAIST Text Corpus (Iida et al, 2007) in Japanese. The NomBank project (Meyers et al, 2004) provides coarse annotations for some of the possessive con st ructions in the Penn Treebank, but only those that meet their criteria. A principled solution to this problem is to use an SRL system for nominal predicates trained using NomBank (Meyers et al., 2004).
The most clearly relevant study is Light et al (2004) where the focus is on introducing the problem, exploring annotation issues and outlining potential applications rather than on the specificities of the ML approach, though they do present some results using a manually crafted substring matching classifier and a supervised SVM on a collection of Medline abstracts. To further elucidate the nature of the task and improve annotation consistency, we have developed a new set of guidelines, building on the work of Light et al (2004).  As a baseline classifier we use the substring matching technique of (Light et al, 2004), which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part ,possibl, further investigation, unlikely ,putative, insights, point toward, promise and propose. Previous studies (Light et al, 2004) showed that the detection of hedging can be solved effectively by looking for specific keywords which imply that the content of a sentence is speculative and constructing simple expert rules that describe the circumstances of where and how a key word should appear. Results obtained adding external dictionaries In our final model we added the keywords used in (Light et al, 2004) and those gathered for our ICD 9-CM hedge detection module. Baseline 1 denotes the substring matching system of Light et al (Light et al, 2004) and Baseline 2de notes the system of Medlock and Briscoe (Medlock and Briscoe, 2007). For clinical free texts, Baseline 1 is an out-domain model since the keywords were collected for scientific texts by (Light et al, 2004) . Our finding that token unigram features are capable of solving the task accurately agrees with the the results of previous work son hedge classification ((Light et al, 2004), (Med 287 lock and Briscoe, 2007)), and we argue that 2-3 word-long phrases also play an important role as hedge cues and as non-speculative uses of an otherwise speculative keyword as well (i.e. to resolve an ambiguity). In recent years, there has been increasing interest in the speculative aspect of biomedical language (Light et al, 2004, Wilbur et al, 2006, Medlock and Briscoe, 2007). In general, these studies focus on issues regarding annotating speculation and approach the problem of recognizing speculation as a text classification problem, using the well-known bag of words method (Light et al 2004, Medlock and Briscoe, 2007) or simple substring matching (Light et al, 2004). Light et al (2004) explore issues with annotating speculative language in biomedicine and out line potential applications. Light et al (2004) report low inter-annotator agreement in distinguishing low speculative sentences from highly speculative ones. First, we used the substring matching method reported in Light et al (2004), which labels sentences containing one of more of the following as speculative: suggest, potential, likely, may, at least ,inpart ,possibl, further investigation, unlikely ,putative, insights, point toward, promise and propose (Baseline1). For example, on line product and movie reviews have provided a rich context for analyzing sentiments and opinions in text (see Pang and Lee (2008) for a recent survey), while tentative, speculative nature of scientific writing, particularly in biomedical literature, has provided impetus for recent research in speculation detection (Light et al, 2004). We collected cue phrases for such a content shifted sentence detection from the dataset adapters can be found as the supplementary material 765 works of Chapman et al (2007), Light et al (2004) and Vincze et al (2008) and from the experiments of Farkas and Szarvas (2008) and Farkas et al (2009). Light et al (2004) present a study on an notating hedges in biomedical documents. The overall inter-annotator agreement was = 0.65, which is similar to what Light et al (2004) report but worse than Medlock and Briscoe' s (2007) results. And as we expected, the feature that represents whether the word is in the hedge list or not is very useful especially in hedge cue finding, indicating that methods based on a hedge cue lists (Light et al, 2004) or keyword selection (Szarvas, 2008) are quite significant way to accomplish such tasks. Light et al (2004) started to do annotations on biomedicine article abstracts, and conducted the preliminary work of automatic classification for uncertainty.
We omit discussion here of the corpus currently in production by the University of Pennsylvania and the Children's Hospital of Philadelphia (Kulick et al 2004), since it is not yet available in finished form. A current project at the University of Pennsylvania and the Children's Hospital of Philadelphia (Kulick et al 2004) is producing a corpus that follows many of these basic principles. For example, the BioText (Rosarioand Hearst, 2004) corpus has no specific annotation guideline and contains several inconsistencies, while PennBioIE (Kulick et al, 2004) is very specific to a particular sub-domain of diseases. The first dataset (PBIO) is based on the annotations of the PENNBIOIE corpus for biomedical entity extraction (Kulick et al, 2004). In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al, 2003), PennBioIE (Kulick et al, 2004), and GENETAG (Tanabe et al, 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suit able for text mining tasks that other researchers need to address. Our annotation guidelines are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al, 2004). For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain (Kulick et al 2004) as well as a test set of 5000 tokens (200 sentences) to parse (eng-lish_pchemtbtb_test.conll). Their corpus contains MEDLINE abstracts on the inhibition of the enzyme CYP450 (Kulick et al, 2004), specifically those abstracts that contain at least one overlapping and one discontinuous annotation. Both the GENIA corpus (Kim et al, 2003) and the BioIE cytochrome P450 corpus (Kulick et al, 2004) come with named entity annotations that include a proportion of chemicals, and at least a few abstracts that are recognis able as chemistry abstracts. The BioIE P450 corpus (Kulick et al, 2004), by contrast, includes chemicals, proteins and other sub stances such as foodstuffs in a single category called substance.  Notable examples of corpus construction projects for the biomedical domain are PennBioIE (Kulick et al, 2004) and GENIA (Kim et al, 2003). Similar to the approach in (Miller et al, 2000) and (Kulick et al, 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2.  First, there is some evidence suggesting that standoff annotation and embedded XML are the two most highly preferred corpus annotation formats, and second, these for mats are employed by the two largest extant curated biomedical corpora, GENIA (Kim et al, 2001) and BioIE (Kulick et al, 2004). The development data was 200 sentences of labeled biomedical oncology text (BIO, the ONCO portion of the Penn Biomedical Treebank), as well as 200K unlabeled sentences (Kulick et al, 2004). Based on an analysis of the PennBioIE corpus (Kulick et al, 2004), detailed distributional results are provided on alternation patterns for several nominalizations with high frequency of occurrence in biomedical text, such as activation and treatment. From the sub language biology domain, we used the oncology part of the PENNBIOIE corpus (Kulick et al,2004) and removed all but three gene entity subtypes (generic, protein, and rna). (Co hen et al, 2008) investigated syntactic alternations of verbs and their nominalized forms which occurred in the PennBioIE corpus (Kulick et al, 2004), whilst keeping PASs of the PASBio in their minds. The BioFrameNet (Dolbey et al, 2006) is an at tempt to extend the FrameNet with specific frames to the bio-medical domain, and to apply the frames to corpus annotation. Similar attempt of constructing integrated corpora is being done in University of Pennsylvania, where a corpus of MEDLINE abstracts in CYP450 and oncology domains where annotated for named entities, POS, and tree structure of sentences (Kulick et al, 2004).
(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences.   It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). A refinement of such technique was presented in (Taskar et al, 2004). Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences.  Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions.
LSA Match: v and M (v) are distributionally similar according to a freely available Latent Semantic Indexing package,2 or for verbs similar according to VerbOcean (Chklovski and Pantel, 2004). We will consider to use a supervised learning approach, as well as the similar features employed for temporal relation classification task, in addition to lexical information (e.g. WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)) and the existing causal signals. For event terms, we first find the root verbs of deverbal nouns and then measure verb similarity by using the fine-grained relations provided by VerbOcean (Chklovski and Pantel, 2004), which has proved useful in summarization (Liu et al, 2007). In particular, we could emulate the approach used in VerbOcean (Chklovski and Pantel 2004). The work on VerbOcean is similar to our research in the use of the Web for acquiring relationships (Chklovski and Pantel, 2004). Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs. For example, Chklovski and Pantel (2004) loosely define ENABLEMENT as a relation that holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2 and gives two examples: assess: :review and accomplish: :complete. Additionally, a wide variety of relationship-specific classifiers have been proposed, including pattern-based classifiers for hy ponyms (Hearst, 1992) ,meronyms (Girju, 2003), synonyms (Lin et al, 2003), a variety of verb relations (Chklovski and Pantel, 2004), and general purpose analogy relations (Turney et al, 2003). Chklovski and Pantel (2004) used patterns like 'x-ed by y-ing' ('obtained by borrowing') to get co-occurrence data on candidate pairs from the Web. Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin's thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. Work was also done on relations be tween verbs (Chklovski and Pantel, 2004). Hence, when exploring very specific relation ship types or very generic, but not widely accepted, types (like verb strength), many researchers resort to manual human-based evaluation (Chklovski and Pantel, 2004). 30 relations are noun compound relationships as proposed in the (Nastase and Szpakowicz, 2003 ) classification scheme, and 5 relations are verb-verb relations proposed by (Chklovski and Pantel, 2004). Other types of relations that have been studied by pattern-based approaches include question answer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al, 2003), general purpose analogy (Turney et al, 2003), verb relations (including similarity, strength, antonym, enable ment and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al, 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney, 2007). We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization. Chklovski and Pantel (2004) address the automatic acquisition of verb-verb pairs and their relations from the web. An ablation study that formed part of the official RTE 5 evaluation attempted to evaluate the contribution of publicly available knowledge resources such as WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and DIRT (Lin and Pantel, 2001) used by many of the systems. The latter utilized several resources for matching hypothesis terms with text terms: WordNet, VerbOcean (Chklovski and Pantel, 2004), utilizing two of its relations, as well as an acronym database ,number matching module, co-reference resolution and named entity recognition tools. Chklovski and Pantel (2004) used patterns to extract a set of relations between verbs, such as similarity, strength and antonymy.
Moreover, the TEASE collection of entailment rules (Szpektor et al, 2004) consists of 136 templates provided as input, plus all the learned templates.  TEASE (Szpektor et al, 2004) discovers binary relation templates from the Web base don sets of representative entities for given binary relation templates. Szpektor et al (2004) applies a similar, with no seed lists, to extract automatically entailment relationships between verbs, and Etzioni et al (2005) report very good results extracting Named Entities and relationships from the web. Lin and Pantel (2001) and Szpektor et al (2004) proposed methods to obtain entailment templates by using a single monolingual resource. DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al, 2004) report accuracies of50.1% and 44.3% respectively compared to our average accuracy across two annotators of 70.79%. In this paper we use TEASE (Szpektor et al, 2004), a state of-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules.  TEASE (Szpektor et al., 2004) discovers dependency sub-parses from the Web, based on sets of representative entities for a given lexical item. Yet the current precision of acquisition algorithms is typically still mediocre, as illustrated in Table 1 for Dirt (Lin and Pantel, 2001) and TEASE (Szpektor et al, 2004), two prominent acquisition algorithms whose outputs are publicly available. Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). We applied the instance-based methodology to evaluate two state-of-the-art unsupervised acquisition algorithms, DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al, 2004), whose output is publicly available. Szpektor et al (2004) describe the TEASE method for extracting entailing relation templates from the Web. Other types of relations that have been studied by pattern-based approaches include question answer relations (such as birthdates and inventor) (Ravichandran and Hovy, 2002), synonyms and antonyms (Lin et al, 2003), general purpose analogy (Turney et al, 2003), verb relations (including similarity, strength, antonym, enable ment and temporal) (Chklovski and Pantel, 2004), entailment (Szpektor et al, 2004), and more specific relations, such as purpose, creation (Cimiano and Wenderoth, 2007), LivesIn, and EmployedBy (Bunescu and Mooney, 2007). Many recent efforts have also focused on extracting semantic relations between entities, such as entailments (Szpektor et al 2004), is-a (Ravichandran and Hovy 2002), part-of (Girju et al 2006), and other relations. Similar methods have also been used by Ibrahim et al (2003) and Szpektor et al (2004). For instance, the precisions of the para phrase patterns reported in (Lin and Pantel, 2001), (Ibrahim et al, 2003), and (Szpektor et al, 2004) are lower than 50%. On-line usage of web queries is less frequent and was used mainly in semantic acquisition applications: the discovery of semantic verb relations (Chklovski and Pantel, 2004), the acquisition of entailment relations (Szpektor et al, 2004), and the discovery of concept-specific relationships (Davidov et al, 2007). Next, we implemented a prototype that utilizes a state-of-the-art method for learning entailment relations from the web (Szpektor et al, 2004), the Minipar dependency parser (Lin, 1998) and a syntactic matching module. Taking a step further, the TEASE algorithm (Szpektor et al, 2004) provides a completely unsupervised method for acquiring entailment relations from the Web for a given input relation (see Section 5.1).
Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). (Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner.  Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010).
 We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. Another approach focused on sentence extraction (Fung and Cheung, 2004). A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004): parallel indicates sentence-aligned texts that are in translation relation; noisy characterizes two documents that are never the less mostly bilingual translations of each other; topic corresponds to documents which share similar topics, but that are not translation of each others and very-non that stands for rather unrelated texts. We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004).  Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity.
Syntactic frame as described by Xue and Palmer (2004) Table 3. The candidate argument extraction method used for the FrameNet data, (as mentioned in 4) was adapted from the algorithm of Xue and Palmer (2004) applied to dependency trees. For PropBank we use the algorithm of Xue and Palmer (2004) applied to dependency trees. Also, we considered some of the features designed by (Pradhan et al, 2005): First and Last Word/POS in Constituent, Subcategorization, Head Word of Prepositional Phrases and the Syntactic Frame feature from (Xue and Palmer, 2004). Hence we now prune our set, by keeping only the siblings of all of the verb's ancestors, as is common in supervised SRL (Xue and Palmer, 2004). The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al, (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). For example, Xue and Palmer (2004) reported that SRL performance dropped more than 10% when they used syntactic features from an automatic parser instead of the gold standard parsing trees. In order to reduce the number of candidate arguments in the identification step, I apply the filtering technique of Xue and Palmer (2004), trivially adopted to the dependency syntax formalism. To save time, we use a pruning stage (Xue and Palmer, 2004) to filter out the constituents that are clearly not semantic arguments to the predicate. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. Though several pruning algorithms have been raised (Xue and Palmer, 2004), the policies are all in global style. In this paper, a statistical analysis of Penn Prop Bank indicates that arguments are limited in a local syntax sub-tree rather than a whole one. Also, we considered some of the features designed by (Pradhan et al, 2004): First and Last Word/POS in Constituent, Subcategorization, Head Word of Prepositional Phrases and the Syntactic Frame feature from (Xue and Palmer, 2004). This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task. To reduce the complexity, Zhao et al (2009) reformulated a pruning algorithm introduced by Xue and Palmer (2004) for dependency structure by considering only direct dependents of a predicate and its ancestors as argument candidates. (Xue and Palmer, 2004) found out that different features suited for different sub-tasks of SRL ,i.e. argument identification and classification. As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004). Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument. The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). For our baseline SRL model, we adopt the features used in other state-of-the-art SRL systems, which include the seven baseline features from the original work of Gildea and Jurafsky (2002) ,additional features taken from Pradhan et al (2005), and feature combinations which are inspired by the system in Xue and Palmer (2004). For instance, many systems used the pruning strategy described in (Xue and Palmer, 2004) and other systems used the soft pruning rules described in (Pradhan et al, 2005a).
Swier and Stevenson (2004) innovated with an unsupervised approach to the problem, using a boot strapping algorithm, and achieved 87% accuracy. VerbNet and its semantic features have been used in a variety of NLP applications, such as semantic role labeling (Swier and Stevenson, 2004), inferencing (Zaenen et al, 2008), verb classification (Joanis et al, 2008), and information extraction (Maynard et al, 2009). Early unsupervised approaches to the SRL problem include the work by Swier and Stevenson (2004), where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface. There has been little research on semi-supervised learning for SRL.We refer to He and Gildea (2006) who tested active learning and co-training methods, but found little or no gain from semi-supervised learning, and to Swier and Stevenson (2004), who achieved good results using semi-supervised methods, but tested their methods on a small number of Verb Net roles, which have not been used by other SRL systems. To the best of our knowledge no system was able to reproduce the successful results of (Swier and Stevenson, 2004) on the PropBank role set. There are also some methods for unsupervised semantic role labeling (Swier and Stevenson, 2004), (Abend et al, 2009) that easily adapt across domains but their performances are not comparable to supervised systems. For example, VerbNet (derived from Levin? s [1993] work, Kipper et al, 2008) is widely used for a number of semantic processing tasks, including semantic role labeling (Swier and Stevenson, 2004), the creation of semantic parse trees (Shi and Mihalcea, 2005), and implicit argument resolution (Gerber and Chai, 2010). Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. Swier and Stevenson (2004) present an unsupervised method for labeling the arguments of verbs with their semantic roles. Swier and Stevenson (2004) were the first to introduce unsupervised SRL in an approach that used the VerbNet lexicon to guide unsupervised learning. Finally, Swier and Stevenson (2004) per form unsupervised semantic role labeling by using hand-crafted verb lexicons to replace supervised semantic role training data. Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al, 2000) verb lexicon, in addition to supervised parses. Swier and Stevenson (2004) induce role labels with a bootstrapping scheme where the set of labeled instances is iteratively expanded using a classifier trained on previously labeled instances. Swier and Stevenson (2004) were the first to introduce an unsupervised semantic role labeling system. This is achieved by adopting the scoring method of Swier and Stevenson (2004), in which we compute the portion Frame of frame slots that can be mapped to an extracted argument, and the portion% Sent of extracted arguments from the sentence that can be mapped to the frame. For comparison, we also apply the iterative algorithm developed by Swier and Stevenson (2004), using the same bootstrapping parameters. For ease of comparison, we use the same verbs as in Swier and Stevenson (2004), except that we measure performance over a much larger superset of verbs. As a point of comparison, we apply the iterative back off model from Swier and Stevenson (2004), trained on 20% of the BNC, with our frame matcher and test data. Early unsupervised approaches to the SRL task include (Swier and Stevenson, 2004), where theVerbNet verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.
In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). Researchers employ the existing SMT models for PG (Quirk et al, 2004). Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. Quirk et al (2004) first recast paraphrase generation as monolingual SMT. The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding.
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. In Japanese WS, unknown words are usu ally dealt with in an on line manner with the unknown word model, which uses heuristics 183 depending on character types (Kudo et al,2004). In Japanese, our unknown word model relies on heuristics based on character types and word length to generate word nodes, similar to that of MeCab (Kudo et al., 2004).  Kudo et al (2004) use SVMs to morphologically tag Japanese. The four parallel corpora were tokenized and lemmatized, for Japanese with the MeCab morphological analyzer (Kudo et al, 2004), and for English with the Freeling analyzer (Padr? et al, 2010), with MWE, quantities, dates and sentence segmentation turned off. In our approach, N-best candidates for each training example are produced with the CRF++ software (Kudo et al, 2004). Following Kudo et al (Kudo et al, 2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab1, to our POS/PROTEIN tagging task. Segmentation for Japanese is a successful field of research, achieving the F-score of nearly 99% (Kudo et al, 2004). One would notice that the baseline score is much lower than the score previously reported regarding newspaper articles (Kudo et al, 2004). This area of research may be considered almost completed, as previous studies reported the F-score of nearly 99% (Kudo et al, 2004). The sequential tagger used in this paper is CRF++ (Kudo et al, 2004). Previous work (Kudo et al, 2004) showed CRFs outperform generative Markov models and discriminative history-based methods in JWS. Kudo et al (2004) modified CRFs for non-segmented languages like Japanese which have the problem of word boundary ambiguity. As conventional sequential tagging problems, such part-of-speech tagging and phrase chunking, we employ the conditional random fields (CRF) as learners (Kudo et al, 2004). Regarding the two state-of-the-art word segmentation systems, one is JUMAN,  a rule-based word segmentation system (Kurohashi and Nagao, 1994), and the other is MECAB, a supervised word segmentation system based on CRFs (Kudo et al, 2004). The performance of the two word segmentation baselines (JUMAN and MECAB) is significantly worse in our task than in the standard word segmentation task, where nearly 99% precision and recall are reported (Kudo et al, 2004). Kudo et al (2004) studied Japanese word segmentation and POS tagging using conditional random fields (CRFs) and rule based unknown word processing. To demonstrate our method, we compare to several well-known structural learning algorithms, like CRF (Kudo et al, 2004), and SVM-HMM (Joachims et al, 2009) on two well-known data, namely, CoNLL-2000 syntactic chunking, SIGHAN-3 Chinese word segmentation tasks. Following Kudo et al (2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab2, to our POS tagging task.
So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). If we extend these positional tags to include POS information ,segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2004) shown that, compared with performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging. As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively. The features we use to build the classifier are generated from the templates of Ng and Low (2004). The table's upper column lists the templates that immediately from Ng and Low (2004). Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). Ac cording to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: b: the begin of the word m: the middle of the word e: the end of the word s: a single-character word. In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004). Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target. Note that the templates of Ng and Low (2004) have already contained some lexical-target ones. Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved F measure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation. In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). For example, Li et al (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it is beneficial to integrate word segmentation and part-of-speech tagging into one model. In this bakeoff, our models built for the tasks are similar to that in the work of Ng and Low (2004). For this task, because of the time limitation as mentioned in the previous section, we could only port our implemented model by using a part of the feature set which was used in the word-based tagger discussed in the work of Ng and Low (2004). Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005).  Similar to (Ng and Low, 2004), we found the overall F measure only goes up a tiny bit, but we do find a significant OOV recall rate improvement.
Chelba and Acero (2004) first traina classifier on the source data. (Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases.  Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text.
They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}.  An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004).
Earlier experiments with graph-based ranking algorithms for text summarization, as previously re ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.   The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality.  Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005). Lex PageRank (Erkan and Radev, 2004) is one of such methods. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004).  
(Koehn, 2004) gives empirical evidence that these give accurate estimates for Bleu statistics. The significance test on translation performance was per formed by the bootstrap method (Koehn, 2004) with a 5% significance level. Our BLEU score improvements of 1.2 to 1.9 points are statistically significant according to the paired bootstrap re sampling method (Koehn, 2004) with n= 1000 and p=0.01. The 95% confidence intervals as calculated by bootstrap re sampling (Koehn, 2004) are shown for each of the results. The 95% confidence intervals of our scores, computed by bootstrap re sampling (Koehn, 2004), indicate that a score increase of more than 1 BLEU is statistically significant. The improvement in BLEU is statistically significant (p= 0.01) using the paired bootstrap re sampling significance test (Koehn, 2004). Similarly to the distortion penalty in the conventional phrase based decoder (Koehn 2004b), the distortion cost of jumping from a word at position i to another word at position j, d (i, j), is proportional to the distance between i and j ,e.g., |i-j|. Decoding is based on a beam search algorithm similar to that of the phrase-based MT decoder (Koehn 2004b).  Using bootstrap re sampling (Koehn, 2004), the improvements in BLEU, TER, as well as the linear combination used in tuning are statistically significant at at least p =.05. Statistical significance in BLEU differences was tested by paired bootstrap re-sampling (Koehn, 2004). These intervals were computed following the boot strap technique described in (Koehn, 2004). Confidence intervals at 95% confidence level following (Koehn, 2004). We perform a bootstrap re sampling significance test (Koehn, 2004) on the output predictions of the local classifiers with and without the inference model. To see whether an improvement is statistically significant, we also conduct significance tests using the paired bootstrap approach (Koehn, 2004). To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrap approach (Koehn, 2004). All improvements on two test sets are statistically significant by the bootstrap resampling (Koehn, 2004). To measure whether the difference between system performance is statistically significant, we use bootstrap re sampling with 100 samples with the t test (Koehn, 2004). Statistical significance on the BLEU scores was tested using pairwise bootstrap sampling (Koehn, 2004). The marked systems produce statistically significant improvements as measured by bootstrap re sampling method (Koehn, 2004) on BLEU over the baseline system.
In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data. Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.   Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex. TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction. PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight. Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable. Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009). In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy. More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words. If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper. As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming. Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year. As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term. Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004). Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction. The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004). We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004). In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.
For instance, instead of representing the polarity of a term using a binary value, Mullen and Collier (2004) use Turney's (2002) method to assign a real value to represent term polarity and introduce a variety of numerical features that are aggregate measures of the polarity values of terms selected from the document under consideration. Mullen and Collier (2004) used SVMs and expanded the feature set for representing documents with favor ability measures from a variety of diverse sources.  Mullen and Collier (Mullen and Collier, 2004) integrated PMI values, Osgood semantic factors and some syntactic relations into the features of SVM.  Another method is to use proximal information of the query and the word, using syntactic structure such as dependency relations of words that provide the graphical representation of the text (Mullen and Collier, 2004). Mullen and Collier (2004) manually annotated named entities in their dataset (i.e. title of the record and name of the artist for music record reviews), and utilized presence and position features in their ML approach. Mullen and Collier (2004), for example, uses WordNet to add information about words found within text, and consequently reports improved classification performance in a sentiment analysis task.
 SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser. Figure 6 shows the performance of WASP com pared to four other algorithms: SILT (Kate et al,2005), COCKTAIL (Tang and Mooney, 2001), SCIS SOR (Ge and Mooney, 2005) and Zettlemoyer and Collins (2005). Of course, other annotations (Ge and Mooney, 2005) carry more explicit forms of semantics. The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). Ge and Mooney (2005) introduced an approach, SCISSOR, where the composition of meaning representations is guided by syntax. Extended back-off levels for the semantic parameter PL1 (Li| ...), using the same notation as in Ge and Mooney (2005). Here, we only describe changes made to SCISSOR for re ranking, for a full description of SCISSOR see Ge and Mooney (2005). Baseline ResultsTable 2 shows the results comparing the baseline learner SCISSOR using both the back-off parameters in Ge and Mooney (2005) (SCISSOR) and the revised parameters in Section 2.2 (SCISSOR+). Ge and Mooney also presented a statistical method (Ge and Mooney, 2005) by merging syn tactic and semantic information. Structure representation A tree structure representation incorporated with semantic and syntactic information is named semantically augmented parse tree (SAPT) (Ge and Mooney, 2005). As defined in (Ge and Mooney, 2005), in an SAPT, each internal node in the parse tree is annotated with a semantic label. The procedure of generating a logical form using a SAPT structure originally proposed by (Ge and Mooney, 2005) and it is expressed as Algorithm 1.  SCISSOR (Ge and Mooney, 2005) takes syntactic parses rather than NL strings and attempts to translate them into MR expressions.
Its architecture is based on the top system in the 2005 CoNLL shared task (Koomen et al, 2005), modified to process raw text using lower level processors but maintaining 6 good real time performance. Details of them can be found in (Koomen et al, 2005). Only recently we have been able to test Koomen et al (2005) SRL tool. This module is re trained in our SRC experiments, using parameters described in (Koomen et al, 2005). Combining the output from several different systems has been shown to be beneficial (Koomen et al, 2005). These were first annotated with semantic roles using a state of-the-art semantic role labeling system (Koomen et al, 2005). Koomen et al (2005) adopted the outputs of multiple SRL systems (each on a single parse tree) and combined them into a coherent predicate argument output by solving an optimization problem. Basedon that work, Koomen et al (2005) combined several SRL outputs using ILP method. On the other hand, probabilistic inference processes, which have been successfully used for SRL (Koomen et al, 2005), mandate that each individual candidate argument be associated with its raw activation, or confidence, in the given model. Koomen et al (2005) used a 2 layer architecture similar to ours. Koomen et al (2005) combined several SRL outputs using ILP method.
Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. Similarities are captured from different viewpoints: DP-HWC (i) -l This metric corresponds to the HWC metric presented by Liu and Gildea (2005). This metric corresponds to the STM metric presented by Liu and Gildea (2005). Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). This finding has been previously reported, among others, in Liu and Gildea (2005). The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. We use three different kinds of metrics: DR-STM Semantic Tree Matching, a la Liu and Gildea (2005), but over DRS instead of over constituency trees. The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set. Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. This phenomenon has been previously observed by Liu and Gildea (2005). This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement.
Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005). Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence. Results are presented in terms of BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005 ) metrics. We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines. In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique. Another approach is taken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006). Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002), WER, PER and TER (Snover et al, 2006). In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium? s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al, 2003), Translation Error Rate (TER) (Snover et al, 2006) 1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. We also used the paraphrase database TERp for METEOR (Banerjee and Lavie, 2005). For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n= 4) (Papineni et al, 2001), NIST (n= 5) (Lin and Hovy, 2002), GTM F1-measure (e= 1, 2) (Melamed et al, 2003), 1-WER (Nie? en et al, 2000), 1-PER (Leusch et al, 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). BLEU (Papineni et al, 2002), TER (Snover et al, 2006) and METEOR (Banerjee and Lavie, 2005) scores will be reported. In this paper ,translation quality is evaluated according to (1) the BLEU metrics which calculates the geometric mean of n gram precision by the system output with respect to reference translations (Papineni et al, 2002), and (2) the METEOR metrics that calculates uni gram overlaps between translations (Banerjee and Lavie, 2005). There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003). The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. e.g. Meteor (Banerjee and Lavie, 2005). Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure. For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al, 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie,2005).
The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similarity) between hypothesis and text ,e.g., (Corley and Mihalcea, 2005).   Corley and Mihalcea (2005) proposed a hybrid method by combining six existing knowledge-based methods. Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dec tors: the exact matching between tokens or lemmas, a similarity between tokens based on their edit distance, the derivation ally related form relation and the verb entailment relation in WordNet, and, finally, a WordNet-based similarity (Jiang and Conrath, 1997). Experimental results lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005). First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline ,i.e. 50%. The first backup strategy is a straightforward BoW method that we will not present in this paper (see more details in (Corley and Mihalcea, 2005)). In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. The system performance reported in (CM05; (Corley and Mihalcea, 2005)), which is among the best we are aware of, is also included for comparison. This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005). Otherwise, WordNet (Miller, 1995) similarities (as in (Corley and Mihalcea, 2005)) and different relation between words such as verb entailment and derivational morphology are applied. Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures. Also, we will try different similarity score functions for both the clustering and the anchor approaches, as those surveyed in Corley and Mihalcea (2005). The semantic similarity formula from (Corley and Mihalcea, 2005) defines the similarity of a pair of documents differently depending on with respect to which text it is computed.
We will present an algorithm for determinizing weighted finite tree recognizers, and use a variant of the procedure found in (Huang and Chiang, 2005) to obtain -best lists of trees that are weighted correctly and contain no repetition.  We follow the third algorithm in Huang and Chiang (2005), where first a traditional Viterbi-chart is created, which enumerates in an efficient way all possible sub derivations. In terms of decoding time, even though we used Algorithm 3 described in (Huang and Chiang, 2005), which lazily generated the N-best translation candidates, the decoding time tended to be increased because more rules were available during cube pruning. The latter function uses bi nary lazy enumeration in a manner similar to (Huang and Chiang, 2005), and relies on two global variables: I and L. The k= 200-best parses at the top cell of the chart are calculated using the efficient algorithm of (Huang and Chiang, 2005). This technique utilizes a bunch of linguistic features to re-rank the k-best (Huang and Chiang 2005) output on the forest level or tree level. We could also have used the more efficient k-best hyper graph parsing technique by Huang and Chiang (2005), but we have not yet incorporated this into our implementation. N-best list not the lazy algorithm of (Huang and Chiang, 2005). It is found to be well handled by the K-Best parsing method in Huang and Chiang (2005). Those weighted tree languages are recognizable and there exist algorithms (Huang and Chiang, 2005) that efficiently extract the k-best parse trees (i.e., those with the highest probability) for further processing. k-best lists are extracted from the CRF trellis using the lazy enumeration algorithm of Huang and Chiang (2005). Translation hyper graphs are generated by each baseline system during the MAPde coding phase, and 1000-best lists used for MERT algorithm are extracted from hyper graphs by the k-best parsing algorithm (Huang and Chiang, 2005). Although Viterbi and k-best extraction algorithms are often expressed as INSIDE algorithms with the tropical semiring ,cdec provides a separate derivation extraction framework that makes use of a &lt; operator (Huang and Chiang, 2005). The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005). In a certain sense, described in greater detail below, this precomputation of exact heuristics is equivalent to the k-best extraction algorithm of Huang and Chiang (2005). By exploiting a local ordering amongst derivations, we can be more conservative about combination and gain the advantages of a lazy successor function (Huang and Chiang, 2005). This triggering is similar to the lazy frontier used by Huang and Chiang (2005). As a baseline, we compared KA∗ to the approach of Huang and Chiang (2005), which we will call EXH (see below for more explanation) since it requires exhaustive parsing in the bottom-up pass. While formulated very differently, one limiting case of our algorithm relates closely to the EXH algorithm of Huang and Chiang (2005).
This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005).  In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures. In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time.
Kim and Hovy (2006) integrated verb information from FrameNet and incorporated it into semantic role labeling. Kim and Hovy (2006) and Bethard et al (2005) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al, 2003) for both opinion holder and opinion target extraction. Due to data sparseness, Kim and Hovy (2006) expand FrameNet data by using an unsupervised clustering algorithm. Kim and Hovy (2006 ) used a FrameNet-based semantic role labeler to deter mine holder and topic of opinions. Kim and Hovy (2006 )useda FrameNet-based semantic role labeler to deter mine holder and topic of opinions. Kim and Hovy (2006 ) used a FrameNet-based semantic role labeler to deter mine holder and topic of opinions. Kim and Hovy (2006) use structural features of the language to identify opinion entities. A more linguistically motivated approach was taken by Kim and Hovy (2006) through identifying opinion holders and targets with semantic role labeling. Also Framenet (Ruppenhofer et al (2010)) is used as a resource in opinion mining and sentiment analysis (Kim and Hovy (2006)).  Kim and Hovy (2006) identifies opinion holders and targets by using their semantic roles related to opinion words. Some work such as (Kim and Hovy, 2006) has explored the connection to role labeling. Kim and Hovy (2006) use a machine translation system and subsequently use a subjectivity analysis system that was developed for English.  A notable exception is the work of Kim and Hovy (2006). Opinion holder is usually an entity that holds an opinion, and opinion target is what the opinion is about (Kim and Hovy, 2006). Kim and Hovy (2006) proposed to map the semantic frames of FrameNet into opinion holder and target for only adjectives and verbs. This means that we are actually searching for all triples{ source, target, opinion} in this sentence (Kim and Hovy, 2006) and throughout each document in the corpus. Bethard et al (2004) and Kim and Hovy (2006) explore the usefulness of semantic roles provided by FrameNet (Fillmore et al 2003). Bethard et al (2004) use this resource to acquire labeled training data while in (Kim and Hovy, 2006) FrameNet is used within a rule-based classifier mapping frame-elements of frames to opinion holders.
They measure compositionality as a combination of two similarity values: firstly, similar to (Katz and Giesbrecht, 2006), the similarity (cosine similarity) between the context of a VNC and the contexts of its constituent words. Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainning data. The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal usages of a given expression in the training data. The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal us ages of a given expression in the training data. Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006). Note that our results are noticeably higher than those reported by Cook et al (2007), Fazly et al (To appear) and Katz and Giesbrecht (2006) for similar supervised classifiers. Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors. There are several studies relevant to detecting compositionality of noun-noun, verb-particle and light verb constructions and verb noun pairs (e.g. Katz and Giesbrecht (2006)). Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. The performance of the supervised one is obtained by the method of Katz and Giesbrecht (2006). Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. Among the earliest studies on token-based classification were the ones by Hashimoto et al (2006) on Japanese and Katz and Giesbrecht (2006) on German. Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. Such techniques either do not use any information regarding the linguistic properties of MWEs (Birkeand Sarkar, 2006), or mainly focus on their non compositionality (Katz and Giesbrecht, 2006). In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manually annotated training data. However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional. We also compare our unsupervised methods against the supervised method proposed by Katz and Giesbrecht (2006). Our results using 1NN, 72:4%, are comparable to those of Katz and Giesbrecht (2006) using this method on their German data (72%). L-NCF depends highly on the accuracy of the automatically acquired canonical forms, it is not surprising that these two methods perform 5This was also noted by Katz and Giesbrecht (2006) in their second experiment.
Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). This solution requires larger applicability contexts (Marcu et al, 2006). Addressing the issues in Galley et al (2006), Marcu et al (2006) create an x Rs rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure. Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006) or auxiliary rules (Liu et al, 2007). We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). (Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. Inside the Moses toolkit, three different statistical approaches have been implemented: phrase based statistical machine translation (PB SMT) (Koehn et al 2003), hierarchical phrase based statistical machine translation (Chiang,2007) and syntax-based statistical machine translation (Marcu et al 2006). Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of non syntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al,2006). Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even out perform them in some cases (Marcu et al, 2006). Following Galley et al (2006)' s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. We build two translation systems: One using tree-based models without additional linguistic annotation, which are known as hierarchical phrase based models (Chiang, 2005), and another system that uses linguistic annotation on the target side, which are known under many names such as string-to-tree models or syntactified target models (Marcu et al, 2006).
We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al, 2006), and binary features to indicate phrase pair presence within different components. We also use the fix-discount method in Foster et al (2006) for phrase table smoothing. First, we used several types of phrase table smoothing in the WMT 2007 system because this proved helpful on other translation tasks: relative frequency estimates, Kneser-Neyand Zens-Ney-smoothed probabilities (Foster et al, 2006). We use a version from Foster et al (2006), modified from (Koehn et al,2003), which is an average of pairwise word translation probabilities. Foster et al (2006) applied ideas from language model smoothing to the translation model. We are only aware of one work that performs a systematic comparison of smoothing techniques in phrase-based machine translation systems (Foster et al., 2006). This may lead to an over-reliance on unreliable distributions, which can be ameliorated by smoothing (e.g., Foster et al (2006)). The maximum likelihood estimates are smoothed using Good-Turing discounting (Foster et al, 2006). The phrase translation model probabilities are smoothed according to one of several techniques as described in (Foster et al, 2006) and identified in the discussion below. In addition, a number of different phrase table smoothing algorithms were used: no smoothing, Good-Turing smoothing, Kneser-Ney 3 parameter smoothing and the log linear mixture involving two features called Zens-Ney (Foster et al, 2006). This is plausible since a similar effect (a decrease in the benefit of smoothing) has been noted with phrase table smoothing (Foster et al, 2006). In addition, we applied phrase table smoothing as described in Foster et al (2006). The phrase based translation model is similar to that of Koehn, with the exception that phrase probability estimates P (s? |t?) are smoothed using the Good-Turing technique (Foster et al, 2006). From this point of view, TMG can also be seen as a TM smoothing technique based on multiple TMs instead of single one such as Foster et al (2006). Some literatures have paid attention to this issue as well, such as Foster et al (2006) and Mylonakis and Simaan (2008). This can happen, if we define features that penalize longer phrase pairs, such as lexical weighting, or if we apply smoothing (Foster et al 2006). Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting as described in Foster et al (2006). Smoothing is obviously one possibility (Foster et al, 2006). Absolute discounting is a popular smoothing method for relative frequencies (Foster et al, 2006). The phrase translation probabilities p (e1|f )and p (f |e2) are estimated by maximum likelihood estimation and smoothed using Good-Turing smoothing (Foster et al, 2006).
Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005). In this paper, we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al, 2006) in the domain adaptation task given by the CoNLL 2007. Prettenhofer and Stein (2010) investigate cross lingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al, 2006). There has been a lot of work in domain adaption for NLP (Dai et al, 2007) (Jiang and Zhai, 2007) and one suitable choice for our problem is the approach based on structural correspondence learning (SCL) as in (Blitzer et al, 2006) and (Blitzer et al, 2007b). These features can either be identified with heuristics (Blitzer et al, 2006) or by automatic selection (Blitzer et al, 2007b). However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al, 2006). Recent work by McClosky et al (2006) and Blitzer et al (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. SCL is the structural correspondence learning technique of Blitzer et al (2006). Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al, 2006), which is an extension of the method described by (Ando and Zhang, 558 2005). SCL (Blitzer et al, 2006) is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification. The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al, 2006). As in (Blitzer et al, 2006), we found it necessary to scale up the SCL features to increase their utilization in the presence of the raw features; however, it was difficult to guess the optimal scaling factor without having access to labeled target data. In this way our problem resembles the part-of-speech tagging task (Blitzer et al, 2006), where the category of each word is predicted using values of the left, right, and current word token. (Blitzer et al, 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. Of course, it is a known fact that machine learning techniques do not transfer well across different domains (e.g., Blitzer et al (2006)). Finally, we compare two domain adaptation approaches to utilize unlabeled speech data: bootstrapping, and Blitzer et al's Structural Correspondence Learning (SCL) (Blitzer et al, 2006). In contrast with bootstrapping, SCL (Blitzer et al,2006) uses the unlabeled target data to learn domain independent features. Blitzer et al (2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of Speech tagger. For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al, 2006). Blitzer et al (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain.
For dependency parsing, McDonald and Pereira (2006) proposed a method which can incorporate some types of global features, and Riedel and Clarke (2006) studied a method using integer linear programming which can incorporate global linguistic constraints. However, work in dependency parsing (Riedel and Clarke, 2006) has demonstrated that it is possible to use ILP to perform efficient inference for very large programs when used in an incremental manner. However, recent work (Riedel and Clarke, 2006) has shown that even exponentially large decoding problems may be solved efficiently using ILP solvers if a Cutting-Plane Algorithm (Dantzig et al, 1954) is used. Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al (2009) recently introduced alternative LP and ILP formulations. Riedel and Clarke (2006) tackled the MAP problem for dependency parsing by an incremental approach that starts with a relaxation of the problem, solves it, and adds additional constraints only if they are violated. ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. Another attempt to overcome the problem of complexity with ILP models is described in (Riedel and Clarke, 2006) (dependency parsing). In contrast, generic NP-hard solution techniques like Integer Linear Programming (Riedel and Clarke, 2006) know nothing about optimal substructure. Riedel and Clarke (2006) showed that dependency parsing can be framed as Integer Linear Program (ILP), and efficiently solved using an off-the shelf optimizer if a cutting plane approach is used. Compared to the representation Riedel and Clarke (2006), this bound has the benefit a small polynomial number of constraints. Our formulation is inspired by Martins et al 2009, and hence uses fewer constraints than Riedel and Clarke (2006). We suggest scaling techniques that allow to optimally learn such graphs over a large set of typed predicates by first decomposing nodes into components and then applying incremental ILP (Riedel and Clarke, 2006). Another solution for scaling ILP is to employ incremental ILP, which has been used in dependency parsing (Riedel and Clarke, 2006). For instance, to improve the accuracy further, more global constrains capturing the subcategorization correct could be integrated as in Riedel and Clarke (2006). Riedel and Clarke (2006) cast dependency parsing as an ILP, but efficient formulations remain an open problem. If it is extended to labeled parsing (a straightforward extension), our formulation fully subsumes that of Riedel and Clarke (2006), since it allows using the same hard constraints and features while keeping the ILP polynomial in size. Rather than adding exponentially many constraints, one for each potential cycle (like Riedel and Clarke, 2006), we equivalently replace condition 3 by 3. all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graph based parser, and a refinement of the latter, due to Martins et al (2008), which attempts to approximate non-local features. We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. The average runtime (across all languages) is 0.632 seconds per sentence, which is in line with existing higher-order parsers and is much faster than the runtimes reported by Riedel and Clarke (2006). The standard approach to framing dependency parsing as an integer linear program was introduced by (Riedel and Clarke, 2006), who converted the MST parser of (McDonald et al 2005) to use ILP for inference. The key idea is to build a complete graph consisting of tokens of the sentence where each edge is weighted by a learned scoring function.
In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. votes on the bill under discussion (Thomas et al, 2006). (Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. The same applies to the task of subgroup detection (as done by (AbuJbara et al., 2012), (Anand et al, 2011) or (Thomas et al, 2006)) . In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000), (Laver and Garry, 2000), (Laver et al, 2003) or (Sim et al, 2013). Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al (2006). However, like other cascaded approaches (e.g., Thomas et al (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. We also use the U.S. Congressional floor debates transcripts from Thomas et al (2006). For our experiments, we evaluate our methods using the speaker based speech-segment classification setting as described in Thomas et al (2006). Datasets in the required format for SVMsle are available at http: //www.cs.cornell.edu/ ?ainur/data.html. In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1: Summary of the experimental results for the Movie Reviews (top) and U.S. Congressional Floor Debates (bottom) datasets using SVMsle, SVMslew/ Prior and SVMslefs with and without proximity features.  Thomas et al (2006) presented a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates. Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al (2006). Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates. The first baseline is based on the work of (Thomas et al 2006). We used the speaker agreement component presented in (Thomas et al 2006) as a baseline. Other work that has considered different discourse functions in sentiment analysis, have experimented on detecting arguments (Somasundaran et al, 2007) and the stance of political debates (Thomas et al, 2006). Stances in online debates: Somasundaran and Wiebe (2009), Thomas et al (2006), Bansal et al (2008), Burfoot et al (2011), and Anand et al (2011) proposed methods to recognize stances in on line debates. Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels).
Various attempts have been made to incorporate discourse relations into sentiment analysis: Pang and Lee (2004) explored the consistency of subjectivity between neighboring sentences; Mao and Lebanon (2007), McDonald et al (2007), and Tackstrom and McDonald (2011a) developed structured learning models to capture sentiment dependencies between adjacent sentences; Kanayama and Nasukawa (2006) and Zhou et al (2011) use discourse relations to constrain two text segments to have either the same polarity or opposite polarities; Trivedi and Eisenstein (2013) and Lazaridou et al (2013) encode the discourse connectors as model features in supervised classifiers. Kanayama and Nasukawa (2006) posited that polar clauses with the same polarity tend to appear successively in contexts. As mentioned in Section 2, Kanayama and Nasukawa (2006) validated that polar text units with the same polarity tend to appear together to make contexts coherent. Kanayama and Nasukawa (2006) bootstrap subjectivity lexicons for Japanese by generating subjectivity candidates based on word co-occurrence patterns. In (Kanayama and Nasukawa, 2006), the authors propose an algorithm to automatically expand an initial opinion lexicon based on context coherency, the tendency for same polarities to appear successively in contexts. Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other. Finally, Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. The second type of relations are word-to-expression relations: e.g., some words appear in expressions that take on a variety of polarities, while other words are associated with expressions of one polarity class or another. In relation to previous research, analyzing word-to-word (intra-expression) relations is most related to techniques that determine expression-level polarity in context (e.g., Wilson et al (2005)), while exploring word-to-expression (inter-expression) relations has connections to techniques that employ more of a global-view of corpus statistics (e.g., Kanayama and Nasukawa (2006)). While most previous research exploits only one or the other type of relation, we propose a unified method that can exploit both types of semantic relation, while adapting a general purpose polarity lexicon into a domain specific one. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. (Kanayama and Nasukawa, 2006) reported that it was appropriate in 72.2% of cases. Kanayama and Nasukawa used both intra and inter-sentential co-occurrence to learn polarity of words and phrases (Kanayama and Nasukawa,2006). Kanayama and Nasukawa (2006) use syntactic features and context coherency, the tendency for same polarities to appear successively, to acquire polar atoms. Other related work is concerned with subjectivity analysis. Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. Kanayama and Nasukawa (2006) improved this work by using the idea of coherency. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created.  Next, the target-specific polarity of adjectives is determined using Hearst-like patterns. Kanayama and Nasukawa (2006) introduce polar atoms: minimal human-understandable syntactic structures that specify polarity of clauses. More advanced methods such as (Kanayama and Nasukawa, 2006) adopt domain knowledge by extracting sentiment words from the domain-specific corpus.
One exception is Choi et al (2006), which proposed an ILP approach to jointly identify opinion holders, opinion expressions and their IS-FROM linking relations, and demonstrated the effectiveness of joint inference. Most similar to our method is Choi et al (2006), which jointly extracts opinion expressions, holders and their IS-FROM relations using an ILP approach. Similar to the preprocessing approach in (Choi et al,2006), we filter pairs of opinion and argument candidates that do not overlap with any gold standard relation in our training data. This makes our ILP formulation advantageous over the ILP formulation proposed in Choi et al (2006), which needs m binary decisions for a candidate span, where m is the number of types of opinion entities, and the score for each possible label assignment is obtained by the sum of raw scores from m independent extraction models. We adopted the evaluation metrics for entity and relation extraction from Choi et al (2006), which include precision, recall, and F1-measure according to overlap and exact matching metrics. It can be viewed as an extension to the ILP approach in Choi et al (2006) that includes opinion targets and uses simpler ILP formulation with only one parameter and fewer binary variables and constraints to represent entity label assignments. For example, our model failed to identify the IS-ABOUT relation (offers, general aid) from the following sentence Powellhad contacted ... and received offers of [gen formulation in Choi et al (2006) on extracting opinion holders, opinion expressions and IS-FROM relations, and showed that the proposed ILP formulation performs better on all three extraction tasks. Opinion Finder (Wilson et al, 2005a) (Version1.4): We used the +/labels assigned by its contextual polarity classifier (Wilson et al, 2005b) to create +/states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al, 2006) to produce mental (M) states. However since the contextual information in a domain is specific, the model got by their approach cannot easily converted to other domains. Choi et al (2006) used an integer linear programming approach to jointly extract entities and relations in the context of opinion oriented information extraction. Choi et al (2005) and Choi et al (2006) explore conditional random fields, Wieg and and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence. Similarly, Choi et al (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently applied tree kernel learning methods on a combination of syntactic and semantic role trees for the same task. Choi et al (2006) is an extension of Choi et al (2005) in that opinion holder extraction is learnt jointly with opinion detection. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assigning a polarity to a given opinion expression, similar to that described by Wilson et al (2009). The first of the two baselines extracts opinion expressions using a sequence labeler similar to that by Breck et al (2007) and Choi et al (2006). Choi et al (2006) address the task of extracting opinion entities and their relations, and incorporate syntactic features to their relation extraction model. For the task of subjective expression detection, Choi et al (2006) and Breck et al (2007) used syntactic features in a sequence model. Similarly, Choi et al (2006) successfully used a PropBank-based semantic role labeler for opinion holder extraction. Others extend the token-level approach to jointly identify opinion holders (Choi et al 2006), and to determine the polarity and intensity of the opinion expressions (Choi and Cardie, 2010).
Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006).  Columns 1 - 3 were predicted using the tagger of Ciaramita and Altun (2006). grained distinctions of WN (Hearst and Schutze,1993) (Peters et al, 1998) (Mihalcea and Moldovan, 2001) (Agirre et al, 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997) (Ciaramita and Johnson, 2003) (Villarejo et al, 2005) (Curran, 2005) (Ciaramita and Altun, 2006). In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates text with a 46-label tag set. We used the implementation available from http: //sourceforge.net/projects/supersensetag, more details on this tagger can be found in (Ciaramita and Altun, 2006). This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy (Ciaramita and Altun, 2006). This annotation was performed automatically using the SuperSense Tagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses. We recommend mate-tools (Bjorkelund et al, 2009) and SuperSenseTagger (Ciaramita and Altun, 2006). We used Ciaramita and Altun's Su per Sense Tagger (Ciaramita and Altun, 2006) to tag the supersenses. This is the coarse lexicographic category label, elsewhere denoted supersense (Ciaramita and Altun, 2006), which is the terminology we use. we use the out puts of SuperSense Tagger (Ciaramita and Altun,2006), which is optimised for assigning the super senses described above, and can outperform a WNF style baseline on at least some datasets. We use SuperSenseTagger (Ciaramita and Altun,2006) as our NER tagger. SUPERSENSE LEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. A detailed description of the features used and the tagger can be foundin (Ciaramita and Altun, 2006). In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). WSD are those reported by (Ciaramita and Altun, 2006). Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us 468.
Patwardhan and Pedersen (2006) create aggregate co-occurrence vectors for a WordNet sense by adding the co-occurrence vectors of the words in its WordNet gloss. An extension by Patwardhan and Pedersen (2006) differentiated context word senses and extended shorter glosses with related glosses in WordNet.  One possible explanation for the unsuitability of the measures of (Patwardhan and Pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors. Other measures have been proposed that utilize the text in WordNet's definitional glosses, such as Extended Lesk (Banerjee and Pedersen, 2003) and later the Gloss Vectors (Patwardhan and Pedersen, 2006) method. It is worth noting that in their experiments, (Patwardhan and Pedersen, 2006) report that the Vector method has rank correlation coefficients of .91 and .90 for MC and RG, respectively, which are also top performing values.  To take account of these similarities WordNet-based similarity measures are used (Patwardhan and Pedersen, 2006). Patwardhan and Pedersen (2006) introduce a vector measure to determine the relatedness between pairs of concepts. The system generated rankings were compared with gold standard data created via Amazon Mechanical Turk.The Duluth systems relied on the Gloss Vec tor measure of semantic relatedness (Patwardhanand Pedersen, 2006) as implemented in WordNet: :Similarity (Pedersen et al, 2004). Gloss Vectors measure (Patwardhan and Pedersen, 2006) is calculated as a cosine (9) between context vectors vi and vj of concepts ci and cj. Patwardhan and Pedersen (2006) evaluate six knowledge-based measures on the task of word sense disambiguation and report the same result. Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. Effectively, this formalizes the notion that two concepts related to a third concept is also semantically related, which is similar to the hypothesis proposed by Patwardhan and Pedersen (2006) in their method based on second-order context vectors. In addition to WktWiki, we operate with 2 baseline measures relying on WordNet glosses available in a WORDNET::SIMILARITY package: Gloss Vectors (Patwardhan and Pedersen, 2006). For this work, we used the Context Vector measure (Patwardhan and Pedersen, 2006). (Patwardhan and Pedersen, 2006) cosine of the angle between the co-occurrence vector computed from the definitions around the two synsets. For each pair of nodes (u, v) in the graph, we compute the semantic similarity score (using WordNet) between every pair of dependency relation (rel: a, b) in u and v as: s (u, v)= reli u ,relj v reli=relj WN (ai ,aj) WN (bi ,bj), where rel is a relation type (e.g., nsubj) and a, b are the two arguments present in the dependency relation (b does not exist for some relations). WN (wi ,wj) is defined as the WordNet similarity score between words wi and wj. The edge weights are then normalized across all edges in the 2There exists various semantic relatedness measures based on WordNet (Patwardhan and Pedersen, 2006).
Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009). Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing, from morphological segmentation (Goldwater et al, 2009) to opinion modelling (Lin et al, 2006). We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. The support vector machine (SVM), NB B and LSPM results are taken directly from Lin et al (2006). We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL): We use the GUEST part of the BitterLemons corpus (Lin et al, 2006), containing 296 articles published in 2001-2005 on http: //www.bitterlemons.org by more than 200 different Israeli and Palestinian writers on issues re lated to the conflict. Bitter Lemons International (BL-I): We collected 150 documents each by a different per2Ratings are from :http: //www.OnTheIssues.org/.  For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). These experiments were conducted in political debate corpus (Lin et al 2006). (Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. As it is easy for a human to identify the perspective of an author (Lin et al, 2006), this measure facilitated the annotation task. In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science. They often belong to controversial subjects (e.g., religion, terrorism, etc.) where the same event can beseen from two or more opposing perspectives, like the IsraeliPalestinian conflict (Lin et al, 2006).
The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). 19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi, 2006) in our experiments. Currently there are about a dozen input/output conversion filters available, covering various existing data formats including the TigerXML format, the for mats of the Penn Treebank (Marcus et al, 1994), the CoNLL-X shared task format (Buchholz and Marsi, 2006), and the formats of the Latin Dependency (Bamman and Crane, 2006), Sinica (Chu Ren et al, 2000), Slovene Dependency (D? zero ski et al, 2006) (SDT), and Alpino (van der Beek et al., 2002) tree banks.  We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. Datasets and Evaluation Our experiments are run on five different languages: Chinese (ch), Danish (da), Dutch (nl), Portuguese (pt) and Swedish (sv) (da ,nl, pt and sv are free data sets distributed for the 2006 CoNLL Shared Tasks (Buchholz and Marsi, 2006)). Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. Penn Treebank (Marcus et al, 1993) the HPSG LinGo Redwoods Treebank (Oepen et al, 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). The Spanish corpus was parsed using the MST dependency parser (McDonald et al, 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al, 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005).
Bick (2006) used the lowercased FORM if the LEMMA is not available, Corston-Oliver and Aue (2006) a prefix and Attardi (2006) a stem derived by a rule-based system for Danish, German and Swedish. Use only some components, e.g. Bick (2006) uses only case, mood and pronoun subclass and Attardi (2006) uses only gender, number, person and case. The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. Speech tagger described in Dell Orletta (2009) and dependency parsed by the DeSR parser (Attardi,2006) using Support Vector Machine as learning algorithm. This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shift/Reduce parser. Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006) .Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time.  However, other non-projective parsers such as (Attardi, 2006) follow a constructive approach and can be analysed deductively. However, the goal of that transition is different from ours (selecting between projective and non projective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic for example, the LEFT ARC transition can not be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. Non-projective transitions that create dependency arcs between non-contiguous nodes have been used in the transition-based parser by Attardi (2006). DeSR (Attardi, 2006) is an incremental deterministic classifier-based parser. This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency. The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. Table 1: The number of non-projective relations of various degrees for several treebanks (training sets), as reported by the parser of Attardi (2006). We turn next to describe the equivalence between our system and the system in Attardi (2006). While in the previous sections we have described a tabular method for the transition system of Attardi (2006) restricted to transitions of degree up to two, it is possible to generalize the model to include higher degree transitions. We build upon DeSR, the shift-reduce parser described in (Attardi, 2006). (Attardi, 2006)) have been introduced for handling non-projective dependency trees: i.e., trees that can not be drawn in the plane without crossing edges. ULISSE was tested against the output of two really different data-driven parsers: the first order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm.
 Introduce through post-processing, e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al, 2006). Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. The highest score on parsing German in the CoNLL-X shared task was obtained by the system of McDonald et al (2006) with a LAS of 87.34 based on the TIGER tree bank, but we want to stress that these results are not comparable due to different data sets (and a different policy regarding the inclusion of punctuation). The constituency versions were evaluated according to the labeled recall (LR), labeled precision (LP) and labeled F-score (LF). McDonald et al (2006) use an additional algorithm. Regarding the data-driven parsers, we have made use of MaltParser (Nivre et al, 2007b) and MST Parser (McDonald et al, 2006), two state of the art dependency parsers representing two dominant approaches in data-driven dependency parsing, and that have been successfully applied to typologically different languages and tree banks (McDonald and Nivre, 2007). In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)'s parser, (McDonald et al., 2006)'s parser, and so on. But whereas the spanning tree parser of McDonald et al (2006) and the pseudo-projective parser of Nivre et al (2006) achieve this performance only with special pre or post-processing, the approach presented here derives a labeled non-projective graph in a single incremental process and hence at least has the advantage of simplicity. Moreover, it has better time complexity than the approximate second-order spanning tree parsing of McDonald et al (2006), which has exponential complexity in the worst case (although this does not appear to be a problem in practice). McDonald et al (2006) use post-processing for non-projective dependencies and for labeling. As described in (McDonald et al, 2006), we treat the labeling of dependencies as a sequence labeling problem. ULISSE was tested against the output of two really different data driven parsers: the first order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm. It should be noted that McDonald et al (2006) use a richer feature set that is incomparable to our features.  The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al, 2006).  Entries marked with are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al (2006), Martins et al (2008), Martins et al (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different). The specific graph-based model studied in this work is that presented by McDonald et al (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation. More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f (i, j, l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al, 2005a; McDonald et al, 2006).
The pseudo-projective approach (Nivre and Nilsson, 2005): Transform non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al, 2006). Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. Firstto Third-Order Features The feature templates of first to third-order features are mainly drawn from previous work on graph based parsing (McDonald and Pereira, 2006), transition-based parsing (Nivre et al, 2006) and dual decomposition-based parsing (Martins et al, 2011). includes the most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald (2012) and Zhang et al (2013). Both methods are sometimes combined (Nivre et al, 2006b). We represented features with a parameter format partly inspired by MaltParser (Nivre et al, 2006a). These parameters are identical to Nivre et al (2006b) to enable a comparison of the scores. We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X. Following (Nivre et al, 2006), the encoding scheme called Head in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. We used the base feature model defined in (Nivre et al, 2006) for all the languages but Arabic, Chinese, Czech, and Turkish. For Arabic, Chinese, and Czech, we used the same feature models used in the CoNLL-X 948 shared task by (Nivre et al, 2006), and for Turkish we used again the base feature model but extended it with a single feature: the part-of-speech tag of the token preceding the current top of the stack. Talbanken05 is a Swedish tree bank converted to dependency format, containing both written and spoken language (Nivre et al, 2006a). For each token, Talbanken05 contains information on word form, part of speech, head and dependency relation, as well as various morphosyntactic and/orlexical semantic features. As our baseline, we use the settings optimized for Swedish in the CoNLL-X shared task (Nivre et al,2006b), where this parser was the best performing parser for Swedish. MaltParser (Nivre et al, 2006) is a language independent system for data-driven dependency parsing which is freely available. It is based on a deterministic parsing strategy in combination with tree bank-induced classifiers for predicting parsing actions. For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al, 2006). We use Nivre et al, (2006)'s dependency parser. Unigram label information has been used in MaltParser (Nivre et al, 2006a; Nivre, 2006). In principle, it is restricted to projective dependency forests, but it can be used in conjunction with the pseudo-projective transformation (Nivre et al2006) in order to capture a restricted subset of non projective forests. Table 4: Comparison of the parsing accuracy (LAS and UAS, excluding punctuation) of Nivre's arc-eager parser with projective buffer transitions (NE+LBA/RBA) and the parser with the pseudo-projective transformation (Nivre et al2006) decide between both with the restricted feature in formation available for buffer nodes. To further put the obtained results into context, Table 4 compares the performance of the arc-eager parser with the projective buffer transition most suitable for each dataset with the results obtained by the parser with the pseudo-projective transformation by Nivre et al2006) in the CoNLL-X shared task, one of the top two performing systems in that event.
DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. This avoids segmentation problems encountered by DeNero et al (2006). 140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al, 2006)) which must generate the alignment on top of the segmentations. As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations. We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003). Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006).  If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006).
Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features. Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. Figure 2: Classification results with respect to d. We used GIZA++ to produce alignments, enabling us to compare using a DPR model against a baseline lexicalized reordering model (Koehn et al., 2005) that uses MLE orientation prediction and a discriminative model (Zens and Ney, 2006) that utilizes an ME framework. Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006).  The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). Following models were applied: n-gram posteriors (Zens and Ney, 2006b), sentence length model, a 6-gram LM and single word lexicon models in both normal and inverse direction. Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a). The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase. j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase. Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier.
The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006). We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English. For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference. The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006). For the bitext-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006). Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3). We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006).  We use the same method described in (Koehn and Monz, 2006) to perform the significance test. We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006). The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002). Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006). We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006). A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). described in (Koehn and Monz, 2006). For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006). The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008).
 Other models use more syntactic information (string-to-tree, tree to-string, tree-to-tree, string-to-dependency etc.) to capture the structural difference between language pairs, including (Yamada and Knight, 2001), (Zollmann and Venugopal, 2006), (Liu et. al. 2006), and (Shen et. al. 2008). In particular we have added support for Zollmann and Venugopal (2006)'s syntax-augmented machine translation. Thrax extracts both hierarchical (Chiang, 2007) and syntax-augmented machine translation (Zollmann and Venugopal, 2006) grammars. An SAMT grammar (Zollmann and Venugopal, 2006) is similar to a Hiero grammar, except that the nonterminal symbol set is much larger, and its labels are derived from a parse tree over either the source or target side in the following manner. The SAMT implementation of Zollmann and Venugopal (2006) includes a several-thousand-line Perl script to extract their rules. To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. Table 1: Rules and their sequences of phrase pairs and nonterminals Previous work has attempted to weaken the con text free assumption of the synchronous context free grammar formalism, for example using syn tactic non-terminals (Zollmann and Venugopal, 2006). Our hierarchical systems consist of a syntax-augmented system (SAMT) that includes target-language syntactic categories (Zollmann and Venugopal, 2006) and a Hiero-style system with a single non-terminal (Chiang, 2007). For example, syntax is successfully integrated into hierarchical SMT (Zollmann and Venugopal,2006). Zollmann and Venugopal (2006) started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span. Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of epsilon-containing and higher rank grammars is left to future work. Zollmann and Venugopal (2006) allow rules to be extracted where non-terminals do not exactly span a target constituent. This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation. Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments available from the original trees without learning process. One example of modifying the SCFG nonterminal set is seen in the Syntax-Augmented MT (SAMT) system of Zollmann and Venugopal (2006). SAMT (Zollmann and Venugopal, 2006) introduces heuristics to create new non-constituent labels, but these heuristics introduce many complex labels and tend to add rarely-applicable rules to the translation grammar. Syntax-Augmented Machine Translation (SAMT; Zollmann and Venugopal, 2006) solves this problem with heuristics that create new labels from the phrase structure parse: it labels for the establishment of as IN+NP+IN to show that it is the con catenation of a noun phrase with a preposition on either side. The Syntax-Augmented Machine Translation (SAMT) model (Zollmann and Venugopal, 2006) extracts more rules than the other syntactic model by allowing different labels for the rules. This restriction may be relaxed by adding constituent labels such as DET+ADJ or NPDET to group neighboring constituents or indicate constituents that lack an initial child, respectively (Zollmann and Venugopal, 2006).
In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once. Huang et al (2006) study a TSG-based tree-to-string alignment model. Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance. The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006). To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template. Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006). Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. We push the idea behind this method further and make the following contributions in this paper: We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase based system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al, 2006). We test our methods on two large-scale English-to-Chinese translation systems: a phrase-based system and our tree-to-string system (Huang et al, 2006). Our data preparation follows Huang et al (2006): the training data is a parallel corpus of 28.3M words on the English side, and a trigram language model is trained on the Chinese side. We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side. For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. Compared with its string-based counterparts, tree-based decoding is simpler and faster: there is no need for synchronous binarization (Huang et al, 2009b; Zhang et al, 2006) and tree parsing generally runs in linear time (Huang et al, 2006). Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009). The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006).
(Goldberg and Zhu, 2006) adapt semi-supervised graph-based methods for sentiment analysis but do not incorporate lexical prior knowledge in the form of labeled features. Some of the work is not related to discourse at all (e.g., lexical similarities (Takamura et al, 2007), morphosyntactic similarities (Popescu and Etzioni, 2005) and word-based measures like TF IDF (Goldberg and Zhu, 2006)). Thus, instead of directly learning a classification function, we learn a regression function - similar to (Goldberg and Zhu, 2006) - that is then used for ranking the hypotheses. To perform rating inference on reviews, Goldberg and Zhu (2006) created a graph on both labeled and unlabeled reviews, and then solved an optimization problem to obtain a smooth rating function over the whole graph. Compared with methods which do not exploit the relationship be tween samples, experiments showing advantages of graph-based learning methods can be found 1210 in (Rao and Ravichandran, 2009), (Goldberg and Zhu, 2006), (Tong et al, 2005), (Wan and Xiao,2009), (Zhu and Ghahramani, 2002) etc. When labeled data are scarce, such graph-based transductive learning methods are especially useful. Our approach accounts for intercategory relationships from the outset of classifier design, rather than addressing this issue with later adjustments. Goldberg and Zhu (2006) proposed a semisupervised learning approach to the rating inference problem in scenarios where labeled training data is scarce. First, we compare two graph-based algorithms in cross-domain SC settings: the algorithm exploited in (Goldberg and Zhu, 2006), which seeks document sentiments as an output of an optimisation problem (OPTIM) and the algorithm adopted by (Wu et al2009), that uses ranking to assign sentiment scores (RANK). The RANK algorithm (Wu et al 2009) is based on node ranking, while OPTIM (Goldberg and Zhu, 2006) determines solution of graph optimisation problem. For more details on the problem solution see (Goldberg and Zhu, 2006). Following (Goldberg and Zhu, 2006) and (Pang and Lee, 2005) we consider 2 types of document representations: feature-based: this involves weighted document features. In NLP, label propagation has been used for word sense disambiguation (Niu et al, 2005), document classification (Zhu, 2005), sentiment analysis (Goldberg and Zhu, 2006), and relation extraction (Chen et al, 2006). nodes (Goldberg and Zhu, 2006). However, similar approaches have been proven rather efficient on other tasks such as document level sentiment classification (Goldberg and Zhu, 2006) and word sense disambiguation (Agirre et al, 2006).
For assigning classes, we use the Chinese Whispers (CW) graph-clustering algorithm, which has been proven useful in NLP applications as described in (Biemann 2006). We use the co-occurrence based graph clustering framework introduced in (Biemann, 2006). We use Chinese Whispers (Biemann, 2006), a special case of MCL that performs the iteration in a more aggressive way, with an optimized linear complexity with the number of graph edges. The weight applied to each edge connecting vertices vi and vj (collocations cab ,cde) is the maximum of their conditional probabilities (max (p (cab|cde), p (cde|cab))). Finally, the graph is clustered using Chinese whispers (Biemann, 2006). To cluster the graph we used the Chinese Whispers clustering tool (Biemann, 2006), whose algorithm does not require to pre-set the desired number of clusters and is reported to outperform other algorithms for several NLP tasks. To generate the projection, sentences were represented as vectors of terms weighted by their frequency in each sentence. As described in (Biemann, 2006), the neighborhood graph is clustered with Chinese Whispers. The first is Chinese Whispers (CW; Biemann (2006)), a randomized graph-clustering algorithm which like the HRG also takes as input a graph with weighted edges. Chinese Whispers (CW) (Biemann, 2006) was used to cluster the graph. The graph is partitioned into vertex clusters representing semantic roles using a variant of Chinese Whispers, a graph-clustering algorithm proposed by Biemann (2006). A wide range of methods exist for finding partition sin graphs (Schaeffer, 2007), besides Chinese Whispers (Biemann, 2006), which could be easily applied to the semantic role induction problem. The Chinese Whispers algorithm itself (Biemann, 2006) has been previously applied to several tasks including word sense induction (Klapaftis and M., 2010) and unsupervised part-of-speech tagging (Christodoulopoulos et al, 2010). Graph partitioning is realized with a variant of Chinese Whispers (Biemann, 2006) whose details are given below. In Biemann and Teresniak (2005) and more detailed in Biemann (2006a), the Chinese Whispers (CW) Graph Clustering algorithm is described, which is a randomized algorithm with edge-linear run-time. In Biemann (2006b), the tagger output was directly compared to supervised taggers for English, German and Finnish via information-theoretic measures. Chinese Whispers (CW) (Biemann, 2006) is a parameter-free graph clustering method that has been applied in sense induction to cluster the cooccurrence graph of a target word (Biemann, 2006), as well as a graph of collocations related to the tar get word (Klapaftis and Manandhar, 2008). For clustering the graphs of CWU and CWW we employ, Chinese Whispers (Biemann, 2006). To obtain a clustering of nouns, we used Chinese Whispers (Biemann, 2006), a randomized graph-clustering algorithm.
For instance, incorporating WSD predictions into an MT decoder based on inversion transduction grammars (Wu, 1997) such as the Bracketing ITG based models of Wu (1996), Zens et al (2004), or Cherry and Lin (2007) would present an intriguing comparison with the present work. Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. Our first pruning technique is broadly similar to Cherry and Lin (2007a). For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-to-one word alignments. The heuristic method is based on the Non Compositional Constraint of Cherry and Lin (2007). Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the bitext space to constraint ITG phrases. Therefore, researches like Cherry and Lin (2007), Haghighi et al (2009) and Zhang et al (2009) tackle this problem by enriching ITG, in addition to word pairs, with pairs of phrases (or blocks). For some restricted combinatorial spaces of alignments those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al,2004) inference can be accomplished using polynomial time dynamic programs. Similarly, Cherry and Lin (2007) use ITG for pruning. Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al, 382 2009), we further reduced AER on the test set to 12.25.
This alternate decoding path model was developed by Birch et al (2007). However, Birch et al (2007) showed that this approach captures the same re-ordering phenomena as lexicalized re-ordering models, which were not included in the baseline. Birch et al (2007) then investigated source-side CCG super tag features, but did not show an improvement for Dutch-English. Lastly, Koehn and Schroeder (2007) reported improvements from using multiple decoding paths (Birch et al, 2007) to pass both tables to the Moses SMT decoder (Koehn et al, 2003), instead of directly combining the phrase tables to perform domain adaptation. We have also shown in passing that the linear interpolation of translation models may work less well for translation model adaptation than the multiple paths decoding technique of (Birch et al, 2007). Finally, Birch et al (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. Birch et al (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al, 2007). We built two separate phrase tables for the two bi-texts, and we used them in the alter native decoding path model of Birch et al (2007). Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side ,translating a morphologically-poor language (English) to a morphologically-rich language (Greek). Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the super tags on the target language side, English. Probabilistic models for using only source tags were investigated by Birch et al (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) super tags as factors on the input words, but in this case English was the target language. In order to reduce these problems, decoding needed to consider alternative paths to translation tables trained with less or no factors (as Birch et al (2007) suggested), so as to cover instances where a word appears with a factor which it has not been trained with. Supertagging encapsulates more contextual information than POS tags and Birch et al (2007) report improvements when comparing a super tag language model to a baseline using a word language model only. Hassan et al (2007) and Birch et al (2007) use super tag n-gram LMs. Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. Factored translation models have also been used for the integration of CCG super tags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).
In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights. Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate. For details, refer to (Foster and Kuhn, 2007). In (Foster and Kuhn, 2007), two kinds of linear mixture were described: linear mixture of language models (LMs), and linear mixture of translation models (TMs). Some of the results reported above involved linear TM mixtures, but none of them involved linear LM mixtures. In (Foster and Kuhn, 2007) two basic settings are investigated: cross-domain adaptation, in which a small sample of parallel in-domain text is assumed, and dynamic adaptation, in which only the current input source text is considered. Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice. In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1. (Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006). Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling. Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). Mixture-modelling for language models is well established (Foster and Kuhn, 2007). Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007).
(Callison-Burch et al, 2007) reported that the inter coder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation. While this single-scale relative ranking is perhaps faster to annotate and reaches a higher inter and intra-annotator agreement than the (absolute) fluency and adequacy (Callison-Burch et al, 2007), the technique and its evaluation are still far from satisfactory. The Moses system with a 4-gram language model and a distance-6 lexical reordering model ("lex RO") scores similarly to state-of-the-art systems of this type on the test 2007 French English data (Callison-Burch et al, 2007). This behavior appears to be consistent on the test 2007 and nc-test2007 data sets across systems (Callison-Burch et al, 2007). This evaluation was inspired by the sentence ranking evaluation in Callison-Burch et al (2007). The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008).  Callison-Burch et al (2007) show that ranking sentences gives higher inter-annotator agreement than scoring adequacy and fluency. In the WMT 2007 shared task evaluation campaign (Callison-Burch et al, 2007) domain adaptation was a special challenge. Although BLEU has played a crucial role in the progress of MT research, it is becoming evident that BLEU does not correlate with human judgement well enough, and suffers from several other deficiencies such as the lack of an intuitive interpretation of its scores. During the recent ACL-07 workshop on statistical MT (Callison-Burch et al, 2007), a total of automatic MT evaluation metrics were evaluated for correlation with human judgement. Finally, when evaluated on the datasets of the recent ACL 07 MT workshop (Callison-Burch et al, 2007). We gather the correlation results of these metrics from the workshop paper (Callison-Burch et al, 2007), and show in Table 1 the overall correlations of these metrics over the Europarl and News Commentary datasets. A complete description on WMT-07 evaluation campaign and dataset is available in Callison-Burch et al (2007). Different from Callison-Burch et al (2007), where Spear man's correlation coefficients were used, we use here Pearson's coefficients as, instead of focusing on ranking; this first evaluation exercise focuses on evaluating the significance and noisiness of the association, if any, between the automatic metrics and human-generated scores. See Callison-Burch et al (2007) for details on the human evaluation task.  Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al (2007)). Though it does, at least in principle, seem possible to mine HTER annotations for more information system comparison (Callison-Burch et al, 2007), and word alignment (Ahrenberg et al, 2003). The method of manually scoring the 11 submitted Chinese system translations of each segment is the same as that used in (Callison-Burch et al, 2007). A new human evaluation measure has been proposed to roughly estimate the productivity increase when using each of the systems in a real scenario, grounded on previous works for human evaluation of qualitative factors (Callison-Burch et al, 2007).
Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue. Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. (Koehn and Schroeder, 2007) used two language models and two translation models: one in-domain and other out-of-domain to adapt the system. (Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). There were three different adaptation measures: First, the turker-generated development set was used for optimizing the weights of the decoding meta parameters, as introduced by Koehn and Schroeder (2007). 
Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007). We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data. For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). The translation quality is measured by three MT evaluation metrics: TER (Snover et al, 2006), BLEU (Papineni et al, 2002), and METEOR (Lavie and Agarwal, 2007). We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1: Feature sets for regression learning can easily retrain the learner under different conditions, therefore enabling our method to be applied to sentence-level translation selection fro many sets of translation systems without any additional human work. The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality. This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences. The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al, 2002) with a maximum N gram length of 4, TER (Snover et al, 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter). In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures. The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows: Section 2 provides background on chart realization in Open CCG using a corpus-derived grammar. Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available.
This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entailment framework (Giampiccolo et al, 2007). The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al,2007). Recognizing textual entailment is to determine whether a sentence (sometimes a short paragraph) can entail the other sentence (Giampiccolo et al, 2007). The average accuracy of the systems in the RTE-3 challenge is around 61% (Giampiccolo et al, 2007). Of course, if examples were also annotated with explanations in a consistent format, this could form the basis of a new evaluation of the kind essayed in the pilot study in (Giampiccolo et al, 2007). We used the data from three recognizing textual entailment challenge: RTE2 (Bar-Haim et al, 2006), RTE3 (Giampiccolo et al, 2007), and RTE5, along with the standard split between training and test sets. For example, the best systems in RTE2 and RTE3 (Giampiccolo et al, 2007) have an accuracy 10% higher than the others but they generally use resources that are not publicly available. These features include whether the two strings are the same, two terms have the same stem, the similarity between the two terms either based on WordNet or distributional statistics (Lin, 1998). To learn the alignment model for nouns, we annotated the noun alignments for the development data used in PASCAL RTE-3 Challenge (Giampiccolo et al., 2007) and trained a logistic regression model based on the features in Table 1. A first step towards a more comprehensive notion of entailment was taken with RTE-3 (Giampiccolo et al,2007), when paragraph-length texts were first included and constituted 17% of the texts in the test set. Typical examples of such relations are given in (Giampiccolo et al, 2007) or those holding between question and answer. As a second application-oriented evaluation we measured the contributions of our (filtered) Wikipedia resource and WordNet to RTE inference (Giampiccolo et al, 2007). In RTE-3 (Giampiccolo et al, 2007), where some paragraph-long texts were included, inter sentential relations became relevant for correct inference. For semantically oriented tools such as SRL systems, it is important to also assess their results w.r.t. the task which they are meant support namely reasoning: Do the semantic representations built by SRL help in making the correct inferences? Can they be used, for instance, to determine whether a given sentence answers a given question? or whether the content of one sentence follow from that another? As explained in (Giampiccolo et al, 2007), entailment recognition is a first, major step towards answering these questions.  see the reports on RTE-1 (Dagan et al, 2005), RTE-2 (Bar-Haim et al, 2006), RTE-3 (Giampiccolo et al, 2007), the RTE-3 PILOT (Voorhees, 2008), RTE-4 (Giampicolo et al, 2008), and RTE-5 (TAC, 2009) The problem of bias is quite general and widely known. Given text T and hypothesis H, the task consists on determining whether or not H can be inferred by T (Giampiccolo et al, 2007). CSR axioms Several examples of the RTE3challenge can be solved by applying CSR (Table 5). The rest of this section depicts the axioms involved in detecting entailment for each pair. The RTE main task addressed this issue by including a candidate entailment pair in the test set only if multiple annotators agreed on its disposition (Giampiccolo et al, 2007). RTE organizers reported an agreement rate of about 88% among their annotators for the two-way task (Giampiccolo et al, 2007). Second, the ranking may be combined with target language information in order to choose the best translation, thus improving translation quality. We position the problem of generating alternative texts for translation within the Textual Entailment (TE) framework (Giampiccolo et al, 2007). Textual Entailment (TE) has recently become a prominent paradigm for modeling semantic inference, capturing the needs of a broad range of text understanding applications (Giampiccolo et al., 2007).
More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). (Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. The work of Chodorow et al (2007) and T & C 08 treat the tasks of preposition selection and error detection as a classification problem. A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the preposition, plus the head verb in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available (Chodorow et al, 2007). Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. While this improvement may seem small, it is in part due to the difficulty of the problem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. Preposition errors are common among new English speakers (Chodorow et al, 2007).
The object of the sense induction task of SENSEVAL-4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). We evaluate our method on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task ,i.e. supervised evaluation. We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and Soroa, 2007). We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from Onto Notes. While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 20071 (Agirre and Soroa, 2007). The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007).
To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007. For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%. The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007). Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007). Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods. The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). For SemEval 2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1. This will allow to asses its applicability to realistic tasks, such as query processing or document indexing. Experimental Set-up In order to measure accuracy, the Senseval 2007 coarse WSD dataset (Navigli et al, 2007) has been employed. We report our results in terms of precision, recall and F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al, 2007).  For the SemEval workshop, only 6 of 15 systems performed better than this baseline on the nouns (Navigli et al, 2007), all of which used MFS as a back off strategy and an external sense tagged data set. All systems performing better than the MFS used the heuristic as a back off strategy when unable to output a sense (Navigli et al, 2007). Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al, 2007)). In the most recent SemEval 2007 (Navigli et al, 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007). Two authors of (Navigli et al, 2007) independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%. We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. Table 1: Performance on SemEval-2007 coarse-grained all-words WSD (Navigli et al, 2007).
Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007). Our evaluation framework is inspired by the lexical substitution task (McCarthy and Navigli, 2007), where a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. (OOT) evaluation metrics defined by McCarthy and Navigli (2007). The first one, the LEXSUB-PARA dataset, is a small subset of the Lexical Substitution corpus (McCarthy and Navigli, 2007) that was specifically created for this task. While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over context insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. For more information on LEXSUB, see McCarthy and Navigli (2007). Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. SWAT-E is the best system for out often, as several of the items that were emphasized through duplication were also correct. The results are much higher than for LEXSUB (McCarthy and Navigli, 2007). The past work which is most similar to ours is derived from the lexical substitution track of SemEval 2007 (McCarthy and Navigli, 2007). In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus, which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). To evaluate the system's ability to come up with suitable substitutes from scratch, we use the measures designed to evaluate systems that took part in the original English lexical substitution task (McCarthy and Navigli, 2007). We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) data, and compare it to a random baseline and E&P's state of the art model. Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. We use it because we want to compare our model with E&P. P10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007). We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sentiment, using the Valentino approach and associated resources.
UNN-WePS achieved an average purity of 0.6, and inverse purity of 0.73 in Semeval Task 13, achieving seventh position out of sixteen competing systems (Artiles et al 2007). We have described a system, UNN-WePS that disambiguates individuals in web pages as required for Semeval task 13 (Artiles et al 2007). We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al 2007), WePS2 Testing (Javier et al 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). We adopted the standard data sets used in the First Web People Search Clustering Task (WePS1) (Artiles et al, 2007) and the Second Web People Search Clustering Task (WePS2) (Artiles et al, 2009). We consider the problem of disambiguating person names in a Web searching scenario as described by the Web People Search Task in SemEval 2007 (Artiles et al, 2007). Two different evaluation measures are reported as described by the task: F=0.5 is a harmonic mean of purity and inverse purity of the clustering result, and F? =0.2 is a version of F that gives more importance to inverse purity (Artiles et al, 2007). Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval 2007 WEPS task (Artiles et al, 2007) and Semeval-. In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). The data we have used for training our system were made available in the framework of the SemEval (task 13: Web People Search) competition (Artileset al, 2007). For both categories the number of target output clusters equals (number of RIPPER output clusters+ the number of documents*0.2). Although the clustering results with the best set tings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al, 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (Artiles et al, 2007). Hence the performance reported here is comparable to the official evaluation results (Artiles et al, 2007). The goal of the Web People Search task (Artiles et al 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al, 2007). The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008). Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al, 2007): a search engine user types in a person name as a query. Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query. The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work. Similar IR features are also used by other WePS systems as they are more robust to the variety of web pages (Artiles et al, 2007).
The 2007 TempEval competition tries to address this question by establishing a common corpus on which research systems can compete to find temporal relations (Verhagen et al, 2007). For instance, TempEval (Verhagen et al, 2007) only labeled relations between events that syntactically dominated each other. Our task is similar to task A and C of TempEval-1 (Verhagen et al 2007) in the sense that we attempt to identify temporal relation between events and time expressions or document dates. challenge held at the SemEval 2007 Workshop (Verhagen et al, 2007). In order to drive forward research on temporal relation identification, the SemEval 2007 shared task (Verhagen et al, 2007) (TempEval) included the following three tasks. Much recent work on temporal relations revolved around the TimeBank and TempEval (Verhagen et al., 2007). After several evaluation campaigns targeted at temporal processing of text, such as MUC, ACE TERN and TempEval-1 (Verhagen et al, 2007), the recognition and normalization task has been again newly reintroduced in TempEval-2 (Pustejovsky& amp; Verhagen, 2009). Temporal information processing is a topic of natural language processing boosted by recent evaluation campaigns like TERN2004,1 TempEval-1 (Verhagen et al, 2007) and the forthcoming TempEval-22 (Pustejovsky and Verhagen, 2009). TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. A first attempt to standardize this task was the 2007 TempEval competition (Verhagen et al, 2007). TempEval07 (Verhagen et al, 2007) integrated 14 TLINK relations into three: before, after, and. Previous research such as Verhagen et al (2007) using three reltions as target relations showed from 60% to 80% performance according to TLINKtypes. A previous module for temporal analysis was developed and integrated into the English grammar (Hagege and Tannier, 2008), and evaluated during TempEval campaign (Verhagen et al, 2007). Although we have not yet evaluated our tagging of relative dates, the system on which our current date normalization is based achieved good results in the TempEval (Verhagen et al., 2007) campaign. The TempEval track consists of three different tasks described in (Verhagen et al 2007). The relevance of temporal information has been reflected in specialized conferences (Schilder et al, 2007) and evaluation forums (Verhagen et al, 2007). See (Verhagen et al, 2007) for details. Accordingly, the performance results given in (Verhagen et al, 2007) are reported using metrics of precision, recall and F-measure. Mani et al (2006), Chambers et al (2007) and some of the TempEval 2007 participants (Verhagen et al, 2007). The main challenges involved in this task were first addressed during TempEval-1 in 2007 (Verhagen et al, 2007).
We call this setting 'SemEval' because the SemEval-2007 competition (Pradhan et al, 2007) was performed using this configuration. Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. English Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). It outperforms most of the systems participating in the task (Pradhan et al., 2007). Experimental results are provided for two datasets: the Semeval-2007 lexical sample task (Pradhanetal., 2007) and the Turk bootstrap Word Sense Inventory (TWSI1, (Biemann and Nygaard, 2010)).  In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007). We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al, 2007). The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90% (Pradhan et al, 2007). Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al, 2007). Specifically, they addressed these issues: (i) independently modeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (ii) exploiting external knowledge acquired from unlabeled data, with the purpose of drastically reducing the amount of labeled 3http: //download.wikimedia.org/enwiki/ 20090306 4 A context corresponds to a line of text in the Wikipedia dump and it is represented as a paragraph in a Wikipedia article. For unsupervised WSD (applied to text only), we use WordNet: :SenseRelate: :TargetWord, here after PBP (Patwardhan et al, 2007), the highest scoring unsupervised lexical sample word sense disambiguation algorithm at SemEval07 (Pradhan et al., 2007).
Therefore, there is nowadays a pressing need to adopt learning approaches to extend the coverage of the FrameNet lexicon by automatically acquiring new LUs, a task we call LU induction, as recently proposed at SemEval-2007 (Baker et al, 2007). We also tested our models on a realistic gold-standard set of 24 unknown LUs extracted from the SemEval-2007 corpus (Baker et al., 2007). Johansson and Nugues (2007) presented the best performing system at SemEval 2007 (Baker et al, 2007), and Das et al (2010) improved performance, and later set the current state of the art on this task (Das et al,2014). Previous shared tasks have shown that frame-semantic SRL of running text is a hard problem (Baker et al, 2007), partly due to the fact that running text is bound to contain many frames for which no or little annotated training data are available. It is one of the main reason for the performance drop of supervised SRL systems inout-of-domain scenarios (Baker et al, 2007) (Johansson and Nugues, 2008). LU induction has been integrated at SemEval-2007 as part of the Frame Semantic Structure Extraction shared task (Baker et al, 2007), where systems are requested to assign the correct frame to a given LU, even when the LU is not yet present in FrameNet. In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicate argument annotations (a superset of the SemEval shared task dataset; Baker et al, 2007). We will also use our measures in applications, to check their effectiveness in supporting various tasks, e.g. in mapping frames across Text and Hypothesis in RTE, in linking related frames in discourse, or in inducing frames for LU which are not in FrameNet (Baker et al, 2007). They used the mapping in the Semeval-2007 task on frame-semantic structure extraction (Baker et al, 2007) in order to find target words in open text and assign frames. Crespo and Buitelaar (2008) carried out an automatic mapping of medical-oriented frames to WordNet synsets applying a Statistical Hypothesis Testing to select synsets attached to a lexical unit that were statistically significant using a given reference corpus. Our parser achieves the best published results to date on the SemEval 07 FrameNet task (Baker et al, 2007). More details can be found in Baker et al (2007). Recent work on frame-semantic parsing in which sentences may contain multiple frames to be recognized along with their arguments has used the SemEval 07 data (Baker et al, 2007). Domain-oriented semantic structures are valuable assets because their representation suits information needs in the domain; however, the extraction of such structures is difficult due to the large gap between the text and these structures. On the other hand, the extraction of linguistically oriented semantics from text has long been studied in computational linguistics, and has recently been formalized as Semantic Role Labeling (Gildea and Jurafsky, 2002), and semantic structure extraction (Baker et al, 2007) (Surdeanu et al, 2008). A variety of methods have been developed for semantic role labeling with reasonably good performance (F 1 measures in the low 80s on standard test collections for English; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al, 2007) for an overview of the state-of-the-art). Considering how the performance of supervised systems degrades on out-of-domain data (Bakeret al, 2007), not to mention unseen events, semi supervised or unsupervised methods seem to offer the primary near-term hope for broad coverage semantic role labeling. We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al (2007) for an overview). A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al, 2007) and work by Das et al (2010). The approaches are too numerous to list; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al, 2007) for an overview of the state of-the-art. We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al (2007) for an overview). SVM classifiers were trained to identify the arguments and label them with appropriate roles. Recently, since the release of full-text annotations in SemEval 07 (Baker et al, 2007), there has been work on identifying multiple frames and their corresponding sets of arguments in a sentence.
Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). The marginal p (yi=k|x;theta) can be computed by dividing this score by Zx (McDonald and Satta, 2007). Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NP hard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. Second, McDonald and Satta (2007) propose an O (n5) algorithm for computing the marginals, as opposed to the O (n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves. For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees.
Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al, 2008). Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. In the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on SMT (Callison-Burch et al, 2008), called newstest 2008 in the following. This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al, 2008). PC Translator this year and also in Callison-Burch et al (2008). The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008). RWTH participated in this shared task with the two most promising metrics according to the previous experiments ,i.e. POSBLEU and POSF, and the detailed results can be found in (Callison-Burch et al, 2008). The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. The system level evaluation procedure follows WMT08 (Callison-Burch et al., 2008), which ranked each system submitted on WMT08 in three types of tasks: Rank: Human judges candidate sentence ranking order of quality. test set in WMT08 (Callison-Burch et al, 2008). For German, Spanish and Czech we use the news test sets proposed in (Callison-Burch et al 2010), for French and Italian the news test sets presented in (Callison-Burch et al 2008), for Arabic, Farsi and Turkish, sets of 2,000 news sentences extracted from the Arabic-English and English-Persian datasets and the SE-Times corpus. Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make (Callison-Burch et al, 2008). Following Callison-Burch et al (2008), we assigned a score to each of the 11 MT systems based on how of ten its translations were judged to be better than or equal to any other system. Detailed token and type statistics can be found in Callison-Burch et al (2008). the model is tuned with mert (Bertoldi, et al) 5) the official test set from ACL WMT 2008 (Callison-Burch et al, 2008), consisting of 2000 sentences, is used as test set. We followed the benchmark assessment procedure in WMT and NIST MetricsMaTr (Callison-Burch et al, 2008, 2010), assessing the performance of the propose devaluation metric at the sentence level using ranking preference consistency, which also known as Kendall's rank correlation coefficient, to evaluate the correlation of the proposed metric with human judgments on translation adequacy ranking. Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al, 2008). For details, see Callison-Burch et al (2008). only ,i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al, 2008). Both resources are taken from the shared translation task in WMT-2008 (Callison-Burch et al, 2008).
Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models: Stanford Segmenter: this model, trained by Chang et al (2008), treats CWS as a binary word boundary decision task. Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al (2008). This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008).  we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al, 2008). We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels.
The experiments led on the alignment methods were evaluated on the development corpus using MGIZA++ (Gao and Vogel, 2008), a multi-thread version of GIZA++ (Och and Ney, 2003) which also allows previously trained IBM alignments models to be applied on the development and test corpora. We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). We extend the multi-thread GIZA++ (Gao and Vogel, 2008) to load the alignments from a modified corpus file. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. is the translation probability score (as the one given for instance by GIZA++ (Gao and Vogel, 2008)). Then, we applied a parallel version of GIZA++ (Gao and Vogel, 2008) that gave us the translation dictionaries of content words only (nouns, verbs, adjective and adverbs) at word form level. We will continue exploration on these directions. The extended GIZA++ is released to the research community as a branch of MGIZA++ (Gao and Vogel, 2008), which is available online. In this section, we will explain how to build a transliteration module on the extracted transliteration pairs and how to integrate it into MGIZA++ (Gao and Vogel, 2008) by interpolating it with the t table probabilities of the IBM models and the HMM model. Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005). For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA instance, with the parallel option ,i.e. one MGIZA in stance per pair direction running in parallel. This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. The parallel sentences are aligned using MGIZA++ (Gao and Vogel, 2008) and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. For this, we create a parallel corpus consisting of n translation hypotheses and n copies of the corresponding source text, both lowercased and detokenized. We compute the word alignment with MGIZA++ (Gao and Vogel, 2008), based on the word alignment model from the primary corpus that we have previously saved to disk. After training a phrase table from the word aligned corpus with Moses, the lexical weights and translation probabilities are rescored, using the sufficient statistics (i.e. the word, phrase and word/phrase pair counts) of both the primary and the secondary corpus.
In English, this kind of typed dependencies has been introduced by de Marneffe and Manning (2008) and de Marneffe et al (2006). While previous work uses the Stanford CoreNLP toolkit to identify characters and extract typed dependencies for them, we found this approach to be too slow for the scale of our data (a total of 1.8 billion tokens); in particular, syntactic parsing, with cubic complexity in sentence length, and out-of-the-box co reference resolution (with thousands of potential antecedents) prove to be 3All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable. Bjorne et al. showed that deep dependency analyses in the well-established Stanford Dependency (SD) scheme (de Marneffe and Manning, 2008) can successfully be utilised in extracting graphs that express semantic entities as node sand relationship arguments as edges but are limited to one node per syntactic token. The native Penn TreeBank output of Bikel's and McClosky's parser was converted to the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). In this stage, we connect the triggers extracted with appropriate arguments using rules defined with the Stanford dependency (SD) scheme (de Marneffe and Manning, 2008). More recent approaches to compression introduce reordering and paraphrase operations (e.g. ,dencies (Briscoe, 2006) while there are over 50 Stanford Dependencies (de Marneffe and Manning, 2008). In this paper, we describe 1) a new dependency conversion (Section 3) of the Penn Treebank (Marcus, et al, 1993) along with the associated dependency label scheme, which is based upon the Stanford parser's popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). By far the most prominent of these is the Stanford typed dependency scheme (de Marneffe and Manning, 2008). This semantic representation can be extracted from the user input by our understanding component via a robust hybrid approach: either via a number of surface patterns containing regular expressions or via patterns reflecting the syntactic analysis of a dependency parser (de Marneffe and Manning, 2008). Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence. We now describe how we build the syntactic relatedness trie (SRT) that forms the scaffolding for the probabilistic models needed to identify sentiment-bearing words via syntactic constraints extracted from a dependency parse (Kubler et al, 2009). We use the Stanford Parser (de Marneffe and Manning, 2008) to produce a dependency graph and con sider the resulting undirected graph structure over words. 3.3.1 Dependency Structures The first set of these features include typed dependency structures (de Marneffe and Manning, 2008) which describe the grammatical relationships between words. To obtain dependency trees, we passed the Stanford constituency trees through the Stanford constituency-to-dependency converter (de Marneffe and Manning, 2008). We use both the original dependency paths and their collapsed Stanford Dependencies forms (de Marneffe and Manning, 2008). As recent work in the BioNLP 2009 shared task has shown (Kim et al,2009), domain-adapted parsing benefits information extraction systems. The native output of the C&C parser is converted into the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). We also establish the applicability of the PropBank scheme to the clinical sub language with its many atypical characteristics, and finally, we find that the PropBank scheme is compatible with the Stanford Dependency scheme of de Marneffeand Manning (2008a; 2008b) in which the under lying tree bank is annotated. The tree bank of Haverinen et al is annotated in the Stanford Dependency (SD) scheme of de Marneffe and Manning (2008a; 2008b). These relations are labeled using traditional grammatical concepts (subject, object, modifier) that are arranged into an inheritance hierarchy (de Marneffe and Manning, 2008a, Sec. In so far, recovering SD relations from phrase-structure (PS) trees have used a range of structural cues such as positions and phrase-labels (see, for instance, the software of de Marneffe and Manning (2008a)).
This strategy is similar to the one employed by Carreras et al (2008) to prune the search space of the actual parser.  This approach can be seen as trade-off between phrase based reranking experiments (Collins, 2000) and the approach of Carreras et al (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism. Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text). Carreras et al (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graph based dependency parser provides the coarse pass, with the fine pass being a far-more-expensive tree adjoining grammar.  2.2? Petrov& amp; Klein (2007) 6.2 Carreras et al (2008) Unk This Paper Baseline 100.7 Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9: Comparison of running times on the English test set, where the time for loading model sis excluded.  Suzuki et al (2009) and phrase-structure annotations in the case of Carreras et al (2008).    (Carreras et al., 2008) and edge annotation (Huang, 2008). Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al2009), or the marginals computed from a simpler parsing model (Carreras et al2008). The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al, 2008)). Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). To continue our example, the resulting entry would be as follows: es gibt? S NP there VP is To give a more formal description of how syn tactic structures are derived for phrases, first note that each parse tree t is mapped to a TAG derivation using the method described in (Carreras et al, 2008). 
Following the successful approaches taken by the participants of the CoNLL-2008 shared task (Surdeanu et al, 2008) on monolingual syntactic and semantic dependency analysis, we designed and implemented our CoNLL-2009 SRL only system with pipeline architecture. We chose to use maximum entropy algorithm in this step because of its success in the CoNLL-2008 shared task (Surdeanu et al, 2008). Our first attempt is to directly apply the state of-art SRL system (Meza-Ruiz and Riedel, 2009) that trained on the CoNLL 08 shared task dataset (Surdeanu et al, 2008), hereafter called SRL-BS, to news tweets. Domain-oriented semantic structures are valuable assets because their representation suits information needs in the domain; however, the extraction of such structures is difficult due to the large gap between the text and these structures. On the other hand, the extraction of linguistically oriented semantics from text has long been studied in computational linguistics, and has recently been formalized as Semantic Role Labeling (Gildea and Jurafsky, 2002), and semantic structure extraction (Baker et al, 2007) (Surdeanu et al, 2008). CoNLL 2008 shared task (Surdeanu et al, 2008) first introduced the predicate classification task, which can be regarded as the predicate sense disambiguation. Perhaps the most immediately promising resource is the CoNLL shared task data from 2008 (Surdeanu et al, 2008) which has syntactic dependency annotations, named-entity boundaries and the semantic dependencies model roles of both verbal and nominal predicates. These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al (2008). In 2008, the shared task (Surdeanu et al, 2008) used a unified dependency based formalism, which modeled both syntactic dependencies and semantic roles for English. In all sections, we will mention some of the differences between last year's and this year's tasks while keeping the text self-contained whenever possible; for details and observations on the English data, please refer to the overview paper of the CoNLL-2008 Shared Task (Surdeanu et al, 2008) and to the references mentioned in the sections describing the other languages. (Surdeanu et al, 2008), (Burchardt et al, 2006) and (Kawahara et al, 2002). The English corpus is almost identical to the corpus used in the closed challenge in the CoNLL-2008 shared task evaluation (Surdeanu et al, 2008). The complete merging process and the conversion from the constituent representation to dependencies is detailed in (Surdeanu et al, 2008). LGS denotes a logical subject in a passive construction (Surdeanu et al, 2008). Table 1 shows SRL performance for the local model described above, and the full global CCG-system described by Boxwell et al (2009). We use the method for calculating the accuracy of Propbank verbal semantic roles described in the CoNLL-2008 shared task on semantic role labeling (Surdeanu et al, 2008). In a second experiment, we applied the feature discovery procedure to the English corpus from CoNLL 2008 (Surdeanu et al, 2008), a dependency corpus converted from the Penn Tree bank and the Brown corpus. The CoNLL 2008 shared task (Surdeanu et al, 2008) was on joint parsing and semantic role labeling, but the best systems (Johansson and Nugues, 2008) were the ones which completely decoupled the tasks. This is a purely syntactic resource, but we can also include this tree bank in the category of multistratal resources since the PropBank (Palmer et al, 2005 )andNomBank (Meyers et al, 2004) projects have an notated shallow semantic structures on top of it. Dependency-converted versions of the Penn Tree bank, PropBank and NomBank were used in the CoNLL-2008 Shared Task (Surdeanu et al, 2008), in which the task of the participants was to produce a bistratal dependency structure consisting of surface syntax and shallow semantics. We applied the bistratal search method in Algorithm 3 on the data from the CoNLL-2008 Shared Task (Surdeanu et al, 2008). Here A0 represents the seller, and A1 represents the things sold (CoNLL 2008 shared task, Surdeanu et al, 2008). The submitted parser is simpler than the submission in which I participated at the CoNLL 2008 shared task on joint learning of syntactic and semantic dependencies (Surdeanu et al., 2008), in which we used a more complex committee based approach to both syntax and semantics (Samuelsson et al, 2008).
From the results of CoNLL-2008 shared task, the top system by (Johansson and Nugues, 2008) also used two 30 different subsystems to handle verbal and nominal predicates, respectively. One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-of-the-art syntactic parser described in (Johansson and Nugues, 2008) 7 (it is referred to Johansson) and an integrated parser described as the following (referred to MST ME). Note that the reranking may slightly improve the syntactic performance according to (Johansson and Nugues, 2008). The comparison indicates that our integrated system outputs a result quite close to the state-of-the-art by the pipeline system of (Johansson and Nugues, 2008) as the same syntactic structure input is adopted. It is worth noting that our system actually competes with two independent sub-systems of (Johansson and Nugues, 2008), one for verbal predicates, the other for nominal predicates. Each sentence was parsed by the LTH dependency parser (Johansson and Nugues, 2008a), which we trained on a Swedish tree bank (Nilsson et al, 2005). Semantic role classifiers rely heavily on lexical features (Johansson and Nugues, 2008b), and this may lead to brittleness; in order to increase robustness, we added features based on hierarchical clusters constructed using the Brown algorithm (Brown et al, 1992). The dependency parser of this demonstration is a further development of Carreras (2007) and Johansson and Nugues (2008). For the predicate identification, we used the features suggested by Johansson and Nugues (2008). Toutanova et al (2008), Johansson and Nugues (2008), and Bjorkelund et al (2009) presented importance of capturing non-local dependencies of core arguments in predicate-argument structure analysis. SRL based on FrameNet is thus not a novel task, although very few systems are known capable of completing a general frame-based annotation process over raw texts, noticeable exceptions being discussed for example in (Erk and Pado, 2006), (Johansson and Nugues, 2008b) and (Coppola et al., 2009). Most of the employed learning algorithms are based on complex sets of syntagmatic features, as deeply investigated in (Johansson and Nugues, 2008b). More recently, the-state-of-art frame-based semantic role labeling system discussed in (Johansson and Nugues, 2008b) reports a 19% drop in accuracy for the argument classification task when a different test domain is targeted (i.e. NTI corpus).  In (Johansson and Nugues, 2008b) the impact of different grammatical representations on the task of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined. In all experiments, the FrameNet 1.3 version and the dependency based system using the LTH parser (Johansson and Nugues, 2008a) have been employed. In the evaluation of the AC task, accuracy is computed over the nodes of the dependency graph, in line with (Johansson and Nugues, 2008b) or (Coppola et al, 2009). An interesting result is that a per-node accuracy of 86.3 (i.e. only 3 points under the state of-the art on the same data set, (Johansson and Nugues, 2008b)) is achieved. The above empirical findings are relevant if compared with the outcome of a similar test on the NTI collection, discussed in (Johansson and Nugues,2008b). Notice that in this paper only the training portion of the NTI data set is employed as reported in Table 2 and results are not directly comparable to (Johansson and Nugues, 2008b).
These systems were selected from WMT09 (Callison-Burch et al, 2009). To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al 2009). To train our models we use the freely available corpora (when possible): Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al 2006), DGTTM3, Opus (Tiedemann, 2009), SE-Times (Tyers and Alperen, 2010), Tehran English-Persian Parallel Corpus (Pilevar et al 2011), NewsCorpus (Callison-Burch et al 2009), UN Corpus (Rafalovitch and Dale, 2009), CzEng0.9 (Bojar and Z ?abokrtsky?, 2009), English-Persian parallel corpus distributed by ELRA4 and two ArabicEnglish datasets distributed by LDC5. We trained two SMT systems, SMT content and SMTtitle, using the Europarl V4 German-English data as training corpus, and two different development sets: one made of content sentences, News Commentaries (Callison-Burch et al 2009), and the other made of news titles in the source language which were translated into English using a commercial translation system. The recent Fr-En 109 (Callison-Burch et al, 2009) corpus aggregates huge numbers of parallel French English sentences from the web. Evaluation campaigns like WMT (Callison-Burch et al, 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. We test our metrics in the setting of the WMT 2009 evaluation task (Callison-Burch et al, 2009). Finally, we plan to repeat this experiment over other test beds with document structure, such as those from the 2009 Work shop on Statistical Machine Translation shared task (Callison-Burch et al, 2009) and the 2009 NIST MT Evaluation Campaign (Przybocki et al,2009). We use the data collected during three Workshops on Statistical Machine Translation: WMT08 (Callison Burch et al, 2008), WMT09 (Callison-Burch et al, 2009) and WMT10 (Callison-Burch et al, 2010). The results shown in the remainder of this paper are reported in terms of case insensitive BLEU which showed last year a better correlation with human judgments than case sensitive BLEU for the two languages we con sider (Callison-Burch et al, 2009). To train our models based on Moses we used the freely available corpora: Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al, 2006), Opus (Tiedemann, 2009), News Corpus (Callison-Burch et al, 2009). We train a baseline phrase-based French-English system using WMT-09 corpora (Callison-Burchetal., 2009) for training and evaluation. The main part of the corpus in this task consists of the Europarl corpus as used in the WMT evaluation (Callison-Burch et al, 2009), with some additional data collected in the scope of the project. Recently, most evaluations of machine translation systems (Callison-Burch et al, 2009) indicate that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method. There have been various evaluation metrics developed and validated for reliability in fields such as MT and summarization (Callison-Burch et al,2009). Rule-based systems could fulfill this role; they are also an attractive choice given their high quality (as judged by human evaluators) in earlier evaluations (e.g. WMT2009 (Callison-Burch et al, 2009)). While this type of evaluation has its advantages, mainly that it is fast and cheap, its correlation with human judgments is often low, especially for translation out of English (Callison-Burch et al, 2009). I mainly take advantage of this type of evaluation as part of participating with my research group in MT 13 shared tasks with large evaluation campaigns such as WMT (e.g. Callison-Burch et al (2009)). Spearman's rank correlation coefficients on the document (system) level between all the metric sand the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. Human judgement of rank has been chosen as the official determinant of translation quality for the 2009 Workshop on Machine Translation (Callison-Burch et al, 2009).
We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. Although the comparison against the Zhu system, which uses syntax-driven machine translation, shows no clear benefit for syntax-based machine translation, it may still be the case that approaches such as Hiero (Chiang et al, 2005) and Joshua (Li et al, 2009), enhanced by dissimilarity based re-ranking, would improve over our current system. We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). We use the Joshua implementation of the method for decoding (Li et al, 2009). Our hybrid machine translation system combines translation output from: a) the Lucy RBMT system, described in more detail in (Alonso and Thurmair, 2003), and b) one or several other MT systems, e.g. Moses (Koehn et al, 2007), or Joshua (Li et al., 2009). Decoding is carried out with Joshua (Li et al, 2009), an open-source platform for SCFG-based MT. Due to engineering limitations in decoding with a large grammar, we apply three additional error correction and filtering steps to every system. A Hiero-style decoder Joshua (Li et al, 2009) is also used in our experiments. However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al, 2009). Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009). We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). Joshua (Li et al, 2009): A decoder written in Java by the John Hopkins University. The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. Decoding was carried out in Joshua (Li et al, 2009), an open-source framework for parsing-based MT. We managed our experiments with LoonyBin (ClarkandLavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines. The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We perform experiments using the open-source MT toolkit Joshua (Li et al, 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). While (a) is not true in our setting because Xi is a hyper graph (which is ambiguous), Li et al (2009b) show how to approximate a hyper graph representation of p (x |yi) by an unambiguous WFSA. We report results on Chinese-to-English translation tasks using Joshua (Li et al, 2009a), an open-source implementation of Hiero (Chiang, 2007). We use Joshua, a Java-based open source implementation of the hierarchical decoder (Li et al, 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrase model.perl script packed with Moses2 (Koehn et al., 2007). In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG).
See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. Bertoldi and Federico (2009) used monolingual data for adapting existing translation models to translation of data from different domains. In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009).  For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses. Further approaches to domain adaptation for SMT include adaptation using in-domain language mod els (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007).  Three experiments involving the Twitter language model confirm Bertoldi and Federico (2009)'s findings that the language model was most helpful. As has been observed before by Bertoldi and Federico (2009), it did not matter whether the synthetic data were used on their own or in addition to the original training data.
Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. Lemma is added later in the TERplus extension (Snover et al 2009).  Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. Synonym relations are defined according to WordNet (Miller et al, 1990), and paraphrase matches are given by a lookup table use din TERplus (Snover et al, 2009). Lemma is added later in the TERplus extension (Snover et al, 2009). Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4. The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments. However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm. These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. TERp (Snover et al, 2009): Translation Edit. The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy. This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis. Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997).
Our goal was to investigate the performance of a memory-based approach to the event extraction task, using only the information available in the training corpus and modelling the task applying an approach similar to the one that has been applied to tasks like semantic role labeling (Morante et al, 2008) or negation scope detection (Morante and Daelemans, 2009). In keeping with the evaluation presented by Morante and Daelemans (2009), the number of perfectly identified negation scopes is measured separately as the percentage of correct scopes (PCS). The best reported performance to date on the BioScope full papers corpus was presented by Morante and Daelemans (2009), who achieved an F1 score of 70.9 with predicted negation signals, and an F1 score of 84.7 by feeding the manually annotated negation cues to their scope finding system. Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). The effect of negation has been broadly studied in NLP (Morante and Daelemans, 2009) and sentiment analysis (Jia et al, 2009). Still, Morante and Daelemans (2009), for example, used various classifiers (Memory-based Learners, Support Vector Machines, and Conditional Random Fields) to detect negation cues and their scope. Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. Compared with negation scope finding, negation signal finding is much simpler and has been well resolved in the literature, e.g. with the accuracy of 95.8% -98.7% on the three sub corpora of the Bioscope corpus (Morante and Daelemans, 2009). Morante et al (2008) and Morante and Daelemans (2009) pioneered the research on negation scope finding by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a negation signal. For example, given golden negation signals on the Bioscope corpus, Morante and Daelemans (2009) only got the performance of 50.26% in PCS (percentage of correct scope) measure on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS measure on the clinical reports subcorpus (6.6 words per sentence on average). Morante and Daelemans (2009) further improved the performance by combing several classifiers. For detailed statistics about the three subcorpora, please see Morante and Daelemans (2009). Following the experimental setting in Morante and Daelemans (2009), the abstracts sub corpus is randomly divided into 10 folds so as to perform 10-fold cross validation, while the performance on both the papers and clinical reports subcorpora is evaluated using the system trained on the whole abstracts subcorpus. Although Morante and Daelemans (2009) reported the performance of 95.8%-98.7% on negation signal finding, it lowers the performance of negation scope finding by about 7.29%-16.52% in PCS measure.  For example, given golden negation cues on the Bioscope corpus, Morante and Daelemans (2009a) only got the performance of 50.26% in PCS on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS on the clinical reports sub corpus (6.6 words per sentence on average). Speculation Scope Learning Similar to Morante and Daelemans (2009a), Morante and Daelemans (2009b) formulated speculation scope identification as a chunking problem which predicts whether a word in the sentence is inside or outside of the speculation scope, with proper post-processing to ensure consecutiveness of the speculation scope.   
The system uses the Illinois Entity Tagger (Ratinov and Roth, 2009) and Orthomatcher from the GATE framework for within a document co-reference resolution. We use the IOBES notation (Ratinov and Roth, 2009) to represent NE mentions with label sequences, thereby NER is formalized as a multi class classification problem in which a given token is classified into IOBES labels. Finally both the training and test data were sentence-segmented and word-tokenized by NLTK (Bird and Loper, 2004), dependency parsed by the Stanford Parser (Klein and Manning, 2003), and NER-tagged by the Illinois Named Entity Tagger (Ratinov and Roth, 2009) with an 18-label type set. Cited authors of each paper are extracted from the reference section and automatically identified by a named entity recognizer tuned for citation extraction (Ratinov and Roth, 2009). Ratinov and Roth (2009) also investigate design challenges for named entity recognition, and showed that other design choices, such as the representation of output labels and using features built on external knowledge, are more important than the learning model itself. In this work, we use the Brown clustering algorithm (Brown et al, 1992), which has been shown to improve performance in various NLP applications such as dependency parsing (Koo et al., 2008), named entity recognition (Ratinov and Roth, 2009), and relation extraction (Boschee et al., 2005). Ratinov and Roth (2009) have shown for the CoNLL-2003 shared task that Greedy decoding (i.e., beam search of width 1) is competitive to the widely used Viterbi algorithm while being over 100 times faster at the same time. Such mention type information as shown on the left of Figure 1 can be obtained from various sources such as dictionaries, gazetteers, rule-based systems (Stro ?tgen and Gertz, 2010), statistically trained classifiers (Ratinov and Roth, 2009), or some web resources such as Wikipedia (Ratinov et al, 2011). However, in practice, outputs from existing mention identification and typing systems can be far from ideal.  NE tags We automatically annotate the sentences with named entity (NE) tags using the named entity tagger of (Ratinov and Roth, 2009). From a sentence, we gather the following as candidate mentions: all nouns and possessive pronouns, all named entities annotated by the NE tagger (Ratinov and Roth, 2009), all base noun phrase (NP) chunks, all chunks satisfying the pattern: NP (PP NP)+, all NP constituents in the syntactic parse tree, and from each of these constituents, all substrings consisting of two or more words, provided the sub strings do not start nor end on punctuation marks. These mention candidates are then fed to our mention entity typing (MET) classifier for type prediction (more details in Section 6.3). For NER we used the Illinois Named Entity Tagger (Ratinov and Roth, 2009) on the highest setting (that achieved 90.5 F1 score on the CoNLL03 shared task). used the state-of-the-art named entity tagger of Ratinov and Roth (2009) to label the text. We also used the CBC word clusters of Pantel and Lin (2002) as additional gazetteers and Brown cluster features as used by Ratinov and Roth (2009) and Koo et al. The LBJ Tagger is based on a regularized average perceptron (Ratinov and Roth, 2009). For example, the average F1 of the Stanford NER (Finkel et al, 2005) drops from 90.8% (Ratinov and Roth, 2009) to 45.8% on tweets, while Liuetal. Our gazetteers comes from (Ratinov and Roth, 2009). As an unlabeled adaptation method to address feature sparsity, we add cluster-like features based on the gazetteers and word clustering resources used in (Ratinov and Roth, 2009) to bridge the source and target domain. For example, (Ratinov and Roth, 2009) only use the cluster-like features to address the feature sparsity problem, and (Finkel and Manning, 2009) only use target labeled data without using gazetteers and word-cluster information. The work (Ratinov and Roth, 2009) also combines their system with several document-level features.
The BioScope corpus has been used to train and evaluate automatic classifiers (e.g. Ozgur and Radev (2009) and Morante and Daelemans (2009)) with promising results. Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while Ozgur and Radev (2009) developed a rule-based system that exploits syntactic patterns. Most systems following Morante and Daelemans (2009) used three class labels (F) IRST, (L) AST and NONE. The following summarizes the steps we took to achieve this goal. Similarly to previous work in hedge cue detection (Morante and Daelemans, 2009), we first convert the task into a sequential labeling task based on the BIO scheme, where each word in a hedge cue is labeled as B-CUE, I-CUE, or O, indicating respectively the labeled word is at the beginning of a cue, inside of a cue, or outside of a hedge cue; this is similar to the tagging scheme from the CoNLL-2001 shared task. To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequential labeling following previous work (Morante and Daelemans, 2009). In Morante and Daelemans (2009), the hedge detection task is solved as two consecutive classification tasks. As the baseline classifier, we use the Cue Dictionary proposed in Morante and Daelemans (2009), classifying each occurrence of those words as a cue. Also, the task of hedge detection (Morante and Daelemans, 2009) can be solved separately, in the original sentences, after the interacting pairs have been found. Parts of the system are similar to that of Morante and Daelemans (2009) both make use of machine learning to tag tokens as being in a cue or a scope. The presence of potential clause ending words, used by Morante and Daelemans (2009), is included as a feature type with values: whereas, but, although, nevertheless, notwithstanding, how ever, consequently, hence, therefore, thus, instead, otherwise, alternatively, furthermore, moreover, since. Vincze et al (2008) created a publicly available annotated corpus of biomedical papers, abstracts and clinical data called BioScope, parts of which were also used as training data for the CoNLL10 shared task, building on the dataset and annotation scheme used for evaluation by Medlock and Briscoe (2007). Morante and Daelemans (2009) use the BioScope corpus to approach the problem of identifying cues and scopes via supervised machine learning. Morante and Daelemans (2009) presented a meta-learning system that finds the scope of hedge cues in biomedical texts. Shallow linguistic features are introduced in their experiments. Morante and Daelemans (2009) present their research on identifying hedge cues and their scopes. More experiments could be found in their paper (Morante and Daelemans, 2009). The main difference between the two systems is that Morante and Daelemans (2009) perform the second phase with a machine learner, whereas Ozgur and Radev (2009) perform the second phase witha rule-based system that exploits syntactic information. The approach to resolving the scopes of hedge cues that we present in this paper is similar to the approach followed in Morante and Daelemans (2009) in that the task is modelled in the same way. A difference between the two systems is that this system uses only one classifier to solve Task 2, whereas the system described in Morante and Daelemans (2009) used three classifiers and a metalearner. Another difference is that the system in Morante and Daelemans (2009) used shallow syntactic features, whereas this system uses features from both shallow and dependency syntax. It is not really possible to compare the scores obtained in this task to existing research previous to the CoNLL-2010 Shared Task, namely the results obtained by Ozgur and Radev (2009) on the BioScope corpus with a rule-based system and byMorante and Daelemans (2009) on the same corpus with a combination of classifiers. Morante and Daelemans (2009) report percentage of correct scopes for the full text data set (42.37), obtained by training on the abstracts data set, whereas the results presented in Table 5 are reported in Fmeasures and obtained in by training and testing on other corpora.
Due to the increasing demand on deep understanding of natural language text, negation recognition has been drawing more and more attention in recent years, with a series of shared tasks and workshops, however, with focus on cue detection and scope resolution, such as the BioNLP 2009 shared task for negative event detection (Kim et al, 2009) and the ACL 2010 Workshop for scope resolution of negation and speculation (Morante and Sporleder, 2010), followed by a special issue of Computational Linguistics (Morante and Sporleder, 2012) for modality and negation. In the BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009), the most frequent predicates were nominals. The BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009), the first large scale evaluation of biomedical event extraction systems, drew the participation of 24 groups and established a standard event representation scheme and datasets. The BioNLP 09 Shared Task (Kim et al., 2009) was the first shared task that provided a consistent data set and evaluation tools for extraction of such biological relations. The inclusion of full papers in the datasets is the only difference from Task of the BioNLP 2009 shared task (BioNLP09ST1) (Kim et al., 2009), which used the same task definition and abstracts dataset. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to aboutness annotation such as semantic roles (Carreras and Marquez, 2004) or the BioNLP2009 task (Kim et al, 2009) where negated relations are also labelled as positive. In recent years, several challenges and shared tasks have included the extraction of negations, typically as part of other tasks (e.g. the BioNLP 09 Shared Task 3 (Kim et al 2009)). We use molecular events as a case study and experiment on the BioNLP 09 data, which comprises a gold standard corpus of research abstracts manually annotated for events and negations (Kim et al 2009). The corpus used in this study is provided by the BioNLP 09 challenge (Kim et al 2009). The recent BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009a) (below, BioNLP shared task) represented the first community-wide step toward the extraction of fine-grained event representations of information from biomolecular domain publications (Ananiadou et al, 2010). To estimate the capacity of the newly annotated resource to support the extraction of the targeted PTM events and the performance of current event extraction methods at open-domain PTM extraction, we performed a set of experiments using an event extraction method competitive with the state of the art, as established in the BioNLP shared task on event extraction (Kim et al, 2009a; Bjorne et al., 2009). We note that while these results fall notably below the best result reported for Phosphorylation events in the BioNLP shared task, they are comparable to the best results reported in the task for Regulation and Binding events (Kim et al, 2009a), suggesting that the dataset allows the extraction of the novel PTM events with Theme and Site arguments at levels comparable to multi-argument shared task events. Lee et al (2008) present E3Miner, a tool for automatically extracting information related to ubiquitination, and Kim et al (2009b) present a preliminary study adapting the E3Miner approach to the mining of acetylation events. It should be noted that while studies targeting single specific PTM types report better results than found in the initial evaluation presented here (in many cases dramatically so), different 25extraction targets and evaluation criteria complicate direct comparison. For the BioNLP 09 Shared Task (Kim et al, 2009), the first in the ongoing series, the organisers provided the participants with automatically generated syntactic analyses for the sentences from the annotated data. The term hedging is often used as an umbrella term to refer to an array of extra-factual phenomena in natural language and is the focus of the CoNLL-2010 Shared Task on Hedge Detection. The CoNLL-2010 Shared Task on Hedge Detection (Farkas et al, 2010) follows in the steps of the recent BioNLP? 09 Shared Task on Event Extraction (Kim et al, 2009), in which one task (speculation and negation detection) was concerned with notions related to hedging in biomedical abstracts. The BioNLP? 09 Shared Task on Event Extraction (Kim et al, 2009) dedicated a task to detecting negation and speculation in biomedical abstracts, based on the GENIA event corpus annotations. The majority of our corpora are available in the common stand-off style format introduced for the BioNLP 2009 Shared Task (BioNLP 09 ST) (Kim et al, 2009). A recent shared task in biomedical text mining, the BioNLP 09 Shared Task on Event Extraction (Kim et al, 2009), showed that the biomedical natural language processing (BioNLP) community is greatly interested in heading towards the extraction of deep, semantically rich relationships. The approach of classifying identified events into whether they fall under negation or speculation was followed by Sauri and Pustejovsky (2009) and the participants of the BioNLP? 09 Shared Task (Kim et al., 2009). The BioNLP 2009 Shared Task, a recent bio-molecular event extraction task, is one such task: analysis showed that the application of a parser correlated with high rank in the task (Kim et al, 2009).
Since we were additionally interested in determining colour signatures for emotions (Section 7), we chose to annotate all of the 10,170 word-sense pairs that Mohammad and Turney (2010) used to create their word emotion lexicon. For example, Mohammad and Turney (2010) study the affects words can evoke in people's minds, while Bollen et al (2011) study various moods ,e.g., tension, depression, beyond simple dichotomy of positive and negative sentiment.  In this work, we make this transition more explicit and intentional, by introducing a novel connotation lexicon. Mohammad and Turney (2010) focussed on emotion evoked by common words and phrases. Two main differences are: (1) our work aims to discover even more subtle association of words with sentiment, and (2) we present a nearly unsupervised approach, while Mohammad and Turney (2010) explored the use of Mechanical Turk to build the lexicon based on human judgment. For instance, our co-occurrence method is an adaption of a technique applied in sentiment analysis (Turney and Littman, 2003), which has recently been shown to work for formality (Brooke et al, 2010), a dimension of stylistic variation that seems closely related to readability. Taboada et al (2011) validate their sentiment lexicon using crowd sourced judgments of the relative polarity of pairs of words, and in fact crowd sourcing has been applied directly to the creation of emotion lexicons (Mohammad and Turney, 2010). Mohammad and Turney (2010) created a crowd sourced term emotion association lexicon consisting of associations of over 10,000 word-sense pairs with eight emotions joy, sadness, anger, fear, trust, disgust, surprise, and anticipation argued to be the basic and prototypical emotions (Plutchik, 1980). The Mohammad and Turney (2010) lexicon also has associations with positive and negative polarity. According to Mohammad and Turney (2010), adverbs and adjectives are some of the most emotion-inspiring terms. Mohammad and Turney (2010) compiled emotion an notations for about 4000 words with eight emotions (six of Ekman, trust, and anticipation). The questions are phrased exactly as described in Mohammad and Turney (2010). The questions were phrased exactly as described in Mohammad and Turney (2010). If an annotator answers Q1 incorrectly, then in formation obtained from the remaining questions is discarded. Emotional connotation works exactly in the same way, but in this case word-emotion associations are taken from (Mohammad and Turney, 2010).
Crowdsourcing services, such as Amazon Mechanical Turk (MTurk) and CrowdFlower, have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010). Corpus creation using AMT has numerous precedents now; see i.e. Callison-Burch and Dredze (2010) and Heilman and Smith (2010b). A number of researchers have recently experimented with the use of Amazon Mechanical Turk (AMT) to create and evaluate human language data (Callison-Burch and Dredze, 2010). Over the last several of years, Mechanical Turk, introduced by Amazon as artificial artificial intelligence, has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). A number of mechanisms for quality control have been proposed for use with Mechanical Turk annotation (Callison-Burch and Dredze, 2010a). That paper appeared in a NAACL 2010 workshop organized by Callison-Burch and Dredze (2010), focusing on MTurk as a source of data for speech and language tasks. Callison-Burch and Dredze (2010) provide an overview of various tasks for which MTurk has been used, and offer a set of best practices for ensuring high-quality data.
The data for French, German, and Spanish are from the 2010 Workshop on Statistical Machine Translation (Callison-Burch et al, 2010). For the segment level, we followed (Callison-Burch et al, 2010) in using Kendall's rank correlation coefficient. Our choice of metrics was based on their popularity in the MT community, their performance in open competitions such as the NIST Metrics MATR challenge (NIST, 2008) and the WMT shared evaluation task (Callison-Burch et al, 2010), their availability, and their relative complementarity. On the evaluation data in (Sennrich, 2011), this system significantly outperformed MEMT (Heafield and Lavie, 2010), which was among the best-performing system combination tools at WMT 2010 (Callison-Burch et al, 2010). In this paper, we apply the same approach to a different translation scenario, namely the WMT 2011 shared task. Machine Learning methods over previously released evaluation data have been already used for tuning complex statistical evaluation metrics (e.g. SVM-Rank in Callison-Burch et al (2010)). With coefficients?= 0.60 and?= 0.23, our metric performs relatively low compared to the other metrics of WMT10 (indicatively iBLEU:?= 0.95,?= 0.39 according to Callison-Burch et al (2010). Though, it still has a position in the list, scoring better than several other reference-aware metrics (e.g.of?= 0.47 and?= 0.12 respectively) for the particular language pair. We should note that we are not capable of fully investigating this case based on the current set of experiments, because all of the systems in our data sets have shown acceptable scores (11-25 BLEU and 0.58-0.78 TERp according to Callison-Burch et al (2010)), when evaluated against reference translations. According to the system-level correlation with human judgments (Tables 1 and 2), it ranks top for the out-of-English task and very close to the top for the into-English task (Callison-Burch et al, 2010). Spearman's rank correlation coefficients on the document (system) level between all the metric sand the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks.  To train them we use the freely available corpora: Europarl (Koehn, 2005), JRC-Acquis (Steinberger et al., 2006), CzEng0.9 (Bojar and? Zabokrtsky?, 2009), Opus (Tiedemann, 2009), DGT-TM5 and News Corpus (Callison-Burch et al, 2010), which results in more than 4 million sentence pairs for each model. Our system was tested on the News test set (Callison-Burch et al, 2010) released by the organizers of the 2010 Workshop on Statistical Machine Translation.  For instance, the corpora made available for recent machine translation evaluations are in the order of 1 billion running words (Callison-Burch et al 2010). Experiments were carried out on two corpora: TED (Paul et al 2010) and News Commentary (NC) (Callison-Burch et al 2010). Training data used for ROSE is from WMT10 (Callison-Burch et al, 2010) human judged sentences. The synonym matching is computed using WordNet (Fellbaum, 1998) and the paraphrase matching is computed using paraphrase tables (Callison-Burch et al, 2010). The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. 1249 Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation (WMT10) in four directions,{Czech, French, German, Spanish} -to English (Callison-Burch et al, 2010), and we found comparable performance to the conventional confusion network based system combination in two language pairs, and statistically significant improvements in the others. We ran our experiments for the WMT10 system combination task using e four language pairs, {Czech, French, German, Spanish} -to-English (Callison-Burch et al, 2010).
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed. Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector. The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. Let us start by setting the syntactic relation that we want to focus on for the purposes of this study: following Guevara (2010) and Baroni and Zamparelli (2010), I model the semantic composition of adjacent Adjective-Noun pairs expressing attributive modification of a nominal head. Guevara (2010) and Mitchell and Lapata (2010). A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)). Guevara (2010) and Zanzotto et al (2010) propose the full additive model (full add), where the two vectors to be added are pre-multiplied by weight matrices: c= Au+Bv Since the Mitchell and Lapata and full add models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be com posed into the phrase of interest. Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzottoet al (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors: f comp R (~u, ~v)= W 1 ~u+W 2 ~v. We will further use the following equivalent formulation: f comp R (~u, ~v)= W R [~u; ~v] where W R? R d? 2dand [~u; ~v] is the vertical concatenation of the two vectors (using Matlab notation).
Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available. Recent work by Clarke et al (2010) and Liang et al. Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers. In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance. To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments. Due to space consideration, we provide a brief description (see (Clarke et al, 2010) for more details). We restrict the possible assignments to the decision variables, forcing the resulting output formula to be syntactically legal, for example by restricting active variables to be type consistent, and forcing the resulting functional composition to be acyclic and fully connected (we refer the reader to (Clarke et al, 2010) for more details). However, when trained using the noisy supervision, our method achieves substantially more accurate translations than a state-of-the-art semantic parser (Clarke et al, 2010) (specifically, 80.0% in F-Score compared to an F-Score of 66.7%). It continues sampling a specification tree for each text specification until it finds one which successfully reads all of the input examples. The second baseline Aggressive is a state-of the-art semantic parsing framework (Clarke et al, 2010). The framework repeatedly predicts hidden structures (specification trees in our case) using a structure learner, and trains the structure learner based on the execution feedback of its predictions. As in Clarke et al (2010), we obviate the need for annotated logical forms by considering the end-to-end problem of mapping questions to answers. At the same time, representations such as FunQL (Kate et al, 2005), which was used in Clarke et al (2010), are simpler but lack the full expressive power of lambda calculus. We first compare our system with Clarke et al (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs. One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010).
The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. Whereas the CoNLL-2010 shared task (Farkas et al, 2010) annotated all occurrences of weasels as uncertainty markers, we acknowledge the possibility of sources (e.g. citations) that actually nullify the weasel. We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2: Features used in sentence-level sentiment prediction.  We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations: (1) an SVM (RBF kernel) that is employed for identifying sentiment-bearing sentences (Hassan et al, 2010), and (dis) agreement detection (Yin et al, 2012) in on line debates; (2) a Linear CRF for (dis) agreement identification in broadcast conversations (Wang et al, 2011). Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. Councill et al (2010) present a supervised scope detector using their own annotation. As annotation tool, we use Jubilee (Choi et al,2010). This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora. The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note.   The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well. The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection.
They used a combination of a minimum word frequency threshold and Categorical Proportional Difference as a feature selection method and achieved the highest accuracy of 83.33% on a hand labeled test dataset. (Agarwal et al, 2011) performed three class (positive, negative and neutral) classification of tweets. We use the emoticons list provided by (Agarwal et al, 2011) in their research. We use the emoticons list provided by (Agarwal et al, 2011) in their research. Other resources for sentiment detection include the Dictionary of Affect in Language (DAL) to score the prior polarity of words, as in Agarwal et al (2011) on social media data. Johansson and Moschitti (2010) and Agarwal et al (2011) process sentences and tweets respectively. In (Agarwal et al, 2011) a study was conducted on a reduced corpus of tweets labelled manually.
In the current BioNLP 11 Shared Task1 (Kim et al, 2011), we demonstrate its generalizability to different event extraction tasks by applying what is, to a large extent, the same system to every single task and subtask. The BioNLP Shared Task 2011 (BioNLP ST 11) (Kim et al, 2011a), the follow-up event to the BioNLP 09 Shared Task (Kim et al, 2009), was organized from August 2010 (sample data release) to March 2011. For compatibility with the BioNLP ST 09 and its repeat as the GE task in 2011 (Kim et al, 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST? 09 data. To date, most approaches to the BioNLP event extraction task (Kim et al, 2011a) use a single model to produce their output. We describe the Stanford entry to the BioNLP2011 shared task on biomolecular event extraction (Kim et al, 2011a). The Infectious Diseases (ID) task of the BioNLPShared Task 2011 (Kim et al, 2011a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases. These are a superset of those targeted in the BioNLP ST 09 and its repeat, the 2011 GE task (Kim et al, 2011b). Nevertheless, extraction performance for the top systems is comparable to the state-of-the-art results for the established BioNLP ST 09 task (Miwa et al, 2010) as well as its repetition as the 2011 GE task (Kim et al, 2011b), where the highest overall result for the primary evaluation criteria was also 56% F score for the FAUST system (Riedel et al, 2011). The five top-ranking systems participated also in the GE task (Kim et al, 2011b), which involves asubset of the ID extraction targets. We participated in the BioNLP-ST 2011 (Kim et al, 2011a), and applied a graph matching-based approach (Liu et al, 2010) to tackling the Task 1 of the GE NIA event extraction (GE) task (Kim et al, 2011b), and the core task of the Epigenetics and Post-translational Modifications (EPI) task (Ohta et al, 2011), two main tasks of the BioNLP-ST 2011. This section presents our results on the GE and the EPI tasks (Kim et al, 2011b; Ohta et al, 2011) respectively. Different experimental methods in processing the obtained event rules are described for the purpose of improving the precision of both tasks and increasing the recall of the EPI task. The Epigenetics and Post-translational Modifications (EPI) task is a shared task on event extraction from biomedical domain scientific publications, first introduced as a main task in the BioNLP Shared Task 2011 (Kim et al, 2011a). The EPI task focuses on events relating to epigenetic change, including DNA methylation and hi stone methylation and acetylation (see e.g. Basic modification events are defined similarly to the PHOSPHORYLATION event type targeted in the 09 and the 2011 GE and ID tasks (Kim et al, 2011b; Pyysalo et al, 2011b), with the full task extending previously defined arguments with two additional ones, Sidechain and Contextgene. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability of PTM and DNAmethylation resources other than those specifically introduced for the task and the PHOSPHORYLATION annotations in the GE task (Kim et al, 2011b), no participant chose to apply other corpora for training. The BioNLP 2011 Shared Task ((Kim et al, 2011)) series generalized this defining a series of tasks involving more text types, domains and target event types. For example, Miwa et al (Miwa et al, 2010b) reported a significant improvement with binding events, achieving 50% of performance level. The task introduced in BioNLP-ST 2009 was re named to Genia event (GE) task, and was hosted again in BioNLP-ST 2011, which also hosted four other IE tasks and three supporting tasks (Kim et al, 2011).  There are 4 event extraction tracks: in addition to the GENIA track that again focuses on transcription factors (Kim et al, 2011b), the epigenetics and post translational modification track (EPI) focuses on events relating to epigenetic change, such as DNAmethylation and hi stone modification, as well asother common post-translational protein modifications (Ohta et al, 2011), whereas the infectious diseases track (ID) focuses on bio-molecular mechanisms of infectious diseases (Pyysalo et al, 2011a). This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al, 2011a). For Genia (GE) Task 1 (Kim et al, 2011b) we achieve the second best results.
The BioNLP Shared Task 2011 (BioNLP ST 11) (Kim et al, 2011a), the follow-up event to the BioNLP 09 Shared Task (Kim et al, 2009), was organized from August 2010 (sample data release) to March 2011. For compatibility with the BioNLP ST 09 and its repeat as the GE task in 2011 (Kim et al, 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST 09 data. The GE task (Kim et al, 2011) preserves the task definition of BioNLP-ST 2009, arranged based on the Genia corpus (Kim et al, 2008). The Infectious Diseases (ID) task of the BioNLP Shared Task 2011 (Kim et al, 2011a) is an information extraction task focusing on the biomolecular mechanisms of infectious diseases. These are a superset of those targeted in the BioNLP ST 09 and its repeat, the 2011 GE task (Kim et al, 2011b). Nevertheless, extraction performance for the top systems is comparable to the state-of-the-art results for the established BioNLP ST 09 task (Miwa et al, 2010) as well as its repetition as the 2011 GE task (Kim et al, 2011b), where the highest overall result for the primary evaluation criteria was also 56% F score for the FAUST system (Riedel et al, 2011). The five top-ranking systems participated also in the GE task (Kim et al, 2011b), which involves a subset of the ID extraction targets. We describe the Stanford entry to the BioNLP 2011 shared task on biomolecular event extraction (Kim et al, 2011a). We participated in the BioNLP-ST 2011 (Kim et al, 2011a), and applied a graph matching-based approach (Liu et al, 2010) to tackling the Task 1 of the GE NIA event extraction (GE) task (Kim et al, 2011b), and the core task of the Epigenetics and Post-translational Modifications (EPI) task (Ohta et al, 2011), two main tasks of the BioNLP-ST 2011. This section presents our results on the GE and the EPI tasks (Kim et al, 2011b; Ohta et al, 2011) respectively. Different experimental methods in processing the obtained event rules are described for the purpose of improving the precision of both tasks and increasing the recall of the EPI task. This paper presents the UMass entry to the BioNLP 2011 shared task (Kim et al, 2011a). For Genia (GE) Task 1 (Kim et al, 2011b) we achieve the second best results.  There are 4 event extraction tracks: in addition to the GENIA track that again focuses on transcription factors (Kim et al, 2011b), the epigenetics and post translational modification track (EPI) focuses on events relating to epigenetic change, such as DNAmethylation and hi stone modification, as well as other common post-translational protein modifications (Ohta et al, 2011), whereas the infectious diseases track (ID) focuses on bio-molecular mechanisms of infectious diseases (Pyysalo et al, 2011a). The BioNLP 2011 shared task GENIA Task1 (BioNLP11ST-GE1) (Kim et al, 2011) focuses on extracting events from abstracts and full papers. Domain event extraction has been advanced in particular by the BioNLP Shared Task (ST) events (Kim et al, 2011a; Kim et al, 2011b), which have introduced common task settings, datasets, and evaluation criteria for event extraction. By the primary evaluation criteria, the highest performance achieved in the 2009 task was 51.95% F-score, and a 57.46% F score was reached in the comparable 2011 task (Kim et al, 2011b). This BioNLP ST 2009 formulation of the event extraction task was followed also in three 2011 main tasks: the GE (Kim et al, 2011c), ID (Pyysalo et al,2011a) and EPI (Ohta et al, 2011) tasks. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al, 2011a). The first alternative criterion has also been previously considered in the GE task evaluation (Kim et al., 2011c); the latter has, to the best of our knowledge, not been previously considered in domain event extraction.
This paper describes our entry to the 2011 CoNLL closed task (Pradhan et al, 2011) on modeling unrestricted coreference in OntoNotes. We have updated the publicly available CoNLL coreference scorer 1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al, 2011) and 2012 (Pradhan et al, 2012) participant sin the official track, where participants had to automatically predict the mentions. The proposed BLANC is highly positively correlated with the 1http: //code.google.com/p/reference-coreference-scorers 2 The order is kept the same as in Pradhan et al (2011) and Pradhan et al (2012) for easy comparison. We follow the CoNLL2011 scheme to select TRAIN, DEV and TEST datasets (Pradhan et al,2011). (Pradhan et al, 2011) presents challenges that go beyond previous definitions of the task. An overview of all systems participating in the CONLL-2011 shared task and their results is provided by Pradhan et al (2011). In this paper we present SUCRE (Kobdani and Schutze, 2010) that is a modular coreference resolution system participating in theCoNLL-2011 Shared Task: Modeling Unrestricted Coreference in OntoNote (Pradhan et al., 2011). This paper describes our coreference resolution system participating in the close track of CoNLL 2011 shared task (Pradhan et al, 2011). The most frequent one is the constituent's head that solvers need then to extract using ad-hoc rules; see the CoNLL 2011 shared task (Pradhan et al, 2011), for instance. Our system builds on an earlier system that we evaluated in the CoNLL 2011 shared task (Pradhan et al, 2011), where we optimized significantly the solver code, most notably the mention detection step and the feature design. The CoNLL 2011 Shared Task (Pradhan et al,2011) is dedicated to modeling unrestricted coreference in OntoNotes. Both the CoNLL-2011 (Pradhan et al, 2011) and CoNLL 2012 (Pradhan et al, 2012) shared tasks focus on resolving coreference on the OntoNotes corpus. a) Non-anaphoric detection modules b) Pronominal resolution module The data used for training as well as testing was provided CoNLL-2001 shared task (Pradhan et al, 2011), (Pradhan et al, 2007) organizers. Plenty of machine learning algorithms such as Decision tree (Ng and Cardie,2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. Meanwhile, the CoNLL-2011 shared task on English language show that a well-designed rule-based approach can achieve a comparable performance as a statistical one (Pradhan et al, 2011). This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al, 2011). This was the official metric in the CoNLL-2011 shared task (Pradhan et al2011). We followed the CoNLL-2011 evaluation methodology, that is, we removed all singleton clusters, and apposition/copular relations before scoring. We evaluated the systems on three different settings: only on entity clusters, only on event clusters, and on the complete task, i.e., both entities and events. While models other than mention-pair have been proposed (Culotta et al, 2007), none performs clearly better as evidenced by recent shared evaluations such as SemEval 2010 (Recasens et al, 2010) and CoNLL 2011 (Pradhan et al, 2011). We applied this solver to the closed track of the CoNLL 2011 shared task (Pradhan et al,2011). This task (Pradhan et al, 2011) has set a harder challenge by only considering exact matches to be correct. In this paper, we present a learning approach to coreference resolution of named entities (NE), pronouns (PRP), noun phrases (NP) in unrestricted text according to the CoNLL-2011 shared task (Pradhan et al, 2011).
In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however.  Our second baseline is the Stanford resolver (Lee et al2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al2011). Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). They also outperform the learning-based systems of Sapena et al (2011) and Chang et al (2011), and perform competitively with Lee's system (Lee et al 2011). In this paper, we have chosen two coreference resolution systems: Stanford's Multi-Pass Sieve Coreference Resolution System (Lee et al, 2011) (henceforth, Stanforddcoref) and ARKref (O'Connor and Heilman, 2011). which, along with the absence of Froggy in the name gazetteer for the system (Lee et al, 2011), would lead to both precision and recall errors for Froggy, as we observed. attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. The Stanford tools perform part of speech tagging (Toutanova et al, 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al, 2005), and coreference resolution (Lee et al, 2011). We use the part-of speech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003), and the coreference resolution system (Leeetal., 2011). Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent.  For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. We filter arcs by simply adapting the sieves method proposed in (Lee et al, 2011). Sieves 2 to 7 are obtained from (Lee et al, 2011). Note that this and several other rules rely on coreference information, which we obtain from two sources: (1) chains generated automatically using the Stanford Deterministic Coreference Resolution System (Lee et al 2011) 5, and (2) manually identified coreference chains taken directly from the annotated Switchboard dialogues. In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al, 2011) is a multi-pass system that applies tiers of deterministic co reference sieves from highest to lowest precision (Lee et al, 2011).
Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al, 2011). Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. And researchers have grown increasingly concerned that automatic metrics have a strong bias towards preferring statistical translation outputs; the NIST (2008, 2010), MATR (Gao et al, 2010) and WMT (Callison-Burch et al, 2011) evaluations held during the last five years have provided ample evidence that automatic metrics yield results that are inconsistent with human evaluations when comparing statistical, rule-based, and human outputs. Provided for the Sixth EMNLP Workshop on Statistical Machine Translation (Callison-Burch et al 2011) extracted from the annotated NPs in the Penn Tree bank 3.0 corpus. These systems have been dominating the area in the recent years (Callison-Burch et al, 2011). 2 Domain Source Target# Test Language Language sets Europarl (Koehn, 2005) Fr, De, Es En 4 En Fr, De, Es KFTT (Neubig, 2011) Jp, En En, Jp 2 EMEA (Tiedemann, 2009) Da, De En 4 News (Callison-Burch et al, 2011) Cz, En, Fr, De, Es Cz, En, Fr, De, Es 3Table 2: The translation systems used for the curve fitting experiments, comprising 30 language-pair and do main combinations for a total of 96 learning curves. (Callison-Burch et al, 2011) The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks. The tables in the Appendix of Callison-Burch et al (2011) report p-values of up to 1%, computed for every pairwise comparison in the dataset. For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7). shared pilot task, this did not hold (Callison-Burch et al, 2011). To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al 2007) for the WMT 2011 German-English translation task (Callison-Burch et al 2011). Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al, 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al 2011). The example is taken from the parallel corpus of English and Haitian Kre`yol text messages used in the 2010 Shared Task for the Workshop on Machine Translation (Callison-Burch et al, 2011), which is the corpus used for evaluation in this paper. The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al, 2011). Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al, 2011). (Callison-Burch et al, 2011). For the Haitian Creole English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al, 2011). It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al, 2011). Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion net work decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al, 2011).
Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics: BLEU (Papineni et al, 2002), NIST (Doddington, 2002), TER (Snover et al, 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms). For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006). Inverted Automatic Scores: For each Spanish system output sentence, we translate it to English and get its scores of BLEU and METEOR (Denkowski and Lavie, 2011). We report two translation measures: BLEU (Papineni et al 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011). The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language. We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level.  We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level.
We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments. The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states. Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference. The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime. The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011). Inference was carried out using the language modeling library described by Heafield (2011). We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011). The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua. Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets. With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002). This was used to create a KenLM (Heafield, 2011). In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application. Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011). Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011). n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3. Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011). For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing.
This contribution has been built based on the data released for the Quality Estimation task of the Workshop on Machine Translation (WMT) 2012 (Callison-Burch et al, 2012). The two-scale scoring for adequacy and fluency used in NIST evaluation has been abandoned by some evaluation campaigns, most notably the WMT shared task series, see Koehn and Monz (2006) through Callison-Burch et al (2012) 1. Callison-Burch et al (2012) report for several automatic metrics on the whole WMT12 English-to-Czech dataset, the best of which correlates at?= 0.18. In line with our observation, Czech-to-English correlations reported by Callison-Burch et al (2012) are higher: the best metric achieves 0.28 and aver ages 0.25 across four source languages. On the other hand, it is quite possible that the WMT-style rankings taken as the gold standard are of a disputable quality themselves, see Section 3.1 or the detailed report on inter annotator agreement and a long discussion on interpreting the rankings in Callison-Burch et al (2012). It is possible that Callison-Burch et al (2012) use some what different METEOR settings apart from the different subset of the data. Nevertheless, in the context of this Quality Evaluation Shared task (see (Callison-Burch et al, 2012) for a detailed description) we have also used supervised learning as a final stage, in order to submit results which can be compared to other methods (see? 4). We investigate the use of various similarity measures for evaluating the quality of machine translated sentences. Table 2 shows the best results among the configurations we have tested (expressed using the official evaluation measures, see (Callison-Burch et al, 2012) for details). The TCD-M5P-resources-only submission ranked 5th (among 17) in the ranking task, and 5th among 19 (tied with two other systems) in the scoring task (Callison-Burch et al, 2012). Unfortunately the TCD-M5P-all submission contained an error.13 Below are the official results for TCD-M5P-resources-only and the corrected results for TCD-M5P-all: In four cases in which Google n-grams formed the reference data, the scores were computed using the wrong language (Spanish instead of English) as the reference. The WMT 2012 shared task on QE for MT (Callison-Burch et al, 2012) required participants to score and rank a set of automatic English to Spanish translations output by a state of-the-art phrase based machine translation system. An analysis of the Pearson correlation of the baseline features (Callison-Burch et al, 2012) with human quality assessments shows that the two strongest individual predictors of post-editing effort are the n-gram language model perplexities estimated on source and target sentences.
There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). This estimate could be used as a starting point for a more detailed alignment algorithm such as word_align (Dagan et al, 1993). This concept of alignment has been also used for tasks like automatic vocabulary derivation and corpus alignment (Dagan et al, 1993). In Ido Dagan et al (1993) noisy points were filtered out by deleting frequent words. However, (Dagan et al, 1993) have shown that knowledge of target-text length is not crucial to the model's performance. In the recent years, there have been a number of papers considering this or similar problems: (Brown et al, 1990), (Dagan et al, 1993), (Kay et al, 1993), (Fung et al, 1993). 
Superficially, the architecture of our system conforms to the standard emerged in natural anguage generation (NLG) (as expressed, for instance in Reiter, 1994) in that it includes the stages of content specification, text planning and surface generation (realization). This separation is now fairly standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994). The summarizer's architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. This assumes the existence of a separate higher-level process to produce such a representation, following the canonical pipeline architecture of a full generation system (Reiter, 1994). In the community of NLG, there is a broad consensus that the generation of natural language should be done in three major steps [Reiter, 1994]. Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline. Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter& amp; Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al (1999), Soon et al (2001)) .In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. Narratological aspects influence on all architectural modules [Reiter, 1994] or representation levels [Cahill et al, 2000] of NLG. The RAGS project initially set out to develop a reference architecture based on the three-stage pipeline suggested by Reiter (Reiter, 1994). In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided. Although most generation systems pipeline decisions (Reiter, 1994), we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions. Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). Furthermore, Reiter (1994), who reviews the architecture of some models of natural language generation, shows that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects. In this paper we ground on two of these common aspects, namely the distinction between what-to-say and how-tosay (De Smedt, Horacek& amp; Zock, 1996) and the use of a pipeline architecture, which divides the generation process& quot; into multiple modules, with information flowing in a &apos; pipeline &apos; fashion from one module to the next& quot; (Reiter, 1994). There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994).
(Brill, 1995) presents a rule-based part-of-speech tagger for unsupervised training corpus. The fact that observations and prior knowledge are useful for part-of-speech tagging is well understood (Brill, 1995), but the approach of estimating an initial transition model only from unambiguous word pairs is novel. In the meantime, (Brill 1995a) (Brill 1995b) proposed a method to acquire context-dependent POS disambiguation rules and created an accurate tagger, even from a very small annotated text by combining supervised and unsupervised learning.  Later results (e.g. Brill (1995)) seemed to indicate that other methods of unsupervised learning could be more effective (although the work of Banko and Moore (2004) suggests that the difference may be far less than previously assumed). Transformation based learning (TBL) (Brill, 1995) is a machine learning approach for rule learning. In its most general setting, the TBL hypothesis is not a classifier (Brill, 1995). (Brill, 1995) uses lexicon for initial annotation of the training corpus, where each word in the lexicon has a set POS tags seen for the word in the training corpus. Brill (1995b) proposed an unsupervised tagger based on transformation based learning (Brill, 1995a), achieving accuracies of above 95%. We will also study the effect of other window sizes and the combination of this unsupervised approach with minimally-supervised approaches such as (Brill 1995) (Smith and Mann 2003). Research into unsupervised part-of-speech tagging with a tag dictionary (sometimes called weakly supervised POS tagging) has been going on for many years (cf Merialdo (1994), Brill (1995)), but generally using a fairly small tag set. Unsupervised part-of-speech tagging, as defined above, has been attempted using a variety of learning algorithms (Brill 1995, Church, 1988, Cutting et. al. 1992, Elworthy, 1994 Kupiec 1992, Merialdo 1991). For our comparison of unsupervised tagging methods, we implemented the HMM taggers described in Merialdo (1991) and Kupiec (1992), as well as the UTBL tagger described in Brill (1995).
Brill and Resnik (1994) applied Error-Driven Transformation Based Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. As we have argued in Zavrel and Daelemans (1997), this corresponds exactly to the behavior of the Back-Off algorithm of Collins and Brooks (1995), so that it comes as no surprise that the accuracy of both methods is the same. The results of Brill's method on the present benchmark were reconstructed by Collins and Brooks (1995). Collins and Brooks (1995) used a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results). The perhaps underwhelming human performance is partially due to misclassifications by the Treebank assemblers who made these determinations by hand, and also unclear cases, which we discuss in the next section. Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model. Abney, Schapire, and Singer (1999) used the dataset from Collins and Brooks (1995) with a boosting algorithm and achieved 85.4% accuracy. Their algorithm also was able to order the specific data points by how much weight they were assigned by the learning algorithm. Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. Collins and Brooks (1995) used a supervised back-off model to achieve 84.5% precision on the Ratnaparkhi test set. However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition: 72.2% according to (Collins and Brooks, 1995). (Collins and Brooks, 1995) also present a model with multiple back offs. Later, Collins and Brooks (1995) achieved 84.5% accuracy by employing a backed-off model to smooth for unseen events. In our experiments, we only considered features that contained P since the preposition is the most important lexical item (Collins and Brooks, 1995). We describe the different classifiers below: cl base: the baseline described in Section 7.2clR1: uses a maximum entropy model (Ratnaparkhi et al, 1994) clBR5: uses transformation-based learning (Brill and Resnik, 1994) cl CB: uses a backed-off model (Collins and Brooks, 1995 )clSN: induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 )clHR6: uses lexical preference (Hindle and Rooth, 1993 )clR2: uses a heuristic extraction of unambiguous attachments (Ratnaparkhi, 1998) cl Pl: uses the algorithm described in this paper Our classifier outperforms all previous unsupervised techniques and approaches the performance of supervised algorithm. The accuracy is reported in (Collins and Brooks, 1995). p (Rjright; a; b) =# (R; right; a; b)# (right; a; b) (6) e.g. for the Verb-PP attachment relation pobj (following (Collins and Brooks, 1995) including the description noun 7) p (pobjjright; verb; prep ;desc: noun) =# (pobj; right; verb; prep ;desc: noun)# (right; verb; prep ;desc: noun) The distance (measured in chunks) between a head and a dependent is a limiting factor for the probability of a dependency between them. For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995). We used the same training and test data as Collins and Brooks (1995). Our approach can be seen as an extension of (Collins and Brooks, 1995) from PP-attachment to most dependency relations. Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994).
For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). The more recent set of techniques includes multiplicative weight update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al, 1993, Golding, 1995, Golding and Schabes, 1996). The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996). All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995). Table 6 shows 3An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al, 1993) for testing. A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) and performs similar to Golding and Schabes (1996). The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus. Golding (1995) builds a classifier based on a rich set of context features. Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from 140 common spelling confusions among sets such as there, their, and they &apos; re. We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).
For instance, work on word sense disambiguation i corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and ad verbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups. JIGSAW nouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. Although the assessment of semantic similarity using a dictionary database as knowledge source has been recognized as providing significant cues for word clustering (Resnik 1995b) and the determination of lexical cohesion (Morris& amp; Hirst, 1991), its relevance for word disambiguation in running text remains relatively unexplored. Resnik (1995a) defines the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus. At present, we are trying to integrate the word sense disambiguation method proposed in (Resnik, 1995) into our system. Semantic tags are assigned from on-line thesaura like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992). The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al 1994, Ng& amp; Hian 1996, and Grishman et al 1994) and for performing disambiguation (Resnik 1995 and Leacock et ai. Finally, disambiguating the direct object according to WordNet categories, e.g., Resnik (1995), would improve the accuracy of using these categories to disambiguate verbs. There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). Eventually, the sense supported by those patterns which are semantically closer to the context in question is selected as the most likely one (see, among others, [Dolan, 1994], [Resnik, 1995a, 1995b], [Agirre and Rigau, 1996], [Sanfilippo, 1997]). Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al, 1994), (Resnik, 1995)).
Following Ramshaw and Marcus (1995), the current dominant approach is formulating chunking as a classification task, in which each word is classified as the (B)eginning, (I)nside or (O) outside of a chunk. NP chunks in the shared task data are BaseNPs, which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus (1995). Transformation-based learning (TBL) was originally introduced via the Brill part-of-speech tagger (Brill, 1992) and has since been applied to a wide variety of NLP tasks, including binary phrase structure bracketing (Brill, 1993), PP-attachment disambiguation (Brill and Resnik, 1994), base NP chunking (Ramshaw and Marcus, 1995), dialogue act tagging (Samuel et al1998), and named entity re cog nition (Black and Vasilakopoulos, 2002). Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshawand Marcus, 1995), but determining sub-NP structure is rarely addressed. Ramshaw and Marcus (Ramshaw and Marcus,1995) first represented base noun phrase recognition as a machine learning problem. Both the IOB representation (Ramshaw and Marcus, 1995) and the Start/End representation (Kudo and Matsumoto, 2001) are popular. Meanwhile, it is common for NP chunking tasks to represent a chunk (e.g., NP) with two labels, the begin (e.g., B-NP) and inside (e.g., I-NP) of a chunk (Ramshaw and Marcus, 1995). They mention that the resulting shallow parse tags are somewhat different than those used by Ramshaw and Marcus (1995), but that they found no significant accuracy differences in training on either set. Training and testing were performed using the noun phrase chunking corpus described in Ramshaw & Marcus (1995) (Ramshaw and Marcus, 1995). Ramshaw and Marcus (1995) first introduced the machine learning techniques to chunking problem. After the work of Ramshaw and Marcus (1995), many machine learning techniques have been applied to the basic chunking task, such as Sup port Vector Machines (Kudo and Matsumoto, 2001), Hidden Markov Model (Molina and Pla 2002), Memory Based Learning (Sang, 2002), Conditional Random Fields (Sha and Pereira, 2003), and so on. The noun phrase extraction module uses Brill &apos; s POS tagger [Brill (1992)] and a base NPchunker [Ramshaw and Marcus (1995)]. Noun phrases were extracted using Ramshaw and Marcus &apos; s base NPchunker [Ramshaw and Marcus (1995)]. Five chunk tag sets, IOB1, IOB2, IOE1, IOE2 (Ramshaw and Marcus, 1995) and SE (Uchimoto et al, 2000), are commonly used. text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinesesegmentation (Peng et al, 2004). (IOB) encoding originating from (Ramshaw and Marcus, 1995). Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences. Next, a rule-based text chunker (Ramshaw and Marcus, 1995) is applied on the tagged sentences to further identify phrasal units, such as base noun phrases NP and verbal units VB. Given a weight vector w, the scorew? f (x, y) ranks possible labelings of x, and we denote by Yk, w (x) the set of k top scoring labelings for x. We use the standard B, I, O encoding for named entities (Ramshaw and Marcus, 1995). The NP chunks in the shared task data are base-NP chunks which are non-recursive NPs, a definition first proposed by Ramshaw and Marcus (1995).
We employ several orthographic metrics widely used in this research area: the edit distance (Levenshtein, 1965), the longest common subsequence ratio (Melamed, 1995) and the XDice metric (Brew and McKelvie, 1996). Accuracy and coverage roughly correspond to Melamed's precision and percent correct respectively (Melamed, 1995). Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al, 1996)) ,statistical alignment models (e.g. IBM models from statistical machine translation (Brown et al, 1993)), or string similarity measures (e.g. the longest common sub-sequence ratio (Melamed,1995)). This measurement is called longest common subsequence ratio [Melamed, 1995]. Similarity is normalised by dividing the length of the common subsequence by the length of the longer string (Melamed, 1995). Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: LCSR (s1 ,s2)= |LCS (s1 ,s2) |max (|s1|, |s2|) where LCS (s1 ,s2) is the longest common subsequence of s1 and s2, and |s| is the length of s. Following Nakov et al (2007), we combined the LCSR similarity measure with competitive linking (Melamed, 2000) in order to extract potential cog 76nates from the training bi-text. We used the Longest Common Subsequence Ratio (LCSR) to measure similarity (Melamed, 1995). The aligner linked two words to each other only if neither of them was on the function word list and their longest common subsequence ratio (Melamed, 1995) was at least 0.75. recognition on the bag-of-words translation task was measured directly, using Bitext-Based Lexicon Evaluation (BIBLE: Melamed, 1995). 104 kind of BIBLE evaluation has been estimated at 62% precision and 60% recall (Melamed, 1995). Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995). Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them. Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (LCSR), defined as follows: LCSR (s 1, s 2)= |LCS (s 1, s 2)| max (|s 1|, |s 2|) where LCS (s 1, s 2) is the longest common subsequence of s 1 and s 2, and |s| is the length of s. We retained as likely cognates all pairs for which LCSR was 0.58 or higher; that value was found by Kondrak et al (2003) to be optimal for a number of language pairs in the Europarl corpus. Al-Onaizan et al (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed,1995) described in (Tiedemann, 1999) as a similarity measure.
The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank:2 TreeTagger (Schmid, 1994), which uses decision trees, TnT (Brants, 2000), which uses a hidden Markov model, SVMtool (Gimenez and Ma`rquez, 2004), which is based on support vector machines, and ACOPOST (Schroder, 2002), implementing the memory-based model of Daelemans et al (1996). We use the Memory Based Tagger (Daelemans et al, 1996) System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. To investigate the effect of using automatically assigned tags, we trained MBT, a memory-based tagger (Daelemans et al,1996), on the training portions of our 10-fold cross validation experiment for the maximal data and let it predict tags for the test material. In that table, TBL stands for Brill &apos; s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw. The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al, 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al (1996). Comparison of generalization performances in terms of F-score of MBL on the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memory based learning for part-of-speech tagging as MBT (Daelemans et al, 1996). For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used. The CoNLL data differs slightly from the original Alpino tree bank as the corpus has been part-of-speech tagged using a Memory-Based-Tagger (Daelemans et al, 1996). The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al, 1999) as well as possible. The data consists of words, entity tags and part of-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al, 1996). We used the provided POS annotation in Dutch (Daelemans et al, 1996) and a minimally supervised tagger (Yarowsky and Cucerzan, 2002) for Spanish to restrict the space of words accepted by the discriminators (e.g .is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the first word in the sentence). We employ MBT, a memory-based tagger-generator and tagger (Daelemans et al, 1996) to produce apart-of-speech (PoS) tagger based on the ATB1corpus2. A particular instantiation, MBT, was proposed in (Daelemans et al, 1996). We use the Memory-Based Tagger (Daelemans et al, 1996) trained on the Brown corpus to compute the part-of speech tags. Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al, 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (Kubler and Zhekova, 2009). This fact prohibits the feeding of the training algorithms with patterns that have the form: (Tagi_2, Tagi_b Tagi, Tagi.~, Manual_Tagi), which is the ease for similar systems that learn POS disambiguation (e.g., Daelemans et al, 1996). Comparing our tree-induction algorithm and IGTREE, the algorithm used in MBT (Daelemans et al, 1996), their main difference is that IGTREE produces oblivious decision trees by supplying an a priori ordered list of best features instead of re-computing the best feature during each branching, which is our case. The MBT POS tagger (Daelemans et al, 1996) is used to provide POS information. This material was POS-tagged using MBT (Memory Based Tagger) (Daelemans et al,1996). Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set.
This includes basic linguistic problems such as morphological analysis (van den Bosch et al., 1996), parsing (Zelle and Mooney, 1996), word sense disambiguation (Mooney, 1996), and anaphora resolution (Aone and Bennett, 1996). Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney,1996) and (Pedersen and Bruce, 1997). Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by naive Bayes models (e.g., Mooney (1996), Chodorowetal. Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al, 1985, Gallant, 1991, Leacock et al, 1993, and Mooney, 1996).
Since the raw Penn Treebank data contains many inconsistencies in its annotations (cf. Ratnaparkhi, 1996), a single inconsistency in a test set tree will very likely yield a zero percent parse accuracy for the particular test set sentence. For both tree banks, we convert from constituent to dependency format using pennconverter (Johansson and Nugues, 2007), and generate POS tags using the MXPOST tagger (Ratnaparkhi, 1996). We started with a maximum entropy based tagger that uses features very similar to the ones proposed in Ratnaparkhi (1996). Ratnaparkhi (1996: 134) suggests use of an approximation summing over the training data, which does not sum over possible tags:& quot; h E f j= 2 P (~) p (ti lhi) f j (hi, ti )i=1 However, we believe this passage is in error: such an estimate is ineffective in the iterative scaling algorithm. The features that define the constraints on the model are obtained by instantiation of feature templates as in Ratnaparkhi (1996). They are a subset of the features used in Ratnaparkhi (1996). The feature templates in Ratnaparkhi (1996) that were left out were the ones that look at the previous word, the word two positions before the current, and the word two positions after the current. Model Overall Unknown Word Accuracy Accuracy Baseline, 96.72% 84.5% J Ratnaparkhi 96.63% 85.56% (1996) Table 3 Baseline model performance This table also shows the results reported in Ratnaparkhi (1996: 142) for Convenience. This may stem from the differences between the two models &apos; feature templates, thresholds, and approximations of the expected values for the features, as discussed in the beginning of the section, or may just reflect differences in the choice of training and test sets (which are not precisely specified in Ratnaparkhi (1996)). One conclusion that we can draw is that at present the additional word features used in Ratnaparkhi (1996) looking at words more than one position away from the current do not appear to be helping the overall performance of the models. Some are the result of inconsistency in labeling in the training data (Ratnaparkhi 1996), which usually reflects a lack of linguistic clarity or determination of the correct part of speech in context. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e.,? =2 in the equation above). A number of different sequential learning frameworks have been tried, yielding 96-97% accuracy: Lafferty et al (2001) experimented with conditional random fields (CRFs) (95.7% accuracy), Ratnaparkhi (1996) used a maximum entropy sequence classifier (96.6% accuracy), Brants (2000) employed a hidden Markov model (96.6% accuracy), Collins (2002) adopted an averaged perception discriminative sequence model (97.1% accuracy). Feature templates as in (Ratnaparkhi, 1996),. The best result known to us is achieved by Toutanova [2002] by enriching the feature representation of the MaxEnt approach [Ratnaparkhi, 1996]. For instance, implementing an efficient version of the MXPOST POS tagger (Ratnaparkhi, 1996) will simply involve composing and configuring the appropriate text file reading component, with the sequential tagging component, the collection of feature extraction components and the maximum entropy model component. In this bakeoff, our basic model is based on the framework described in the work of Ratnaparkhi (1996) which was applied for English POS tagging. We have explained elsewhere (Clark, 2002) how suitable features can be defined in terms of the  word ,pos-tag pairs in the context, and how maximum entropy techniques can be used to estimate the probabilities, following Ratnaparkhi (1996). Given the parallel corpus, we tagged the English words with a publicly available maximum entropy tagger (Ratnaparkhi, 1996), and we used an implementation of the IBM translation model (Al Onaizan et al, 1999) to align the words. The C&C supertagger is similar to the Ratnaparkhi (1996) tagger, using features based on words and POS tags in a five-word window surrounding the target word, and defining a local probability distribution over supertags for each word in the sentence, given the previous two super tags.
In Goodman (1996), an efficient parsing strategy is given that maximizes the expected number of correct constituents.  Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively. This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005). Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper. (Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set.  The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996). Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002). Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees. Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting.   The most probable parse can be estimated by iterative Monte Carlo sampling (Bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of a sentence (Bod 1995, Simaa'as; an 1995) or the labelled recall parse of a sentence (Goodman 1996).
The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997).  The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. 
For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. (Fung and McKeown 1997, Kikui 1999, Zhao and Vogel 2002) extracted bilingual word senses, lexicon and parallel sentence pairs from such corpora. In Fung and McKeown (1997), a translation model applied to a pair of unrelated languages (English/Japanese) with a random selection of test words, many of them multi-word terms, gives a precision around 30% when only the top candidate is proposed. For example, in Rapp (1995), Fung and McKeown (1997), Morin et. al.
WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al 2011), and semantic role labeling (Gildea and Jurafsky, 2002). I followed (Resnik, 1993)/ (Resnik, 1997) who defined selectional preference as the amount of information a verb provides about its semantic argument classes. The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept for a word from an underlying ontology such as WordNet (Resnik, 1997). Resnik (1997) described a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes.  These promising results enable a number of future researches: (1) larger scale experiments with different measures and semantic similarity models (e.g. (Resnik, 1997)); (2) improvement of the overall efficiency by exploring feature selection methods over the SK, and (3) the extension of the semantic similarity by a general (i.e. non binary) application of the conceptual density model. Techniques for automatically detecting selections preferences have been discussed in (McCarthy and Carrol, 2003) and (Resnik, 1997). A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al, 2008), textual inference (Pantel et al, 2007), word-sense disambiguation (Resnik,1997), and many more. We adopted the association measure proposed by Resnik (1993) and successfully applied to a number of tasks in NLP including word sense disambiguation (Resnik, 1997). To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). We used the training sets, test sets, and evaluation method described in (Resnik, 1997). Automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997).
In this section, we investigate the performance of two maximum entropy classifiers (Ratnaparkhi, 1997), one for determining whether a noun phrase has a determiner or not and the other for selecting the appropriate determiner if one is needed. There are two canonical parsers that fall into this category: the decision-tree parser of (Magerman, 1995), and the maximum-entropy parser of (Ratnaparkhi, 1997). The closest relative of our framework is the maximum-entropy parser of Ratnaparkhi (Ratnaparkhi, 1997). The Chinese parse trees are produced by a maximum entropy based parser (Ratnaparkhi, 1997). We demonstrate this by comparing our k-best lists to those in (Ratnaparkhi,1997), (Collins, 2000) and the parallel work by Charniak and Johnson (2005) in several ways, including oracle reranking and average number of found parses. Ratnaparkhi (1997) introduced the idea of oracle re ranking: suppose there exists a perfect re ranking scheme that magically picks the best parse that has the highest F-score among the top k parses for each sentence.  A maximum entropy parser (Ratnaparkhi, 1997) parser is then built and tested. The maximum entropy parser (Ratnaparkhi, 1997) is used in this study, for it offers the flexibility of integrating multiple sources of knowledge into a model. The maximum entropy parser (Ratnaparkhi, 1997) parses a sentence in three phases: (1) it first tags the input sentence. The SLM was trained on 20M words of WSJ text automatically parsed using the parser in (Ratnaparkhi, 1997), binarized and enriched with headwords and NT/POS tag information as explained in Section 2.2 and Section 3. The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data. The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) dier in their details but are based on similar features. Even though some parsers effectively exhibit linear behavior in sentence length (Ratnaparkhi, 1997), fast statistical parsers such as (Henderson, 2004) still take around 1.5 seconds for sentences of length 35 in tests that we made. This is a true pipeline approach, as was done in other successful parsers, e.g. (Ratnaparkhi, 1997), in that the classifiers are trained on individual decisions rather than on the overall quality of the parser, and chained to yield the global structure. Finally, our parser is in many ways similar to the parser of Ratnaparkhi (1997).  On the application side, (log) linear parsing models have the potential to supplant the currently dominant lexicalized PCFG models for parsing by allowing much richer feature sets and simpler smoothing, while avoiding the label bias problem that may have hindered earlier classifier-based parsers (Ratnaparkhi, 1997). The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). Chunks as a separate level have also been used in Collins (1996) and Ratnaparkhi (1997).
For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997). For example, Goodman (1997) suggests using a coarse grammar consisting of regular non-terminals, such as NP and VP, and then non-terminals augmented with head-word information for the more accurate second-pass grammar. In best-first parsing, this priority is called a figure-of-merit (FOM), and is based on various approximations to P (e|s), the fraction of parses of a sentence s which include an edge e (though see Goodman (1997) for an alternative notion of FOM). As in most other statistical parsing systems we therefore use the pruning technique described in Goodman (1997) and Collins (1999: 263-264) which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). Beam thresholding (Goodman, 1997) is a simple and effective technique for pruning edges during parsing. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various 893 speed-related enhancements (Goodman, 1997) have been applied. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al, 2005) and quick check (Malouf et al, 2000). The terms alpha and beta are the thresholds of the number and the beam width of lexical entries, and theta is the beam width for global thresholding (Goodman, 1997). The terms alpha and beta are the thresholds of the number and the beam width of lexical entries, and theta is the beam width for global thresholding (Goodman, 1997). We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al, 2005) and other techniques for deep parsing. However, if many iterations are required to obtain a parse, the utility of starting with a low beam and iterating becomes questionable (Goodman, 1997). A paper closely related to ours is Goodman (1997). (Solsona et al, 2002)) or to prune the search space by adjusting a beam width during parsing itself (Goodman, 1997). A prime example of this idea is from Goodman (1997), who describes a method for producing a simple but crude approximate grammar of a standard context-free grammar. However, M1 is usually not preferred in practice (Goodman, 1997). However, if combined with other inexact pruning techniques like beam-pruning (Goodman, 1997) or coarse-to-fine parsing (Charniak et al, 2006), binarization may interact with those pruning methods in a complicated way to affect parsing accuracy.
Melamed (1997b), however, proposes a method for the recognition of multi word compounds in bi texts that is based on the predictive value of a translation model.   Instead, we develop a new scoring criterion, based on Melamed (1997). Melamed (1997) and Lin (1999) have done some research on non compositional phrases discovery. Some applications that would benefit from knowing this distinction are machine translation (Imamura et al, 2003), finding paraphrases (Bannardand Callison-Burch, 2005), (multilingual) information retrieval (Melamed, 1997a), etc. Melamed (1997b) measures the semantic entropy of words using bi texts. Melamed (1997a) investigates various techniques to identify non-compositional compounds in parallel data. This measure is equivalent to translational entropy (Melamed, 1997b). Some of the most sophisticated work on this aspect of problem again seems to be that of Melamed (1997). Melamed (1997) investigates techniques for identifying non-compositional compounds in English-French parallel corpora and emphasises that translation models that take non compositional compounds into account are more accurate.
Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). The corpus-based algorithm that we used to build the semantic lexicon (Riloff and Shepherd, 1997) requires five seed words as input for each semantic category, and produces a ranked list of words that are statistically associated with each category. For more details of this algorithm, see (Riloff and Shepherd, 1997). In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997). (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. For example, Riloff and Shepherd (Riloff and Shepherd, 1997) developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind. Many of the successful methods follow the unsupervised iterative bootstrapping framework (Riloff and Shepherd, 1997). NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). The idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in Riloff and Shepherd (1997) and Roark and Charniak (1998). Both Hearst (1992) and Riloff and Shepherd (1997) use unparsed text. Riloff and Shepherd (1997) suggested using conjunction and appositive data to cluster nouns; however, they approximated this data by just looking at the nearest NP on each side of a particular NP. The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). Riloff and Shepherd (1997) presented a corpus based method that can be used to build semantic lexicons for specific categories. Following the work of (Riloff and Shepherd, 1997), we adopted the following evaluation setting.
Once the corpus has been processed, clusters are repeatedly merged using HAC with the aver age link criteria, following (Pedersen and Bruce,1997). Here we are following (Pedersen and Bruce, 1997), who likewise took this approach to feature representation. Context Representations SenseClusters supports two different representations of context, first order context vectors as used by (Pedersen and Bruce, 1997) and second order context vectors as suggested by (Schutze,1998). But our model does have a natural preference for the most frequent sense in the thesaurus training corpus, which is a useful heuristic for word sense disambiguation (Pedersen and Bruce, 1997). Previous work in word sense discrimination has shown that contexts of an ambiguous word can be effectively represented using first order (Pedersen and Bruce, 1997) or second order (Schutze, 1998) representations.   (Pedersen and Bruce, 1997) and (Pedersen and Bruce,1998) propose a (dis) similarity based discrimination approach that computes (dis) similarity among each pair of instances of the target word. (Schutze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty's method) fares well. The objective of this research is to extend previous work in discrimination by (Pedersen and Bruce, 1997), who developed an approach using agglomerative clustering. We believe that this is an aggressive number of senses for a discrimination system to attempt, considering that (Pedersen and Bruce, 1997) experimented with 2 and 3 senses, and (Schutze, 1998) made binary distinctions. Our method of name discrimination is described in more detail in (Pedersen et al, 2005), but in general is based on an unsupervised approach to word sense discrimination introduced by (Purandare and 25 Pedersen, 2004), which builds upon earlier work in word sense discrimination, including (Schutze, 1998) and (Pedersen and Bruce, 1997). For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target. An evaluation was carried out on the full 27,132 instance train+test data set using the SenseClusters evaluation methodology, which was first defined in (Pedersen and Bruce, 1997).   In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination.
Term frequency (Luhn, 1958), lexical chains (Barzilay and Elhadad, 1997), location of the sentences (Edmundson, 1969) and the cue phrases (Teufel et al, 1997) are used to determine the important lexical units. Lexical chains, which capture relationships between related terms in a document, have shown promise as an intermediate representation for producing summaries (Barzilay and Elhadad, 1997). Various applications in Natural Language Processing, such as Question Answering (Novischi and Moldovan, 2006), Topic Detection (Carthy, 2004), and Text Summarization (Barzilay and Elhadad, 1997), rely on semantic relatedness (similarity or distance) measures either based on word nets and/or corpus statistics as a resource. Ever since Morris and Hirst (1991)'s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993). For each top scored chain, Barzilay and Elhadad (1997) extract econometrics statistsical methods economic analysis case studies methods measurement evaluation statistical data data analysis cartography data collection surveys censures Figure 2. Barzilay and Elhadad (1997) segment the original text and construct lexical chains that sentence which contains the first appearance of a chain member. Conceptual units can also be defined out of more basic conceptual units, based on the co-occurrence of important concepts (Barzilay and Elhadad, 1997) or syntactic constraints between representations of concepts (Hatzivassiloglou et al, 2001). A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al, 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al, 2000). In particular, in the biomedical domain Reeve et al (2007) adapt the lexical chaining approach (Barzilay and Elhadad, 1997) to work with UMLS concepts, using the MetaMap Transfer Tool to annotate these concepts. In particular, they have successfully been used in the field of Automatic Text Summarization (Barzilay and Elhadad, 1997). But, as Barzilay and Elhadad (1997) point at, the use of a part-of-speech tagger could eliminate wrong inclusions of words such as read, which has both noun and verb entries in WordNet. So, Barzilay and Elhadad (1997) propose the first dynamic method to compute Lexical Chains. As a consequence, Silber and McCoy (2002) propose a linear time version of (Barzilay and Elhadad, 1997) lexical chaining algorithm. Their evaluation shows that their algorithm is more accurate than (Barzilay and Elhadad, 1997) and (Silber and McCoy, 2002) ones. Like in (Barzilay and Elhadad, 1997), we define a chain score which is defined in Equation 16 where |chain| is the number of words in the chain. A more fine-grained model of coherence might include proper anaphora resolution (Lee et al, 2011), which is still an unsolved task for scientific texts, and also include models of lexical coherence such as lexical chains (Barzilay and Elhadad, 1997) and entity coherence (Barzilay and Lapata, 2008). Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). In automatic text summarization, synonymous words are employed to identify repetitive information in order to avoid redundant contents in a summary (Barzilay and Elhadad, 1997). Examples include summarization (Barzilay and Elhadad, 1997), question answering (Ramakrishnan et al, 2003) and machine translation (Chan and Ng, 2007). Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions (Barzilay and Elhadad, 1997).
Also, if we want to select the most important parts of a text, sentences might prove again to be too large segments (Marcu, 1997a; Teufel and Moens, 1998): in some cases, only one of the clauses that make up a sentence should be selected for summarization.    This kind of approach has been very popular in summarization; however the difficulty of this task often requires more complex representations, and different kinds of models to learn relevance in text have been proposed, such as discourse-based (Marcu, 1997) or network-based (Salton et al, 1997) models and many others. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al, 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu, 1997), while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (Hatzivassiloglou et al, 2001). We intend to investigate any potential linkages between the word groups in the texts and other theories that provide pre-determined structures of text, such as Rhetorical Structure Theory (Marcu, 1997). RST can be used in sentence selection for single document summarization [Marcu, 1997]. Ono et al (1994), T'sou et al (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST). Ono et al (1994), T'sou et al (1992) and Marcu (1997) focus on discourse structure in summarization using the Rhetorical Structure Theory (RST, Mann and Thompson 1986). Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al, 1973), probabilistic measures for token salience (Salton et al, 1997), and the use of implicit discourse structure (Marcu,1997). Theories such as RST have been popular for sometime as a way of describing the multi-levelled rhetorical relations that exist in text, with relevant applications such as automatic summarization (Marcu, 1997) and natural language generation (Knott and Dale, 1996).
Our initial experiments with the integration of GermaNet (Hamp and Feldweg, 1997), the evolving German version of WordNet, seem to confirm the positive results described for WordNet (de Buenaga Rodriguez et al, 1997) and will thus be extended. To divide adjectives into groups, Tsvetkov et al (2014) use 13 top-level classes from the adapted taxonomy of Hundsnurscher and Splett (1982), which is incorporated in GermaNet (Hamp and Feldweg, 1997). Synonyms in GermaNet (Hamp and Feldweg, 1997) receive the score 1. GermaNet (Hamp and Feldweg, 1997) is a large lexical database, where words are associated with POS in formation and semantic sorts, which are organized in a fine-grained hierarchy. For German, we address this issue by introducing two further types of features into our model based on the GermaNet resource (Hamp and Feldweg, 1997).  This paper presents an algorithm for finding systematic polysemous classes in WordNet (Miller et al 1990) and GermaNet (Hamp and Feldweg 1997) - a semantic database for German similar to WordNet. The system currently uses two different terminological onto logies WordNet (Fellbaum, 1998) and GermaNet (Hamp and Feldweg, 1997) as chaining resources which have been mapped onto the database for mat. We select the hyper parameters of our model using an independent development set, which we extract from the lexical resource GermaNet (Hamp and Feldweg, 1997).
We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin's CogNIAC (Baldwin 1997) approach which features. This result is comparable with the results described in (Baldwin 1997). For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%). The approach for applying the rules is similar to the one proposed by Baldwin (1997). The CogNIAC algorithm (Baldwin, 1997) was designed for high-precision AR. RAP (Kennedy and Boguraev, 1996), Baldwin's pronoun resolution method (Baldwin, 1997) and Mitkov's knowledge-poor pronoun resolution approach (Mitkov, 1998b). Since the original version of CogNiac is non-robust and resolves only anaphors that obey certain rules, for fairer and comparable results we implemented the resolve-all version as described in (Baldwin, 1997). Baldwin's CogNiac (Baldwin, 1997) is a knowledge poor approach to anaphora resolution based on a set of high confidence rules which are successively applied over the pronoun under consideration. This definition is slightly different from the one used in (Baldwin, 1997) and (Gaizauskas and Humphreys, 2000). As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison. Evaluation results for both evaluation modes are 76 given in traditional precision, recall and f-score, which are similar to (Baldwin, 1997). Pronoun resolution is carried out using the Glencova Pronoun Resolution algorithm (Halpin et al, 2004), based on a series of rules similar to the CogNIAC engine (Baldwin, 1997), but without gender information based rules since this is not provided by the Penn Treebank tag set. As a consequence, current anaphor resolution implementations mainly rely on constraints and preference heuristics which employ information originated from morphosyntactic or shallow semantic analysis ,e.g. in Baldwin (1997).
(Gonzalo et al., 1998) pointed out some more weaknesses of WordNet for Information Retrieval purposes, in particular the lack of domain information and the fact that sense distinctions are excessively fine-grained for the task. Then, following previous studies (e.g., (Gonzaloet al, 1998)), we use the synsets relations in Word Net for query expansion. The LKB can be used, among others, for monolingual and cross-lingual information retrieval, which has been demonstrated in other projects (Gonzalo et al, 1998). Gonzalo et al (1998) cite this failure to model related senses in order to explain why their study into the effects of ambiguity showed radically different results to Sanderson (1994). Gonzalo et al (1998) showed in an experiment, where words were manually disambiguated, that a substantial increase in performance is obtained when query words are disambiguated, before they are expanded. (Gonzalo et al, 1998) demonstrates an increment in performance over an IR test collection using the sense data contained in SemCor over a purely term based model. The KB can be used, among others, for monolingual and cross-lingual information retrieval, which was demonstrated by (Gonzalo et al, 1998). In another work, Gonzalo et al (1998) used a manually sense annotated corpus, SemCor, to study the effects of incorrect disambiguation.
In the Conceptual Case Frame Acquisition project (RiloffandSchmelzenbach, 1998), extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions. In related unsupervised tasks, Riloff and colleagues have learned case frames for verbs (e.g., Riloff and Schmelzenbach, 1998), while Gildea (2002) has learned role slot mappings (but does not apply the knowledge for the labelling task). There are number of different existing approaches for identifying semantic roles, varying from traditional parsing approaches, for example using HPSG grammars and Lexical Functional Grammars, that strongly rely on manually developed grammars and lexicons, to data-driven approaches, for example AutoSlog (Riloff and Schmelzenbach, 1998). Applying such a model to information extraction, in AutoSlog Riloff (1993) builds a list of patterns for filling in semantic slots in a specific domain, as well as a method for automatic acquisition of case frames (Riloff and Schmelzenbach, 1998).
Some of the data comes from the parsed files 2-21 of the Wall Street Journal Penn Treebank corpus (Marcus et al, 1993), and additional parsed text was obtained by parsing the Wall Street Journal text using the parser described in Charniak et al (1998). Using this information, the model described in (Charniak et al 1998) is P (s|h, t, l). Charniak et al (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Therein, the idea of coarse-to-fine parsing (Charniak et al, 1998) is extended to handle the repeated parsing of the same sentences. One paper that focuses on efficiency of statistical parsing is Charniak et al (1998). Charniak et al (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. This binarization process is similar to the one described in (Charniak et al, 1998). Here, we observe an effect seen in previous work (Charniak et al (1998), Petrov and Klein (2007), Petrov et al (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). Two grammars are equivalent if they define the same probability distribution over strings (Charniak et al, 1998). As shown in Charniak et al (1998), we can binarize explicitly and use intermediate symbols to replace dotted rules in chart parsing. Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. I-TRIE is a non-deterministic left branching trie with weights on rule entry as in Charniak et al (1998). Following (Charniak et al, 1998), we parsed unseen sentences of length 18-26 from the Penn Treebank, using the grammar induced from the remainder of the treebank. On the other hand, the more complex, tuned FOM in (Charniak et al, 1998) is able to parse all of these sentences using around 2K edges, while BF requires 7K edges. The complex FOMs in (Charniak et al, 1998) require somewhat more online computation to assemble.  In the training phase, each target-style parse tree in the training data is transformed into a binary tree (Charniak et al, 1998) and then decomposed into a (golden) action-state sequence. This measure is used by Charniak et al (1998) and Klein and Manning (2003b). With respect to chart parsing, (Charniak et al, 1998) report that their parser can achieve good results while producing about three times tile mininmm number of edges required to produce the final parse. Related approaches are used in Hall (2004) and Charniak and Johnson (2005).
 A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003).  For instance, maximum entropy may be used when a high diversity of knowledge sources are to be taken into account (Borthwick et al, 1998). It is straightforward to see that this problem may be resolved using dynamic programming, as did Borthwick et al (1998).
Treebank texts contain complete structural parsers, POS tags, and annotation of the antecedents of definite pronouns (added by Ge et al 1998). Ge et al (1998) implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs's (1978) seminal syntax-based pronoun resolution algorithm. Ge et al [1998] also present a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model. Ge et al (1998) uses a non-parametrized statistical model to find the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank. Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al, 1998) assumes that correct parse trees are given. In other cases, these modules are integrated by means of statistical (Ge et al, 1998) or uncertainty reasoning techniques (Mitkov, 1997). (Ge et al 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. Thus the size of the annotated data (3,115 personal pronouns, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English coreferential expressions in (Ng and Cardie, 2002)). Exceptions are existant but few (2.5%): abstract pronouns (such as that in English) referring to non neuter or plural NPs, plural pronouns co-referring with singular collective NPs (Ge et al, 1998), antecedent and anaphor matching in natural gender rather than grammatical gender. Ge et al (1998) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already. Like (Ge et al, 1998), Strube (1998) evaluates on ideal hand annotated data.  Ge et al (1998)'s probabilistic approach combines three factors (aside from the agreement filter): the result of the Hobbs algorithm, Mention Count dependent on the position of the sentence in the article, and the probability of the antecedent occur ring in the local context of the pronoun. The choice of entities may reasonably be considered to be independent given the mixing weights, but how we realize an entity is strongly dependent on context (Ge et al, 1998). Ge et al (1998) exploit a similar idea to assign gender to proper mentions. Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference annotated corpus (Ge et al, 1998). There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al, 1998), or contextual role-knowledge (Bean and Riloff, 2004). Incorporating context only through the governing constituent was also done in (Ge et al, 1998). Ge et al (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. Their factors are taken from Ge et al (1998), with two exceptions.
Mellish et al (1998) investigate the problem of determining a discourse tree for a set of elementary speech acts which are partially constrained by rhetorical relations. Mellish et al (1998) (and subsequently Karamanis and Manurung 2002) advocate genetic algorithms as an alternative to exhaustively searching for the optimal ordering of descriptions of museum artefacts. Following previous work (Mellish et al, 1998) we used a single fitness function that scored candidates based on their coherence. The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. For example, the measure from (Mellish et al, 1998) looks at the entire discourse up to the current transition for some of their cost factors. Mellish et al (1998) advocate stochastic search as an alternative to exhaustively examining the search space. As in the case of Mellish et al (1998) we construct an acceptable ordering rather than the best possible one. Mellish et al (1998) made the point that even this restricted approach would soon become intractable with more than a small set of facts when one allows weak RST relations such as Joint and Elaboration into the model. In the late 1990s, Chris Mellish implemented the first stochastic text planner (Mellish et al 1998). The evaluation function of Mellish et al (1998) also was calculated over a sum of local features of the tree, although a wider set of features were involved. For instance, the evaluation function of Mellish et al (1998) assigned +3 for each instance of subject-repetition. Genetic algorithms are also used in [Mellish et al, 1998] where the authors state the problem of given a set of facts to convey and a set of rhetorical relations that can be used to link them together, how one can arrange this material so as to yield the best possible text. Mellish et al (1998) advocate stochastic search methods for document structuring.
However, as various researchers have pointed out (Harabagiu et al, 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. Work on the Extended WordNet project (Harabagiu et al, 1999) is achieving substantial progress in making the information in WordNet more explicit. For example, in eXtended WordNet (Harabagiu et al 1999), the rich glosses in WordNet are enriched by disambiguating the nouns, verbs, adverbs, and adjectives with synsets. In the sections to follow we describe a mechanism for automating the extraction of these relationships (in the same vein as (Harabagiu et al 1999), and for using them to generative apt interpretations for metaphors involving WordNet entries. Harabagiu et al (1999) proposed a scheme for attaching sense tags to predicates within the framework of transforming WordNet glosses into a logical form. Endeavors such as that of Harabagiu et al (1999), in which each of the textual glosses in WordNet (Fellbaum, 1998) is linguistically analyzed to yield a sense-tagged logical form, is an example of the former approach. For example, in eXtended WordNet (Harabagiu et al 1999), the glosses in WordNet are enriched by disambiguating the nouns, verbs, adverbs, and adjectives with synsets. We obtain the data of the I relation from eXtended WordNet (Harabagiu et al, 1999), an automatically sense-disambiguated version of WordNet in which every term occurrence in every gloss is linked to the synset it is deemed to belong to. The transformation of WordNet into a graph based on the I relation would of course be nontrivial, but is luckily provided by eXtended WordNet (Harabagiu et al, 1999), a publicly available version of WordNet in which (among other things) each term sk occurring in a WordNet gloss (except those in example phrases) is lemmatized and mapped to the synset in which it belongs. The eXtended WordNet (Harabagiu et al, 1999) project aims to transform the WordNet glosses into a format that allows the derivation of additional semantic and logic relations. In parallel, efforts have been made to enrich WordNet by adding information in glosses (Harabagiu et al, 1999). Hence, it is not very surprising that they were criticized, as in (Harabagiu et al, 1999), for not being suitable for Natural Language Processing. They use sense disambiguated glosses provided by eXtended WordNet (Harabagiu et al, 1999) to link synsets by starting with positive (or negative) sentiment concepts in order to find other concepts with positive (or negative) sentiment values.
To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al, 1999). The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al, 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al, 2003). We view our use of part-of-speech patterns as a natural extension to the introduction of structural elements to statistical machine translation by Wang [1998] and Och et al [1999]. The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al, 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al, 2003). Zens et al (2004) introduce a left-to-right decoding algorithm with ITG constraints on the alignment template system (Och et al, 1999). A significant source of errors in statistical machine translation is the word reordering problem (Och et al, 1999). The blocks are simpler than the alignment templates in (Och et al, 1999) in that they do not have any internal structure. We take the intersection of the two alignments as described in (Och et al, 1999). A similar block selection scheme has been presented in (Och et al, 1999). We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al, 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005). The phrase extraction heuristic then extracts all the bi phrases that are compatible with the word alignment (Och et al, 1999). Second, using a heuristic proposed in (Och et al, 1999), all the aligned phrase pairs (x?, a?, y?) satisfying the following criteria are extracted: (1) x? and y? consist of consecutive words of x and y, and both have length at most k, (2) a? is the alignment between words of x? and y? induced by a, (3) a? contains at least one link, and (4) there are no links in a that have just one end in x? or y?.  Only phrases that conform to the so-called consistent alignment restrictions (Och et al, 1999) are extracted. Och et al (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. Re-ordering effects across languages have been modeled in several ways, including word-based (Brown et al, 1993), template-based (Och et al, 1999) and syntax-based (Yamada, Knight, 2001). To implement our phrase extraction technique, the maximum approximation alignments were combined with the union operation as described in (Och et al, 1999), resulting in a dense but inaccurate alignment map as measured against a human aligned gold standard. This method of phrase pair extraction was originally described by Och et al (1999). Once an alignment is obtained, phrases which satisfy the inverse projection constraint are extracted (although earlier this constraint was called consistent alignments (Och et al, 1999)). The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data (Och et al, 1999).
Cardie and Wagstaff (1999) combined the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm. Cardie and Wagstaff (1999) describe an unsupervised clustering approach to noun phrase coreference resolution in which features are assigned to single noun phrases only. The feature semantic class used by Cardie and Wagstaff (1999) seems to be a domain-dependent one which can only be used for the MUC domain and similar ones. Cardie and Wagstaff (1999) report a performance of 53.6% F-measure (evaluated according to Vilain et al (1995)). However, the cost is the decrease in performance to about 53% F-measure on the same data (Cardie and Wagstaff, 1999) which may be unsuitable for a lot of tasks. Although approaches to coreference resolution that rely only on clustering could easily enforce transitivity (as in Cardie and Wagstaff (1999)), they have not performed as well as state-of-the-art approaches to coreference.  Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. The closest comparable unsupervised system is Cardie and Wagstaff (1999) who use pairwise NP distances to cluster document mentions. Similarly, approaches to coreference resolution (Cardie and Wagstaff, 1999) use clustering to identify groups of references to the same entity. The system of Cardie and Wagstaff (1999) uses the node distance in WordNet (with an upper limit of 4) as one component in the distance measure that guides their clustering algorithm. For the first subtask we use the same set of features as in Cardie and Wagstaff (1999). Coreference resolution on text datasets is well studied (e.g., (Cardie and Wagstaff, 1999)). We employ a set of verbal features that is similar to the features used by state-of-the-art coreference resolution systems that operate on text (e.g., (Cardieand Wagstaff, 1999)). Coreference resolution is often performed in two phases: a binary classification phase, in which the likelihood of coreference for each pair of noun phrases is assessed; and a partitioning phase, in which the clusters of mutually coreferring NPs are formed, maximizing some global criterion (Cardie and Wagstaff, 1999). The verbal features that we have included are a representative sample from the literature (e.g., (Cardie and Wagstaff, 1999)). An unsupervised approach to the resolution of definite NPs was applied by Cardie and Wagstaff (1999). Cardie and Wagstaff (1999) describe an unsupervised clustering approach to noun phrase coreference resolution in which features are assigned to single noun phrases only. The feature semantic class used by Cardie and Wagstaff (1999) seems to be a domain-dependent one which can only be used for the MUC domain and similar ones. Cardie and Wagstaff (1999) report a performance of 53.6% F-measure (evaluated according to Vilain et al (1995)).
Some have assumed only partially tagged training corpora (Merialdo, 1994), while others have begin with small tagged seed word lists (such as Collins and Singer (1999) and Cucerzan and Yarowsky (1999) for named-entity tagging). The bootstrapping methods for language independent NER of Cucerzan and Yarowsky (1999) have a similar effect. Cucerzan and Yarowsky (1999) exploit morphological and contextual patterns to propose a language-independent solution to NER. Cucerzan and Yarowsky (1999) built a cross language NER, and the performance on English was low compared to supervised single-language NER such as Identi Finder. It has been previously attempted by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999). We can also exploit what Cucerzan and Yarowsky (1999) call the one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document.  Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora. The NER task for Hindi has been explored by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999). Tries have previously been used in both supervised (Patrick et al, 2002) and unsupervised (Cucerzan and Yarowsky, 1999) named entity recognition. Earlier papers have taken a character-level approach to named entity recognition (NER), notably Cucerzan and Yarowsky (1999), which used prefix and suffix tries, though to our knowledge incorporating all character grams is new. Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. The NER task for Hindi has been explored by Cucerzan and Yarowsky in their language independent NER work which used morphological and contextual evidences (Cucerzan and Yarowsky, 1999). Previous work (Cucerzan and Yarowsky, 1999) was done using the complete words as features which suffers from a low recall problem. Prefix and suffix tries were also used previously (Cucerzan and Yarowsky, 1999). The core model utilized, extended and evaluated here is based on Cucerzan and Yarowsky (1999). Because the core model has been presented in detail in Cucerzan and Yarowsky (1999), this paper focuses primarily on the modifications of the algorithm and its adaptation to the current task. The bootstrapping stage (5) uses the initial or current entity assignments to estimate the class conditional distributions for both entities and contexts along their trie paths, and then re-estimates the distributions of the contexts/entity-candidates to which they are linked, recursively, until all accessible nodes are reached, as presented in Cucerzan and Yarowsky (1999). This paper has presented and evaluated an extended bootstrapping model based on Cucerzan and Yarowsky (1999) that uses a unified framework of both entity internal and contextual evidence.
Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999). (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting).  Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on. DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus. (Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify. In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition.  Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999). While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collins and Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention). This can be either a fixed number of added unlabelled examples (Blum and Mitchell, 1998), the performance drop on a control set of labelled instances, or a filter on the disagreement of h1 and h2 in classifying U (Collins and Singer, 1999). In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999).  Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998). This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999). This criterion was used in a lightly-supervised NE recognizer (Collins and Singer, 1999). (6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here. We use Collins and Singer (1999) for our exact specification of Yarowsky. This is not clearly specified in Collins and Singer (1999), but is used for DL-CoTrain in the same paper.
Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy. Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999).  (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers. A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999). This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization. Henderson and Brill (1999) also reported that context did not help them to outperform simple voting. (Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction).  Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000). Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees. Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper. Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees. (Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined. (Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined.
This extension will require using a more selective alignment technique (similar to that of (Hatzivassiloglou et al., 1999)). Another approach (Hatzivassiloglou et al,1999) has been to use a machine learning algorithm in which features are based on combinations of simple features (e.g., a pair of nouns appear within 5 words from one another in both texts). We use SimFinder (Hatzivassiloglou et al, 1999) for sentence clustering and the f-measure for word overlap to compare noun phrases. We use SimFinder (Hatzivassiloglou et al, 1999) for sentence clustering and its similarity metric to evaluate cluster quality; SimFinder outputs similarity values (simvals) between 0 and 1 for pairs of sentences, based on word overlap, synonymy andn-gram matches. We then cluster the simplified sentences withSimFinder (Hatzivassiloglou et al, 1999). At the level of short passages or sentences, (Hatzivassiloglou et al, 1999) goes beyond N-gram, taking advantage of WordNet synonyms, as well as ordering and distance between shared words. For instance, Hatzivassiloglou et al (1999) trained a classifier for paraphrase detection, though their performance only reached roughly 37% recall and 61% precision. In the context of multi document summarization, SimFinder (Hatzivassiloglou et al, 1999) identifies sentences that convey similar information across in put documents to select the summary content. An obvious choice for a baseline in this task is the following: any two sentences are considered aligned if their cosine similarity exceeds a certain threshold. We also compare our algorithm with two state-of the-art systems, SimFinder (Hatzivassiloglou et al, 1999) and Decomposition (Jing, 2002). To this end, we intend to implement a second-pass analysis that would rerank the candidates produced by fuzzy inverted generation by computing text similarities over short passages such as those propose din (Hatzivassiloglou et al, 1999). Hatzivassiloglou et al (1999) proposed to use linguistic features as indicators of text similarity to address the problem of sparse representation of sentences.
This experiment is repeated on two different existing systems, which are reported in Buchholz et al (1999) and Carroll et al (1999), respectively. For example, the Buchholz et al (1999) system ignores verb complements of verbs and is designed to look for relationships to verbs and not GRs that exist between nouns, etc. For example, in our experiments, the Buchholz et al (1999) system uses the annotation np-sbj to indicate a subject, while the Ferro et al (1999) system uses the annotation subj.  As an example, take the results of using the Buchholz et al (1999) system on the 1391GR instance training set. When using the Buchholz et al (1999) system, the improvement in the other modifier is now no longer statistically significant. We rerun rule learning on the smaller (1391 GR instance) training set with a Union of the Buchholz et al (1999) and Carroll et al (1999) systems's translated GR annotations. TheIaU F-score is statistically signicantly better than the F-scores with either using Buchholz et al (1999) (IaB) or not using any initial annotations (NI). Preiss (2003) compares the parsers of Collins (2003) and Charniak (2000), the GR finder of Buchholz et al (1999), and the RASP parser, using the Carroll et al (1998) gold-standard. Buchholz et al (1999) achieve 71.2 F-score for grammatical relation assignment on automatically tagged and chunked text after training on about 40,000 Wall Street Journal sentences. The computation of grammatical relations from shallow parsers or chunkers is still at an early stage (Buchholz et al, 1999, Carroll et al, 1998) and there are few other robust semantic processors, and none in the medical domain. 
